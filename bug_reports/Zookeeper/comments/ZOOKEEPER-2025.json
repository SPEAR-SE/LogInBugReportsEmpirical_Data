[Attached PDF describing the issues we observed., Regarding the "unreasonable length", is it related to ZOOKEEPER-1513?, The error message is the same, but stack trace is very different. In https://issues.apache.org/jira/browse/ZOOKEEPER-1513, the stack trace appears to be during initialization, whereas in the stack trace we observed, it is during normal operation, presumably when a follower is forwarding a write to the leader. I'm not expert in the code though, so I could be misreading that., I wish everyone had reports like that when they create a jira. :-) 

Let me have a look at it..., While attempting to reproduce a connection storm with different zk clients (so the client behavior isn't exactly the same as in the original report), I straced the "NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181" thread, and noticed that time between accept() calls increased as the number of sockets to read() and write() to increased.  It would be in the milliseconds to even hundreds of milliseconds range, and on average I would see 20-40 accept() calls per second.  This was a short sample though, so I need to do more extensive testing, but at that rate, it would make sense why we see the accept queue filling and experiencing ListenOverflows along with syn cookies.

I'm still trying to understand why socket and file descriptor usage skyrocketed to the 64,000 ulimit when there were very few ESTABLISHED connections.  My current theory is that connections would very rapidly become ESTABLISHED, then the client would timeout on a request, and leave the connection in a CLOSE_WAIT state.  Unfortunately we didn't capture the number of sockets in CLOSE_WAIT during the event, so I can't prove it.,   I am reasonably certain I have identified the state that caused the large number of connection events.

  1) A large packet in follower to leader communication evicted the follower from the cluster, causing a near instant 5,000 connection attempts to the remaining nodes. The large packet cause should be identified, but the resulting loss of a single follower node should be a tolerable event.

  2) Every active client on the remaining nodes is moved into a throttled state by the server. The only operation the server is now interested in is connection events and every client is going to be reconnecting. Outstanding requests is not going to go down and we will not close old connections because doIO() is not being called. We hit over 100k outstanding requests. It is not desirable to 100% client throttling.

  3) Eventually, the server node will be out of file handles and fail. The handling of the "Too many files" exception from accept() leads to a rebuild and shuffle of the set of events the server is interested in, but it is very likely that very near the top of the iteration of this set there will be another exception from accept() causing the rebuild and re-shuffle to trigger. It would be more desirable to continue iterating through the set of selected clients in hopes that one of them could close. Otherwise, severe failure is just a matter of time. Particularly in the condition where 100% of clients have been throttled, the set of selected clients will only contain reconnect attempts., Brett,

That is a pretty interesting scenario.

Flavio,

What do you think about that?


, Hi [~bsterner]. Thanks for sharing the ideas.

One question regarding 2).
bq. Outstanding requests is not going to go down and we will not close old connections because doIO() is not being called. 
Could you clarify this?

So I see this problem has two parts:
1. Client reconnect happens instantly in a short time. Back-off would be desired.
2. equal nio.selector seems not proper design in a hind sight. Accepting clients and requests should be separated. And if clients couldn't be accepted any more, a pause would be desired without hindering processing new requests.

Well. Another non-trivial change.., {quote}
Outstanding requests is not going to go down and we will not close old connections because doIO() is not being called.
{quote}

Unless I am misunderstanding, after being throttled (interest for OP_READ removed from the SelectionKey), NIOServerCnxnFactory.run() won't select the client connection to call NIOServerCnxn:doIO(). doIO() is where decrementing outstanding requests and closing a connection starts., I believe that this has been fixed in ZOOKEEPER-1504.
This issue is duplicate to ZOOKEEPER-517.
I have seen this patch in branch-3.5/trunk but not in branch-3.4., Hongchao,

I don't see how that really addresses the problems here.  The throttling code is still very naive.  And the complete throttling of clients with no client back off is also not a great solution since there is no source quench under pretty reasonable scenarios.

It also doesn't address the question of why one server would send an unreasonably large packet to another.

Flavio,

Have you had a chance to look at this?

, 
Has anybody looked at this?

]