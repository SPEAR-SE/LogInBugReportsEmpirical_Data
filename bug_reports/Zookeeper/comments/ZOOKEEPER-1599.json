[It looks like this was caused by ZOOKEEPER-962 and/or ZOOKEEPER-882 in the 3.3 branch (they're lumped into one commit). Prior this commit, 3.3 servers were able to successfully join a 3.4.x quorum. These patches are also present in the 3.4 branch since 3.4.0., Skye, is this something we can address? i.e. make those changes backward compatible, retroactively. ;-), I think I found the problem. In 3.4, the leader syncs with the learner in LearnerHandler.run as follows (very abridged edition):
1. Leader sends diff/snapshot/whatever
2. Leader sends NEWLEADER
2. Leader waits for ACK from learner
3. Leader sends UPTODATE

Learner.syncWithLeader then looks very roughly like:
1. Learner receives diff/snapshot/whatever
2. Learner receives NEWLEADER
3. Learner sends ACK to leader
4. Learner waits for UPTODATE
5. Learner sends another ACK
(2 and 3 are optional for backwards compatibility)


However, in 3.3, LeaderHandler.run looks like:
2. Leader sends diff/snapshot/whatever
3. Leader sends UPTODATE
4. Leader waits for ACK from learner

Likewise, on the Learner side (at least from 3.3.3 onward, which committed ZOOKEEPER-962 and ZOOKEEPER-882):
2. Learner receives diff/snapshot/whatever
3. Learner waits for UPTODATE
4. Learner sends ACK

(There's still some NEWLEADER business in 3.3, but it doesn't seem important so I didn't bother to figure it out)


So, in 3.4, the learner ACKS _before_ the leader sends UPTODATE, and in 3.3 the learner ACKS _after_ the leader sends UPTODATE. The result is that when a 3.3 learner syncs with a 3.4 leader, the leader gets stuck waiting for an ACK and the learner gets stuck waiting for an UPTODATE., Here's a potential fix, but I don't grok Zab well enough to say whether this will create new problems. This patch, which should be applied to 3.4, has the leader send an UPTODATE _before_ waiting for an ACK if the follower is running the old Zab protocol. This means the follower receives an UPTODATE before the leader ZK server starts (i.e., before the leader gets ACKs from a quorum of followers), which smells fishy to me, but maybe it's a better problem to have than the current one. FWIW, this is how the old Zab protocol worked too.

Another option would be to patch 3.3 to speak Zab 1.0. This seems less useful though since you'd have to upgrade 3.3 before safely upgrading to 3.4. , -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560430/ZOOKEEPER-1599.patch
  against trunk revision 1420028.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1292//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1292//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1292//console

This message is automatically generated., In some scenarios, this patch can cause the joining 3.3 server to start serving state that will later be truncated from the log. Can you explain why its important for a 3.3 server to be able to join a 3.4 leader ? I can see why a 3.4 server may need to connect to a 3.3 leader during upgrade to get the state, but why is the other direction important ? 

Thanks,
Alex, I agree with [~shralex], it would be nice to understand the use case for this and preferably not make this change. It would also be great if we eventually could get rid of all this backward compatibility tricks we have for zab. It makes the code more complex. , Cancelling the patch until we understand this issue better. It also doesn't include a test case, so another reason to cancel it., A 3.3 server should be able to join a 3.4 leader so you don't suffer unexpected outages during a rolling upgrade. For example, say you're upgrading three 3.3 servers to 3.4. You upgrade one server to 3.4, then take down the 3.3 leader to upgrade it. If the 3.4 server is elected the new leader, you won't have a quorum until you restart the new 3.4 server. (At least, this should be possible as far as I understand -- am I missing something?)

Given Alexander's comment, I agree that we should not commit the patch I submitted -- better to lose your quorum temporarily than to serve incorrect state., Thanks, Skye. Another reason not to support this is that an earlier version of ZooKeeper server may not always understand or be able to correctly execute commands sent by a leader running a later version. I'm not sure about 3.3 vs 3.4 (MultiOp ?) but in 3.5 I hope to see ZK-107 go in, and then there will be new reconfiguration commands which 3.4 will not be able to process correctly.

One may try to avoid such outages by upgrading the leader last., I actually found this bug while looking at ZOOKEEPER-1495 (ZK client hangs when using a function not available on the server.), because I wasn't sure what currently happens when an upgraded leader sends an unrecognized command to a follower (e.g., multis). It's reasonable for the follower to drop out of the quorum in this case (there's not really any other option), but I still think the follower should be able to join the quorum initially, i.e. we should fix this bug if possible. In the case of new commands, it seems pretty obvious that you shouldn't use them until you finish your upgrade, but it's way less obvious that you have to maintain a 3.3 leader during a rolling upgrade (and even if you try to take down the leader last, something else may cause a new leader to be elected).

It sounds like the patch I submitted causes more problems than it fixes, but maybe there's a different way of going about it?, We could send the software version in FollowerInfo and check it in LeaderHandler. If a leader sees that a follower tries to connect to it with an older software version it just refuses. 

Obviously you can't guarantee that the 3.3 leader is maintained until the end of the upgrade, and in case it doesn't there's going to be some unavailability. But I'm not convinced that its important for the 3.3 server to be able to join the quorum initially, processing commands in some partial way, and only then to drop out.



, There are at least two reasons I can think of why we need to seriously re-consider the direction I see us going in the previous comments:

1) we _explicitly_ guarantee users that we'll support X.Y.# to X.Y+1.# backward compatibility for cases like this

2) availability is a major feature of ZK, downtime during upgrade reduces availability. Granted this can be managed, but it's still the case that we're not providing availability.

We need to provide a smooth upgrade path for users. 3.3 to 3.4 upgrade of the serving cluster is a big part of this.

Can we think harder on this? :-), We were supposed to have ZOOKEEPER-1136 committed to 3.3.4, but we ended up not committing to branch 3.4:

https://issues.apache.org/jira/browse/ZOOKEEPER-1136?focusedCommentId=13104310&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13104310

But, I suppose that if we had, then we would have just moved the problem to 3.2. It could be an option to backport the patch, though. A second option is change nothing and in the case the leader turns to 3.4, then the system will be down until we get a quorum of 3.4 rolled. A third option is the one Alex proposes, but the net result is not very different from the second option, I think.   , If we do backport ZOOKEEPER-1136 to 3.3, don't we just end up with the same problem when moving from 3.3.6 to 3.3.7? (assuming we put the patch into 3.3.7) 

Or is this addressed by including Skye's patch in 3.3.7. But not in 3.4. If I understand correctly that would result in the ability to move forward, but the issue Alex highlighted ("this patch can cause the joining 3.3 server to start serving state that will later be truncated from the log") still exists. It's not a regression though in 3.3, the problem currently exists. However 3.4 forward it would be addressed. Does this make sense?


I don't think "moving the problem to 3.2" is such an issue for the following:

1) 3.2 is very old, likelihood someone doing a rolling upgrade from 3.2 is much less likely than 3.3 to 3.4

2) if 1 is an issue the person can upgrade from 3.2.x to 3.3.6, then upgrade to 3.3.7 (whenever we fix this issue in 3.3) and then upgrade to 3.4.

Was there an explicit reason why we did not backport ZOOKEEPER-1136 to 3.3?, One thing just occurred to me. If we have a 3.4 leader, then it will only be able to make progress if there is a quorum of 3.4 followers. Otherwise, it will drop leadership and a 3.3 leader will be elected back. Assuming that the 3.4 leader has enough 3.4 followers supporting it, any 3.3 follower that gets stuck will eventually be upgraded. Does it make sense? If so, what we are discussing here is not an issue unless you want to have a mix of 3.3 and 3.4 servers for some other reason that is not a rolling upgrade.  , Flavio, how/at what stage will the 3.4 leader detect that it shouldn't count votes from a 3.3 follower ? 
I thought nothing is checked about votes, and this is addressed in ZK-107 (specifically in ZOOKEEPER-1113 which is solved in 107), I think that Flavio is simply pointing out that if 3.4 can follow 3.3, but 3.3 cannot follow 3.4, then a rolling upgrade works.  If you have A,B,C then upgrading A will force B or C to lead.  Assume B WLOG.  When you take down B, A might want to become leader, but will fail so C will lead with A and C in the quorum.  When you take down C after B is back, either A or B can become a 3.4 leader.

Surprisingly, this also works for downgrades.  If you downgrade C, A or B can remain leader.  When you downgrade B, however, A cannot be leader anymore so C will take over while B is down.  Downgrading A now gives you a pure 3.3 cluster.

The question of how a 3.4 leader can be sure not to allow a 3.3 follower while staying otherwise compatible is a different kettle of fish, but obviously critical to the scenario., I see. but what prevents a 3.4 leader to repeatedly trying to become leader and to repeatedly fail :), Ultimately, if somebody else gets a quorum, the 3.4 node should give up trying to become leader., Yes, but there's no guarantee that this will ever happen., A 3.4 leader will very likely repeatedly try and fail to become the leader since voting is basically deterministic (basically if a server is up-to-date and has the highest server id, it will always be elected leader AFAIK). No one else will get a quorum because no one else will be elected leader, so they won't even try.

It sounds like we should patch 3.3 so that it can speak both protocols. This way we're not introducing regressions into 3.4 by supporting the old protocol (even though you'll only hit the bugs when upgrading from 3.3), and if you want to avoid downtime you can first upgrade to 3.3.7 and then to 3.4. Does this make sense?, Hi Skye,

It sounds like its good for this specific case. But in general and also in the context of 1495 I think it might be better simply not to allow a follower of an earlier version to connect to a leader of a later version, rather than to allow it to connect and then drop out of the 
quorum if there's something it doesn't understand. The reason is that at this point the follower might have already participated in commits with the 3.4 leader, and may be needed for safety (so it can't be a follower but we do count on its state for safety, which is weird to me). Moreover, its possible for example that the 3.4 leader has already processed a multi command (or some other new command like reconfiguration) by the time the 3.3 follower joins and then the follower will have to quit during catchup/state-transfer, so in general you don't guarantee availability in this case either. 


Alex
, actually in the case of reconfiguration 1495 is even a bit more problematic -- we have a new client-server command for reconfiguration, but we don't have a new server-to-server command, we just do setData and figure out that its actually a reconfiguration. If the follower doesn't know what a reconfiguration is, the error will be just that it can't write to /zookeeper/config znode since this znode doesn't exist, and from this you'd have to realize that it doesn't support reconfiguration. Also the startup protocol of the leader is changing, as it needs to make sure that if a reconfiguration failed in the middle we'll recover correctly. I'm not sure what will happen if the follower doesn't follow the recovery protocol..., In the common rolling upgrade scenario ops typically upgrades the ensemble first, then the clients. So the case such as multi commands being run against 3.4 prior to the upgrade completing (ensemble), while something that can certainly happen, is up to the user and unlikely to happen if they follow our instructions. If they do so then they are outside the parameters of what we support. However in the common case all they want to do is upgrade the servers w/o service downtime. We should support this. We always have in the past, and we tell people that's a guarantee., [~skye]
bq. A 3.4 leader will very likely repeatedly try and fail to become the leader since voting is basically deterministic (basically if a server is up-to-date and has the highest server id, it will always be elected leader AFAIK). No one else will get a quorum because no one else will be elected leader, so they won't even try.

If there is only one guy that can be elected because of its history, then there will be downtime until it is able to form a quorum of supporters. I'm not convinced that we should make changes to get rid of this scenario and at the same time, I don't think we are violating our compatibility contract in this particular case. It is part of ZooKeeper that if we can't elect a leader, there will be downtime.

[~shralex]
bq. how/at what stage will the 3.4 leader detect that it shouldn't count votes from a 3.3 follower ? 

I was not suggesting that it would "detect". It would simply timeout in the case it can't get enough supporters and drop leadership.

bq. I thought nothing is checked about votes, and this is addressed in ZK-107

Does ZOOKEEPER-107 solve the backward compatibility problem? The issue here for me is mainly backward compatibility., > It would simply timeout in the case it can't get enough supporters and drop leadership.

got it. 

> Does ZOOKEEPER-107 solve the backward compatibility problem?
no. my comment about ZK-107 checking membership was because I didn't understand what you meant :) 

As a side note here's a way to upgrade the system with ZK-107 without any downtime - If you have servers A, B, C running 3.3, you can connect 3 new servers A', B', C' running 3.4 (they must have different ids), and just switch the membership from ABC to A'B'C'. You'll be able to make the switch only once A'B'C' is up-to-date, and the switch will not involve downtime besides a momentary leader handoff (quicker than usual leader change). But because the new ids must be different from the old ones I'm not sure people will use this method. 

, on second thought the method in my last comment requires both old and new software versions to support ZK-107 (because of recovery issues), so totally irrelevant here - sorry., i think we need to be a bit careful about saying that a 3.3 server can work with a 3.4 server without any caveats. a 3.3 server is not going to understand new transactions, so at a minimum we would require that you have an option to turn off new transactions in 3.4 so that it can work with 3.3. is the plan that people would run with a mixed ensemble of 3.3 and 3.4 servers for a long time? or is it just for upgrading?

at some point the amount of cruft and work that we have to put in for a very short term corner case gets completely unwieldy. (we may have hit that point already...)

being able to connect 3.4 follower to a 3.3 leader enables rolling upgrades. if we want more, i think we should make it much more clear what the requirements are. we should also clarify the backward compatibility requirements for servers., btw, even though currently the change from 3.3 to 3.4 is so significant that the 3.3 server can't even connect, in the future we don't know that we'll be so lucky - it may somehow connect and fail later. If we do decide not to allow such connection it would be better to explicitly check for this case, and we can use the "protocol version" already passed in LearnerInfo. Right now its always the same - 0x10000 I think we should increase it with every major release., [~breed] I thought I was pretty clear on the scenario (my recent comment above, requoted here: )

bq. In the common rolling upgrade scenario ops typically upgrades the ensemble first, then the clients. So the case such as multi commands being run against 3.4 prior to the upgrade completing (ensemble), while something that can certainly happen, is up to the user and unlikely to happen if they follow our instructions. If they do so then they are outside the parameters of what we support. However in the common case all they want to do is upgrade the servers w/o service downtime. We should support this. We always have in the past, and we tell people that's a guarantee.

Rolling upgrade is important to allow upgrading the ensemble with high availability. The goal is to do it in a short period of time. The goal is not to mix clients using new features unsupported by the older servers.

I agree with Alex, if we make a significant change such that it's not backward compatible we need to increment the major version number (and the protocol version). That's always been one of our reasons for having a major version number. That said I don't see why we'd need to do so in this case.
, Pat, I'm not sure what you're trying to get out of this discussion at this point. Here is my understanding out of the comments I've seen here, including my own observations:

# The code we have for 3.3 and 3.4 enable rolling upgrades in the way you guys have been suggesting to people. There is this corner case that could cause some downtime, but I have no reason to suspect that it would be severe. 
# Making changes to try to fix the corner case is likely to open a can of worms, so I'd rather leave it alone and it sounds like others too.
# Moving forward, we won't have to depend on rolling upgrades any longer when ZOOKEEPER-107 gets in.

Is there a strong reason to keep discussing this?, [~shralex]
bq. Right now its always the same - 0x10000 I think we should increase it with every major release.

In my understanding, we bump up the version only when there are changes to the protocol. The version of the protocol is supposed to indicate changes to the protocol and not to ZooKeeper overall.

bq. As a side note here's a way to upgrade the system with ZK-107 without any downtime - If you have servers A, B, C running 3.3, you can connect 3 new servers A', B', C' running 3.4 (they must have different ids), and just switch the membership from ABC to A'B'C'. You'll be able to make the switch only once A'B'C' is up-to-date, and the switch will not involve downtime besides a momentary leader handoff (quicker than usual leader change). But because the new ids must be different from the old ones I'm not sure people will use this method. 

Isn't it necessary that the servers running 3.3 also understand reconfigs?

, > Isn't it necessary that the servers running 3.3 also understand reconfigs?

yes - I also figured it out immediately after writing this, and added this comment (above):

> on second thought the method in my last comment requires both old and new software versions to 
> support ZK-107 (because of recovery issues), so totally irrelevant here - sorry.

Whether using the protocol version number or through some new info, I suggest to explicitly detect that the learner has earlier software version and not to allow this connection, for the unlucky case where the connection succeeds and something goes wrong later., As I read it then the current proposal is to leave this issue as it currently stands. The limitation is that a rolling upgrade from 3.3 to 3.4 is supported, however if leadership changes to a 3.4 server during the upgrade, and a minority of the servers are 3.4, the service would go down until a majority of servers had been migrated to 3.4. We don't want to try and fix this for a number of reasons, including; a) it's a case that's easy to identify and fix, unlikely to happen in the typical case, b) attempting to fix it would be complex and open us up to a number of potential bugs and incorrect state.

Going forward (3.5+) this won't be an issue because we can use dynamic reconfig to work around it., I wanted to understand if there is anything in [~shralex] proposal that I'm missing. We check for earlier versions of the protocol in learner handler, but we don't drop the connection. If we do as you suggest, we are just dropping it explicitly instead of timing out, yes? If so, then I'd rather leave as is, since the 3.3 follower might end up making progress in the case the leader is able to form a quorum of 3.4 supporters., > we are just dropping it explicitly instead of timing out, yes? 

yes. 

> I'd rather leave as is, since the 3.3 follower might end up making progress in the case the leader is able to form a quorum of 3.4 supporters.

I don't think I completely understand your comment. If what you say here is possible then I see this as a bad thing - he shouldn't be allowed to make progress even if there's a 3.4 quorum since he might not understand the 3.4 commands. I was actually only thinking of the simpler scenario where 3.3 follower will time out and was worried that in the future a 3.4 follower connecting to a 3.5 leader may not time out by itself and we'll need to explicitly prevent it from connecting to avoid him getting commands that it doesn't understand.


, bq. he shouldn't be allowed to make progress even if there's a 3.4 quorum since he might not understand the 3.4 commands.

If that's the case, then 3.4 is not backward compatible independent of zab. Are the two branches not backward compatible?

bq. I was actually only thinking of the simpler scenario where 3.3 follower will time out and was worried that in the future a 3.4 follower connecting to a 3.5 leader may not time out by itself and we'll need to explicitly prevent it from connecting to avoid him getting commands that it doesn't understand.

I think it has been mentioned here in this jira that consecutive branches need to be backward compatible, so between 3.4 and 3.5 servers they should be able understand each other, there shouldn't be commands they don't understand.   

, A 3.4 server understands 3.3 commands - this is how it can catch up from a 3.3 leader during upgrade. I think this is what backwards compatibility means. But 3.3 server might not understand all 3.4 commands (multiops for example) and similarly a 3.4 server might not understand 3.5 commands (reconfigs for example). This doesn't seem to contradict backwards compatibility - you can't open a docx file with MS Word 2003 :), Can't you export word docs to earlier versions, perhaps missing features? I can't believe how cool word is... Back to the system at hand, I don't understand why backward compatibility should be defined using leaders and followers. It feels like it should be independent of which server is leading., You're right, not really important who's leading - I think that a 3.4 follower connected to 3.3 leader should not ask the leader to execute commands that the leader doesn't understand, or send it messages that it doesn't understand. We should ensure that too.

But we have the constraint that the leader can't choose a subset of the commands to send to a follower - it must send all the log. So the case where the leader is 3.4 and follower is 3.3 seems more complex to get right. 

Out of curiosity I looked on the wikipedia definition of backward compatibility. It says:
> a product or technology is backward or downward compatible if it can work with input generated by an older product or technology.
> The reverse is forward compatibility, which implies that old devices allow (or are expected to allow) data formats generated by new (or future) devices, perhaps without supporting all new features. 

Guaranteeing forward compatibility in our case seems complex.
, Agreed, that's an interesting observation. I'm not aware of forward compatibility being a goal. 

One of the complications with backward compatibility is that we have a replicated service and during these rolling upgrades we can have a mix of 3.3 and 3.4 servers, so in the end we have two types of compatibility: server to server and client to server. If the clients are 3.3, then there should be no problem assuming the ensemble can operate with these two versions and that we have client to server backward compatibility. , It has been a while so this discussion is not exactly fresh for me. I have gone over the comments, and despite the heated discussion it sounds like we converged to not applying the patch proposed here. When upgrading 3.3->3.4, there are cases in which we can have some downtime until the upgrade completes. 

Is my understanding correct? Is it ok if I resolve this jira marking it as won't fix?, We have always endeavored to maintain "N to N+1" (3.N.X to 3.N+1.Y, e.g. 3.1.3 to 3.2.0) compat for ZK - regardless of server or client. I can see the reasoning in this case but we shouldn't relax this goal unless absolutely necessary. And then in exceptions we need to document clearly.

[~skye] do you have any further comments/concerns?

That said I'm fine marking this as "won't fix" however you should update the "release notes" field to capture. (release notes get put into the release documentation)

Thanks all!, Thanks for the feedback, Pat. I don't mind adding the release note, but let me go over a couple of things. The multi-op jira only got into the 3.4 branch, so to make the servers compatible, we would need to backport ZOOKEEPER-965. The other jira that [~skye] claims to be a problem is ZOOKEEPER-882. I had a look at ZOOKEEPER-882 and it should have got into 3.3.3 according to the jira header, but I only see I commit (from me, actually) and not the second, so I suspect the 3.3 branch ended up not having that patch applied.

I'll check what happened with ZOOKEEPER-882 and in the case it really didn't get in, if the patch still applies. In the case it does, would it make sense to have another release for the 3.3 branch? I don't think it solves completely the problem of this jira because the multi-op jira is not in, and it might be some work to get it. , Never mind, I have verified that ZOOKEEPER-882 is in 3.3 and 3.4. Back to the compatibility issue, I think unless we decide to have a 3.3 release with multi-op, I still don't think it makes sense to get this one in. 

Please let me know if we have any disagreement here. If I don't hear anything else by mid next week, I'll mark it as not a problem.

, Closing issues after releasing 3.4.6.]