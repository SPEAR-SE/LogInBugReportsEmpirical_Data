[Here is the config, which is a modified CDH3 version, pretty much standard:

{code}
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
dataDir=/var/zookeeper
# the port at which the clients will connect
clientPort=2181
server.0=hadoop-primary.i.foobar.com:2888:3888
server.1=hadoop-a.i.foobar.com:2888:3888
server.2=hadoop-c.i.foobar.com:2888:3888
{code}, I think this issue is related to ZOOKEEPER-880. It seems that the connections nagios creates start a RecvWorker and a SendWorker, and once they close, they kill RecvWorker but not SendWorker, so for every notification sent there is an orphan RecvWorker.

I see two options:

# Patch it so that it also kills the SendWorker instance;
# Decline connection requests from unknown servers.

I'm also curious to understand why you guys are monitoring the election port., I meant to say that there is an orphan SendWorker, not an orphan RecvWorker., This happened at a customer location, so no idea why they all were polled. After talking to Patrick I also advised switching to the appropriate 4-letter word check (like "stat" to check the node). Patrick suggested though we should issue the JIRA to capture the situation. If this is a dupe from ZOOKEEPER-880 then please close this one as such. Your call., Let's keep them both open until we learn more.

Lars, can you have them turn off Nagios from pinging the ZK ensemble, then have them restart all the zk servers (the entire zk ensemble), then see if this problem still occurs and update this jira with the results? Thanks!, This issue sounds very similar to ZOOKEEPER-880 -- they are also using Nagios to monitor the election port in that case as well., Yes, did request that and will update here as I get details., The trunk contains a monitoring script (src/contrib/monitoring) useful for Nagios, Ganglia and  Cacti. For best results you need a version of ZooKeeper that has the 'mntr' 4letter word (see ZOOKEEPER-744) but it also works with 3.3.x., Hi,

If this problem is reproducible, can you try with the patch attached for ZOOKEEPER-880. Going through the logs, the patch for ZOOKEEPER-880 will only fix part of the problem - it will prevent leak of SendWorker thread.

Logs show that there are 320 RecvWorker threads blocked in a read() from remote socket. 2 of these threads should be legitimate threads to remote servers. Very likely rest of them are from Nagios.

There are 2 cases I can think of that can lead to this situation:
1. Nagios may not be closing a connection. If Nagios was closing connections, then the read() should have received an exception.
2. Monitoring frequency set too high

-Vishal
, Sounds like this was resolved with ZOOKEEPER-880. Please reopen if it happens again. Thanks Lars.]