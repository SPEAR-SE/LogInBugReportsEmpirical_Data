[Contains a partial before-restart log for node #3 (node3_after_restart.log) and an after-restart log for all three nodes., Actually, step 11 above is not correct, according to Flavio on the mailing list.  Node #3 proposed that it become leader, but did not have enough followers., These logs are very difficult to read, but one thing I notice is this log from node1_after_restart:

314258 [Thread-3] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - Address of remote peer: 126
314267 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 215
314267 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 126 (n.leader), 17179869911 (n.zxid), 1 (n.round), LOOKING (n.state), 126 (n.sid), LOOKING (my state)
314267 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification epoch is smaller than logicalclock. n.epoch = 1, Logical clock4
314267 [WorkerSender Thread] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - There is a connection already for server 126
314273 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 215
314273 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 126 (n.leader), 17179869911 (n.zxid), 3 (n.round), LOOKING (n.state), 126 (n.sid), LOOKING (my state)
314273 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification epoch is smaller than logicalclock. n.epoch = 3, Logical clock4
314273 [WorkerSender Thread] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager  - There is a connection already for server 126
314274 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 215
314274 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 37 (n.leader), 17179869831 (n.zxid), 3 (n.round), FOLLOWING (n.state), 126 (n.sid), LOOKING (my state)
314274 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - I'm a participant: 215
QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.QuorumPeer  - FOLLOWING


Specifically, that the n.zxid starts at 17179869911, and goes to 17179869831 when election finishes. 

And in fact node3 does believe it is LEADING according to the logs. It's also confusing because the node3 logs have a bunch of lines like this:

6397 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 215 (n.leader), 17179869918 (n.zxid), 4 (n.round), LOOKING (n.state), 215 (n.sid), LEADING (my state)

Why is my state LEADING, but n.leader is node1, but node1 is also LOOKING?, 
The logs are really very difficult to follow. Also, I think the sequence of events as described do not match with the logs.

Heres what we can see in from node1_after_restart

2048:151124 [QuorumPeer:/0.0.0.0:2888] WARN org.apache.zookeeper.server.quorum.Learner  - Exception when following the leader <==== NODE 2 WENT DOWN HERE
13497:	at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:375)  ===> NODE 3 WENT DOWN HERE
17926:309591 [LearnerHandler-/13.0.0.12:52753] WARN org.apache.zookeeper.server.quorum.LearnerHandler  - ******* GOODBYE /13.0.0.12:52753 ******** ===> NODE2 WENT DOWN HERE AGAIN
18044:310757 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Leader  - Shutdown called ===> NODE1 shutdown again (which is why node 3 became the leader)?

There seem to be a few restarts after this as well
grep -n "tickTime set to 3000" node1_after_restart.log
[...]
23685:1831 [pool-1-thread-3] INFO org.apache.zookeeper.server.quorum.QuorumPeer  - tickTime set to 3000
28491:1479 [pool-1-thread-2] INFO org.apache.zookeeper.server.ZooKeeperServer  - tickTime set to 3000

So anything beyond time 310757 is not as described in the bug report.

Also note that because of ZOKEEPER-975, after receiving notifications from all peers a node can go in LEADING state until it waits for initTime() * tickTime(). Then, it will timeout, start FLE, and go to FOLLOWING state., If you grep for 0000000000000000_record0000003292 and
0000000000000000_record0000003348 in node2_after_restart.log we can
see that znode ending with 3292 was at least accessed before 3348.

2943:153670 [FollowerRequestProcessor:126] TRACE org.apache.zookeeper.server.quorum.FollowerRequestProcessor  - :Fsessionid:0x7e2ec782ff5f0000 type:getDate cxid:0x4d830245 zxid:0xfffffffffffffffe \
txntype:unknown reqpath:/zkrsm/0000000000000000_record0000003292
[...]
4340:156963 [FollowerRequestProcessor:126] TRACE org.apache.zookeeper.server.quorum.FollowerRequestProcessor  - :Fsessionid:0x7e2ec782ff5f0000 type:getDate cxid:0x4d8302bb zxid:0xfffffffffffffffe \
txntype:unknown reqpath:/zkrsm/0000000000000000_record0000003348

The first column above indicates line number. The type (getDate) is a
typo and should be getData.

Similarly, in log3_after_restart:

32497:24471 [ProcessThread:-1] TRACE org.apache.zookeeper.server.PrepRequestProcessor  - :Psessionid:0x252ec7856ce60000 type:getDate cxid:0x4d8300c1 zxid:0xfffffffffffffffe txntype:unknown reqpath\
:/zkrsm/0000000000000000_record0000003292
[...]
37513:27851 [ProcessThread:-1] TRACE org.apache.zookeeper.server.PrepRequestProcessor  - :Psessionid:0x252ec7856ce60001 type:getDate cxid:0x4d8300f3 zxid:0xfffffffffffffffe txntype:unknown reqpath\
:/zkrsm/0000000000000000_record0000003348

I am confused to see that node1_after_restart.log does not have any trace of sequential znodes beyond /zkrsm/0000000000000000_record0000000921 (especially because Node1 was the leader when the znodes in questions were created).

I am also not sure whether to trust the cxid in these messages or not. If I
am not mistaken, they represent the current transaction id as seen by the client.
How can we isolate the transaction that did the creates?
, {quote}
The logs are really very difficult to follow.
{quote}

What would make them easier to follow?  I am about to replace the relative timestamps with wallclock timestamps and I will try to reproduce.  Is there anything else that might help?

{quote}
2048:151124 [QuorumPeer:/0.0.0.0:2888] WARN org.apache.zookeeper.server.quorum.Learner - Exception when following the leader <==== NODE 2 WENT DOWN HERE
13497: at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:375) ===> NODE 3 WENT DOWN HERE
17926:309591 [LearnerHandler-/13.0.0.12:52753] WARN org.apache.zookeeper.server.quorum.LearnerHandler - ******* GOODBYE /13.0.0.12:52753 ******** ===> NODE2 WENT DOWN HERE AGAIN
{quote}

Correct, node2 dies twice during the period I've described.  (Steps #6 and #10.)

{quote}
18044:310757 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Leader - Shutdown called ===> NODE1 shutdown again (which is why node 3 became the leader)?
{quote}

This happens because my node1 client code, running in tandem with the ZK server code, hits an assert failure when it detects the sequence numbers decreasing, and kills the server code as well; then our framework restarts everything.  So this is AFTER the client creates a file with a decreased seqno.  Everything after this point in the node1 file did not contribute to the bug.  (I left it in the log in case it showed anything interesting after the restart.)

{quote}
If you grep for 0000000000000000_record0000003292 and
0000000000000000_record0000003348 in node2_after_restart.log we can
see that znode ending with 3292 was at least accessed before 3348.
{quote}

Yes, while node2 was alive, nodes up to 3348 were accessed.  That 3292 is the original node with that sequence number.  The second one created by node1 does not appear to show up in the logs labeled as such -- I'm not sure how to get the ZookeeperServer to print a log message containing the name of the file it created.

{quote}
I am confused to see that node1_after_restart.log does not have any trace of sequential znodes beyond /zkrsm/0000000000000000_record0000000921 (especially because Node1 was the leader when the znodes in questions were created).
{quote}

It looks to me like node #2 was the leader for a large chunk of time based on these lines in node1_after_restart:

2535 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 126 (n.leader), 8589936862 (n.zxid), 2 (n.round), LEADING (n.state), 126 (n.sid), LOOKING (my state)
...
154676 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 215 (n.leader), 12884902548 (n.zxid), 3 (n.round), LOOKING (n.state), 126 (n.sid), LEADING (my state)

After that, there are a bunch of create messages within /zkrsm, but no GetDatas.

{quote}
I am also not sure whether to trust the cxid in these messages or not. If I
am not mistaken, they represent the current transaction id as seen by the client.
How can we isolate the transaction that did the creates?
{quote}

This confused me as well.  If there's anything I can do to get more accurate logs, please let me know.  I'm not sure if I can reproduce this yet, but I have a strategy I'm planning to try., Could it be this:

A. Notice that suddently node 2 goes in following state:
2533 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 126 (n.leader), 17179869911 (n.zxid), 1 (n.round), LOOKING (n.state), 126 (n.sid), LOOKING (my state)
2533 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 126
2533 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 37 (n.leader), 17179869831 (n.zxid), 3 (n.round), LOOKING (n.state), 37 (n.sid), LOOKING (my state)
2533 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 126
2533 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 37 (n.leader), 17179869831 (n.zxid), 3 (n.round), LEADING (n.state), 37 (n.sid), LOOKING (my state)
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 126, proposed id: 126, zxid: 17179869911, proposed zxid: 17179869911
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Adding vote: From = 126, Proposed leader = 126, Porposed zxid = 17179869911, Proposed epoch = 1
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 37, proposed id: 126, zxid: 17179869831, proposed zxid: 17179869911
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Adding vote: From = 37, Proposed leader = 37, Porposed zxid = 17179869831, Proposed epoch = 3
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - I'm a participant: 126

B. Due to ZOOKEEPER-975, 3 goes in LEADING state because it receives history of old notifications from node 1 and node2.
node3_after_restart:
2653 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 37 (n.leader), 17179869831 (n.zxid), 1 (n.round), LOOKING (n.state), 37 (n.sid), LOOKING (my state)

                            //If have received from all nodes, then terminate
                            if ((self.getVotingView().size() == recvset.size()) &&
                                    (self.getQuorumVerifier().getWeight(proposedLeader) != 0)){
                                self.setPeerState((proposedLeader == self.getId()) ?
                                        ServerState.LEADING: learningState());
                                leaveInstance();
                                return new Vote(proposedLeader, proposedZxid);

                            }

C. After which node 2 starts following node 3 and then node 1 starts following node 3 (because 2 is following 3)

due to:

                    default:
                        /*
                         * There is at most one leader for each epoch, so if a
                         * peer claims to be the leader for an epoch, then that
                         * peer must be the leader (no* arbitrary failures
                         * assumed). Now, if there is no quorum supporting
                         * this leader, then processes will naturally move
                         * to a new epoch.
                         */
                        if(n.epoch == logicalclock){
                            recvset.put(n.sid, new Vote(n.leader, n.zxid, n.epoch));
                            if((n.state == ServerState.LEADING) ||
                                    (termPredicate(recvset, new Vote(n.leader,
                                            n.zxid, n.epoch, n.state))
                                            && checkLeader(outofelection, n.leader, n.epoch)) ){
                                self.setPeerState((n.leader == self.getId()) ?
                                        ServerState.LEADING: learningState());

                                leaveInstance();
                                return new Vote(n.leader, n.zxid);
                            }
                        }
, Hi Jeremy,

I think I know whats going on here. The bug description that I gave in
my earlier comment is causing legitimate transactions to
get truncated.

You were right about node 3 becoming the leader after reboot.
I have given the order of events and my explanation below. Note that
the order of events is slightly different from what you described.

1. After your step 6, node 1 becomes the leader (its ID is > node 3's
id). Heres what node1 is saying in terms of membership and elections rounds.

a) 2048:151124 [QuorumPeer:/0.0.0.0:2888] WARN org.apache.zookeeper.server.quorum.Learner  - Exception when following the leader <==== NODE 2 WENT DOWN HERE (your step #6)
b) 13497:  at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:375)  ===> NODE 3 WENT DOWN HERE (your step #8)
c) 17926:309591 [LearnerHandler-/13.0.0.12:52753] WARN org.apache.zookeeper.server.quorum.LearnerHandler  - ******* GOODBYE /13.0.0.12:52753 ******** ===> NODE2 WENT DOWN HERE AGAIN (Your step #10)

d) 18044 310757 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Leader  - Shutdown called ===> NODE1 *shutdown* again? (This is because it lost majority as explained below)
18045     at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:390)
18046     at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:367)
18047     at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:658)


e) 23685:1831 [pool-1-thread-3] INFO org.apache.zookeeper.server.quorum.QuorumPeer  - tickTime set to 3000 ====> NODE1 restarted again (this is because of your client assert)

2. Notice that the time difference between b) and c) above is only 4.5
seconds. So you rebooted node 2 before node 3 had joined the cluster. 
As a result node 1 lost its majority and gave up leadership. You can
see that shutdown is called from Leader.java:367, which is done when
the leader cannot ping the followers. Your application would have seen
a DISCONNECT before it asserted.

What took node 3 so long to join the cluster? As described in ZOOKEEPER-975
and my previous comment, 3 went into LEADING state because of the bug
in FastLeaderElection. After 3 reboots, nodes 1 and 2 send old
notifications to 3. When 3 receives notifications of all nodes, it
goes in the leading state. Heres the incorrect if condition from FastLeaderElection

                           //If have received from all nodes, then terminate
                           if ((self.getVotingView().size() == recvset.size()) &&
                                   (self.getQuorumVerifier().getWeight(proposedLeader) != 0)){
                               self.setPeerState((proposedLeader == self.getId()) ?
                                       ServerState.LEADING: learningState());
                               leaveInstance();
                               return new Vote(proposedLeader, proposedZxid);

                           }

3. Now, 3 is in LEADING state and it will remain in the LEADING state
until ticktime * initTime or until a majority of followers start
following 3.

In the mean time, 2 boots and starts leader election. 2 receives a
notification from 3, which claims 3 to be the leader.

2533 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 37 (n.leader), 17179869831 (n.zxid), 3 (n.round), LEADING (n.state), 37 (n.sid), LOOKING (my state)
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 126, proposed id: 126, zxid: 17179869911, proposed zxid: 17179869911
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Adding vote: From = 126, Proposed leader = 126, Porposed zxid = 17179869911, Proposed epoch = 1
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - id: 37, proposed id: 126, zxid: 17179869831, proposed zxid: 17179869911
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Adding vote: From = 37, Proposed leader = 37, Porposed zxid = 17179869831, Proposed epoch = 3
2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - I'm a participant: 126

2 blindly believes that 3 is the leader and starts following 3! As a
result, 2's state is rolled-back (because 3 has old zxid). So 2
truncates its transactions logs.

Heres the incorrect if statement:

                   default:
                       /*
                        * There is at most one leader for each epoch, so if a
                        * peer claims to be the leader for an epoch, then that
                        * peer must be the leader (no* arbitrary failures
                        * assumed). Now, if there is no quorum supporting
                        * this leader, then processes will naturally move
                        * to a new epoch.
                        */
                       if(n.epoch == logicalclock){
                           recvset.put(n.sid, new Vote(n.leader, n.zxid, n.epoch));
                           if((n.state == ServerState.LEADING) ||
                                   (termPredicate(recvset, new Vote(n.leader,
                                           n.zxid, n.epoch, n.state))
                                           && checkLeader(outofelection, n.leader, n.epoch)) ){
                               self.setPeerState((n.leader == self.getId()) ?
                                       ServerState.LEADING: learningState());

                               leaveInstance();
                               return new Vote(n.leader, n.zxid);
                           }
                       }

3. Now, 1 is also running leader election (it does this immediately
after loosing majority).

1 now receives votes from 2 and 3 that say that 3 is the leader. So, 1
starts following 3.

18427 314274 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - Receive new notification message. My id = 215
18428 314274 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection  - Notification: 37 (n.leader), 17179869831 (n.zxid), 3 (n.round), FOLLOWING (n.state), 126 (n.sid),       LOOKING (my state)
18429 314274 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection  - I'm a participant: 215
18430 314274 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.QuorumPeer  - FOLLOWING


As a result, 1 ends up truncating its transactions as well. In order
for your client to see 3292 znode, the ensemble would have to
rolled-back at least (3348 - 3291) 57 transactions. We can see that
both 1 and 2 went back (17179869911 - 17179869831) 80 transactions.

In short, there are several places where a peer goes in correct
states. The main reason for this is that it does not always rely on <epoch, id,
zxid> to determine leadership. ZOKEEPER-975 had identified some of
them, and the fix for ZOKEEPER-975 would have prevented this bug.

I already have a working patch for ZOKEEPER-975 that I did not attach to the bug,
because I had implemented it on top of ZOOKEEPER-932. To avoid back
porting and testing efforts, I was hoping that ZOOKEEPER-932 would get
reviewed soon so that I can post a patch for ZOOKEEPER-975. It turns
out that ZOOKEEPER-975 is a blocker, so I will pull out the changes
for ZOOKEEPER-975 and submit it for review. We might have to do more
changes to cleanup FastLeaderElection, but my patch for ZOOKEEPER-975
should at least get rid of the bug that you are seeing.

Let me know if you think I have miscalculated things here., Wow, thanks for the in-depth explanation.  It makes sense to me, in terms of the timeline of events and what could go wrong, but I don't know enough about the Zookeeper code to be able to verify for sure.  I would love to try out a patch for ZOOKEEPER-975 and see if that fixes the problem for me.  (I added myself as a watcher for that bug.), That's a good catch. I was thinking that the only thing we might need to do is remove the case that follows a leader blindly. In my understanding, this is the case that allowed server 2 to follow server 3. Server 1 wouldn't have followed server 3 if there wasn't a quorum supporting 3, since the "if" statement that makes the decision on following the server is guarded by this predicate: (n.epoch == logicalclock)

It sounds like we will need a new release soon...
, I'm not entirely convinced that ZOOKEEPER-975 is the right fix for this problem. In my perception, it might make it less probable, but it still doesn't get rid of the problem. Let me put some thoughts here.

My understanding is that currently we rely upon a safety property from leader election, which says that the server that arises as leader has the highest zxid among a quorum of servers. This is a nice optimization because it speeds up the recovery process. 

The problem I see is that the zxid used in votes to determine leadership may not be the same as the current zxids of servers because of the problem that ZOOKEEPER-975 points to or simply because messages come in late. The former is exactly the case we observed here. Consequently, leader election violated the safety property it was supposed to satisfy.    

My current opinion is that we should be making sure that the leader really has the highest zxid among a quorum before exercising leadership. I believe we do not perform this check today, and this probably implies a deeper change and requires some more thinking.

Regarding the "if" that blindly accepts a leader, I think it should go away. It sounds wrong even without the fix for ZOOKEEPER-975., To follow up on my own comments. Here is how I'm currently thinking. 

For the issue described in this jira to happen, the following needs to hold: 

# We have a broken leader that does not have the highest last zxid in a quorum; 
# There is a quorum containing the broken leader such that for every element of the quorum, the last zxid epoch is the same or smaller compared to the epoch of the broken leader. If this condition doesn't hold, then at least one server will not follow the broken leader; 
# The broken leader must have received at least one notification during leader election that reflect an old state of the system. 

Clearly the problem Vishal has reported matches this description, since repeating notifications causes a server to receive notifications that might reflect a stale state of the system. However, I'm not entirely convinced that we can completely get rid of this problem with a patch for ZOOKEEPER-975 because of scenarios like the following, which at least abstractly seems to work: 

# There are three servers: S1, S2, S3;
# Servers come all from epoch e , and are trying to elect a new leader for e+1; 
# Each server sends one notification to each of the other servers, proposing itself as leader; 
# S2 and S3 receive a notification from S1, and suppose that S1's vote supersedes their own votes. S2 and S3 eventually decide to follow S1; 
# S1 receives notifications from S2 and S3 and decides to lead; 
# S2 follows S1 for a longer time than S3, so S3 lags behind; 
# S1 drops leadership (doesn't matter why); 
# S3 starts leader election and receives the notification S2 sent in step 3; 
# S3 believes it is the leader and starts leading; 
# S2 receives a leading notification from S3 and starts following S3, thus truncating its log (!!!) 

The last step is mainly due to the broken "if" statement I mentioned before. Now, in this example, there was no repetition of notification messages (ZOOKEEPER-975), and yet we've reached a problem. The question is if this run can happen or not. Can you see any step that can't happen? To me, the one step that is not clear is Step 8, but I can't convince myself that it can't happen., Hi Flavio,

I submitted a patch to ZOOKEEPER-975 yesterday, but I forgot to mention it on this bug. I think it addresses both 975 and this bug. I am not sure if you got a chance to take a look at it? (I just noticed that the patch failed on trunk. I will fix that)

The patch removes the two incorrect if conditions that I mentioned above (in addition to changes done for 975). Note - in the diff, for the the second if (where the peer blindly follows), we may have to check the epoch as well. I also posted a question yesterday pointing out a place where we don't send the latest zxid when the cluster is running. I think we should fix that as well.

I was planning to write a test to verify the patch, but I will wait to hear back regarding the diff and questions posted to ZOOKEEPER-975 before proceeding.
, 7.S1 drops leadership (doesn't matter why);
8.S3 starts leader election and receives the notification S2 sent in step 3;

Wouldn't the notification from S2 have n.epoch < logicalclock, causing it to be ignored?, Right, but what if S3 crashes and recovers? In this case, logicalclock is reset and receiving the old message from S2 would cause S3 to lead, as described above. If this is right, then is it only possible if we have message duplicates?, How would S3 get an old message from S2? I don't understand what you mean by message duplicates in this scenario., If I am not mistaken the scenario Flavio presents is as follows:
T0: S1, S2, S3 start FLE. Lets assume that S2's notification is (e=1, xid=1). In the end, S1 wins.
T1: S1(e=1, xid=10),  S2(e=1, xid=10), S3(e=1, xid=5)
T2: S1 and S3 crash
T3: S3 reboots (really fast), starts FLE from <e=0, xid=10>, and receives S2's notification of (e=1, xid=1)
T4: S3 updates its epoch to 1, compares <e=1, xid=1> vs its <e=1, xid=5> and decides to lead.

However, S2 should not follow S3 (after removing the blind following condition).
S2 will follow S3 only if a majority are following S3. In this case they are not.

Now, S2 in turn will receive a "LOOKING" notification from S3 (sent during T3). S2 will also decide to lead. Both will think they are LEADING. They will timeout and start another round of election. This time, S2 will turn out to be the winner. Does this make sense?

I think the problem here is "lastMessage" in QCM. I was thinking about this while working on the patch. As I mentioned in 975, I think we should send the current state <e, lastLoggedZxid> instead of lastMessage. Also, in T3, we might want to initialize logicalClock to epoch stored in transaction logs. What do you think?, Wouldn't S2 receive a FOLLOWING notification from S3, not a LOOKING notification, if S3 ended up following S1 initially? Isn't FOLLOWING the last notification that it sends? In which case

T3: S3 starts FLE from e=0 xid=5, sees a FOLLOWING notification, has no quorum, and goes to round 2. 
T4: Attempt to LEAD from S2 with the higher xid, and cede power. 

Or is the FOLLOWING message somehow not put on the queue?, Er first sentance is backwards should read:
Wouldn't S3 recieve a FOLLOWING notification from S2 not LOOKING if S2 ended up following S1 initially., The FOLLOWING notification does not get qeueued. If I am not mistaken, a peer does not send a notification after finishing FLE.However, I think that my patch for 975 would avoid this problem. In that patch I store only the most recently sent notification (and not all notifications). The most recent notification sent by S2 would be the one where it tells everyone that it is LOOKING and it thinks that S1 is the leader.

We should get rid of lastMessage in  QCM. wherever we send lastMessage, we should send the current state of the peer. So, if the peer is in the middle of FLE, send <epoch,proposedleader, proposedzxid, LOOKING>. Otherwise, send <epich, leader, lastLoggedZxid(), FOLLOWING/LEADING>, Ok, I didn't see that thanks. Out of curiosity, could we save a FOLLOWING notification as our lastMessage then, even if we don't send it? That would give you the "current" state more accurately than a stale LOOKING notification and it seems like it could solve this particular problem., It is unclear to me why you say that a FOLLOWING notification does not get enqueued, Vishal. A peer does send a notification when it receives one from a peer LOOKING through the same mechanism as other notifications.

Also, you're right that it does not send a final notification message. I believe it is this way because the peer will still respond to a notification from a peer that it is looking, so the message is not strictly necessary. , {quote}
It is unclear to me why you say that a FOLLOWING notification does not get enqueued, Vishal. A peer does send a notification when it receives one from a peer LOOKING through the same mechanism as other notifications.
{quote}

My comment was in the context of previous scenario that we were discussing., {quote}Out of curiosity, could we save a FOLLOWING notification as our lastMessage then, even if we don't send it? That would give you the "current" state more accurately than a stale LOOKING notification and it seems like it could solve this particular problem.
{quote}

We could, but that wouldn't give an accurate state as well. A peer might be in the LOOKING state (next FLE round) while the lastMessage gets sent. There will be race conditions. In my opinion, a better way of doing this is to send the current state instead of lastMessage. In which case, I agree with Flavio that sending the final notifications won't be necessary. This will also reduce the number of notifications exchanged.]