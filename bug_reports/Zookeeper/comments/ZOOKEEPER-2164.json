[connectOne is blocking, yes? shall we make it non-blocking?, Good catch!

Actually I'm trying to refactor this part to be non-blocking :) More like:

1. Construct a connector instead of connectOne()
2. Submit the connector to connection manager. Submit() returns a Future.
3. connector is an interface for Netty to roll in.

I would like to take advantage of this JIRA and discuss the design here. Any thoughts?, Sure sounds good. Thank you for driving this Hongchao!, We experienced a related problem.  

In a test-setup with 6 servers (3.4.6) with 2 servers shut down, leader election could take a very long time ( 1 to 2 minutes ) to complete. Once we changed the "cnxTO" variable from 5000ms to 500ms in the QuorumCnxManager, it completed under 10 seconds again.

In a setup with 8 servers (3.4.6) with 2 servers shut down, leader election could take a very long time ( We have experienced more than 10 minutes ! ) to complete and frequently started again immediately after completing.
Monday we will test our cnxTO fix on this setup as well.
, It's on my plan to have a patch for this. I'm currently involved in internal stuff. I should be able to get onto this after that.

At the mean time, it sounds like you have a good testing plan. Would be nice if you can share it. :), Its on my plan to have a patch for this.I'm currently involved in internal stuff., ZOOKEEPER 2164-fast leader election keeps failling., I believe I have run into this issue (zookeeper versions 3.4.6  and 3.4.10).  

These scenarios I've tested  lead me to believe I have the same problem.  I have a 3 node cluster and if the leader is "2" and is stopped, the election will fail and ultimately 1 and 3  respond with  "This ZooKeeper instance is not currently serving requests" from the stat command.  

If 2 is restarted, the cluster returns and 2 becomes the leader .     This appears to be the scenario documented above.  Sometimes 3 will fail to rejoin but if it is restarted it will rejoin the cluster successfully.

Essentially the only electable leader is #2.  The nodes are built as docker containers and orchestrated using Kubernetes.  

I am searching  for a work around or  configuration change that will enable the  cluster to be functional if the existing leader fails  are there are only 2 nodes (out of 3) available.]