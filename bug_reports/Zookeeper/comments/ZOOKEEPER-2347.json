[Stack trace showing the issue, Thanks [~tedyu] for reporting this issue. Also, thanks [~fournc], [~fpj] for the analysis/advice. I'm attaching a patch fixing the issue. I've tried an attempt to simulate the scenario through unit testing, I ran test multiple times and didn't see any deadlock after applying the patch. Please review., To be consistent, I'm reposting the comment I made in the other jira here. 

bq. We have made requestsInProcess an AtomicInteger in ZOOKEEPER-1504, removing the synchronization of the decInProcess method. We should just make the same change here for the 3.4 branch.
, Not sure how I can test this with hbase unit test(s).

As far as I know, zookeeper still uses ant to build while hbase dependency is expressed through maven., bq. As far as I know, zookeeper still uses ant to build while hbase dependency is expressed through maven.

Hi Ted.  The Ant build has a {{mvn-install}} target.  If you're interested in testing with HBase, then I think you could get the current branch-3.4 ZooKeeper code, apply the patch, run {{ant mvn-install}} to install a 3.4.8-SNAPSHOT build to your local repository, and then set up your HBase build to link against ZooKeeper 3.4.8-SNAPSHOT., What are the conditions that trigger this issue?  We've been running with 3.4.7 and so far have not seen any dead-locks with routine server shutdowns, or with tests.  Trying to judge whether we should revert or not., Thanks for the pointer, Chris.

I ran TestSplitLogManager after modifying pom.xml twice which passed. Previously the test hung quite reliably on Mac.

, [~yuzhihong@gmail.com], thank you for the help with testing!, The patch looks good, but I'm not really convinced about the test case. It relies on the interleaving of events to possibly trigger the problem, so it isn't deterministically reproducing the problem in the case it exists. I was thinking that maybe a better way of testing this is to set up a pipeline, populate {{toFlush}} directly, and just call shutdown on the {{ZooKeeperServer}}. If it is possible to do this, then it will be more reliable than submitting a bunch of operations and hoping for the race to kick in. What do you think? , yes, agreed. I've attached another patch modifying the unit tests, please review it again. Thanks!, Rakesh:
Thanks for updating the test case., [~fpj]:
Can you review the patch ?, [~rakeshr] thanks for the update. It looks much better, I have tested and the new test case does hang without the other changes, but there are a few small points I want to raise:

# Do we really need a timeout of 90s? I'd rather have something like 30s or less.
# Typo in {{LOG.error("Exception while waiting to proess req", e);}}
# Please add a description of the dependency cycle that we are testing for. For example, in step 7, you could say that we are testing that SyncRequestProcessor#shutdown holds a lock and waits on FinalRequestProcessor to complete a pending operation, which in turn also needs the ZooKeeperServer lock. This is to emphasize where the problem was and make it very clear.
# Replace {{"Waiting for FinalReqProcessor to be called"}} with {{"Waiting for FinalRequestProcessor to start processing request"}} and {{"Waiting for SyncReqProcessor#shutdown to be called"}} with {{"Waiting for SyncRequestProcessor to shut down"}}.
# There are a couple of exceptions that we catch but do nothing because we rely on the timeout. It is better to simply fail the test case directly if it is a failure rather than rely on a timeout. If you don't like the idea of calling {{fail()}} from an auxiliary class, then we need to at least propagate the exception so that we can catch and fail rather than wait.

I also would feel more comfortable if we get another review here. I'm fairly confident, but given that we've missed this issue before, I'd rather have another +1 before we check in., Thanks [~fpj] for the detailed comments. I've modified the description . Also, used 30 secs timeout. I feel bigger timeout would be needed in case running the tests in slower machines. Could you please review the latest patch again when you get a chance., [~rakeshr] looks good, I just have a few minor asks, please replace accordingly:

# “Tests to verify that ZooKeeper server should be able to shutdown properly…” -> “Test case to verify that ZooKeeper server is able to shutdown properly…”
# “errOccurred” -> “interrupted”
# “InterruptedException while waiting to process request!” -> “Interrupted while waiting to process request”

I still would like to have another committer having a look at this to have a second opinion. Any volunteer, please?, Thanks again [~fpj] for the review comments. Attached another patch addressing the same., It lgtm - thanks [~rakeshr] and [~fpj]. One question though, why use NettyServerCnxnFactory for the test instead of the NIO one (which much more used)?

[~cnauroth]: mind taking a look as well?

Also, how can we validate if the HBase tests now pass?, Thanks [~rgs] for the reviews.

bq. One question though, why use NettyServerCnxnFactory for the test instead of the NIO one (which much more used)?
No specific reason. Test scenario has no relation with either Netty or NIO.

bq. Also, how can we validate if the HBase tests now pass?
Sometime back Ted has updated Hbase test status in jira, please see the [comments|https://issues.apache.org/jira/browse/ZOOKEEPER-2347?focusedCommentId=15063086&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15063086]. Thanks [~yuzhihong@gmail.com] for the test results.
, Assuming there was only test change since I performed validation last year, this should be good to go., +1 for the patch.  Nice work, [~rakeshr]!, Merged:

https://github.com/apache/zookeeper/commit/8fc106507b767c28530a1028b27bd3b25e3aaab8

Thanks [~rakeshr]!]