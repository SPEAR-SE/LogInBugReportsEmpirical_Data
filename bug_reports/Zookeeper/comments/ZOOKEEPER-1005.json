[I should note that several other failures were also simulated, with nodes replaced from time to time. But this particular issue seems to have prevented the zookeeper service from becoming available for a period of 2 hours. After 2 hours the quorum was reestablished and zookeeper recovered gracefully (at first glance)., Hi Alexandre,

Can you attach the zookeeper logs and zoo.cfg files? Also, please describe your test in detail.

I am not 100% sure whether it is a good idea to wait for 40 seconds as you suggested. Heres a scenario to think about:
Assume that we have a slow follower instead of a slow leader. Before the follower enters the for loop in question, there is a change in leader. Now, when the follower attempts to connect to the peer that it thinks is a leader (the old leader), it will fail with the same error as you are seeing. In this case, it is not worth asking the follower to wait for 40 seconds in that loop. Currently, ZK does not  preempt a peer once it has exited leader election (and is either trying to be a leader or a follower).

Note that there have been many fixes in leader election since ZK 3.2.2., Hi Alexandre, We do not assume that a follower must be able to connect to an elected leader. Since a process might crash at any time, there is the possibility that a process crashes right after being elected, so we don't want a follower waiting indefinitely on such leader.

I don't understand how this problem of a follower timing out before the leader exercises its role leads to a 2-hour delay. If a connection from a leader to a follower times out, then the follower goes back to leader election, and a new leader is supposed to arise in a few seconds. 

As Vishal pointed out, there has been some fixes to leader election and to the logic that connects a leader and it followers since 3.2.2. , Hi Vishal,

{quote}
Can you attach the zookeeper logs and zoo.cfg files? Also, please describe your test in detail.
{quote}
Unfortunately our auto-installer process has reinstalled the nodes before I could get complete logs off the machines. I will attach the zoo.cfg shortly. 

{quote}
I am not 100% sure whether it is a good idea to wait for 40 seconds as you suggested. Heres a scenario to think about:
Assume that we have a slow follower instead of a slow leader. Before the follower enters the for loop in question, there is a change in leader. Now, when the follower attempts to connect to the peer that it thinks is a leader (the old leader), it will fail with the same error as you are seeing. In this case, it is not worth asking the follower to wait for 40 seconds in that loop. Currently, ZK does not preempt a peer once it has exited leader election (and is either trying to be a leader or a follower).
{quote}
I agree that the follower should not wait forever, and 40 seconds may be too long. But these settings can be configure in zoo.cfg. I also think that the fact that the leader takes so long to actually start the leader process is a problem (which should be addressed separately, and may be related to our particular failure simulation) and is a definite contributing factor to this issue. 

I am just surprised that the documentation indicates that follower should continue to attempt to connect to the leader for the time specified by {{initLimit}}, but that does not happen in this case. So the choice of initLimit is a worthy debate, but does not seem to change the fact that a follower should be trying to connect to leader until the initLimit time has elapsed (according to documentation). The timeout on the socket is set to {{self.tickTime * self.initLimit}} so this seems to indicate that the initLimit time is adhered to, but only in certain classes of failures.

{quote}
Note that there have been many fixes in leader election since ZK 3.2.2.
{quote}
We are committed to zookeeper 3.2.2 for the moment, since we do not have sufficient resources to test a later zookeeper in time for inclusion in our next product release. We will be implementing other workarounds for the moment, but it seemed like a good idea to raise the issue and see whether this is the desired behavior., Hi Flavio,

{quote}
Hi Alexandre, We do not assume that a follower must be able to connect to an elected leader. Since a process might crash at any time, there is the possibility that a process crashes right after being elected, so we don't want a follower waiting indefinitely on such leader.
{quote}
Yes, I agree with that. My question in this case is not whether the follower should be waiting, but rather how long should the follower wait. It seems to me that the follower may be waiting for a shorter period of time than the documentation seems to indicate it should. Please see my response to Vishal's comment for details.

{quote}
I don't understand how this problem of a follower timing out before the leader exercises its role leads to a 2-hour delay. If a connection from a leader to a follower times out, then the follower goes back to leader election, and a new leader is supposed to arise in a few seconds.

{quote}
Only two of the three zookeeper nodes were up. The follower did go back to leader election after which a new leader was elected with the same conditions as listed in the log entries above. So this cycle of election starts, leader chosen, follower fails to contact leader, leader terminates, election start ... repeated for a very long period of time until the leader eventually started its service in time for the follower to connect.

I have no doubt that some other factor was influencing the leader, and preventing the leader from starting the leader service in a timely fashion. And yes, that has to be sorted out. I think we will be able to work around our problem, so an immediate fix is not the prime concern. At this point I am more interested in determining if this is the correct behavior.
{quote}

{quote}
As Vishal pointed out, there has been some fixes to leader election and to the logic that connects a leader and it followers since 3.2.2.
{quote}
Unfortunately we are committed to zookeeper 3.2.2 for the moment, due to our product release dates and available resources., {quote}
I agree that the follower should not wait forever, and 40 seconds may be too long. But these settings can be configure in zoo.cfg. I also think that the fact that the leader takes so long to actually start the leader process is a problem (which should be addressed separately, and may be related to our particular failure simulation) and is a definite contributing factor to this issue.

I am just surprised that the documentation indicates that follower should continue to attempt to connect to the leader for the time specified by initLimit, but that does not happen in this case. So the choice of initLimit is a worthy debate, but does not seem to change the fact that a follower should be trying to connect to leader until the initLimit time has elapsed (according to documentation). The timeout on the socket is set to self.tickTime * self.initLimit so this seems to indicate that the initLimit time is adhered to, but only in certain classes of failures.
{quote}

Per my understanding, the leader waits for tickTime * initLimit and gives up leadership if a majority of followers do not join the leader in that time. So I think the text is written from leader's perspective. The follower does not wait for that amount of time, but keeps trying to run leader election and then joins the leader. In your case, the follower was not able to join the leader after 5 attempts. The follower will start new a leader election, but that will not result in a new leader election round at the leader. The leader will wait for 40 seconds before giving up leadership. In the meantime, the follower will complete the second round of election, elect the same leader and again try to join the leader. I would expect that this process should complete soon (at least within 40 seconds of leader wait time).

Without logs it will be hard to pinpoint the problem. Can you also take a stack trace at the leader and the follower?

Can you give details about your experiment (you mentioned you also replaced a node). Also, what is your SyncLimit? There have been some fixes in the code path in question (ZOOKEEPER-822 and ZOOKEEPER-900). Higher syncLimit values or failure of a node during leader election can cause significant delays without these fixes.



    Note that there have been many fixes in leader election since ZK 3.2.2.

We are committed to zookeeper 3.2.2 for the moment, since we do not have sufficient resources to test a later zookeeper in time for inclusion in our next product release. We will be implementing other workarounds for the moment, but it seemed like a good idea to raise the issue and see whether this is the desired behavior.
, not a blocker. Moving it out of 3.4 release.]