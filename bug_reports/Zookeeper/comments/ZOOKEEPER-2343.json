[0.0.0.0 cannot be used as a destination IP address.
So I think it is not a bug. 
(Although validating such an address is effective improvement.)


Perhaps it makes sense to use hostnames instead of IP addresses.
, 0.0.0.0 can work in 3.4.6, that's why zookeeper 3.4.6 can work fine in kubernetes.
see this

https://github.com/fabric8io/fabric8-zookeeper-docker/blob/master/config-and-run.sh

 if [ "$SERVER_ID" = "$i" ];then
      echo "server.$i=0.0.0.0:2888:3888" >> /opt/zookeeper/conf/zoo.cfg
    else
      echo "server.$i=zookeeper-$i:2888:3888" >> /opt/zookeeper/conf/zoo.cfg
    fi

Even using hostname can't be acceptable in kubernetes, because Pod and service's hostname can't be the same., In docker, the magic sauce for me was to use externally addressable ip addresses in zoo.cfg.dynamic, then set {{quorumListenOnAllIPs=true}} in zoo.cfg., There's a good post on these issues from Tim Crowder ( [~timrc] ?)
http://timmahnator.tumblr.com/post/115073121775/zoo-keepers-and-dock-workers, I tested the same configuration using ZooKeeper 3.5.4-beta-SNAPSHOT and the problem is not reproduced. 
ZOOKEEPER-2819 introduced new behavior for {{reconfigEnabled}} flag and it allows to resolve your issue., I have just tested this with 3.6.0-SNAPSHOT and I can confirm that it works.  However, with 3 replicas, (on deployment in gcloud Kubernetes Engine), the first server pod to start will report "This ZooKeeper instance is not currently serving requests", but the second and third instance are able to create the quorum.  When the first instance is deleted, on recreation it can join the quorum and subsequently serve requests.
]