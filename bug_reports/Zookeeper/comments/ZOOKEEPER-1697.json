[When I ran into a similar problem, I found that the main culprit is ZOOKEEPER-1324. 

The side-effect of that bug is that it caused the initial set of followers that join the quorum before the quorum is formed to get duplicate NEWLEADER packets. Those followers will take one more additional snapshot after sending NEWLEADER's ACK back to the leader and before serving request. 

This caused the leader to tear down the quorum if syncLimit is low and snapshot taking time is high.

You can look at the follower logs to see that they are taking 2 snapshots to confirm if this is the case.  , Hi [~thawan] I really appreciate the feedback. I do indeed see two snapshotting messages in the follower log.

Say ZOOKEEPER-1324 is fixed, it seems to me that this Jira would still need to be addressed, no? In the case with a very large snapshot file (say twice as large) we could still run into the same issue I describe here. "tear down the quorum if syncLimit is low and snapshot taking time is high". Do you agree or am I missing something?

Thanks again., I'm also not entirely convinced that solving ZOOKEEPER-1324 will solve this current issue. Talking about ZOOKEEPER-1324, I'd really like to have it in soon.

I had a look at trunk, and I don't understand how LearnerHandler#synced() can be the culprit here. Calls to synced() appear in two places in Leader: 

# In the second while loop of lead(), after enough followers have synchronized;
# In isQuorumSynced(), which is used with the RECONFIG command. It seems unrelated to the issue described here.

I looked at trunk because this issue is also marked for 3.5.0.

, Hi [~fpj] thanks for looking. 

bq. I looked at trunk because this issue is also marked for 3.5.0.

Sorry for the confusion. I listed the "Affects Version" as 3.4.3, because that's where I saw it. Marked as fix for both 3.4 and 3.5 because I wanted to make sure we considered it for both. I haven't looked to see if it effects 3.5 though. To be clear I've been looking at 3.4 codebase. In particular 3.4.3 release codebase which is where the issue was seen.

bq. I had a look at trunk, and I don't understand how LearnerHandler#synced() can be the culprit here. Calls to synced() appear in two places in Leader:

Is this still a question for you if you look at the 3.4.3 codebase?

What I see in the logs is that the leader has shut down with the log I mentioned:

{noformat}
org.apache.zookeeper.server.quorum.Leader: Shutdown called
java.lang.Exception: shutdown Leader! reason: Only 0 followers, need 2
{noformat}

All four of the followers were in the middle of snapshotting during this time. Once they complete that task they then attempt to 

{noformat}
        writePacket(ack, true);
{noformat}

I've verified from all four follower logs that this fails for all four in this location - acking to the leader and getting "broken pipe" due to the leader having already shutdown.

Based on my reading of the code:

1) it seems that the followers are busy snapshotting and have not yet ack'd.

2) Because LearnerHandler hasn't seen the ack it has not yet updated tickOfLastAck for the first time (so 0). We're in bootstrapping so we should allow for initLimit (which is configured large relative to syncLimit)

3) Leader lead() method is checking the tickOfLastAck against the syncLimit via the synced() call, which has expired and therefore calls shutdown. I'm suggesting that if initLimit were used instead during this call to synced (but only during the bootstrapping phase, you still want to use syncLimit otw) that the followers would eventually have been able to snapshot, send the ack, and the leader would be happy (tickOfLastAck updated appropriately). In other words we need to extend the time period of the check in synced during the bootstrapping of a learner.

This make sense to you all? If not what am I missing. Thanks!


, I do see this on the leader just prior to the shutdown:

bq. INFO org.apache.zookeeper.server.quorum.Leader: Have quorum of supporters; starting up and setting last processed zxid: ...

So perhaps the first newleader ack (based on Thawan's speculation we are getting 2 dup newleader pkts) does get sent by the learner to the learnerhandler, which allows the leader to make progress. However learnerhandler will wait at the top of the while loop (bottom of run method) for another packet to come before it first updates tickOfLastAck. That packet is the one that's causing the "broken pipe" error message on the follower. i.e. tickOfLastAck is therefore never updated.

Another issue to consider is that the leader.self.tick and tickOfLastAck are both unsynchronized in these code sections...

, It is legitimate that we use syncLimit at the part of the Leader code because when we start that while loop, the leader has already received enough acks for the newleader proposal. After looking more into it, I think that the culprit is ZOOKEEPER-1324 as Thawan suggested. The followers are taking to long to send an ack back because they are taking an additional snapshot.  , ZOOKEEPER-1324 is probably one of the issues, but i think after that gets fixed we can run into the issue that pat identified. we don't set tickOfLastAck until after we receive the first message from the follower after the leader becomes active. after the leader becomes active it will sleep for tickTime/2 and then check to see if the followers are synced, which uses tickOfLastAck. that gives followers a very small window in which to send the first message. perhaps when the leader starts. a simple fix would be to set the tickOfLastAck of all its followers to the current tickTime., bq. a simple fix would be to set the tickOfLastAck of all its followers to the current tickTime.

And to avoid a race, I think we need to reset tickOfLastAck before calling zk.startup()., I've attached a patch for trunk & 3.4 that I think will fix the problem. 

All tests pass on branch34, however I'm running into ZOOKEEPER-1700 on trunk and can't get the tests to run cleanly (only FLETest.testJoin is failing on trunk, but that fails with or without this patch applied)

Please take a look and let me know if this patch makes sense to you for both 3.4 and trunk., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582223/ZOOKEEPER-1697.patch
  against trunk revision 1463329.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1470//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1470//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1470//console

This message is automatically generated., ZOOKEEPER-1700 turned out to be a bug in the FLETest. This patch now passes for me on both branch34 and trunk. There are no updates to existing tests as this is closing a very small timing hole. The existing tests verify this codepath and are passing. [~breed] or [~fpj] et. al. can you take a look at the two patches? Thanks., Hi Pat, It doesn't quite work, I think. Setting the tickOfLastAck where you're setting could still cause the same problem because this loop:

{code}
            /*
             * Wait until leader starts up
             */
            synchronized(leader.zk){
                while(!leader.zk.isRunning() && !this.isInterrupted()){
                    leader.zk.wait(20);
                }
            }
{code}

could still spin for a while, no?. It might work better to set tickOfLastAck for the learners that have ack'ed at that point in Leader#tryToCommit() right before zk.startup().

One smaller comment, perhaps not for this jira, is that I'd rather use QuorumPeer#getTick() rather than accessing tick directly. We are doing it this way in a number of places so it might be better to use it consistently. , [~fpj] that's a good point, a window still exists. (see if this comment/solution solves the problem - I'd rather hold off on "why not tryToCommit" in case we solve this in another way)

Perhaps the original approach is flawed, I tried thinking about this from a different angle and I think I came up with a patch that addresses all concerns. Take a look at the latest patch for trunk/branch34 (attached)

In this case I turn things around - we define a deadline for next ack rather than tracking the last time we heard from the learner. This allows us to take into account the bootstrapping phase, and the fact that the learner is not out of bootstrapping until it reaches the point where the leader receives the response to UPTODATE message. What do you think?, [~phunt] - this seems _much_ clearer and easier to reason about.  , -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582498/ZOOKEEPER-1697.patch
  against trunk revision 1463329.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1474//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1474//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1474//console

This message is automatically generated., Hi Pat, The new approach you propose looks good to me. One advantage is that it concentrates the changes on LearnerHandler and it is easier to read the code this way. 

I like the approach of resetting tickOfLastAck for the second ack and on because the first ack is a bit odd, so resetting is a way of separating the timing of the first ack from the rest.

In your patch, I was actually thinking that this statement:

{code}
tickOfNextAckDeadline = leader.self.tick + leader.self.initLimit;
{code} 

might not be right because of the following argument. An instance of LearnerHandler may take up to initLimit to get to the point that the leader is running. From the point that the LearnerHandler instance crosses the start up while loop, we should grant another syncLimit ticks until next ack because the first might have arrived right before it crosses the while loop, no? If so, then perhaps the initialization of tickOfNextAckDeadline should be instead:

{code}
tickOfNextAckDeadline = leader.self.syncLimit + leader.self.initLimit;
{code} 

Let me know what you think., Thanks [~fpj]

bq. One advantage is that it concentrates the changes on LearnerHandler and it is easier to read the code this way.

Agree. The encapsulation and code flow is much better.

bq. An instance of LearnerHandler may take up to initLimit to get to the point that the leader is running.

I was thinking that the initLimit is the amount of time you allow the servers to bootstrap and start serving, for me this included the initialization as well as the initial "sync" period. I could see going either way, but if we did make the change it seems like it would be odd - we do sock.setSoTimeout(initlimit) in some places (during bootstrapping), based on your comment we should change those as well? Why not have the operator just consider that (initi + sync) in setting the initLimit?, bq. I was thinking that the initLimit is the amount of time you allow the servers to bootstrap and start serving, for me this included the initialization as well as the initial "sync" period.

Agreed, if the leader is not able to get a quorum of supporters within initLimit, it drops leadership. 

bq. we do sock.setSoTimeout(initlimit) in some places (during bootstrapping), based on your comment we should change those as well?

No, I think the setting of socket timeouts is correct, I don't see a problem with it, do you?

bq. Why not have the operator just consider that (initi + sync) in setting the initLimit?

My point is not exactly that. Note that in my comment I'm talking about the first time we set tickOfNextAckDeadline in your patch. In the patch, it sounds like you're giving only one tick to the next ack in the case bootstrapping and syncing takes initLimit. This is the time until the LearnerHandler instance crosses the startup while loop. I'm saying that it should be syncLimit instead.  

, [~fpj] I believe I understand what you are saying, my point was just "what's the definition of initLimit". You've defined it as "leader is not able to get a quorum of supporters within initLimit, it drops leadership."

Looking at our docs they say: http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_clusterOptions

"initLimit: Amount of time, in ticks (see tickTime), to allow followers to connect and sync to a leader. Increased this value as needed, if the amount of data managed by ZooKeeper is large."

afaict our official docs match what's currently in the patch.

Would we not get the same thing as you are suggesting by running with a config of 

initLimit=15
syncLimit=5

vs changing the patch as you suggest and running with 

initLimit=10
syncLimit=5

Our docs seems to be saying you should run with the first config (and the current patch). Perhaps I'm missing something though....
, I'm not sure if you're saying that my statement conflicts with the definition of initLimit in the docs. If followers are not able to sync with the leader, isn't it the case that the prospective leader will not have a quorum and will drop leadership? In any case, I was reading the code, not the docs. 

I don't think the two ways of configuring you're suggesting are equivalent because we use initLimit in a number of other places (Leader#lead() while waiting for a quorum of supporters, for example) and I'm not saying that we should replace that initLimit with initLimit + syncLimit for cases other than ack bookkeeping. Actually, I'm saying that we need syncLimit on top of initLimit for the first ack in LearnerHandler independent of how you choose the value for initLimit. This observation comes from the fact that between start up and the second ack we should allow syncLimit ticks. Say that the leader takes initLimit to form a quorum of supporters. Once the LeanerHandler starts processing messages, it has only a tick to update tickOfNextAckDeadline before the learner is considered out of sync with the patch you're proposing, right?  , bq. only one tick to the next ack

perhaps this is the point i'm missing that you're trying to make. Question: what do we consider "sync'd" wrt bootstrapping? After the first ack, or after the UPTODATE response comes back to the leader. I was assuming the second (uptodate). Is that not correct?, Sorry, I was formulating my previous comment during your response and they crossed. I suspect that we are converging? (I'll wait and let you comment on my previous comment ;-) ), Ah, great question. In my view, the UPTODATE message is the commit of the synchronization phase of recovery, so no ack is actually needed, but we have it for backward compatibility. In the way we are doing it currently, you're right that the first ack received in the LearnerHandler#run() while(true) loop is a response to the UPTODATE so it could be considered part of the synchronization phase. 

However, if we follow this interpretation, there is a discrepancy in what is implemented, I think. The leader must gather a quorum of supporters within initLimit ticks. Once it gets it, the leader starts running and believes that everyone is synced up. But, the UPTODATE message really goes out only after the leader has already started running:

{code}
            /*
             * Wait until leader starts up
             */
            synchronized(leader.zk){
                while(!leader.zk.isRunning() && !this.isInterrupted()){
                    leader.zk.wait(20);
                }
            }
            // Mutation packets will be queued during the serialize,
            // so we need to mark when the peer can actually start
            // using the data
            //
            LOG.debug("Sending UPTODATE message to " + sid);      
            queuedPackets.add(new QuorumPacket(Leader.UPTODATE, -1, null, null));
{code}  

From the leader perspective, it is already established when leader.zk.isRunning() is true and in my understanding we should be giving syncLimit ticks for acks once that predicate is true. Now, in practice a tick should be long enough to enable the leader to get a response for the UPTODATE message. I think what we are really discussing here is definitions, which is great. , bq. I think what we are really discussing here is definitions, which is great.

Yes, absolutely. I don't mean to be a pain here, I'm just not fully groking something. Thanks for bearing with me [~fpj].

So based on my reading of your most recent comment you still feel we should allow "initLimit + syncLimit" during the initial phase, is that right? LMK for sure and I'll update the patch., Here is my take. Setting tickOfNextAckDeadline to initLimit + syncLimit initially is conservative. At the same time, if the follower reacts fast, the learner handler will update tickOfNextAckDeadline it sooner after the sync up phase. Also, it only impacts the time for the current leader to release leadership if the follower crashes after completing the sync up phase. 

I think it is safer to update it, but if you don't update it, it will be a corner case, very unlikely to happen. 

Does it make sense?  , Conservative is good. See the updated patches. [~fpj] if you +1 I'll be happy to do the commit. Thanks!, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582684/ZOOKEEPER-1697.patch
  against trunk revision 1463329.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1476//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1476//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/1476//console

This message is automatically generated., +1, Thanks, Pat. 

b3.4 Committed revision 1481322.

trunk Committed revision 1481325., Integrated in ZooKeeper-trunk #1923 (See [https://builds.apache.org/job/ZooKeeper-trunk/1923/])
    ZOOKEEPER-1697. large snapshots can cause continuous quorum failure
  (phunt via fpj) (Revision 1481325)

     Result = SUCCESS
fpj : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1481325
Files : 
* /zookeeper/trunk/CHANGES.txt
* /zookeeper/trunk/src/java/main/org/apache/zookeeper/server/quorum/LearnerHandler.java
* /zookeeper/trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java
, Closing issues after releasing 3.4.6.]