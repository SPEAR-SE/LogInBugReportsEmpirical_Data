[[~yss] [~xiaoed]

I appreciate the effort for reporting this issue, but the official language of this project is English. Would you please translate the description and comment accordingly?, i've updated, interesting,I will test this issue in my docker.
[~yss] Do you have any more advances? Is it a issue about linux OS probably?, [~yss] Have you tried newer version of stable zookeeper release (e.g. 3.4.13), as well as different versions of OS? 3.4.5 is a pretty old version. , Given the 3.4.5 that has been used in the production environment, it is not realistic to upgrade to a higher version at present. Have similar defects been solved in 3.4.13?
[~hanm], I have seen a futex bug in Linux on the Internet before. However, when there was a problem, no deadlock problem was found with jstack. At present, we have used zk 3.4.5 for 4-5 years, and this bug was encountered for the first time. However, this version of kernel has been upgraded to block related IO drivers. Does this cause CLOSE_WAIT for zk? It looks like zk Server is deadlocked
[~maoling], {quote}Have similar defects been solved in 3.4.13?
{quote}
Previously there were reports about CLOSE_WAIT, but if I remember correctly, most of those cases ended up no actions taken because it was hard to reproduce. 
{quote}It looks like zk Server is deadlocked
{quote}
The thread dump in 1.log file indicates some threads are blocked, but that seems a symptom rather than the cause. If we run out available sockets then some zookeeper threads that involves file IO / socket IO will be blocked. 

 
{quote}Does this cause CLOSE_WAIT for zk?
{quote}
Most of time, long living CLOSE_WAIT connections indicate an application side bug instead of kernel bug - that the connection should be closed but for some reasons the application, after receiving TCP reset from clients can't close the connection - which effectively leaks connections. The upgrade of kernel could be a trigger though. 

 

I am interested to know if any other folks can reproduce this. I currently don't have the environment to reproduce this.

 

Also, [~yss] can you please use zip file instead of rar file for uploading log files? 

Another thing to try is to increase your limit of open file descriptors - seems its currently set as 60? If you increase it (ulimit), you could end up still leaking connections but the server should be available before its running out of sockets., 


27/5000 


 

 





*  increase your limit of open file descriptors 
There are only a few clients on our side, so we do not provide the connection number specifically


If we repeat it here, what information or log should I provide to you for analysis
[~hanm], [~yss]

Not a deadlock? Just a problem about the IO in your env?

Is it easy to reproduce this issue? any advances?]