[ZXID overflowed in prod:

We observed that the ensemble was not receiving any packets during the time of outage, as can be seen in the attachment 'image12.pnp'. It was a grafana graph, with data source from the four-letter word commands. In the meantime, node count dropped by ~10000 and stayed flat at 302,500 after the overflow. The aggregated log is attached as 'zk_20170309_wo_noise.log', which seems to tell that the leader election was finished successfully, quorum formed, and ZK servers started up.

However, we did see a lot of the following errors after the ZK servers went up:
{noformat}
2017-03-09 09:00:12,420 - ERROR [CommitProcessor:2:NIOServerCnxn@180] - Unexpected Exception:
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
        at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77)
        at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:153)
        at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1076)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:404)
        at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:74)


2017-03-09 09:00:13,210 - ERROR [CommitProcessor:1:NIOServerCnxn@180] - Unexpected Exception:
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
        at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77)
        at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:153)
        at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1076)
        at org.apache.zookeeper.server.NIOServerCnxn.process(NIOServerCnxn.java:1113)
        at org.apache.zookeeper.server.WatchManager.triggerWatch(WatchManager.java:120)
        at org.apache.zookeeper.server.WatchManager.triggerWatch(WatchManager.java:92)
        at org.apache.zookeeper.server.DataTree.deleteNode(DataTree.java:594)
        at org.apache.zookeeper.server.DataTree.killSession(DataTree.java:966)
        at org.apache.zookeeper.server.DataTree.processTxn(DataTree.java:818)
        at org.apache.zookeeper.server.ZKDatabase.processTxn(ZKDatabase.java:329)
        at org.apache.zookeeper.server.ZooKeeperServer.processTxn(ZooKeeperServer.java:965)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:116)
        at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:74)
{noformat}

We mitigated the issue by restarting the ensemble, after which we see traffic flowing into the ensemble and the whole system started recovering., We tried running 'ZxidRolloverTest' with different setups but failed to reproduce the issue, so we decided to use same hardware as production. The experiments below were using a 5-node ZK ensemble, with zookeeper.testingonly.initialZxid set to high enough value:

1. With tiny scripts using kazoo, spawn off client processes that each continuously randomly create  ZK nodes and set data on them, generating the same number of connections as production, while having another set of clients randomly read data from the nodes.
   - Result: ZXID overflowed. Leader election happen completed within 5 seconds. Short burst of errors was seen from the client side but the clients recovered right after the election.

2. Set up a 85 node Kafka broker cluster, then trigger overflowing with the same method as in 1.
  - Result: same as 1. The Kafka brokers behaved normal.

3. Set up a test tool to generate ~100k/s messages, and as many consumers as needed to generate the 1500-per-node connection count, for the Kafka cluster. The consumers write consumption offsets to ZK every 10ms. 
  - We noticed that after the ZXID overflowed for a couple of times, the whole system began acting weirdly - metrics from the brokers became sporadic, ISRs became flappy, metrics volume sent by Kafka dropped, etc. See attachment 'message_in_per_sec.png', 'metric_volume.png', 'GC_metric.png' for screenshots.
  - From the 'srvr' stats, latency became '0/[>100]/[>200]', vs. in normal conditions '0/0/[<100]'. Profiling ZK revealed that it was because the ensemble received high QPS of write traffics (presumably from the Kafka consumers) such that the 'submittedRequests' queue (>6500 in queue each time when a new request is added) in 'PrepRequestProcessor' of the leader was filled up, causing even the reads to have high latencies.
  - It looked to us that somehow by electing a new leader when overflowing caused the consumers to align, thus DDOSing the ensemble. However, we have not observed the same behavior after bouncing the leader process BEFORE the overflow. The ensemble should behave similarly in both cases since both call for new leader elections. One difference though, we noticed, was that in the overflow case the leader election port was left open so the downed leader would participate in the new round of leader election. Not sure if it's related but thought might be worth bringing up.

The behavior was observed with both ZK 3.4.6 and 3.4.9, Kafka 0.10.0 and 0.10.2.
    , We would much appreciate your input. Thanks!, [~eefangyicheng]
Good Report.Do you have any advances about this issue?, [~maoling] NaH, I have since shifted focus to other tasks because it didn't seem like I was getting close to reproducing what we saw in prod.

It would be great if you have some ideas that we can try out. We're all ears.]