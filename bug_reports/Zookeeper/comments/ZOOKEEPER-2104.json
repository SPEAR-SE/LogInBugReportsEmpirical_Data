[We found the same issue, but it recovered after 10 minutes, was this expected?
Can we fix this asap?, We are facing the same issue and its took much longer to get recovered . 

So our whole purpose of using Zookeeper cluster for HA has been defeated .

Please suggest . , I'm seeing the same issue. Additional observations: when the ensemble gets into this state, the leader and followers appears to be replicating massive amount of data although the curator clients were mostly idle. The on disk foot print for the log and snapshot are 65MB and 500MB respectively and there are plenty of memory and disk space on the servers. , The logs from the first node in the description indicate that syncing to disk is taking too long. Is the disk device shared between logs and snapshots? It is unclear from these logs why the second node abandoned the leader, but looks like a timeout of the socket.

[~davidlao] you say the servers were replicating massive amounts of data, how did you notice it? Also, were the servers generating too many snapshots and is this due to a traffic spike? Would increasing snapCount help? Could you observe in the logs why the ensemble wasn't able to come back up?, In my case the logs and snapshots are on separate drives. I noticed the net traffics, with the resource monitor app which is part of the Windows OS, were from the leader to followers on port 2888. I was seeing ~15 MB/s traffic between the nodes. Will try to increasing the snapshot count (though I'd think the default 100k transactions is plenty for the particular running workload). 

There are two distinct ERRORs from the log (see attached), one appears to be due to broken connection and the other is java.nio.channels.CancelledKeyException (reported in ZOOKEEPER-1237).  The broken connection error happens only once when server start. 
, Add attachements, The errors in this case don't say much, just that the server can't read/write from the socket. Is there any issue with your disks? Have you checked the disk traffic around the time this happened? I'm assumed this happened once, but let me know if this is reproducible., Unfortunately I've lost a member server and its snapshots and upon restarting the issue is no longer reproducible. I'll keep an eye on this and provide update as appropriate. Thanks for taking a look. , Hi All,

I am also seeing the below error on all of my 3 nodes where zookeeper is installed. Zookeeper is not coming backup. Any thoughts?

```
Apr 26 19:09:13 <hostname> zookeeper: 2016-04-26 19:09:13,966 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@362] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
```, Have a look at your initLimit config. and ensure its large enough to allow for followers to fully sync to the leader. In my case, increasing the initLimit solved the problem.  , [~davidlao] We have the below setting in the initLimit. 

```
# synchronization phase can take
initLimit=10
```, If increasing the initLimit value fixes it, then it is probably the case that you have large snapshots. Could you check the size of your snapshot files?, We just ran into exactly the same issue. 3 nodes cluster, suddenly the cluster went down and all nodes reporting "ZooKeeper is not running".

Is there something I can do to make someone look into this? 
, [~d.freudenberger] please check the comments in this jira if you haven't done it yet.

The "ZooKeeper is not running" messages are due to the server(s) being in leader election. If they can't elect a leader and make progress, then we need to determine why that's the case. To my knowledge, there is nothing to be fixed here unless you provide further evidence of a bug., [~fpj] of course I read through the comments. Zookeeper recovered after ~15 minutes. 20 minutes later (right now) it crashed again and flooding the log file with following errors:

{quote}
2016-07-27 11:49:39,829 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.41.199.233:60522 (no session established for client)
2016-07-27 11:49:39,864 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.41.199.201:60524
2016-07-27 11:49:39,865 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@354] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2016-07-27 11:49:39,865 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.41.199.201:60524 (no session established for client)
2016-07-27 11:49:40,095 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.41.199.217:37339
2016-07-27 11:49:40,096 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@354] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2016-07-27 11:49:40,098 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.41.199.217:37339 (no session established for client)
2016-07-27 11:49:40,245 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.41.199.63:33360
2016-07-27 11:49:40,245 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@354] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2016-07-27 11:49:40,245 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.41.199.63:33360 (no session established for client)
2016-07-27 11:49:40,317 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.41.199.111:34965
2016-07-27 11:49:40,320 [myid:2] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@354] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2016-07-27 11:49:40,320 [myid:2] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.41.199.111:34965 (no session established for client)
2016-07-27 11:49:40,346 [myid:2] - WARN  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:Follower@89] - Exception when following the leader
java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:83)
	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
	at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:152)
	at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:272)
	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:72)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:740)
2016-07-27 11:49:40,347 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:Follower@166] - shutdown called
java.lang.Exception: shutdown Follower
	at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:166)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:744)
2016-07-27 11:49:40,347 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:FollowerZooKeeperServer@139] - Shutting down
2016-07-27 11:49:40,347 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:ZooKeeperServer@419] - shutting down
2016-07-27 11:49:40,348 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:QuorumPeer@670] - LOOKING
2016-07-27 11:49:40,352 [myid:2] - INFO  [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:FileSnap@83] - Reading snapshot /var/lib/zookeeper/version-2/snapshot.3300799266
{quote}

The size of the snapshot (/var/lib/zookeeper/version-2/snapshot.3300799266) is 147 mb. Not sure if this is considered "large" for zookeeper. initLimit is set to 10, tickTime is 2000.

What else can I provide?



, [~d.freudenberger] right, 147mb isn't large. But, this indicates that the follower has timed out waiting on the leader to sync up:

{noformat}
2016-07-27 11:49:40,346 [myid:2] - WARN [QuorumPeer[myid=2]/0:0:0:0:0:0:0:0:2181:Follower@89] - Exception when following the leader
java.net.SocketTimeoutException: Read timed out
{noformat}

You may want to have a look at the prospective leader logs to see if you spot anything odd. If the log files aren't too large, then you may consider posting them here.

Also, is the snapshot you checked the latest one? do all snapshots have roughly that size? About devices, are you using a single device or a dedicated device to the txn log. , [~fpj] the referenced snapshot is the latest one and all snapshots have around the same size. The log files look pretty much all the same to me. But maybe your eyes will catch sth. that looked good to me. You can download the log files for all nodes if you like (https://s3-eu-west-1.amazonaws.com/files.rebuy-cdn.de/logs.tgz). The transaction log is written to the same device. This is sth. we can't really change in our current setup.
, Is it possible this is a clock drift problem? The logs you've provided end at 12:13:35 for node1, 12:18:31 for node 2, and 12:14:11 for node3. I can't remember if this degree of clock drift causes issues or not, [~fpj] do you recall?, [~fournc] good point, I have seen clock drift causing problems like that, essentially causing the socket to timeout on the read., It's hard to tell if this is just that the logs were grabbed at different times or if it is clock drift but I would check for clock drift.
I'm also seeing this error though:
2016-07-27 11:47:05,709 [myid:2] - WARN  [SyncThread:2:FileTxnLog@321] - fsync-ing the write ahead 
log in SyncThread:2 took 1111ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide

It's also taking over 10 seconds to read the snapshot on startup, which is not a good sign. Flavio's advice to increase the initLimit is probably good., Yeah, your init limit needs to be longer. They're not getting into quorum because it takes longer than 20s to sync. Dunno why the original node crashed but if you increase initLimit that should solve this problem., I encountered the same problem.  Is any fead back about whether increase the init limit values will work? thanks.
]