[[~ongardie]: thanks for reporting this. In the example given though:

{code}
  zk = new ZooKeeper(...)
  // The library establishes a TCP connection.
  zk.createAsync("/data-23857", "...", callback)
  // The library/kernel closes the TCP connection because it times out, and
  // the create of /data-23857 is doomed to fail with ConnectionLoss. Suppose
  // that it never reaches the server.
  // The library establishes a new TCP connection.
  zk.createSync("/pointer", "/data-23857")
  // The create of /pointer succeeds.
{code}

Callback should be called with ConnectionLossException before createSync() is send to the server, because internally all requests — sync or async — are serialized through the same queue.

It sounds like the assumption here is that a createSync() should fail if a previous createAsync() call failed? That should be left to the application, no? Internally, all replies/events are delivered in order so ordering shouldn't be broken — no?, bq. Callback should be called with ConnectionLossException before createSync() is send to the server, because internally all requests — sync or async — are serialized through the same queue. --[~rgs]

I've observed that subsequent requests do not block on the return of the callback from prior requests. In other words, I can't count on my /data-23857 callback to see and process the ConnectionLoss before my /pointer request is sent out on the new connection. (And even if I could, that programming model would be error-prone and burdensome.)

BTW, whether the second create is sync or async isn't important in the example. The problematic situation could arise even if both were createAsync.

bq. It sounds like the assumption here is that a createSync() should fail if a previous createAsync() call failed? That should be left to the application, no? Internally, all replies/events are delivered in order so ordering shouldn't be broken — no? --[~rgs]

Can you clarify?, Thanks for starting this discussion, [~ongardie].

Some of the issues you're describing, we only had the opportunity to discuss in more detail in the book. It is really not self-promotion, and it is where we had a chance to talk more about some of the subtleties we have.

Let me first acknowledge that it is possible that we submit asynchronously operations OP1 and OP2, such that OP2 is executed and OP1 isn't. This can happen in the case that the connection is lost while the client is processing OP1, the callback is set to deliver the connection loss event, a new connection is created, and OP2 is submitted concurrently. ZooKeeper in this scenario makes two important guarantees: 1) it notifies the application that there has been an error for OP1; 2) it doesn't reorder the execution of OP1 and OP2. If they both execute, it will follow the order of submission.

If you're building an application, then you have two choices with the current API. You can be optimistic and submit asynchronous operations in a tight loop. In the case you fall in the situation I describe above, then you might need to execute some recovery procedure. If as an application designer you don't want or can't execute such a procedure, then you'll have to execute one at a time.

In your first example, you're mixing the synchronous and the asynchronous API. We don't actually enforce that an application uses one or the other, but we do not recommend to mix synchronous and asynchronous calls precisely because of the reasons you're raising.

About your proposal, it is fairly extreme, so I'm -1 as is. However, there is one thing we have discussed at some point, but never really got to do, is to fail all operations upon a connection loss and only re-enable upon an explicit indication from the application, like a {{reenableOps}} call. It achieves the same effect of dropping a suffix of operations and does not require all the changes you're proposing. It should be a fairly simple change to make and it is compatible with the current API. , bq. ZooKeeper provides "FIFO client order: all requests from a given client are executed in the order that they were sent by the client."

One more constraint should be added: all requests from a given client in *the same session* are guaranteed being executed in FIFO order as they were sent. Requests across sessions are not guaranteed FIFO order. In the case of two consecutive quests with a connection loss between the two, there will be two sessions after client reconnect, thus both requests will not be guaranteed executed FIFO., i think it's a good idea to document this issue in this jira. it would be really nice to surface this to clients in a way that they both realize the problem and they have a way to deal with it.

the nice thing about it is that it is a client side issue. the server maintains its guarantees. since you are implementing your own client you can actually experiment with different ideas.

it sounds to me that getConnection() and reenableOps() are basically the same. right? or are you proposing that when you get a ZKConnection object you can invoke the zookeeper operations on that?

i think this is really only an issue for async methods, since synchronous methods execute ... synchronously, thus one at a time. i kind of like the idea of getting a object that only has async methods that you can have a strong guarantee of FIFO execution.

one problem i see with reenableOps is that it affects everything using the zookeeper handle, not just the ops in question., bq. i think it's a good idea to document this issue in this jira.

Documenting is a good suggestion, but probably better on the official or docs or at least wiki. I think that's what you're suggesting, yes?

bq. it sounds to me that getConnection() and reenableOps() are basically the same. right?

I need to step back to respond to this and say how I have interpreted the original proposal. The description says:

bq. Add a method ZooKeeper.getConnection() returning a ZKConnection object. ZKConnection would wrap a TCP connection.

My interpretation here is that in {{SendThread}}, instead of starting a connection automatically once the connection is declared lost, we need to change the logic to have the application enabling the reconnection. If we let the application do it and the application takes long, then the session might end up expiring. I'm not convinced that it is a good idea to let the application take control over the reconnection.

The different approach with {{reenableOps}} consists of leaving the automatic reconnection as we have today, but in the {{queuePacket}} method where we have this:

{code}
            if (!state.isAlive() || closing) {
                conLossPacket(packet);
            } else {
                // If the client is asking to close the session then
                // mark as closing
                if (h.getType() == OpCode.closeSession) {
                    closing = true;
                }
                outgoingQueue.add(packet);
            }
{code}

we change the predicate of the {{if}} block to evaluate to true also in the case operations haven't been re-enabled after a connection loss. We might want to have a new state that represents the client being connected but disabled and possibly a parameter that disables this behavior to provide what we have currently for compatibility. I'd say that the default behavior is what we have today, but I don't feel strongly about it.

bq. one problem i see with reenableOps is that it affects everything using the zookeeper handle, not just the ops in question.

I'm not sure what you have in mind when you refer to *everything*, [~breed]. If you're alluding to the fact that it also affects the sync api, then the configuration switch enables turning it on and off. The discussion below provides a different option, though.

bq. i kind of like the idea of getting a object that only has async methods that you can have a strong guarantee of FIFO execution.

I agree, it shouldn't be difficult to separate out so that we have a synchronous handle and an asynchronous handle. We would need to keep a {{ZooKeeper}} class that instantiates the two for backward compatibility., Thanks for the quick replies.

bq. In your first example, you're mixing the synchronous and the asynchronous API. We don't actually enforce that an application uses one or the other, but we do not recommend to mix synchronous and asynchronous calls precisely because of the reasons you're raising.

In my three-line example, it doesn't matter whether the second create is synchronous (mixing) or asynchronous (not mixing). Isn't it fine to issue a series of asynchronous calls ending with a synchronous call?

Summarizing the *{{reenableOps()}}* proposal so far, we have:
 - A new configuration option (say, "requireReenableOps") for an application to opt into calling {{reenableOps()}}. This would preserve backwards compatibility for existing clients, so the default would be to keep today's behavior.
 - Upon a connection loss, the client library would still reconnect automatically and issue pings as needed. However, with the configuration option set, it would continue to fail all requests (both synchronous and asynchronous).
 - After the application calls {{reenableOps()}}, the client library would permit new requests to use the new connection.

This would work, but in terms of API design, I can think of a couple of drawbacks:

- If we consider an application that's spread across various modules and/or thread boundaries, it has to be carefully designed with respect to {{reenableOps()}}, as this is a global property. I think this is what [~breed] was getting at with "it affects everything using the zookeeper handle". Specifically, every module/thread must be ready for a call to {{reenableOps()}} before it is safe to invoke, and until such time, the application won't make much progress.

- Most asynchronous code will need to know whether it's running under requireReenableOps=false or requireReenableOps=true. This might complicate a reusable library's internals and API, which may need to have different behavior depending on this setting.

The *{{getConnection()}}* proposal doesn't have these problems. Some modules of an application can continue issuing synchronous calls on the ZooKeeper object, others can issue (deprecated) asynchronous calls on the ZooKeeper object, and yet others can use ZKConnection objects, moving to new connections at their own pace with no application-level synchronization.

bq. it sounds to me that getConnection() and reenableOps() are basically the same. right? or are you proposing that when you get a ZKConnection object you can invoke the zookeeper operations on that?

You would invoke the operations directly on the ZKConnection object.

I do like the idea of the library automatically reconnecting for pings, and we could apply that to the {{getConnection()}} proposal too:
- The library would reconnect automatically to issue pings, as it does now.
- {{ZooKeeper.getConnection()}} would return the current ZKConnection object, which wraps the current TCP connection.
- Upon a connection loss, all subsequent operations on the same ZKConnection would return a Connection Loss error. The library would internally reconnect automatically, but it would not make this TCP connection availble via the old ZKConnection.
- Upon noticing a connection loss, the application would need to call {{ZooKeeper.getConnection()}} again to get the new ZKConnection object.

The ZKConnection object certainly needs to export the asynchronous methods that are in ZooKeeper today. I think it should also export the synchronous methods. I think these are both well-defined and useful in ZKConnection, as in my original example with {{createAsync()}} followed by {{createSync()}}.

I think the ZooKeeper class should continue to export non-deprecated synchronous methods as well. These would just be convenience wrappers that invoke {{getConnection()}} followed by the synchronous operation on that. Much ZooKeeper code is entirely synchronous, and as such, does not depend on FIFO client order. I see no reason to force churn upon that code.

I still think the asynchronous calls on the ZooKeeper class should be deprecated, since applications cannot rely on FIFO client order when using them. At a minimum, these should have a very clear warning to this effect, and applications should be encouraged to get a ZKConnection and then make asynchronous calls on that.
, thanx diego, you did express well what i was trying to say. i also like your proposal. there are probably more details to work out, like how would it look for the C api? i like how it encapsulates nicely the relation between a sequence of operations, and your example does make a compelling argument for also including the sync api.

do we have some applications that we can use to validate the api? it would be nice to validate the design before we standardize it.

what i meant by "i think it's a good idea to document this issue in this jira" is that it's good that we have this jira to discuss the problem and potential solutions., bq. In my three-line example, it doesn't matter whether the second create is synchronous (mixing) or asynchronous (not mixing). Isn't it fine to issue a series of asynchronous calls ending with a synchronous call?

Maybe, it depends on where you make the synchronous call. If you call it from a callback, then you may end up re-ordering the results. After checking it again, I don't think it is super important for this discussion. In your simple example, you're right that it doesn't matter whether the second call is sync or async.

I like the idea of not making the behavior global and configurable via switch. In particular, the configuration switch is error prone. I still think that mixing API use and different behavior, even across modules, with the same ZK handle isn't good practice, but I'm fine with leaving it up to the application to decide, though.

On {{getConnection}}, I have a few high-level points:

# I like the fact that it is all client side and it makes it clear to the application that requests are being sent over a given connection. Once the connection drops, a proper suffix of the operations will receive an error.
# I'd rather not have the sync api and the async api over different objects. The proposal has the sync api in the zookeeper handle and the async api in the connection object. I'll discuss this some more below.
# I don't like much the idea of exposing a "connection". The abstraction we have historically is a session and the fact that a session can have multiple connections over time is mostly transparent to the application, except for the connection loss event, which makes it clear that there are multiple connections going on underneath.

On the second point, we don't necessarily have to move the async api to such a connection object. We could instead have something like a token and the token essentially represents a connection. If the connection drops, then the token is invalidated. With such a token, we will be adding a new call for each operations to the handle, something like:

{code}
public void create(final String path, 
                            byte data[],
                            List<ACL> acl,
                            CreateMode createMode,
                            StringCallback cb,
                            Object ctx,
                            FIFOToken token);

{code}

and also a call to get the current token:

{code}
public FIFOToken getFifoToken() {
    return this.fifoToken;
}
{code}

Again, the main reason for suggesting this is to enable such FIFO-enforced calls in the handle API rather than having a connection object.

Another point I was thinking about with respect to the use of {{getConnection}} is the following. If I'm a developer, where should I call it and when? In principle, we can do it upon receiving a {{SyncConnected}} event via the default watcher (transition from {{Disconnected}} to {{SyncConnected}}. It might not be possible to do it at that point in the case I have one or more threads submitting operations, though. The applications will need to pause before we get the new connection, otherwise we can have a mix of threads sending to the invalid connection while others send to the new connection. In the case it is ok that the FIFO order guarantee is per thread, we can have each thread individually calling {{getConnection}} (or whatever other variant we end up agreeing upon) upon receiving a connection loss error. 

The last point of the list is important because we have proposed quite some time back to remove connection loss by resending upon reconnecting in ZOOKEEPER-22. If we resend, then we won't be erroring out requests with a connection loss event upon dropping a connection. If we do it, then we avoid the gaps that we are discussing in this issue in the first place. ZOOKEEPER-22 needs work on the server-side, though. On the positive side, the client-side API changes are minimal. 

, bq. I don't like much the idea of exposing a "connection". The abstraction we have historically is a session and the fact that a session can have multiple connections over time is mostly transparent to the application, except for the connection loss event, which makes it clear that there are multiple connections going on underneath. --[~fpj]

Nobody wants ZOOKEEPER-22 more than I do ;). It would solve this issue and make correct ZooKeeper usage so much simpler. But until ZOOKEEPER-22 is done, the concept of a connection is not hidden from applications, and exposing connections may be the most straightforward and honest thing to do. Even with the FIFO token, the application would need to know to grab a new FIFO token after seeing a connection loss event, so FIFO tokens can't be a fully separate concept from connections.

Otherwise, the FIFO token vs {{getConnection()}} are equivalent in terms of their expressiveness, so I see the choice as a question of aesthetics.


bq. Another point I was thinking about with respect to the use of getConnection is the following. If I'm a developer, where should I call it and when? In principle, we can do it upon receiving a SyncConnected event via the default watcher (transition from Disconnected to SyncConnected. It might not be possible to do it at that point in the case I have one or more threads submitting operations, though. The applications will need to pause before we get the new connection, otherwise we can have a mix of threads sending to the invalid connection while others send to the new connection. In the case it is ok that the FIFO order guarantee is per thread, we can have each thread individually calling getConnection (or whatever other variant we end up agreeing upon) upon receiving a connection loss error. --[~fpj]

I'd argue strongly for the latter approach of using connections/FIFO tokens only locally. Applications should grab a connection/FIFO token when they're doing asynchronous work, then forget it and grab another one later when needed.

If someone did want the FIFO client order guarantee across threads, they might be insane, but they can share their connection/FIFO token across threads. I don't think this will be common usage.

Taking the example from page 4 of the paper, here's some possible code:

{code}
public void newLeader(ZooKeeper zk, ArrayList<string> config) {
    while (true) {
        ZKConnection conn = zk.getConnection()
        // "The new leader makes the configuration change by deleting ready, ..."
        conn.deleteAsync("/ready", callback=logStuff)
        // "...updating the various configuration znodes, ..."
        for (int i = 0; i < config.size(); i++) {
            conn.createAsync("/config-" + i, config.get(i), EPHEMERAL, callback=logStuff)
        }
        // "...and creating ready."
        try {
            conn.createSync("/ready", "", EPHEMERAL)
        } except (ConnectionLoss) {
            continue;
        }
        return;
    }
}
{code}

Note that this code is grabbing a connection (FIFO token) before starting its pipeline, using it locally, then dropping it. In case of a connection loss, it gets another one.

This could be more sophisticated by keeping track of which operations had previously succeeded in the callbacks, but it shouldn't be necessary.

We could also sprinkle in:
{code}
        if (conn.isLost()) {
            continue;
        }
{code}
as an optimization/pessimization, but again, it shouldn't be necessary.

\\
----
\\

bq. do we have some applications that we can use to validate the api? it would be nice to validate the design before we standardize it. --[~breed]

Good point. I think we'd want well-intentioned applications that make async calls, maybe some that care about FIFO order and some that don't. Any ideas?


There are a couple more alternatives I want to raise for discussion:

One possible stance would be: _Well, this adds too much complexity, so you can't have FIFO client order until ZOOKEEPER-22 is done_, WONTFIX.

Another more extreme stance would be to eliminate the FIFO client order guarantee altogether. A big question I've been wondering about is whether FIFO client order is useful in real world applications, and whether it's worth the implementation trouble.

FIFO client order provides two potential benefits:
 - Leveraging FIFO client order may improve application performance. It does not add significant runtime cost in general, yet it can reduce round-trip stalls for applications that leverage it.
 - FIFO client order may simplify reasoning about applications by reducing the possible state space that they operate in.

On the other hand:

 - FIFO client order adds complexity to the implementation. It has to be guaranteed at every level of the stack. This issue shows how it was lost in the highest levels of the client API. Even if we fixed this issue, Curator or other wrappers might still throw away FIFO client order at even higher levels. And at lower levels, though the current implementation maintains FIFO client order, this has widespread implications restricting its design, including its consensus algorithm, its threading model, and its choice of transport protocol.
 - Relatedly, solving ZOOKEEPER-22 would be simpler by eliminating the FIFO client order guarantee, as the library could retry operations in any order (both reads and writes).
 - How many applications rely on FIFO client order? Users didn't seem to notice this bug. Does that imply they're doing just fine without FIFO client order, or are the conditions for this bug so rare that it did not come up in production or testing?
 - Using FIFO client order correctly seems difficult. My above sample code, for instance, looks simple but is quite subtle and fragile. It assumes the asynchronous operations will succeed as long as the connection is not lost (existing znodes in the way? ACLs? quotas after ZOOKEEPER-451?), and it only avoids leaving garbage behind on client crashes by using ephemeral nodes. Compare to a version that issues all the createAsync requests, blocks on their successful completion (possibly recovering from errors in nontrivial ways), then creates the /ready node. I think it'd be easier to see why that version works, at the runtime cost of an extra round trip delay.

I know this is only tangentially related to this issue, but I'm really interested in hearing your thoughts., bq. Some of the issues you're describing, we only had the opportunity to discuss in more detail in the book. It is really not self-promotion, and it is where we had a chance to talk more about some of the subtleties we have. --[~fpj]

Indeed, page 116-117 of the ZooKeeper: Distributed Process Coordination book mentions this issue. It suggests:

bq. If Op2 depends on Op1, ... wait for a successful execution of Op1 before submitting Op2...this approach is safe, but it adds a performance penalty..., Although unrelated to the API semantic discussion above, you could use {{multi()}} to have multiple updates to be performed atomically. That would be as good as the async op followed by the sync op, in that you only have to wait for 1 operation to be synced/complete.

Eg: 
{code:java}
List<Op> operations = new ArrayList<>();
operations.add(Op.create("/data-23857", "..."));
operations.add(Op.create("/pointer", "/data-23857"));
zk.multi(operations);
{code}, multi will handle some of the use cases, but a simple one that it doesn't handle is if you want to implement swap:

zk.getData(znode, ...)
zk.setData(znode, ...)

you can't do that with multi (and i don't think we should extend multi to do it :)

mutli also doesn't handle the case when you are updating lots of data and would go over max packet size.

, i would love to see ZOOKEEPER-22 fixed, but i don't think it will be fixed anytime soon. (it would be awesome to be surprised though :)

@diego perhaps you could implement your idea in your go client implementation and propose it again if it works out well? i like the getConnection proposal., I think it would be a lot less disruptive for applications to fix ZOOKEEPER-22. Why can't we simply jointly work on getting ZOOKEEPER-22 fixed? ]