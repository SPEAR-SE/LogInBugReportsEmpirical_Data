[Rakesh, would your patch attached to ZOOKEEPER-1100 fix this problem?  I think it would, I just want to make sure . . ., If your client throws an OOM error, there's no guarantee that you will be able to do anything at all beyond that point. It's not clear to me what you hope to do about it. What are the users going to do when they can't act themselves due to the OOM state?, I'll preface this by saying that I know nothing about this code, and probably have it all wrong.

Here's what I saw in production at a customer using ZK 3.3.3.  Because we are terrible people, we embed Zookeeper into a JVM running a bunch of other code.  One server in a three-node cluster used up all of its memory (due to non-ZK code).  The JVM did not crash, it just stayed up, still listening on its Zookeeper ports.  The server kept sitting there spawning new threads to handle incoming connections, but it never closed those connections.  Incoming client connections (from C clients running elsewhere) timed out, and so they would try to re-connect to other servers -- that's fine.  The problem, from what I could tell, was that incoming server connections just seemed to freeze.  The two remaining Zookeeper servers in the cluster were unable to make any progress because they were trying to connect to the third server who never responded to them.  So even though there were two servers that were perfectly fine, they became stuck too and could never form a new majority, and so all of Zookeeper was down until we manually restarted the server that had run out of memory.

Clearly, there are things we need to do in our architecture to avoid this problem, but I was hoping to apply a quick fix to ZK in the meantime to help out -- the most important thing would be to properly close the connection if you're not able to service it.  It's also possible the problem I describe above actually isn't in ClientCnxn.java, but some other server connection file (NIOServerCnxn?), A server ran out of memory? This ticket is for the client code, not the server code. More likely NIOServerCnxn than ClientCnxn as you mention.
 
OOM stuff can cause VMs to behave very strangely, which is why I generally think it's best to fail big and fail fast when it happens. There's not really any sense in trying to "recover" because beyond that point the behavior is pretty non-deterministic. Strange that the other VMs wouldn't form a quorum though... might be interesting to dig into. Feel free to open another ticket with some more info and we can dig into it more., Yes, a server, sorry.  I wrongly assumed ClientCnxn was used on the server side to receive connections from clients.  I am going to try to repro in house and will open a ticket if I can make it happen.  Thanks, and sorry for the confusion., @Camille
Oh Yes! its client side and the user itself is not able to act in case of OOME.

When I raised, I was thinking about the pre-creation of the WatchedEvent() to avoid OOM once again in the catch block, so the user would be getting the 'Disconnected' event., As rakesh told [here|https://issues.apache.org/jira/browse/ZOOKEEPER-1100?focusedCommentId=13234068&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13234068], throwable was handled in catch block, but there is a chance of getting OOME while creating event. we got this specific OOME and SendThread exited and clients hanged.
{noformat}2013-08-25 23:02:41,206 - ERROR [main-SendThread(linux-136:25500):ClientCnxn$1@411] - from main-SendThread(linux-136:25500)
java.lang.OutOfMemoryError: Java heap space
        at java.util.HashMap.resize(HashMap.java:462)
        at java.util.HashMap.addEntry(HashMap.java:755)
        at java.util.HashMap.put(HashMap.java:385)
        at java.util.HashSet.add(HashSet.java:200)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
        at org.apache.zookeeper.ZooKeeper$ZKWatchManager.materialize(ZooKeeper.java:183)
        at org.apache.zookeeper.ClientCnxn$EventThread.queueEvent(ClientCnxn.java:463)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1123){noformat}


So, we need better handling of OOME. 

Any thoughts?, In ACCUMULO-1708, I am trying to work out a way to make Accumulo server processes die if any thread throws an Error.  Accumulo uses Zookeeper and HDFS.  One problem I have run into is that zookeeper and HDFS create threads that could possibly throw OOME.  I thought I was onto something with ThreadGroup.uncaughtException(), but since zookeeper and hdfs threads catch Throwable its a dead end.  If interested, I attached an example called ThreadTest.java to ACCUMULO-1708 that shows an experiment trying to use a thread group.

If zookeeper client side threads always rethrew Errors, then this would ideal for my purposes.   Zookeeper code could still try to take some action in catch.   I suppose this might look like the following.


{code:java}
try{
    //...
} catch (Throwable e)
{
   try{
     //..
     cleanup();
     if(state.isAlive()){
        eventThread.queueEvent(
        new WatchedEvent(Event.EventType.None, Event.KeeperState.Disconnected, null) )
     }
     //....
   }catch (Throwable e) {
     //failure while trying to process failure
     e.printStackTrace();
   }finally{
      throw e;
   }
}
{code}
, The finally in the java code in my prev comment should probably be the following. Also the the second catch should use a different variable name for the exception.

{code:java}
   }finally{
      if(e instanceof Error)
        throw (Error)e;
      //the other likely type is probably a RuntimeException, should this be rethrown?
   }
{code}]