[I found a JIRA that's probably related, liked it above. Also checked Java client, I don't think the problem occurs there., [~fpj] Zookeeper_simpleSystem::testFirstServerDown seems not related to reconfig test - it is only called in TestClient.cc. Are you referring to another test failure in TestReconfigServer.cc? Is it possible that you can post the failed log so I can try to reproduce the specific failed test on my side (my c client tests of 3.5.2 alpha consistently passed for me today.)., [~shralex] Good catch, it is exactly the same problem. The description about a list of two servers, but it is an issue in general that we skip one server of the list every time.

[~hanm] The test case isn't related to reconfiguration, that's correct. However, zh->reconfig is set to 1 initially according to the logic we have implemented. That's what I observed while tracing the execution. The fact that it is set to 1 initially actually changes the lists we are getting the server addresses from (there are _old and _new lists in the handle).

There isn't much in the output, but here is a sample:

{noformat}
2016-07-05 18:35:50,174:42240:ZOO_INFO@log_env@1027: Client environment:zookeeper.version=zookeeper C client 3.5.2
2016-07-05 18:35:50,174:42240:ZOO_INFO@log_env@1031: Client environment:host.name=fpj-test-apache-01
2016-07-05 18:35:50,174:42240:ZOO_INFO@log_env@1038: Client environment:os.name=Linux
2016-07-05 18:35:50,174:42240:ZOO_INFO@log_env@1039: Client environment:os.arch=4.4.0-28-generic
2016-07-05 18:35:50,174:42240:ZOO_INFO@log_env@1040: Client environment:os.version=#47-Ubuntu SMP Fri Jun 24 10:09:13 UTC 2016
2016-07-05 18:35:50,174:42240:ZOO_INFO@log_env@1048: Client environment:user.name=fpj
2016-07-05 18:35:50,174:42240:ZOO_INFO@log_env@1056: Client environment:user.home=/root
2016-07-05 18:35:50,174:42240:ZOO_INFO@log_env@1068: Client environment:user.dir=/home/fpj/code/zookeeper-3.5.2-alpha/src/c
2016-07-05 18:35:50,174:42240:ZOO_INFO@zookeeper_init_internal@1111: Initiating client connection, host=127.0.0.1:22182,127.0.0.1:22181 sessionTimeout=10000 watcher=0x447050 sessionId=0 sessionPasswd=<null> context=0x7ffcc708fec0 flags=0
2016-07-05 18:35:51,174:42240:ZOO_WARN@get_next_server_in_reconfig@1256: [OLD] count=0 capacity=0 next=0 hasnext=0
2016-07-05 18:35:51,174:42240:ZOO_WARN@get_next_server_in_reconfig@1259: [NEW] count=2 capacity=16 next=0 hasnext=1
2016-07-05 18:35:51,175:42240:ZOO_WARN@get_next_server_in_reconfig@1268: Using next from NEW=127.0.0.1:22182
2016-07-05 18:35:51,175:42240:ZOO_ERROR@handle_socket_error_msg@2353: Socket [127.0.0.1:22182] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-07-05 18:35:51,175:42240:ZOO_WARN@get_next_server_in_reconfig@1256: [OLD] count=0 capacity=0 next=0 hasnext=0
2016-07-05 18:35:51,175:42240:ZOO_WARN@get_next_server_in_reconfig@1259: [NEW] count=2 capacity=16 next=1 hasnext=1
2016-07-05 18:35:51,175:42240:ZOO_WARN@get_next_server_in_reconfig@1268: Using next from NEW=127.0.0.1:22181
2016-07-05 18:35:51,175:42240:ZOO_ERROR@handle_socket_error_msg@2353: Socket [127.0.0.1:22181] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-07-05 18:35:51,175:42240:ZOO_WARN@get_next_server_in_reconfig@1256: [OLD] count=0 capacity=0 next=0 hasnext=0
2016-07-05 18:35:51,175:42240:ZOO_WARN@get_next_server_in_reconfig@1259: [NEW] count=2 capacity=16 next=2 hasnext=0
2016-07-05 18:35:51,175:42240:ZOO_WARN@get_next_server_in_reconfig@1279: Failed to find either new or old
2016-07-05 18:35:51,175:42240:ZOO_ERROR@handle_socket_error_msg@2353: Socket [127.0.0.1:22182] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-07-05 18:35:51,175:42240:ZOO_ERROR@handle_socket_error_msg@2353: Socket [127.0.0.1:22182] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-07-05 18:35:51,176:42240:ZOO_ERROR@handle_socket_error_msg@2353: Socket [127.0.0.1:22182] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-07-05 18:35:51,176:42240:ZOO_ERROR@handle_socket_error_msg@2353: Socket [127.0.0.1:22182] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
<This line keeps repeating>
{noformat}

No server seems to be up for the client to connect, which I don't understand the reason, but I've focused mostly on why the address is the same after some point rather than alternating between the two addresses., [~fpj] Just to confirm that I am seeing the same failures of TestClient.cc in the VM your provided consistently. Not sure why same tests passed on my own cluster consistently. Will report back after I dig a little more into this., [~shralex] [~hanm]

bq. From my brief digging, my feeling was that the java way of doing it was better: statichostprovider is the only one that increments pointers and
gives out addresses and the caller doesn't do any of this... But this may
be too much of a change for C.

I think that if we make the change in ZOOKEEPER-1856, we will have the calls to addrvec_next concentrated in zoo_cycle_next_server. The only exception is the processing of read-only state in zookeeper_interest:

{noformat}
if (zh->state == ZOO_READONLY_STATE) {
            int idle_ping_rw = calculate_interval(&zh->last_ping_rw, &now);
            if (idle_ping_rw >= zh->ping_rw_timeout) {
                zh->last_ping_rw = now;
                idle_ping_rw = 0;
                zh->ping_rw_timeout = min(zh->ping_rw_timeout * 2,
                                          MAX_RW_TIMEOUT);
                if (ping_rw_server(zh)) {
                    struct sockaddr_storage addr;
                    addrvec_peek(&zh->addrs, &addr);
                    zh->ping_rw_timeout = MIN_RW_TIMEOUT;
                    LOG_INFO(LOGCALLBACK(zh),
                             "r/w server found at %s",
                             format_endpoint_info(&addr));
                    handle_error(zh, ZRWSERVERFOUND);
                } else {
                    addrvec_next(&zh->addrs, NULL);
                }
            }
            send_to = min(send_to, zh->ping_rw_timeout - idle_ping_rw);
        }
{noformat}

Here we call handle_error, but I'm thinking that we don't really want to skip to the next server, so it is ok that we don't call addrvec_next in handle_error. If it is, then we can simply call zoo_cycle here. The call to addrvec_next is advancing the pointer but not changing zh->addr_cur, so we can't call replace it with zoo_cycle. I need to think a bit more about how to remove that addrvec_next call., I am thinking we can do this:
# Remove addrvec_next in handle_error. Make it clear that handle_error only does clean up / logging work. If we need to try another server, we need explicitly call zoo_cycle_server or addrvec_next. 

# For read only state processing, I am thinking we could do this (after addrvec_next is removed from handle_error), because if we are here then the current server is not usable anyway (either it is a dead server or a read-write server, and we need a read only server if I understand this logic correctly), so we'd move on and try another server.
{code}
                if (ping_rw_server(zh)) {
                    struct sockaddr_storage addr;
                    addrvec_peek(&zh->addrs, &addr);
                    zh->ping_rw_timeout = MIN_RW_TIMEOUT;
                    LOG_INFO(LOGCALLBACK(zh),
                             "r/w server found at %s",
                             format_endpoint_info(&addr));
                    handle_error(zh, ZRWSERVERFOUND);
                } 
                addrvec_next(&zh->addrs, NULL);
{code}, bq. if we are here then the current server is not usable anyway (either it is a dead server or a read-write server, and we need a read only server if I understand this logic correctly), so we'd move on and try another server

If the client is in read-only mode and it finds a read-write server, then it should prefer the read-write server. That's one thing that is confusing me here, if it is finds a read-write server, then it should stick to it and transition away from read-only., I see, I got the logic wrong - just checked Java code and it does same thing (prefer RW server to read only server) by throwing an exception.

bq. That's one thing that is confusing me here, if it is finds a read-write server, then it should stick to it and transition away from read-only.

I think the C code does this - if ping_rw_server(zh) != 0 then we find a rw server and the server address can be retrieved through addrvec_next(&zh->addrs, &zh->addr_cur), because the ping_rw_server uses the addrvec_peek to try next address. What handle_error does is to transition away from the current server (read-only) to the new read-write server, by cleaning up the current state (as if there is an error) and shift the next address to try to the next address, which is validated by ping_rw_server(zh) to be a read-write server. 

So I guess we can still remove addrvec_next in handle_error and explicitly call addrvec_next:
{code}
 if (ping_rw_server(zh)) {
                    struct sockaddr_storage addr;
                    addrvec_peek(&zh->addrs, &addr);
                    zh->ping_rw_timeout = MIN_RW_TIMEOUT;
                    LOG_INFO(LOGCALLBACK(zh),
                             "r/w server found at %s",
                             format_endpoint_info(&addr));
                    handle_error(zh, ZRWSERVERFOUND);
                    addrvec_next(&zh->addrs, &zh->addr_cur);
                } else {
                    addrvec_next(&zh->addrs, NULL);
                }
{code}
, [~hanm] it sounds right, but I liked [~shralex] observation and ideally we should advance the pointer of the server list in a single place. If we can remove the calls to {{addrvec_next}} here and only call {{zoo_cycle_next_server}}, then I believe we would pretty much have it. I think we can replace the first call to {{addrvec_next}} in the RO block you shared with {{zoo_cycle_next_server}}, but not the second if I'm reading this correctly., bq. we should advance the pointer of the server list in a single place. 
Agree.

bq. I think we can replace the first call to addrvec_next in the RO block you shared with zoo_cycle_next_server
zoo_cycle_next_server does more than addrvec_next - and in some cases the addrvec_next might not get called if zoo_cycle_next_server returns earlier within the reconfig logic and in this case the current server addr is reset to NULL. So it seems to me that addrvec_next and zoo_cycle_next_server is not semantically equivalent. 

bq. but not the second if I'm reading this correctly.
Yeah, I think the addrvec_next(&zh->addrs, NULL) just change the state of the zh-addrs vector without changing the state of the current server in zh handle - zoo_cycle_next_server will change the state of the current server, so both are not semantically equivalent.

, {quote}
zoo_cycle_next_server does more than addrvec_next - and in some cases the addrvec_next might not get called if zoo_cycle_next_server returns earlier within the reconfig logic and in this case the current server addr is reset to NULL. So it seems to me that addrvec_next and zoo_cycle_next_server is not semantically equivalent.
{quote}

True, they aren't semantically equivalent, but that actually could be a bug. What if the client is in RO mode and reconfig is enabled? Shouldn't it try follow the same process in zoo_cycle_next_server to find an RW server?

{quote}
Yeah, I think the addrvec_next(&zh->addrs, NULL) just change the state of the zh-addrs vector without changing the state of the current server in zh handle - zoo_cycle_next_server will change the state of the current server, so both are not semantically equivalent.
{quote}

We could either leave as is or have a separate function to advance that only takes one parameter. The latter sounds a bit unnecessary. Another option is to have a second parameter for zoo_cycle_next_server so that we update according to the second parameter rather than always updating zh->addr_cur. Does it work?, bq. but that actually could be a bug. 
Good point.

bq. have a second parameter for zoo_cycle_next_server so that we update according to the second parameter rather than always updating zh->addr_cur.
I like this solution which parameterizes zoo_cycle_next_server such that what state it's changing is explicit in the interface, and use zoo_cycle_next_server instead of the low level addrvec function because we could also be in reconfig mode when this 'else' branch is executed., [~hanm] thanks for the feedback. are you interested in providing the patch for this issue or should I go ahead and work on one? I'm happy either way., [~fpj] I am working on a patch, will attach the patch in a day or two., Attach a patch. While doing this I think keep zoo_cycle_next_server its current signature is simpler implementation wise, otherwise we'll change a public API and also need modify a couple of other places in code (like get_next_sever_in_reconfig). It also fixes an issue that previously addrvec_next was not properly protected if we are in reconfig mode. If we are, we'll just let existing reconfig logic handling server shifts and then retry when reconfig is finished., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12818439/ZOOKEEPER-2466.patch
  against trunk revision 1750739.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3275//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3275//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3275//console

This message is automatically generated., [~hanm] as you explained, there is a behavior change since with RO status the client won't look for an RW server until the reconfig completes. Say that it disconnects from the RO server, though, maybe because the RO server has been removed from the configuration. What happens in this case? I'd expect that either it looks for another RO server or it waits until the reconfig completes, although the former is more aligned with the behavior we describe in the documentation.

Also, I was thinking if we need a test case for this. We found out about this problem through a test case, but apparently it doesn't always fails. It might be worth checking that the order of servers is as expected. , [~fpj] 
bq. What happens in this case?

What will happen depends on the state of the new server that the client connects to after reconfig. 
* If the new server is a read only server (does not matter if this is the same read-only server the client previously connects to, or a new read-only server client reconnects to, after load balancing), then client will continue seeking a RW server, after reconfig is finished. 
* If the new server is a read-write server, then we are done.

Specifically, for the case where the current RO server is taken out during reconfig, the error handling logic will take care of retry connect to another server, so we will finally end up at the previous cases I just listed. Does this make sense to you?

bq. I was thinking if we need a test case for this.
I agree - actually I think this patch fix a bug where we could change state of zh->addr_cur during reconfig, without protection, so potential data races leading to undefined behavior. Would be good to have a test case cover this. Java client might have a similar issue (because RO was introduced before Reconfig feature.).
For existing test case I'll double check and see if we can add / improve it to have a deterministically failing case cover this scenario. I'll work on adding both cases.


, [~hanm] from your latest message, I understand that this patch still needs some more work wrt:
# Test case
# Java client

I'm canceling the patch for now., Yes those are on my list - I'll submit a new one next week. , Hi [~fpj] a late update on this one:
Designed a test case to always make the bug reveal itself - the reason why we did not see the bug happen deterministically is because of the probabilistic nature of {code}get_next_server_in_reconfig{code} where it might return a working server, or not. The bug can be deterministically reproduced if we taking the probability out of equation by always making {code}get_next_server_in_reconfig{code} return none zero, and this can be achieved if all servers are down. So, the updated test case first make sure all servers are down and zk client can't get connected; then it started the server and verify client can connect. 

Tested with and without the patched change in zookeeper.c: without the change the new test always fail and with the change the new test passes my stress test of 300 runs., Also [~fpj] I'd like to tackle the work related to Java client in a separate JIRA as this JIRA is specifically to C client and concern on Java client is more about reconfig + read-only other than the issue described here. What do you think?, +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12822864/ZOOKEEPER-2466.patch
  against trunk revision 1755379.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3342//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3342//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3342//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12822864/ZOOKEEPER-2466.patch
  against trunk revision 2fa315b7d0ed65828479fcdcc9e76ca8552fba4a.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3681//console

This message is automatically generated.]