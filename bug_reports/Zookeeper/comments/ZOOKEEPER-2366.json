[[~timothyjward], thank you for the bug report.  Let's try to target a fix for 3.5.2, which is the next release., This test case highlights the issue outlined in the bug, and shows that it affects both the Netty and Nio connection providers., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12787954/zookeeper.patch
  against trunk revision 1729259.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3047//console

This message is automatically generated., [~timothyjward]: thanks for the writing the test case! Mind generating the patch with something like:

{code}
git diff --no-prefix HEAD~1.. > ZOOKEEPER-2366.patch
{code}

so that it applies cleanly and CI can run.. Thanks!, I updated the patch to apply cleanly - ZOOKEEPER-2366.patch, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12791407/ZOOKEEPER-2366.patch
  against trunk revision 1733525.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3082//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3082//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3082//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12791407/ZOOKEEPER-2366.patch
  against trunk revision 1743978.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3163//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3163//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3163//console

This message is automatically generated., I think there is a confusion here. The patch that [~timothyjward] proposed is a test case to illustrate the problem, it is not a fix. We need to produce a fix and use the test case to verify., Attaching a patch for this issue. I'm a bit puzzled by the error handling for the reconfigure method. It sounds like it is swallowing exceptions and let the caller continue normally. In this patch, I have made changes to propagate the exception, and I think this is fine, but I'd appreciate if someone else could have a look., +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12805471/ZOOKEEPER-2366.patch
  against trunk revision 1743978.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3168//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3168//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3168//console

This message is automatically generated., Hello [~fpj].  Thank you for picking up this patch.  I think the approach of propagating the exceptions looks like the right direction.  Here are a few comments.

{code}
            try {
                acceptThread.join();
            } catch (InterruptedException e) {
                LOG.error("Error joining old acceptThread when reconfiguring client port " + e.getMessage());
            }
{code}

This wasn't introduced by your patch, but it's generally a bad practice to catch an {{InterruptedException}} and neither propagate it nor re-raise the interrupted status.  Other layers of code might be expecting to react to thread interruption.  Since we're touching this code, do you think we should add a call to {{Thread.currentThread().interrupt()}}?

{code}
            LOG.error("Error reconfiguring client port to " + addr + " " + e.getMessage());
{code}

This is also not a problem introduced by your patch, but can we pass along the {{e}} as the second argument to log the full stack trace for better diagnosis?

{code}
       try {
           LOG.info("binding to port " + addr);
           parentChannel = bootstrap.bind(addr);
           localAddress = addr;
       } catch (Exception e) {
           LOG.error("Error while reconfiguring", e);
           throw new IOException(e.getMessage());
       } finally {
           oldChannel.close();
       }
{code}

If there is an exception thrown from the try body, and then {{oldChannel.close()}} also throws an exception, then the caller will receive the second exception.  Generally, the first exception is probably more interesting for root cause information, so I think it's preferrable to catch and log the exception in the finally block, or if you prefer, only throw from the finally block if the whole try block was successful.

{code}
            try { 
                self.processReconfig(newQV, designatedLeader, zk.getZxid(), true);
            } catch (IOException e) {
                return false;
            }
{code}

Does this exception need to be logged at this layer, or do all exceptions already get logged somewhere within the {{QuorumPeer#processReconfig}} call?

I think the test works for covering this change, but I see some incorrect indentation.  Would you please clean that up?, Thanks for the patch. A few comments:
- In case the new port is taken, perhaps its better to leave the old port in effect ? (rather than to remain without a client port at all like now, which is expected in the new test)
- Error message in Netty method is less informative. Consider making messages consistent in both methods.
- I'm not sure simply returning false in  Leader.java is sufficient. That operation may forever remain in the queue ? I'm not sure about this since the
new test somehow works.
- Indentation seems a bit weird in the changes in QuorumPeer and in the test.
, looks like the operation is removed from outstanding beforehand, so that's not a problem, but I'm still
concerned about this for two reasons: its commit isn't being sent out, so followers will have a hole in the sequence,
and, if you see how tryToCommit is used, you'll see that there may be operations queued behind the reconfig that already have acks and by returning
false you're preventing those from being committed., sorry, one more thing: quorumpeer.processreconfig is used not only by Leader.java but also in other places (follower, observer, learner etc). In fact the new test changes the follower's port, so I suspect that it doesn't even exercise the new code in Leader.java! , This is probably why errors were silent - introducing a proper failure flow for operations during commit might have been too complicated. For example, you must change the port to the blocked one in the config since a quorum accepted the op (unlike what I suggested previously before thinking more about it - you can't retain the old one open, this is exactly this bug).  For simplicity I'd consider not throwing exceptions, logging as before and just fixing the leak. But perhaps there's a better solution., bq. In case the new port is taken, perhaps its better to leave the old port in effect ? (rather than to remain without a client port at all like now, which is expected in the new test)

Maybe so, my concern is that processReconfig-> reconfigure is called upon processing commit and activate and in the case there is a problem binding to the port, the current code will simply return successfully from reconfigure like if the processing of the message had succeeded, which it hasn't. The behavior sounds wrong to me, does it sound right to you?

bq. Error message in Netty method is less informative. Consider making messages consistent in both methods.

Sure, I can change it.

bq. I'm not sure simply returning false in Leader.java is sufficient. That operation may forever remain in the queue ? I'm not sure about this since the
new test somehow works.

Sounds like you have some more insight further down.

bq. Indentation seems a bit weird in the changes in QuorumPeer and in the test.

Ok, will fix, I think I see the concern, but it doesn't sound right either to not fail the reconfig operation in the case the processing of the operation doesn't succeed. That's actually the reason why I was concerned about this error handling. It sounds like either the tests aren't capturing reconfig failure scenarios properly or there is another code path to recover from these errors during the execution of the reconfiguration. There is something here that I'm missing., Sure, I noticed that it is called in all those places. When you say that the Leader code isn't being executed, what's your concern exactly? From what I can tell, if there is an error in NIOServerCnxnFactory.reconfigure(), the exception will propagate all the way to lead() and lead() will propagate up forcing the leader to resign, which I think it is fine if the server isn't behaving correctly., I need to convince myself that we will get the correct behavior if we keep swallowing the exception. Right now my sense is that this isn't right, which led me to propose this patch, but I was actually counting on having to tweak it more because it feels like it needs some thinking. There is no rush, let's do this right., Thanks for the comments, [~cnauroth]. Let's agree first on the right way to fix this and then I'll make sure these are addressed., let me try to clarify:

- as you know, in ZK operations fail only (a) at the leader and (b) during pre-processing. Here the situation is different - (a) the reconfig could be trying to alter a follower's port, which may be taken (the leader has no way to know that), and (b) this is detected during commit time when the change is attempted. At that point, the operation is committed for all practical purposes (quorum already accepted), there is no way to abort it. What can we do without redesigning ZK ? my suggestion is to keep logging an error and not propagate it up (because you can't do anything about it anyway) but of course to close the port, which is what the bug was opened for and what the patch does. The alternative is perhaps to bring down that server because its state can't reflect a committed operation, but I think its an overkill :)

- Why I said that the new "return false" in Leader.java isn't exercised by the test: because the test scenario is trying to change a follower's port, so processReconfig fails to change it at the follower, the leader will execute processReconfig just fine, since all it does is update the config in memory.

- returning false in Leader.java is wrong. My comments can probably be better there, but false has a different meaning - it indicates that there are not enough acks. Operations do not fail at the commit stage. 

- The test can be improved a bit to also try to reconfig the leader's client port and see what happens when its taken. Right now it only checks for follower. Its possible to do that without adding too much code if you do the flow in a loop - once for leader and once for follower. I think its also ok without this extension though.

, That's good insight.

bq. At that point, the operation is committed for all practical purposes (quorum already accepted), there is no way to abort it. What can we do without redesigning ZK ?

I'd say that this part is fine, I'm ok with letting it commit. I'm less ok with the server that is supposed to change the port failing to do it and keep operating regularly. For example, if we are changing the port of a follower and the port change fails, then I'd say that the follower should stop following, and a bit more drastically perhaps even exit.

bq. The alternative is perhaps to bring down that server because its state can't reflect a committed operation, but I think its an overkill.

I actually think this is a problem. If the user changes something on the server, then the user expects to either see the change reflected or an immediate indication of an error. Granted that we are printing an error message, but I think we can do better than that to call the admin attention to the problem right away.

bq. the test scenario is trying to change a follower's port, so processReconfig fails to change it at the follower, the leader will execute processReconfig just fine, since all it does is update the config in memory. Returning false in Leader.java is wrong. My comments can probably be better there, but false has a different meaning - it indicates that there are not enough acks. Operations do not fail at the commit stage.

Got it, and I don't claim that bit is correct, I'll have a closer look at the Leader code.

bq. The test can be improved a bit to also try to reconfig the leader's client port and see what happens when its taken. Right now it only checks for follower. Its possible to do that without adding too much code if you do the flow in a loop - once for leader and once for follower. I think its also ok without this extension though.

Ok.
 , Sure. But keep in mind that many clients could be connected to the follower, so shutting down may be disruptive.
And more importantly, if this is the leader we're talking about (failed attempt to change leader's client port), then shutting it down
will bring the whole system down, and it may not recover since it is very likely to keep trying to become the leader and fail
due to the taken client port. So unless there's some way to raise an alert, maybe we should just document in the manual 
that when changing the client port of a server the admin should always verify that it worked the port by opening CLI or similar.

BTW, I briefly looked what happens when other ports are changed: for leading port change, the leader will resign, and for quorum port the leader election is restarted., Let's see if we can agree on the behavior. Without reconfiguration, if there is an IOException while starting the cnxn factory, then the server will exit. You're saying that upon a reconfiguration of the client port, the situation is different because the server might already have a number of clients connected to it. But, if we are to close the old port regardless of whether binding to the new port succeeded, then we'll be losing those clients connections in any case. I get the point of the server being re-elected and failing every time, we do want to avoid that scenario, so either we log and proceed or we exit the server. 

, I don't think we're loosing the connected clients since the port is only used to accept connections, but then another random port is used to actually communicate. I'm not 100% sure, but if I remember correctly this is how it works. , Yeah I also say this in the manual, so I must have checked it "When the client port of a server is modified, it does not drop existing client connections. New connections to the server will have to use the new client port.", [~shralex], thank you for your comments.  I missed a lot of subtlety in the error handling here when I reviewed.

bq. At that point, the operation is committed for all practical purposes (quorum already accepted), there is no way to abort it. What can we do without redesigning ZK ?

Thinking out loud, so I'm probably missing something, but can we do something like bind to the new port during the proposal, but still keep the old port listening too?  Then, during commit, we'd transition the accept thread to the new already-bound port and close the old one.  The effect I'm trying to achieve is moving the bind failure to proposal time, so it won't get ack'd as successful, and therefore the quorum won't accept it.

Of course, this is a much heavier change and maybe strays towards "redesigning ZK".

bq. so either we log and proceed or we exit the server.

Thinking about the operational model, I could see "exit the server" easily escalating to "exit the ensemble."  If the admin accidentally reconfigs to a commonly bound port, such as 8080 for an HTTP server, then there is a strong possibility that all servers in the ensemble would hit the bind failure and exit., If we pursued something like this, then we'd open up another edge case to worry about: a port getting bound on a subset of servers, but not accepted by the quorum, so then the bound port is left lingering on that subset of servers., I think its actually worse - consider a reconfig changing just one client port, of one of the servers. All the other servers don't need to perform any operation besides updating the config in their memory, so they won't have any issue. The single server may object because it can't bind to the port, but the leader only needs a majority vote, so his objection may not arrive on time for the commit.

I just want to point out that there is no voting in Paxos or ZooKeeper. Servers don't say no to the leader's proposals. Protocols in which servers can say no are distributed commit protocols (e.g., 2 phase commit, 3 phase commit, E3PC, etc.) but ZooKeeper solves consensus, in which servers don't have a Yes/No vote or veto powers., bq. I just want to point out that there is no voting in Paxos or ZooKeeper.

Absolutely.  As I think more about trying to push the bind into the proposal phase, it just opens up more failure modes with no clear solutions.  As you said, this is exactly what multi-phase commit addresses, but then we've got a very different system on our hands.

I'm slowly coming around to the opinion that we should follow your earlier suggestion to leave the error handling as is and simply ensure the old socket gets closed to prevent the leak.  This would allow the admin to recover by reconfiguring back to the old port (or a different new port).  It's unusual in that the server can't reflect committed state, but the alternative of halting increases the severity of the problem.  I don't see a cleaner alternative that wouldn't cause a huge impact on overall ZooKeeper architecture.

[~fpj], what are your current thoughts?, It doesn't actually bother me that we don't abort the command, as [~shralex] points out, we don't have the notion of abort. The bit the bothers me is the fact that it transparently fails and we rely on the administrator to go check logs or perform manual tests to make sure that it worked. Bringing the server down is indeed sounding like a bad idea, but not surfacing the problem in a fairly visible manner is also not ideal. , bq. ...not surfacing the problem in a fairly visible manner is also not ideal.

Do you think we could solve this part with an enhancement to do some additional sanity checks from the client side after the reconfig completes?, Since this has turned out to be trickier than simply ensuring close of a socket, and we haven't reached consensus on a solution, I'd like to suggest that we defer out of the upcoming 3.5.2 release candidate.  Are there any objections?, It sounds like the three of us agree that 1. we must close the socket and log the error 2. We shouldn't abort or crash the server. 3. we should think of a way to make the administrator aware of the problem (documentation / client-side checking / some kind of alert / something else)

1 and 2 sound straightforward and require simplifying the patch and removing some stuff. 3 is something we're not sure about. 
So perhaps it could make sense to have 1+2 go into the 3.5.2 and 3 should be a separate jira. Up to you guys., [~shralex], thanks for the reply.

Yes, I agree on all 3 points.  I also agree that 1+2 in 3.5.2 is a viable plan, with 3 to go into a separate JIRA targeted to a later release.  If others disagree, then I'd also be comfortable deferring the whole thing in the interest of building consensus and moving ahead with a 3.5.2 release candidate.

[~fpj], what is your opinion?, I'm fine with not fixing the third point for 3.5.2. I've been thinking that we could have an admin tool for reconfiguration, and the tool could use a new 4lw to pull the result of the execution of the latest successful reconfig command. Because we don't have an error path for the execution of the reconfiguration itself, the viable way I see is to store it and enable a client to pull it. Would that work?, I've reverted the change back to no propagate the exception, but only log the error message. I'll do the client side change in another jira because I don't want it blocking this issue. 

I've added a few reformatting changes to this jira, but if it bothers anyone, I'm happy to not do them here., +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12811336/ZOOKEEPER-2366.patch
  against trunk revision 1748630.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3189//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3189//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3189//console

This message is automatically generated., [~fpj], thank you for the new patch.

This is a repeat of an earlier comment: This wasn't introduced by your patch, but it's generally a bad practice to catch an {{InterruptedException}} and neither propagate it nor re-raise the interrupted status. Other layers of code might be expecting to react to thread interruption. Since we're touching this code, do you think we should add a call to {{Thread.currentThread().interrupt()}}?

{code}
    public void reconfigure(InetSocketAddress addr) {
       Channel oldChannel = parentChannel;
       try {
           LOG.info("binding to port " + addr);
           parentChannel = bootstrap.bind(addr);
           localAddress = addr;
       } catch (Exception e) {
           LOG.error("Error while reconfiguring", e);
       } finally {
           oldChannel.close();
       }
    }
{code}

I don't see a checked exception raised in that code block.  Is the catch block there in anticipation of an unchecked exception that you've seen?

Hmmm... this isn't directly related to your patch, but something seems odd here.  The Netty bind and close calls are asynchronous, so when this method returns, it's not guaranteed that the bind and close have completed.  I wonder if we have a race condition lurking.

A very minor nitpick:

*NIOServerCnxnFactory.java:*
{code}
            LOG.info("binding to port " + addr);
{code}

{code}
                LOG.error("Error joining old acceptThread when reconfiguring client port " + e.getMessage());
{code}

{code}
            LOG.error("Error reconfiguring client port to " + addr + " " + e.getMessage());
{code}

*NettyServerCnxnFactory:*
{code}
           LOG.info("binding to port " + addr);
{code}

Since we're touching these lines of code anyway for the reformatting, let's take the opportunity to switch to the SLF4J calling convention with {} token substitutions.

[~shralex], would you also like to review the latest approach?, Thanks [~cnauroth].

bq. Since we're touching this code, do you think we should add a call to Thread.currentThread().interrupt()?

The try/catch block wasn't there for the netty cnxn factory, I've added it in this patch because the bind call does seem to throw some runtime exceptions so I wanted to make sure we at least log the error in the case something goes wrong. But, according to the documentation, {{ServerBootstrap.bind}} does not throw {{InterruptedException}}, or am I reading it wrong?

bq. The Netty bind and close calls are asynchronous, so when this method returns, it's not guaranteed that the bind and close have completed.

If the bind is asynchronous, then the server will simply not accept client connection requests until it completes. As for the close, the worst that can happen is that it accepts a connection request through the old port after the reconfigure call completes. I don't see a problem, but you let me know if I'm missing anything.

bq. let's take the opportunity to switch to the SLF4J calling convention with {} token substitutions

Yep, done.

, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12811707/ZOOKEEPER-2366.patch
  against trunk revision 1748630.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3191//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3191//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3191//console

This message is automatically generated., Thanks for the updated patch [~fpj].

The comment about thread interruption referred to this piece of code in {{NIOServerCnxnFactory}}:

{code}
            try {
                acceptThread.join();
            } catch (InterruptedException e) {
                LOG.error("Error joining old acceptThread when reconfiguring client port {}",
                            e.getMessage());
            }
{code}

Sorry if I was unclear about that.  Again, this isn't introduced by your patch, so feel free to defer this part of the feedback if you prefer.

I was concerned that the asynchronous bind might be a source of flaky tests, but really the client-side retry logic would just take care of it.  I think you're right that it's not really a problem.

+1 for the patch.  I leave it to you to decide whether or not to address the thread interruption.  I'd be +1 either way.

[~shralex], do you have further feedback?, I'll make the change you're requesting and submit a new patch, [~cnauroth]. , [~cnauroth] added the interrupt call., +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12812439/ZOOKEEPER-2366.patch
  against trunk revision 1748630.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3237//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3237//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3237//console

This message is automatically generated., +1 for the latest patch.  Thank you, [~fpj].

[~shralex] or any other committers, can we possibly get someone to do a second review by 12 noon Thursday, 6/23 PST, so that we can include this for the deadline I set out for the 3.5.2 release candidate?, Hi Chris, I'll try to do a review later today. Sorry for the delay but I'm
traveling.


, [~shralex], no worries.  Thank you!, bq. can we possibly get someone to do a second review by 12 noon Thursday, 6/23 PST, so that we can include this for the deadline I set out for the 3.5.2 release candidate?

A quick reminder that we need this in for the release candidate [~shralex], Two comments about the test:
1. It still checks follower's client port change, but not leader's client port change, which we may also want to do as suggested in the comments above. 
2. There's a try block without a catch. 
  try (ServerSocket ss2 = new ServerSocket()) {
+                ss2.bind(new InetSocketAddress(getLoopbackAddress(), oldClientPort));
+            }

Is this intentional ? The test also throws an exception.
Maybe the intention is to bind the old port and then unbind when ss2 goes out of scope. Is this release instantanious or could it
fail the following code that needs the old port vacant ? Maybe we should just remove this try block ?

Another question: could Thread.currentThread().interrupt() somehow crash the server ? 
, Regarding extending the test, if you don't want to do it now because of time constraints, maybe just add a TODO in the code or something ?, Thanks for the review [~shralex]. I have added the test case for the leader. I had to fix the call to {{updateThreadName}} in {{QuorumPeer}} because it was giving me a NPE for the leader test case as {{getLocalAddress}} is null when the port reconfiguration changes.

bq. try (ServerSocket ss2 = new ServerSocket()) { + ss2.bind(new InetSocketAddress(getLoopbackAddress(), oldClientPort)); + } Is this intentional ? The test also throws an exception.

It is intentional, it is just defining the scope of ss2 to be that block so that we can check the status of the port.

bq. could Thread.currentThread().interrupt() somehow crash the server ?

I think so, but we don't want to swallow {{InterruptedException}} in general. If it turns out to be a problem, then we will fix it, but I'd say that we do as [~cnauroth] suggested., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12812837/ZOOKEEPER-2366.patch
  against trunk revision 1748630.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3246//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3246//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3246//console

This message is automatically generated., Thanks Flavio. +1 from me, Hey all, sorry for joining the party late. One quick note is that in NIOServerCnxnFactory.reconfigure():

{code}
....
+            acceptThread.setReconfiguring();
+            oldSS.close();
+            acceptThread.wakeupSelector();
...
{code}

sounds like that first oldSS.close() call should be wrapped with a try/catch IOException given it's best effort too (it could be gone by that time, no?).

Other than, +1., [~rgs] I'm not sure whether you missed it or I'm not getting the comment, but the close call is within a try/catch block:

{noformat}

try {
            this.ss = ServerSocketChannel.open();
            ss.socket().setReuseAddress(true);
            LOG.info("binding to port " + addr);
            ss.socket().bind(addr);
            ss.configureBlocking(false);
            acceptThread.setReconfiguring();
            oldSS.close();
            acceptThread.wakeupSelector();
            try {
                acceptThread.join();
            } catch (InterruptedException e) {
                LOG.error("Error joining old acceptThread when reconfiguring client port {}",
                            e.getMessage());
                Thread.currentThread().interrupt();
            }
            acceptThread = new AcceptThread(ss, addr, selectorThreads);
            acceptThread.start();
        } catch(IOException e) {
            LOG.error("Error reconfiguring client port to {} {}", addr, e.getMessage());
            try {
                oldSS.close();
            } catch (IOException sse) {
                LOG.error("Error while closing old server socket.", sse);
            }
        }
{noformat} 

Assuming that everything is ok, you get to commit it. :-), Thanks very much, everyone.  I'll make sure to commit this before creating a 3.5.2 release candidate, unless another friendly volunteer beats me to it.  :-), [~fpj]: what I am saying is that closing the (old) socket should always be best effort, the outer try/catch means that acceptThread.wakeupSelector() (and what comes after) would be skipped, which doesn't sound right.

I think it should be like this:

{code}
    private void tryClose(ServerSocketChannel s) {
          try {
                s.close();
          } catch (IOException sse) {
                LOG.error("Error while closing server socket.", sse);
          }
    }

    public void reconfigure(InetSocketAddress addr) {
         ServerSocketChannel oldSS = ss;        
         try {
            this.ss = ServerSocketChannel.open();
            ss.socket().setReuseAddress(true);
            LOG.info("binding to port " + addr);
            ss.socket().bind(addr);
            ss.configureBlocking(false);
            acceptThread.setReconfiguring();
            tryClose(oldSS);
            acceptThread.wakeupSelector();
            try {
                acceptThread.join();
            } catch (InterruptedException e) {
                LOG.error("Error joining old acceptThread when reconfiguring client port {}",
                            e.getMessage());
                Thread.currentThread().interrupt();
            }
            acceptThread = new AcceptThread(ss, addr, selectorThreads);
            acceptThread.start();
         } catch(IOException e) {
            LOG.error("Error reconfiguring client port to {} {}", addr, e.getMessage());
            tryClose(oldSS);
         }
{code}

Thus, the outer try/catch is about the rest of the stuff we are doing and closing the old socket is only a best effort thing. , [~rgs] Got it, in the case it all goes well but closing the old socket throws an exception, then we would be skipping the subsequent calls. I'll make the change., Can I get a +1 here, please?, +1, thanks [~fpj].

[~cnauroth]: i'll go ahead and merge this. , [~cnauroth]: actually, can't merge from here, I am on the wrong laptop, sorry :-( , +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12812901/ZOOKEEPER-2366.patch
  against trunk revision 1749951.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3248//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3248//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3248//console

This message is automatically generated., [~rgs], thanks very much for that additional catch in your code review.  +1 from me again on the latest patch.  I'll make sure to commit it before creating the release candidate., I have committed this to trunk and branch-3.5.  [~fpj], thank you for the patch.  [~shralex] and [~rgs], thank you for the code reviews., SUCCESS: Integrated in ZooKeeper-trunk #2972 (See [https://builds.apache.org/job/ZooKeeper-trunk/2972/])
ZOOKEEPER-2366: Reconfiguration of client port causes a socket leak. (fpj via cnauroth) (cnauroth: [http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1750025])
* trunk/CHANGES.txt
* trunk/src/java/main/org/apache/zookeeper/server/NIOServerCnxnFactory.java
* trunk/src/java/main/org/apache/zookeeper/server/NettyServerCnxnFactory.java
* trunk/src/java/main/org/apache/zookeeper/server/quorum/Leader.java
* trunk/src/java/main/org/apache/zookeeper/server/quorum/QuorumPeer.java
* trunk/src/java/test/org/apache/zookeeper/test/NettyNettySuiteTest.java
* trunk/src/java/test/org/apache/zookeeper/test/NioNettySuiteTest.java
* trunk/src/java/test/org/apache/zookeeper/test/ReconfigTest.java
]