[Hi [~jiangjiafu] -

Would it be possible to upload the data to JIRA or a gist?

Thanks, zookeeper log of ofs_zk3 , I think this must be a bug, because the PR happens again in my environment., In the recently environment, I found that, zk3 (leader) found the node expired, and then zk2 and zk3 deleted the node, but the transcation is not done in zk1!, I could see ZOOKEEPER-2355 jira talking about similar case of inconsistency in the Follower server. Could you please take a look at this issue analysis. Perhaps [~arshad.mohammad] can share more details on that issue faced in his env., I have a quick look to the 2355, I am not pretty sure these are the same PR.
But from the log I can see that zk1(the problem node) do lost connection to the leader while wring data, and then many transcations are lost too(including the closeSession transcation)., bq. I think this must be a bug, because the PR happens again in my environment.
bq. do lost connection to the leader while wring data, and then many transcations are lost too(including the closeSession transcation).
From your comment, I think this is reproducible in your env. One easy way is to apply 2355 patch and quickly re-test to understand the impact of 2355 fix.

, I found that, the first time the follower try to reconnect to the leader, it sends the peerLastZxid 0x100003748 to the leader and begin to sync the log from 0x100003749, but failed due to network disconnection. The second time the follower try to reconnect to the leader, it sends the peerLastZxid 0x10000385c to the leader, therefore, the log 0x100003749 ~ 0x10000385c is missing!!

, I believe this PR is the same with ZOOKEEPER-2355, thank you for your reminding [~rakeshr]. I will use the patch provided in ZOOKEEPER-2355, and see whether the PR will happen again. I hope the patch can work fine., Hi [~jiangjiafu]

Was your testing with the patch successful, so we can close this Jira as duplicate?]