{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12764079","self":"https://issues.apache.org/jira/rest/api/2/issue/12764079","key":"ZOOKEEPER-2099","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310801","id":"12310801","key":"ZOOKEEPER","name":"ZooKeeper","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310801&avatarId=10011","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310801&avatarId=10011","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310801&avatarId=10011","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310801&avatarId=10011"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10484","id":"10484","description":"Apache ZooKeeper related","name":"ZooKeeper"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-12-30T21:44:25.815+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Oct 25 20:47:05 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/ZOOKEEPER-2099/watchers","watchCount":7,"isWatching":false},"created":"2014-12-29T21:35:58.549+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"3.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12316644","id":"12316644","description":"Dynamic Reconfig, Remove Watches, Local Session","name":"3.5.0","archived":false,"released":true,"releaseDate":"2014-08-04"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12326518","id":"12326518","name":"3.6.0","archived":false,"released":false}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-10-25T20:47:05.100+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312382","id":"12312382","name":"server","description":"General issues with the ZooKeeper server."}],"timeoriginalestimate":null,"description":"When a learner sync's with the leader, it is possible for the Leader to send the learner a DIFF that does NOT contain all the transactions between the learner's zxid and that of the leader's zxid thus resulting in a corruption datatree on the learner.\nFor this to occur, the leader must have sync'd with a previous leader using a SNAP and the zxid requested by the learner must still exist in the current leader's txnlog files.\nThis issue was introduced by ZOOKEEPER-1413.\n\n*Scenario*\nA sample sequence in which this issue occurs:\n# Hosts H1 and H2 disconnect from the current leader H3 (crash, network partition, etc).  The last zxid on these hosts is Z1.\n# Additional transactions occur on the cluster resulting in the latest zxid being Z2.\n# Host H1 recovers and connects to H3 to sync and sends Z1 as part of its FOLLOWERINFO or OBSERVERINFO packet.\n# The leader, H3, decides to send a SNAP because a) it does not have the necessary records in the in-mem committed log, AND b) the size of the required txnlog to send it larger than the limit.\n# Host H1 successfully sync's with the leader (H3). At this point H1's txnlogs have records up to and including Z1 as well as Z2 and up.  It does NOT have records between Z1 and Z2.\n# Host H3 fails; a leader election occurs and H1 is chosen as the leader\n# Host H2 recovers and connects to H1 to sync and sends Z1 in its FOLLOWERINFO/OBSERVERINFO packet\n# The leader, H1, determines it can send a DIFF.  It concludes this because although it does not have the necessary records in its in-memory commit log, it does have Z1 in its txnlog and the size of the log is less than the limit.  H1 ends up with a different size calculation than H3 because H1 is missing all the records between Z1 and Z2 so it has less log to send.\n# H2 receives the DIFF and applies the records to its data tree. Depending on the type of transactions that occurred between Z1 and Z2 it may not hit any errors when applying these records.\n\nH2 now has a corrupted view of the data tree because it is missing all the changes made by the transactions between Z1 and Z2.\n\n*Recovery*\nThe way to recover from this situation is to delete the data/snap directory contents from the affected hosts and have them resync with the leader at which point they will receive a SNAP since they will appear as empty hosts.\n\n*Workaround*\nA quick workaround for anyone concerned about this issue is to disable sync from the txnlog by changing the database size limit to 0.  This is a code change as it is not a configurable setting.\n\n*Potential fixes*\nThere are several ways of fixing this.  A few of options:\n* Delete all snaps and txnlog files on a host when it receives a SNAP from the leader\n* Invalidate sync from txnlog after receiving a SNAP. This state must also be persisted on-disk so that the txnlogs with the gap cannot be used to provide a DIFF even after restart.  A couple ways in which the state could be persisted:\n** Write a file (for example: loggap.<zxid>) in the data dir indicating that the host was sync'd with a SNAP and thus txnlogs might be missing. Presence of these files would be checked when reading txnlogs.\n** Write a new record into the txnlog file as \"sync'd-by-snap-from-leader\" marker. Readers of the txnlog would then check for presence of this record when iterating through it and act appropriately.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12835192","id":"12835192","filename":"ZOOKEEPER-2099.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"created":"2016-10-25T20:24:33.380+0000","size":28749,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12835192/ZOOKEEPER-2099.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12834543","id":"12834543","filename":"ZOOKEEPER-2099.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"created":"2016-10-20T20:44:10.480+0000","size":26606,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12834543/ZOOKEEPER-2099.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12689378","id":"12689378","filename":"ZOOKEEPER-2099-repro.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-29T21:40:59.629+0000","size":12048,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12689378/ZOOKEEPER-2099-repro.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Using txnlog to sync a learner can corrupt the learner's datatree","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14260459","id":"14260459","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"body":"The attach repro patch contains a test that reproduces the issue.  As written, the test fails if a corruption is detected.\n\nIt does the following:\n# Start an ensemble of five hosts (SIDs 1-5)\n# If SID 5 is the leader, move leader to any other host\n# Shutdown SID 5 and one other non-leader host\n# Insert some new znodes\n# Turn on forced SNAPs for syncing with the leader\n# Restart SID 5 and validate it sync'd with a snapshot\n# Shutdown current leader; this results in SID 5 becoming the leader\n# Disable forced snap-on-sync (and make DIFF more likely by adjusting the size limit)\n# Restart the second host that was shutdown in step #3 and validate it sync's with a DIFF.\n# Check for mismatched nodes on the leader and the host that was shutdown in step #3 => there should be a mismatch","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-29T21:40:59.637+0000","updated":"2014-12-29T21:40:59.637+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14261533","id":"14261533","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fournc","name":"fournc","key":"fournc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Camille Fournier","active":true,"timeZone":"Etc/UTC"},"body":"Changing status to get the build to execute with the patch","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fournc","name":"fournc","key":"fournc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Camille Fournier","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-30T21:44:25.815+0000","updated":"2014-12-30T21:44:25.815+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14261577","id":"14261577","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12689378/ZOOKEEPER-2099-repro.patch\n  against trunk revision 1646992.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 5 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.\n\n    -1 release audit.  The applied patch generated 1 release audit warnings (more than the trunk's current 0 warnings).\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2473//testReport/\nRelease audit warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2473//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt\nFindbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2473//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/2473//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-30T22:36:21.202+0000","updated":"2014-12-30T22:36:21.202+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14298005","id":"14298005","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hdeng","name":"hdeng","key":"hdeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hongchao Deng","active":true,"timeZone":"America/Los_Angeles"},"body":"In Scenario Step 7, do you mean\n{code}\nHost H2 recovers and connects to H1\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hdeng","name":"hdeng","key":"hdeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hongchao Deng","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-30T01:05:10.170+0000","updated":"2015-01-30T01:05:10.170+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14299151","id":"14299151","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hdeng","name":"hdeng","key":"hdeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hongchao Deng","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~svoutil] for uploading the repro-patch.\nRegarding the test you added, should the last assert \n{code}\nAssert.assertNotNull(\"Node should also exist on target if there is no corruption but it does NOT\", gapTargetStat);\n{code}\nbe \n{code}\nAssert.assertNull(...);\n{code}\n\nright?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hdeng","name":"hdeng","key":"hdeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hongchao Deng","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-30T19:59:20.516+0000","updated":"2015-01-30T19:59:20.516+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14299237","id":"14299237","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hdeng","name":"hdeng","key":"hdeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hongchao Deng","active":true,"timeZone":"America/Los_Angeles"},"body":"Okay. I see what you are saying now. Do you have a patch for the problem?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hdeng","name":"hdeng","key":"hdeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hongchao Deng","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-30T21:22:22.841+0000","updated":"2015-01-30T21:22:22.841+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14299418","id":"14299418","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"body":"I wrote the test to fail, so assertNotNull is correct since the node will exist on the leader but NOT on the target host.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-31T00:10:58.868+0000","updated":"2015-01-31T00:10:58.868+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14299421","id":"14299421","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"body":"Yeah, nice catch; I've updated it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-31T00:13:02.639+0000","updated":"2015-01-31T00:13:02.639+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14299439","id":"14299439","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"body":"No, not yet.  We've been discussing options, but haven't settled on a plan.\n\nAn extreme option is to truncate/delete all txnlog (and the in-mem committedLog) for zxids < than the start of the snap whenever syncWithLeader results in the SNAP (i.e. this would happen on the learner).  This would work since if this learner became the leader later, then the txnlog would not contain the zxid requested by a stale learner and so a DIFF would not even be possible.  I consider this extreme and more of a last resort because it means deleting txnlog from disk which could impact investigations/backup-scripts/retention-policy/etc.\n\nAnother option would be to track the latest received SNAP zxid somewhere. Then LearnerHandler#syncFollower would compare the requested zxid with the last SNAP zxid and if the requested zxid is less then it would force a SNAP even if the requested zxid existed in the txnlog.\nThe storage location for the latest received SNAP needs to persist with the txnlogs (since it would still need to be known after a host restart).  This could be done by storing it in a separate file in the same directory as the txnlog, or it could be appended to the txnlog at the time of the SNAP.\nThe latter storage option has the benefit that tools like LogFormatter would also see it (and not just the latest but all snap zxids) and be able to handle it. In the case of LogFormatter it could indicate that there is a gap in the txnlog at that point.\n\nI personally prefer appending a special SNAP-OCCURRED record into the txnlog, but have not yet gone through all the investigation to determine whether that would be safe and/or what other changes would be needed since that record should probably use some invalid ZXID in its record (in order to avoid confusion with a valid record with that same zxid on another host). ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=svoutil","name":"svoutil","key":"svoutil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santeri (Santtu) Voutilainen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-31T00:28:57.981+0000","updated":"2015-01-31T00:28:57.981+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/14299505","id":"14299505","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hdeng","name":"hdeng","key":"hdeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hongchao Deng","active":true,"timeZone":"America/Los_Angeles"},"body":"{quote}\nThe storage location for the latest received SNAP needs to persist with the txnlogs (since it would still need to be known after a host restart). This could be done by storing it in a separate file in the same directory as the txnlog, or it could be appended to the txnlog at the time of the SNAP.\n{quote}\n\nCould you clarify the method? I like the second idea because it is more obvious to do.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hdeng","name":"hdeng","key":"hdeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hongchao Deng","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-31T01:08:46.725+0000","updated":"2015-01-31T01:08:46.725+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15177225","id":"15177225","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=phunt","name":"phunt","key":"phunt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Patrick Hunt","active":true,"timeZone":"America/Los_Angeles"},"body":"Cancelling patch as the solution is not included. Good to have the reproducible case though.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=phunt","name":"phunt","key":"phunt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Patrick Hunt","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-03T05:36:47.884+0000","updated":"2016-03-03T05:36:47.884+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15592947","id":"15592947","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"body":"Attached is an initial attempt at fixing this.\n\nI was investigating an issue with massive inconsistency between ZK servers and developed my own test to reproduce the issue. I originally thought our issue was isolated to ephemeral znodes so I didn't find this issue in my searches, but the test I wrote ended up being almost exactly the same as the one provided by [~svoutil]. I did find another detail in my investigation that seems important - this bug not only causes the leader to not send the correct updates to the follower, but the leader can also incorrectly tell the follower to truncate its log, resulting in the loss of even more transactions on the follower. This requires a slightly different sequence of events (a few more steps). I found it easier to use my original test as a base and make some improvements using the test submitted here, so that's why the test I'm submitting looks so different.\n\nh4. Test\n\nHere's a description of what the test does:\n\n# Enable forceSnapSync\n# Set a very high snapshotSizeFactor to guarantee a DIFF if forceSnapSync is off\n# Start 5 servers\n# Create a baseline znode /w (not necessary, but shows where the data loss starts)\n# Shutdown SID 4\n# Create /x while SID 4 is down\n# Shutdown SID 0\n# Create /y while SIDs 0 and 4 are down\n# Start SID 4 (which receives a SNAP from the current leader because of forceSnapSync=true)\n# Create /z while SID 0 is down\n# Disable forceSnapSync\n# Shutdown current leader - SID 4 becomes leader\n# Start SID 0 (which receives a TRUNC from SID 4 without the fix and a SNAP with the fix)\n# Check for the presence of all znodes on all servers (without the fix, SID 0 is missing /x and /y)\n\nMore detail on what goes wrong in step 13:\n\n(Using W = the zxid of the transaction which creates /w, X for /x, etc.)\n\nAt this point, SID 4 has W and Z in its log and it has a snapshot containing the updates from W, X, and Y. It tries to sync with SID 0 (whose last zxid is Y), and iterates through its log until it finds a zxid > Y. It then looks back at the previous log entry (W), sees that W < Y, and tells SID 0 to truncate its log to W. After this, it starts sending updates at Z. SID 0 therefore deletes X and misses Y. The only correct thing for SID 4 to do here is to send a snapshot.\n\nh4. Fix\n\nThe approach of writing a file with the last SNAP received to the data dir and checking that value when trying to sync with a follower seems best. The patch adds code to ZKDatabase to handle this file (called lastSnapReceived). LearnerHandler checks this lastSnapReceived value, and if it falls in the range of transactions a follower needs in syncFollower, a snapshot is sent.\n\nWe desperately need this fix because of the massive issues the bug is causing, so I will be doing as much testing as I can around it before fixing our internal version of ZK. It would be great to also get it polished to a state where it could be included in a future 3.5.x version.\n\nSome big points to discuss:\n* What should ZKDatabase/Learner do if it can't create or write to the file? It currently doesn't handle any exceptions which will result in the Learner stopping. This ensures correctness, but introduces another way for a Learner to fail.\n* What should ZKDatabase/LearnerHandler do if it can't read the file? LearnerHandler currently catches all exceptions and falls back to sending a SNAP. This is always correct, but there will be performance loss in syncing new learners if the file becomes unreadable/corrupted somehow.\n* Is there risk with upgrades or downgrades? It doesn't seem like there should be. Versions without the fix will just ignore the file if it's present in their data dir. Upgrading from a version without the fix to a version with the fix will result in the file being written when initializing the ZKDatabase.\n\nSmaller points I couldn't decide on:\n* Is it acceptable to enforce snapLog being non-null when constructing a ZKDatabase now? I had to modify some unit tests, but I liked that better than a test-only null check in the constructor.\n* Should zookeeper.forceSnapSync and zookeeper.snapshotSizeFactor be settable system properties? The property names were included in the relevant classes but never used, and I wasn't sure if that was intended or not.\n* Is IOUtils a good home for writeLongToFileAtomic since QuorumPeer and ZKDatabase both need that logic now?\n\nPatch generated against master and seems to apply to branch-3.5. Not needed in branch-3.4 since the issue was introduced in 3.5.0\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"created":"2016-10-20T20:44:10.484+0000","updated":"2016-10-20T20:44:10.484+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15593016","id":"15593016","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12834543/ZOOKEEPER-2099.patch\n  against trunk revision cef5978969bedfe066f903834a9ea4af6d508844.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 9 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3495//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3495//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3495//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-10-20T21:07:21.988+0000","updated":"2016-10-20T21:07:21.988+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15593051","id":"15593051","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"body":"Looks like the test needs to be hardened a bit. I think I see the issue - QuorumBase.waitForServerUp doesn't guarantee that the client the test is using to create the nodes is also connected. I ran the test a few dozen times on my machine and saw no failures, but that's obviously not good enough.\n\nAs for the testLE failure, that seems to be a known flaky test (ZOOKEEPER-1932)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"created":"2016-10-20T21:21:10.959+0000","updated":"2016-10-20T21:21:10.959+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15593314","id":"15593314","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hanm","name":"hanm","key":"hanm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hanm&avatarId=26946","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hanm&avatarId=26946","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hanm&avatarId=26946","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hanm&avatarId=26946"},"displayName":"Michael Han","active":true,"timeZone":"America/Vancouver"},"body":"I can't find the other failed test - org.apache.zookeeper.server.quorum.QuorumPeerMainTest.testTransactionLogGap in code base... where is this test coming from? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hanm","name":"hanm","key":"hanm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hanm&avatarId=26946","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hanm&avatarId=26946","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hanm&avatarId=26946","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hanm&avatarId=26946"},"displayName":"Michael Han","active":true,"timeZone":"America/Vancouver"},"created":"2016-10-20T23:10:34.621+0000","updated":"2016-10-20T23:10:34.621+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15593316","id":"15593316","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hanm","name":"hanm","key":"hanm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hanm&avatarId=26946","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hanm&avatarId=26946","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hanm&avatarId=26946","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hanm&avatarId=26946"},"displayName":"Michael Han","active":true,"timeZone":"America/Vancouver"},"body":"Ah never mind, that is the test added into the patch:)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hanm","name":"hanm","key":"hanm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hanm&avatarId=26946","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hanm&avatarId=26946","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hanm&avatarId=26946","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hanm&avatarId=26946"},"displayName":"Michael Han","active":true,"timeZone":"America/Vancouver"},"created":"2016-10-20T23:11:02.014+0000","updated":"2016-10-20T23:11:02.014+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15593326","id":"15593326","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hanm","name":"hanm","key":"hanm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hanm&avatarId=26946","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hanm&avatarId=26946","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hanm&avatarId=26946","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hanm&avatarId=26946"},"displayName":"Michael Han","active":true,"timeZone":"America/Vancouver"},"body":"Should we catch the connection loss exception and retry connecting to make sure the ZK handle used in test is in the connected state? I think all ZK handles were initially in connected state (after calling waitForAll), but maybe for some reasons one or more connections were lost because networking or other issues.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hanm","name":"hanm","key":"hanm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hanm&avatarId=26946","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hanm&avatarId=26946","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hanm&avatarId=26946","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hanm&avatarId=26946"},"displayName":"Michael Han","active":true,"timeZone":"America/Vancouver"},"created":"2016-10-20T23:16:05.111+0000","updated":"2016-10-20T23:16:30.917+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15593379","id":"15593379","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"body":"As a start, I think I should change the test to use waitForOne on the client being used for the operation after shutting down server 4 instead of waitForServerUp. This will at least make sure the test is waiting for the right thing before moving on and creating paths. I do expect the clients to be disconnected when taking down server 4 (since that should be the current leader), and if that's the only source of connection loss, the test might not need to retry.\n\nAs far as I can tell, most other tests don't retry on connection loss. Should they?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"created":"2016-10-20T23:40:48.509+0000","updated":"2016-10-20T23:40:48.509+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15593443","id":"15593443","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hanm","name":"hanm","key":"hanm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hanm&avatarId=26946","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hanm&avatarId=26946","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hanm&avatarId=26946","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hanm&avatarId=26946"},"displayName":"Michael Han","active":true,"timeZone":"America/Vancouver"},"body":"Just did a search on my archived build mails - I see a good amount of tests failed from time to time with 'KeeperErrorCode = ConnectionLoss'. I think the test cases should be made more fault tolerant to such false negatives. I agree that we should not blindly do retry and the retry should be done on a case by case basis. Let me dig more into what this new test and those failed existing tests did..","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hanm","name":"hanm","key":"hanm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hanm&avatarId=26946","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hanm&avatarId=26946","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hanm&avatarId=26946","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hanm&avatarId=26946"},"displayName":"Michael Han","active":true,"timeZone":"America/Vancouver"},"created":"2016-10-21T00:12:18.467+0000","updated":"2016-10-21T00:12:18.467+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15606366","id":"15606366","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"body":"I updated the test so it hopefully won't see connection loss as a result of its own actions. For the two parts of the test where it causes a new leader to be elected, it now looks for the disconnect and reconnect on the correct client to make sure things have recovered completely. Using the existing CountdownWatcher utility class seemed to be the best fit for that, since using QuorumPeerMainTest::waitForOne might miss the disconnection event if the client reconnects while waitForOne is sleeping.\n\nI did add some asynchronous methods to wait for connection or disconnection in CountdownWatcher to be able to start waiting for the disconnection before shutting down the server, avoiding a potential race condition that might cause the disconnection to be missed. They're just simple wrappers that return a Future and kick off waitForDisconnect or waitForConnect in a thread.\n\nI still couldn't get the old version of the test to fail locally, so there might be issues with this one too, but I think it should be an improvement.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=makuchta","name":"makuchta","key":"makuchta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Kuchta","active":true,"timeZone":"America/Denver"},"created":"2016-10-25T20:24:33.384+0000","updated":"2016-10-25T20:24:33.384+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12764079/comment/15606438","id":"15606438","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12835192/ZOOKEEPER-2099.patch\n  against trunk revision cef5978969bedfe066f903834a9ea4af6d508844.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 12 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3499//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3499//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/3499//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-10-25T20:47:05.100+0000","updated":"2016-10-25T20:47:05.100+0000"}],"maxResults":21,"total":21,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/ZOOKEEPER-2099/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i23tuf:"}}