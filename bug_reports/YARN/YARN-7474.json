{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "description": "Fair Scheduler",
            "id": "12322906",
            "name": "fairscheduler",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12322906"
        }],
        "created": "2017-11-11T11:58:41.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378"
            },
            "displayName": "wuchang",
            "key": "wuchang1989",
            "name": "wuchang1989",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2017-11-13T17:27:07.369+0000",
        "customfield_12310222": "1_*:*_1_*:*_263213115_*|*_5_*:*_1_*:*_0",
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "1.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i3mo5r:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Nov 14 13:05:34 UTC 2017",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "Hadoop Version: *2.7.2*\r\nMy Yarn cluster have *(1100TB,368vCores)*  totallly with 15 nodemangers . \r\nMy cluster use fair-scheduler and I have 4 queues for different kinds of jobs:\r\n \r\n{quote}\r\n<allocations>\r\n    <queue name=\"queue1\">\r\n       <minResources>100000 mb, 30 vcores<\/minResources>\r\n       <maxResources>422280 mb, 132 vcores<\/maxResources>\r\n       <maxAMShare>0.5f<\/maxAMShare>\r\n       <fairSharePreemptionTimeout>9000000000<\/fairSharePreemptionTimeout>\r\n       <minSharePreemptionTimeout>9000000000<\/minSharePreemptionTimeout>\r\n       <maxRunningApps>50<\/maxRunningApps>\r\n    <\/queue>\r\n    <queue name=\"queue2\">\r\n       <minResources>25000 mb, 20 vcores<\/minResources>\r\n       <maxResources>600280 mb, 150 vcores<\/maxResources>\r\n       <maxAMShare>0.6f<\/maxAMShare>\r\n       <fairSharePreemptionTimeout>9000000000<\/fairSharePreemptionTimeout>\r\n       <minSharePreemptionTimeout>9000000000<\/minSharePreemptionTimeout>\r\n       <maxRunningApps>50<\/maxRunningApps>\r\n    <\/queue>\r\n    <queue name=\"queue3\">\r\n       <minResources>100000 mb, 30 vcores<\/minResources>\r\n       <maxResources>647280 mb, 132 vcores<\/maxResources>\r\n       <maxAMShare>0.8f<\/maxAMShare>\r\n       <fairSharePreemptionTimeout>9000000000<\/fairSharePreemptionTimeout>\r\n       <minSharePreemptionTimeout>9000000000<\/minSharePreemptionTimeout>\r\n       <maxRunningApps>50<\/maxRunningApps>\r\n    <\/queue>\r\n  \r\n    <queue name=\"queue4\">\r\n       <minResources>80000 mb, 20 vcores<\/minResources>\r\n       <maxResources>120000 mb, 30 vcores<\/maxResources>\r\n       <maxAMShare>0.5f<\/maxAMShare>\r\n       <fairSharePreemptionTimeout>9000000000<\/fairSharePreemptionTimeout>\r\n       <minSharePreemptionTimeout>9000000000<\/minSharePreemptionTimeout>\r\n       <maxRunningApps>50<\/maxRunningApps>\r\n     <\/queue>\r\n<\/allocations>\r\n {quote}\r\n\r\nfrom about 9:00 am, all new-coming applications get stuck for nearly 5 hours, but the cluster resource usage is about *(600GB,120vCores)*，it means，the cluster resource is still *sufficient*.\r\n*The resource usage of the whole yarn cluster AND of each single queue stay unchanged for 5 hours*, really strange. Obviously , if it a resource insufficiency problem , it's impossible that used resource of all queues didn't have any change for 5 hours. So , I is the problem of ResourceManager.\r\n\r\nSince my cluster scale is not large, only 15 nodes with 1100G memory ,I exclude the possibility showed in [YARN-4618].\r\n \r\nbesides that , all the running applications seems never finished, the Yarn RM seems static ,the RM log  have no more state change logs about running applications，except for the log about more and more application is submitted and become ACCEPTED, but never from ACCEPTED to RUNNING.\r\n*The resource usage of the whole yarn cluster AND of each single queue stay unchanged for 5 hours*, really strange.\r\nThe cluster seems like a zombie.\r\n \r\nI haved checked the ApplicationMaster log of some running but stucked application , \r\n \r\n {quote}\r\n2017-11-11 09:04:55,896 INFO [IPC Server handler 0 on 42899] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Getting task report for MAP job_1507795051888_183385. Report-size will be 4\r\n2017-11-11 09:04:55,957 INFO [IPC Server handler 0 on 42899] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Getting task report for REDUCE job_1507795051888_183385. Report-size will be 0\r\n2017-11-11 09:04:56,037 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\r\n2017-11-11 09:04:56,061 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1507795051888_183385: ask=6 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:109760, vCores:25> knownNMs=15\r\n2017-11-11 13:58:56,736 INFO [IPC Server handler 0 on 42899] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Kill job job_1507795051888_183385 received from appuser (auth:SIMPLE) at 10.120.207.11\r\n {quote}\r\n \r\nYou can see that at  *2017-11-11 09:04:56,061* It send resource request to ResourceManager but RM allocate zero containers. Then ,no more logs  for 5 hours. At  13:58， I have to kill it manually.\r\n \r\nAfter 5 hours , I kill some pending applications and then everything recovered，remaining cluster resources can be allocated again, ResourceManager seems  to be alive again.\r\n \r\nI have exclude the possibility of  the restriction of maxRunningApps and maxAMShare config because they will just affect a single queue, but my problem is that whole yarn cluster application get stuck.\r\n \r\n \r\n \r\nAlso , I exclude the possibility of a  resourcemanger  full gc problem because I check that with gcutil，no full gc happened , resource manager memory is OK.\r\n \r\nSo , anyone could give me some suggestions?\r\n ",
        "duedate": null,
        "environment": null,
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
            "id": "2",
            "name": "Critical",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/2"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135"
            },
            "id": "12313722",
            "key": "YARN",
            "name": "Hadoop YARN",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12313722"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378"
            },
            "displayName": "wuchang",
            "key": "wuchang1989",
            "name": "wuchang1989",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989",
            "timeZone": "Etc/UTC"
        },
        "resolution": {
            "description": "A fix for this issue is checked into the tree and tested.",
            "id": "1",
            "name": "Fixed",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1"
        },
        "resolutiondate": "2017-11-14T13:05:34.000+0000",
        "status": {
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "id": "5",
            "name": "Resolved",
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "Yarn resourcemanager stop allocating container when cluster resource is sufficient ",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2017-11-14T13:05:56.000+0000",
        "versions": [{
            "archived": false,
            "description": "2.7.2 release",
            "id": "12332791",
            "name": "2.7.2",
            "releaseDate": "2016-01-25",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12332791"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/YARN-7474/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/YARN-7474/watchers",
            "watchCount": 4
        },
        "workratio": -1
    },
    "id": "13117726",
    "key": "YARN-7474",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13117726"
}