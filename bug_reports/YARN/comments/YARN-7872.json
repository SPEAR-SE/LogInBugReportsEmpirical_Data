[Just a init to trigger Jenkins., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  8m 15s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} branch-2.7.2 Compile Tests {color} ||
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  1m 51s{color} | {color:red} root in branch-2.7.2 failed. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  0m 29s{color} | {color:red} hadoop-yarn-server-resourcemanager in branch-2.7.2 failed with JDK v1.8.0_151. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in branch-2.7.2 failed with JDK v9-internal. {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 31s{color} | {color:green} branch-2.7.2 passed {color} |
| {color:red}-1{color} | {color:red} mvnsite {color} | {color:red}  0m 17s{color} | {color:red} hadoop-yarn-server-resourcemanager in branch-2.7.2 failed. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in branch-2.7.2 failed. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in branch-2.7.2 failed with JDK v1.8.0_151. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in branch-2.7.2 failed with JDK v9-internal. {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  0m  9s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  0m  9s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.8.0_151. {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m  9s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.8.0_151. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v9-internal. {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v9-internal. {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 25s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 2 new + 816 unchanged - 1 fixed = 818 total (was 817) {color} |
| {color:red}-1{color} | {color:red} mvnsite {color} | {color:red}  0m 11s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m  9s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.8.0_151. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v9-internal. {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  0m 10s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v9-internal. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 22s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 15m 17s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:date2018-02-01 |
| JIRA Issue | YARN-7872 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12908800/YARN-7872-branch-2.7.2.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 96c5df8dad1b 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2.7.2 / b165c4f |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 9-internal |
| Multi-JDK versions |  /usr/lib/jvm/java-8-openjdk-amd64:1.8.0_151 /usr/lib/jvm/java-9-openjdk-amd64:9-internal |
| mvninstall | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/branch-mvninstall-root.txt |
| compile | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/branch-compile-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_151.txt |
| compile | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/branch-compile-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk9-internal.txt |
| mvnsite | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/branch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/branch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/branch-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_151.txt |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/branch-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk9-internal.txt |
| mvninstall | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| compile | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-compile-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_151.txt |
| javac | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-compile-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_151.txt |
| compile | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-compile-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk9-internal.txt |
| javac | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-compile-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk9-internal.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| mvnsite | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_151.txt |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk9-internal.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/19564/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk9-internal.txt |
| JDK v9-internal  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/19564/testReport/ |
| Max. process+thread count | 147 (vs. ulimit of 5000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/19564/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, [~leftnoteasy], could you please take a look at this? :)

Appreciate your insights!, [~jlowe], could you please also take a look at this?
Only a little change and seems trunk also has the same issue., [~leftnoteasy] would be better at answering this, as he knows node labels far better than I do.  As I understand it, node labels are effectively hard-partitioning the cluster (especially back in 2.7).  For the simple case of a single node label, it's like smashing two clusters together where one cluster's nodes have the label and the other cluster's nodes do not.  If you want to allocate on the unlabeled nodes, you don't specify any label with your request.  If you want to allocate on the labeled nodes, you specify the label with your request.

In that case node locality and node label are _not_ orthogonal.  For example, a node label can be used to reserve nodes for certain apps.  If any other app comes along with an ANY request and plunks down containers on those nodes, that totally defeats the purpose of that node label.  So I believe this is working as designed.

bq. If not acceptable (i.e. the current behavior is by designed), so, how can we use locality to request container within these labeled nodes?

It's like I stated above.  If you want the resource to be placed on the labeled nodes, put the label in your request.  If you want the request to be placed on unlabeled nodes, omit the label from your request.  If you want the request to go anywhere, labeled or not, I don't think exclusive node labels allow for that functionality, but I may be missing something there.  I do know that allowing apps asking for ANY resource without a label to start using resources on labeled nodes will break some setups.

, Thanks [~jlowe] for the reply :)

I agree that node label is used to partition nodes in the cluster. 

This patch does not break it, since user still can use *node label* to select nodes he wants.

What the patch provides more is that, it can also allow user to use *node locality* (such as a specific node) to select nodes within a specific nodelabel.

For example, we have 2 nodes, 

[Resource: [MemoryMB: [100] CpuNumber: [12]] {color:#14892c}NodeLabel: [persistent]{color} {color:#f79232}HostName: \{A}{color} RackName: \{/default-rack}]

[Resource: [MemoryMB: [100] CpuNumber: [12]] {color:#14892c}NodeLabel: [persistent]{color} {color:#f79232}HostName: \{B}{color} RackName: \{/default-rack}]

Before this patch, user cannot ask for only A node, since A node has a nodelabel persistent and YARN does not allow to specify locality with nodelabel.

After this patch, use can ask for only A node.

So node locality and node label can be orthogonal, and this can provide more controllability to select nodes for user.

 

Besides, queue is used to control which user can access which node label in how much capacity. This patch does not break it, since only the nodes (nodelabel) which can be accessed by target queue can be sent in to LeafQueue#assignContainer.

So the ACL is still under controlled, and the patch just breaks that "labeled nodes can only be allocated for labeled request". 

Overall, my point is that, node locality and node label can be orthogonal for user to select nodes.

., {quote}If not acceptable (i.e. the current behavior is by designed), so, how can we use locality to request container within these labeled nodes?
{quote}
What my problem here is that, given the 2 nodes in my previous comment, how can I just ask for only Node A (NodeLocality = HostName A and RelaxLocality= false) instead of ask for both A and B (NodeLabel = persistent)?, [~yqwang], 

Like [~jlowe] mentioned, currently we don't have an option to say "I don't care about node partition". Non-exclusive node partition has different semantics. The reason why we don't have it is, we need to calculate total pending resource to each partition to make sure preemption can work properly. 

Existing the patch is not backward compatible: it breaks behavior of an app requests locality + node partition. Before your patch, the behavior is, if requested locality is under requested partition, it can be allocated, otherwise it will keep in pending state. After your patch, requested partition will be silently ignored, which is not ideal. And it breaks how we calculate pending resource of each partition.

Unfortunately I cannot think of a way to make it work. The good thing is, In practice it may not be that bad:
 * For traditional batch jobs, like MR/Spark, it will downgrade to rack/offswitch read.
 * For hard locality (relax locality == false), this causes task cannot get allocated. This happens under misconfiguration, which is not normal case as well.

Thoughts?

 , Thanks [~leftnoteasy].
{quote}Existing the patch is not backward compatible: it breaks behavior of an app requests locality + node partition. Before your patch, the behavior is, if requested locality is under requested partition, it can be allocated, otherwise it will keep in pending state. After your patch, requested partition will be silently ignored, which is not ideal. And it breaks how we calculate pending resource of each partition.
{quote}
Seems the behavior is that YARN just *do not allow* to specify node locality + node partition together, so it is meaningless to talk about "ignore one and take another". Please check below code in both 2.7 and trunk:

 
{code:java}
// we don't allow specify label expression other than resourceName=ANY now
if (!ResourceRequest.ANY.equals(resReq.getResourceName())
    && labelExp != null && !labelExp.trim().isEmpty()) {
  throw new InvalidLabelResourceRequestException(
      "Invalid resource request, queue=" + queueInfo.getQueueName()
          + " specified node label expression in a "
          + "resource request has resource name = "
          + resReq.getResourceName());
}
{code}
 

 

So, I think the behavior is the same before and after the patch, i.e.:

If an app requests locality + node partition, an InvalidLabelResourceRequestException will be throw (request failed).

 

What the patch does is that, it allow user to just specify locality and without specify nodelabel to request one specific labeled node.

 

The pending resource of each partition is really a good concern.

Any plan to totally support node locality to work with node label?

 

Thanks again :)

 

 , For deep learning jobs, the performance is very important, so the locality (such as Infiniband) is also very important.

But these jobs also want to use labeled nodes, such as labeled with FPGA (Type), GPU (Type) etc., [~yqwang], 
{code:java}
// we don't allow specify label expression other than resourceName=ANY now
if (!ResourceRequest.ANY.equals(resReq.getResourceName())
    && labelExp != null && !labelExp.trim().isEmpty()) {
  throw new InvalidLabelResourceRequestException(
      "Invalid resource request, queue=" + queueInfo.getQueueName()
          + " specified node label expression in a "
          + "resource request has resource name = "
          + resReq.getResourceName());
}{code}

Actually this is a misunderstanding, we may need to improve comment a bit: 
Currently we support using locality + partition at the same time. But the partition (nodeLabelExpression) should be only set on resourceName == *. Let's say:
{code}
Priority = 1
  ResourceName = ANY, labelExpression = "A" 
  ResourceName = "/rack1", labelExpression = null
  ResourceName = "host1", labelExpression = null, relaxLocality = false
{code} 

In this case, the "host1" hard locality will be respected when host1 is under partition==A. 

We should not silently ignore fields in ResourceRequest, for your requirement, you may take a look at YARN-6592 (which plans to support node attribute YARN-3409), delayed_or (Reference to YARN-6592 design doc), etc. If user specified conflict requirement (like hard locality to a node, but the node is not under specified node partition), scheduler should either reject the resource request (ideal) or keep it pending (current behavior). 

Please let me know your thoughts., For above behavior, you can take a look at {{LocalityAppPlacementAllocator#updateNodeLabels}}:

{code}
  private void updateNodeLabels(ResourceRequest request) {
    String resourceName = request.getResourceName();
    if (resourceName.equals(ResourceRequest.ANY)) {
      ResourceRequest previousAnyRequest =
          getResourceRequest(resourceName);

      // When there is change in ANY request label expression, we should
      // update label for all resource requests already added of same
      // priority as ANY resource request.
      if ((null == previousAnyRequest) || hasRequestLabelChanged(
          previousAnyRequest, request)) {
        for (ResourceRequest r : resourceRequestMap.values()) {
          if (!r.getResourceName().equals(ResourceRequest.ANY)) {
            r.setNodeLabelExpression(request.getNodeLabelExpression());
          }
        }
      }
    } else{
      ResourceRequest anyRequest = getResourceRequest(ResourceRequest.ANY);
      if (anyRequest != null) {
        request.setNodeLabelExpression(anyRequest.getNodeLabelExpression());
      }
    }
  }
{code}, Thanks [~leftnoteasy] for YARN-6592.

Does the "Rich placement constraints" can also be worked for the labeled nodes?

If yes, I think it may also help on this JIRA.

I will take a look in details later.]