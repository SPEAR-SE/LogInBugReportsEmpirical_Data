[This patch adds a configure, "yarn.resourcemanager.zk-jutemaxbuffer-bytes", in yarn-site.xml, so that ZKRMStateStore can set jute.maxbuffer larger than 1MB when connect to ZooKeeper Server. 

Please have a check of the patch, thanks!, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12658387/YARN-2368.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4471//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4471//console

This message is automatically generated., Thanks for your contribution, [~breno.leitao]! Could you explain the condition you faced this problem? If we face this problem very often, it's critical problem of ZKRMStateStore. However, data stored in ZKRMStateStore are small basically. Therefore, I think it's strange that this kind of problem appear. Additionally, if the max data size is fixed, we should make it default value not to face this problem., [~breno.leitao], oops, sorry for calling you wrongly. I tried to mention [~guoleitao].
, Thanks [~ozawa] for your comments. 

I deployed hadoop-2.3.0-cdh5.1.0 with 22-queue fairscheduler on my 20-node cluster. Two resourcemanagers are deployed exclusively on 10.153.80.8 and 10.153.80.18. 

Jobs are submitted from gridmix:
{code}
sudo -u mapred hadoop jar /usr/lib/hadoop-mapreduce/hadoop-gridmix.jar -Dgridmix.min.file.size=10485760 -Dgridmix.job-submission.use-queue-in-trace=true -Dgridmix.distributed-cache-emulation.enable=false  -generate 34816m hdfs:///user/mapred/foo/ hdfs:///tmp/job-trace.json
{code}
job-trace.json is generated by Rumen, with 6,000 jobs, average #maptasks per job is  320 and average #reducetasks is 25.

I found 3 times (gridmix tested more than 3 times) that resourcemanager failed when handle STATE_STORE_OP_FAILED event. At the same time, zookeeper throws out  'Len error IOException'
{code}
... ...
2014-07-24 21:00:51,170 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.153.80.8:47135
2014-07-24 21:00:51,171 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@832] - Client attempting to renew session 0x247678daa88001a at /10.153.80.8:47135
2014-07-24 21:00:51,171 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:Learner@107] - Revalidating client: 0x247678daa88001a
2014-07-24 21:00:51,171 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:ZooKeeperServer@595] - Established session 0x247678daa88001a with negotiated timeout 10000 for client /10.153.80.8:47135
2014-07-24 21:00:51,171 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@863] - got auth packet /10.153.80.8:47135
2014-07-24 21:00:51,172 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@897] - auth success /10.153.80.8:47135
2014-07-24 21:00:51,186 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@354] - Exception causing close of session 0x247678daa88001a due to java.io.IOException: Len error 1813411
2014-07-24 21:00:51,186 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.153.80.8:47135 which had sessionid 0x247678daa88001a

... ...

2014-07-25 22:10:08,919 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.153.80.8:50480
2014-07-25 22:10:08,921 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@832] - Client attempting to renew session 0x247684586e70006 at /10.153.80.8:50480
2014-07-25 22:10:08,922 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@595] - Established session 0x247684586e70006 with negotiated timeout 10000 for client /10.153.80.8:50480
2014-07-25 22:10:08,922 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@863] - got auth packet /10.153.80.8:50480
2014-07-25 22:10:08,923 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@897] - auth success /10.153.80.8:50480
2014-07-25 22:10:08,934 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@354] - Exception causing close of session 0x247684586e70006 due to java.io.IOException: Len error 1530747
2014-07-25 22:10:08,934 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.153.80.8:50480 which had sessionid 0x247684586e70006

... ...

2014-07-26 02:22:59,627 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.153.80.18:60588
2014-07-26 02:22:59,629 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@832] - Client attempting to renew session 0x2476de7c1af0002 at /10.153.80.18:60588
2014-07-26 02:22:59,629 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@595] - Established session 0x2476de7c1af0002 with negotiated timeout 10000 for client /10.153.80.18:60588
2014-07-26 02:22:59,630 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@863] - got auth packet /10.153.80.18:60588
2014-07-26 02:22:59,630 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@897] - auth success /10.153.80.18:60588
2014-07-26 02:22:59,648 [myid:3] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@354] - Exception causing close of session 0x2476de7c1af0002 due to java.io.IOException: Len error 1649043
2014-07-26 02:22:59,648 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.153.80.18:60588 which had sessionid 0x2476de7c1af0002
{code}


  , Passing "-Djute.maxbuffer=<bytes>" in the startup scripts environment (in /etc/hadoop/conf/yarn-env.sh or  /etc/default/hadoop-yarn-resourcemanager) to the YARN_RESOURCEMANAGER_OPTS variable does the trick. It's picked up by the RM binary and effective., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12658387/YARN-2368.patch
  against trunk revision aa92b76.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 5 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/6904//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/6904//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6904//console

This message is automatically generated., Thanks for reporting this, [~guoleitao]. The jutebuffer size depends on the number of applications users want to store in the state-store. I believe YARN-2962 is a better fix. , It is duplicate as YARN-2962. Close this as duplicate and we can discuss the issue there.]