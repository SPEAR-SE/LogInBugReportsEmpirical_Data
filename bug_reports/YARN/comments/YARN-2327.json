[seems running NTP service is a best practice here?, I'm not sure if we can/should be doing much more than recommend NTP and setting time zones properly (if a host has it's TZ env var wrong then it can appear to be hours out, even if it is in sync). NTP sets it host-wide, calculates local clock drift and compensates for it, and has good availability.

FWIW Ant's diagnostics method not only prints out TZ info, it looks at the temp dir and compares its clock with the local time, as the situation "NAS filestore in different time to host" can cause chaos with dependency logic

{code}

-------------------------------------------
 Temp dir
-------------------------------------------
Temp dir is /var/folders/57/xyts0qt105z1f1k0twk6rd8m0000gq/T/
Temp dir is writeable
Temp dir alignment with system clock is -819 ms

-------------------------------------------
 Locale information
-------------------------------------------
Timezone Greenwich Mean Time offset=3600000

{code}

For Hadoop, the problem will be worst in VMs, as their clocks will be jerky and may even go backwards if a VM is moved to another physical host. NTP isn't so good there as its a a use case it doesn't expect.

Ignoring that, the real risk is "ops think a machine is syncing its clock with NTP but isn't". I've seen this happen.

We may want hadoop to help catch that by looking at clocks across a cluster and warning if some is "significantly" off. This could be done with the RM catching the times of hosts when they report in -and flagging when one is beyond a configured threshold. Then it'll be left to that ops team to fix it however they do it in the cluster.

, Building on what Steve said, is this something that would better belong in a monitoring tool like Ganglia or Ambari? Within Hadoop, I think logging a warning for egregious errors like a negative time is good, but I don't think we should do much more than that., As YARN supports more services, this is going to rise in priority.  I agree with steve that it may be hard to fix it in Hadoop, but I think YARN needs to monitor clock skew, which it can do piggybacked on its heartbeat RPC.  That would allow centralized reporting / health checks.  One could argue this belongs in Ambari, shrug.  Depends on how you see the YARN and Ambari missions.

It would also seem that either YARN or ambari should be able to nanny lightweight processes that run on each node (either continuously or periodically).  That would provide a mechanism for running various setup and health checks, etc.  That could be useful in addressing this issue.

, I agree with Andrew and Steve here.  It probably makes sense to have somebody monitor the clock skew between nodes, and warn if it gets too high.

It's worth pointing out that we have very carefully avoided depending on synchronized clocks in HDFS and MR.  If YARN wants to use local clocks to give an approximation of task runtime, that's fine, but we should not depend on too much accuracy there. NTP has its limits.

I think it makes sense to make YARN have its NodeManagers ping back periodically, and complain if their local clocks are too far off (probably we want a granularity of minutes here...)  It fits in well with the other resources YARN is managing, and would allow people to easily diagnose incorrect task runtimes., I'm afraid each component may more or less have made the assumption that clock is synchronized across the cluster. Unfortunately,  the problem that made us be aware of this issue came from MR: an elapsed time that based the the diff of the timestamps on two hosts became negative. Having a try with a cluster whose hosts are completely out of sync in terms of clock may give us the idea how bad the situation is.

According to the prior discussion, it seems to be good to have a mechanism to check the clocks across a cluster, report unacceptable asynchronization, doesn't it? On the other hand, it is arguable whether this mechanism should be part of Hadoop or other cluster monitoring tools.

Anyway, IMHO, it's good to at least let users be aware of the clock synchronization assumption (via jira, mailing list or etc.), and the risks of being out of sync, such that they can be more careful about this factor when setting up a cluster., bq. I'm afraid each component may more or less have made the assumption that clock is synchronized across the cluster. 

Can you explain which components make this assumption, and what happens if it's violated?  So far the only example presented is statistics in YARN., Agreed. Doing math between server clocks is an anti-pattern to be avoided.
We should file JIRAs to fix any instances of that , because the result is
not a usable number given the error bars today.

On Wednesday, July 16, 2014, Colin Patrick McCabe (JIRA) <jira@apache.org>



-- 
---
E14 - typing on glass
, I like some idea of publishing and reporting this. node state is something you could run as a YARN app, do some tracking over time and look for oddities.

what would help there and elsewhere is if the RM would publish its time info via the REST API, as the reference clock,, bq. Can you explain which components make this assumption, and what happens if it's violated? So far the only example presented is statistics in YARN.

Not sure about the HDFS part, but the given example in the description is the problem that happens to MR (see YARN-2251 for more details). For YARN side, at least, NodeHealthStatus#lastHealthReportTime is acquired at NM, has been reported to RM, and will further be sent to AM. If the clock is out of sync, RM and AM will get a incorrect time about the NM health status. It may be more other problematic cases or not. To see what really breaks, we'd better to setup a cluster with asynchronous clocks.
, Let's move this to YARN since that's where the issue is (and the solution too, hopefully.) :)

Also, YARN != MR.  YARN runs a lot of other stuff., YARN itself depends on clock sync today for token-expiry to work. RM doles out a container-token at time T, and AM hands this off to the NM which then makes sure that {{T + token-expiry-interval (defaulting to 10mins) <= current-time}}.

Clearly this assumes that the clocks are synchronized. We can continue to assume the same given the assumption's pervasive nature and warn accordingly. If that is not acceptable, we can slightly improve this to only assume that the clocks proceed at the same rate if not out right synchronized., Thanks, Vinod, that's pretty interesting.  It sounds like this should be documented somewhere in our upstream docs.

It would be interesting to think of a way to avoid this clock dependency in the RM tokens, but I'm not sure if I have any good ideas there.  If we could limit it to assuming that the clocks proceeded at similar rates as you suggested, that would be good... I read a presentation once about attacking NTP servers and clients, so that could be a weak point if we rely on it for security., I'm not sure we should get into this. I would rely on the assumption that NTP is properly configured, including authentication to avoid attacks there., bq. I'm not sure we should get into this. I would rely on the assumption that NTP is properly configured, including authentication to avoid attacks there.

Do we document anywhere that you should use authentication with NTP?  If not, we should add that.

In any case, this JIRA is just about YARN warning about the problem, not about changing auth (as far as I understand)., fair enough, warning is good. BTW, why is this a YARN thing? shouldn't apply this to HDFS as well? (ie DTs), Delegation Tokens are only verified on the servers (NN or RM), so this issue doesn't happen there. The right analogy is block-tokens, which have the same issue.]