[[~ywskycn] Thank you for discussion.

Adding his point also in comments
Set MaxResource equal to cluster capacity (instead of <0,0>) is not practical in real clusters. For example, if we only have two queues, one (Q1) for ad-hoc jobs, and another one (Q2) for production jobs. If we let Q1 and Q2 take over the entire cluster, one possible here: at a given time, Q1 takes 100%, but we have some critical production jobs coming in to Q2. In that case, Q2 needs to wait a while for preempting some resources from Q1. It takes some times. So mostly, we don't set the maximum resource for each queue to 100%, but 70-80% instead, to leave some space.,  Hi [~kasha] any comments in the same. 

Seems in this case the pre-emption logic will not work at all. Only when preemption threshold to really low value the same will work . , I see this as a duplicate or closely related to YARN-3405. [~bibinchundatt] - are you able to try out the patch there and see if it solves the issue for here. I  ll try and get to YARN-3405 later this week. , [~kasha] seems related to YARN-3405 .Will try the patch soon. Would be great if  YARN-3405 gets resolved., Hi [~kasha] Thank you for looking into the same . But YARN-3405 also doesn't change *shouldAttemptPreemption()*  So the primary check of threshold will happen . And subqueue Q1-1 is not preempted since its below threshold  and Q1-2 will starve for resource., Hi [~bibinchundatt]
preemptionUtilizationThreshold default value is 0.8.

As per your scenario, allocated MB is 4Gb and the Total cluster is 10 GB, hence the factor will come as 0.4.

{code}
  private boolean shouldAttemptPreemption() {
    if (preemptionEnabled) {
      return (preemptionUtilizationThreshold < Math.max(
          (float) rootMetrics.getAllocatedMB() / clusterResource.getMemory(),
          (float) rootMetrics.getAllocatedVirtualCores() /
              clusterResource.getVirtualCores()));
    }
    return false;
  }
{code}

So this check will not pass. And this behavior is expected, You could configure a lesser value for preemptionUtilizationThreshold, to kick the preemption for the level2 queues. Preferably we would like to kick preemption when the cluster usage is above some good amount of percentage (80% as per default). , Closing this issue as per comments, [~sunilg] . Thnk you for looking into the issue.]