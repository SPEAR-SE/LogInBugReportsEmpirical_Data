[CC [~andrew.wang] [~vinodkv], [~jlowe]., At this point we just need to get 3.0.0 out, if there are compat issues let's address them in 3.0.1., I was not able to reproduce this with a job using 2.8.3 jars on the cluster, either with a 3.0 job submitter (although I had to workaround HDFS-12920) or with a 2.8.3 job submitter (which just worked). Is it specific to something in 2.9 jars?
, Yes. I also tried with 2.8.3
{{yarn --config /Users/sunilgovindan/install/release/hadoop-2.8.3/etc/hadoop/ jar /Users/sunilgovindan/install/release/hadoop-2.8.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.3-tests.jar sleep  -m 50 -r 1 -mt 100}}. This could run with 3.0 cluster., I was trying tarball of 2.9.0. Let me try 2.8.3 instead to see any differences., No different. Still hit the same issue. Below is my configuration of mapred-site.xml:
{noformat}
<configuration>

  <property>
    <name>mapreduce.job.user.name</name>
    <value>${user.name}</value>
  </property>

  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

  <property>
    <name>mapreduce.application.framework.path</name>
    <value>hdfs:/mapreduce/2.8/hadoop-2.8.3.tar.gz#mr-framework</value>
  </property>

  <property>
    <name>mapreduce.application.classpath</name>
    <value>$PWD/mr-framework/hadoop-2.8.3/share/hadoop/mapreduce/*,$PWD/mr-framework/hadoop-2.8.3/share/hadoop/mapreduce/lib/*,$PWD/mr-framework/hadoop-2.8.3/share/hadoop/common/*,$PWD/mr-framework/hadoop-2.8.3/share/hadoop/common/lib/*,$PWD/mr-framework/hadoop-2.8.3/share/hadoop/yarn/*,$PWD/mr-framework/hadoop-2.8.3/share/hadoop/yarn/lib/*,$PWD/mr-framework/hadoop-2.8.3/share/hadoop/hdfs/*,$PWD/mr-framework/hadoop-2.8.3/share/hadoop/hdfs/lib/*</value>
  </property>

</configuration>
{noformat}
Do I miss something?, [~sunilg], my test here is using MR over distributed cache which is prerequisite for supporting rolling upgrade just as my above configuration shows. I think you were testing a different scenario?, I just re-tested running a job based on the 2.8.3 tarball with both a 3.0.0 job client and a 2.8.3 job client on a 3.0.0 cluster, and both succeeded.

Are there any features that are enabled on the 3.0 cluster that aren't enabled by default?  I'm running the ResourceManager with the same configs I use for 2.x, so I haven't enabled anything in 3.0 that needs to be explicitly configured.  I suspect the issue isn't going to be what's in mapred-site.xml but rather what's in yarn-site.xml.

Even so, I can't readily explain how a 2.x application master is somehow requesting -1 for a memory size.  It's not like we added this field to the application master protocol in 3.0, right?
]