[Thanks [~shahrs87] for reporting this. One doubt, Which ResourceCalculator is used here? Is it Dominant RC., I remember seeing that this was fixed in branch-2 by some of the capacity scheduler work for labels.

I thought this might be fixed by https://issues.apache.org/jira/browse/YARN-3243 but that is included.  

This might be fixed as part of https://issues.apache.org/jira/browse/YARN-3361 which is probably to big to backport totally.

[~leftnoteasy]  Do you remember this issue?

Note that it also shows up in capacity scheduler UI as root queue going over 100%.  I remember when I was testing YARN-3434 it wasn't occurring for me on branch-2 (2.8) and I thought it was one of the above jiras that fixed., bq. Thanks Rushabh S Shah for reporting this. One doubt, Which ResourceCalculator is used here? Is it Dominant RC.
yes., [~tgraves]/[~shahrs87], 

I think the case could happen when container reservation interacts with node disconnect, one example is:
{code}
A cluster has 6 nodes, each node has 20G resource, and usage is
N1-N4, are all used
N5-N6, both of them are used 10G.
An app ask 15G container, assume it is reserved at N5, so total used resource = 20G * 4 + 10G * 2 + 15G (just reserved) = 115G
Then, N6 disconnected, now cluster resource becomes 100G, and used resource = 105G.
{code}

I've just checked fixes, YARN-3361 doesn't have related fixes. And currently we don't have a fix for above corner case. 

Another problem is caused by DRC, from 2.7.1, we have set availableResource = max(availableResource, Resources.none()). 
{code}
    childQueue.getMetrics().setAvailableResourcesToQueue(
        Resources.max(
            calculator, 
            clusterResource, 
            available, 
            Resources.none()
            )
        );
{code}

But if you're using DRC, if a resource has availableMB < 0 and availableVCores > 0, it could report such resource > Resources.None(). We may need to fix this case as well.

Thoughts?, Hi [~wangda], for the first case, should we check availableResource of root queue when a node gets removed? Then if available memory is negative, we proceed to unreserve some resource until the available memory of root queue becomes positive. , [~wangda] I opened a jira YARN-4067 regarding drc issue in 2.7.1, and post a patch., [~leftnoteasy], sorry, was tagging to the wrong user name, Hi [~lichangleo],
I think the comprehensive fix is, when we allocating from reserved container, we should check limits of queue hierarchy as well. Currently the logic is reserving and not check limits, which seems not correct to me.

If we found queue limits will be violated if we allocate the reserved container, we should drop the reservation. Thoughts?

Thanks,
Wangda, [~leftnoteasy] thanks for sharing ideas! However, correct me if I am wrong, in your suggested way, the negative available memory could still stay for a while right ? (after a node is disconnected and before we try to allocate that reserved). Is it too expensive to check queue limits for every node disconnect? Or is it possible to make reserved container usage not count toward calculation of available memory?, Looks like this is a duplicate of YARN-3933. ]