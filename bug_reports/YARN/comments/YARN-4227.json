[Patch to skip the container release if the node is null, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  17m 20s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   8m  0s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  10m 13s | There were no new javadoc warning messages. |
| {color:red}-1{color} | release audit |   0m 16s | The applied patch generated 1 release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 49s | The applied patch generated  1 new checkstyle issues (total was 71, now 72). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 31s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 34s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 27s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |  55m 56s | Tests passed in hadoop-yarn-server-resourcemanager. |
| | |  96m 10s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12765128/YARN-4227.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 5f6edb3 |
| Release Audit | https://builds.apache.org/job/PreCommit-YARN-Build/9357/artifact/patchprocess/patchReleaseAuditProblems.txt |
| checkstyle |  https://builds.apache.org/job/PreCommit-YARN-Build/9357/artifact/patchprocess/diffcheckstylehadoop-yarn-server-resourcemanager.txt |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/9357/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/9357/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/9357/console |


This message was automatically generated., +1 (nonbinding)

The only comment I have is whether it's worth having a unit test for this corner case.  It's very peculiar to the implementation., Updated patch with a test (thanks to [~rchiang] for the base test)
The release audit and check style are not caused by this patch, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  23m 38s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:red}-1{color} | javac |   4m 59s | The patch appears to cause the build to fail. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12766264/YARN-4227.2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 0f5f984 |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/9420/console |


This message was automatically generated., Something wrong on the build system: the patch builds without an issue locally and the error that causes the build to fail is:
{code}
Applied patch hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java cleanly.
 .../scheduler/fair/FairScheduler.java              |    8 ++-
 .../scheduler/fair/TestFairScheduler.java          |   60 ++++++++++++++++++++
 2 files changed, 67 insertions(+), 1 deletion(-)
rm: missing operand
Try 'rm --help' for more information.
{code}, resubmit patch with slight comment change, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  19m 15s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   9m 53s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  11m 36s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 26s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 56s | The applied patch generated  1 new checkstyle issues (total was 71, now 72). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 44s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 37s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 44s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:red}-1{color} | yarn tests |  57m 48s | Tests failed in hadoop-yarn-server-resourcemanager. |
| | | 104m  2s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched |
|   | hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps |
|   | hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12766280/YARN-4227.3.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 5b6bae0 |
| checkstyle |  https://builds.apache.org/job/PreCommit-YARN-Build/9423/artifact/patchprocess/diffcheckstylehadoop-yarn-server-resourcemanager.txt |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/9423/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/9423/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf901.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/9423/console |


This message was automatically generated., +1 (nonbinding) after fixing the checkstyle issue (add 2 spaces to FairScheduler.java, line 862)

The three failed unit tests pass in my tree.
, Updated with a fix for the checkstyle issue found in the patch processing, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  20m 26s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   8m  6s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  10m 51s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 25s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 48s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 32s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 35s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 29s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:red}-1{color} | yarn tests |  54m 30s | Tests failed in hadoop-yarn-server-resourcemanager. |
| | |  98m 47s | |
\\
\\
|| Reason || Tests ||
| Timed out tests | org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup |
|   | org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestNodeLabelContainerAllocation |
|   | org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerNodeLabelUpdate |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12766462/YARN-4227.4.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 40cac59 |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/9434/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/9434/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/9434/console |


This message was automatically generated., The previous statement should also be updatedÂ to handle a null node to avoid a NPE inside it
{noformat}application.unreserve(rmContainer.getReservedPriority(), node);{noformat}
This may need to still process some portion FSAppAttempt#unreserveInternal instead of skipping the entire processing.

The test seems ok. Should we rename blacklist -> remove?

Overall the fix looks ok. Just another bug which indicates until we restructure the code we will have to keep adding bandaids., Is it possible the root cause of this issue is YARN-3675? I think YARN-3675 may cause this issue. If we can get the complete logs for container_1436927988321_1307950_01_000012, we may confirm it. Once the node is removed, all the containers allocated on the node are supposed to be killed. The race condition at YARN-3675 may cause a container allocated on a just removed node.
, I dug through the logs and it looks similar to [YARN-3675] however the time lag is far larger and there are multiple allocations happening in between the removal of the node and the allocation. These allocations happen on other nodes. I would have expected that the node removal would have been processed in that time. 

{code}
2015-09-11 23:47:52,589 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_e11_1438912039098_1027475_01_000012 of capacity <memory:6144, vCores:1> on host XXXX.com:8041, which currently has 0 containers, <memory:0, vCores:0> used and <memory:221184, vCores:36> available, release resources=true
2015-09-11 23:47:52,589 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application attempt appattempt_1438912039098_1027475_000001 released container container_e11_1438912039098_1027475_01_000012 on node: host: XXXX.com:8041 #containers=0 available=221184 used=0 with event: KILL
2015-09-11 23:47:52,596 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Removed node XXXX.com:8041 cluster capacity: <memory:17190912, vCores:2798>
2015-09-11 23:47:52,596 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2015-09-11 23:47:52,598 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e11_1438912039098_1027474_01_002591 Container Transitioned from RUNNING to COMPLETED
2015-09-11 23:47:52,598 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Completed container: container_e11_1438912039098_1027474_01_002591 in state: COMPLETED event:FINISHED
2015-09-11 23:47:52,598 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=my_user	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1438912039098_1027474	CONTAINERID=container_e11_1438912039098_1027474_01_002591
2015-09-11 23:47:52,598 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container container_e11_1438912039098_1027474_01_002591 of capacity <memory:6144, vCores:1> on host YYYY.com:8041, which currently has 5 containers, <memory:24576, vCores:5> used and <memory:159744, vCores:25> available, release resources=true
2015-09-11 23:47:52,598 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application attempt appattempt_1438912039098_1027474_000001 released container container_e11_1438912039098_1027474_01_002591 on node: host: YYYY.com:8041 #containers=5 available=159744 used=24576 with event: FINISHED
2015-09-11 23:47:52,599 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e11_1438912039098_1027474_01_002648 Container Transitioned from NEW to ALLOCATED
2015-09-11 23:47:52,599 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=my_user	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1438912039098_1027474	CONTAINERID=container_e11_1438912039098_1027474_01_002648
2015-09-11 23:47:52,600 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_e11_1438912039098_1027474_01_002648 of capacity <memory:6144, vCores:1> on host YYYY.com:8041, which has 6 containers, <memory:30720, vCores:6> used and <memory:153600, vCores:24> available after allocation
2015-09-11 23:47:52,600 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Null container completed...
2015-09-11 23:47:52,601 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e11_1438912039098_1027474_01_002649 Container Transitioned from NEW to ALLOCATED
2015-09-11 23:47:52,601 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=my_user	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1438912039098_1027474	CONTAINERID=container_e11_1438912039098_1027474_01_002649
2015-09-11 23:47:52,601 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_e11_1438912039098_1027474_01_002649 of capacity <memory:6144, vCores:1> on host YYYY.com:8041, which has 9 containers, <memory:51712, vCores:9> used and <memory:169472, vCores:27> available after allocation
2015-09-11 23:47:52,604 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e11_1438912039098_1027474_01_002650 Container Transitioned from NEW to ALLOCATED
2015-09-11 23:47:52,604 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=my_user	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1438912039098_1027474	CONTAINERID=container_e11_1438912039098_1027474_01_002650
2015-09-11 23:47:52,604 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container container_e11_1438912039098_1027474_01_002650 of capacity <memory:6144, vCores:1> on host XXXX.com:8041, which has 1 containers, <memory:6144, vCores:1> used and <memory:215040, vCores:35> available after allocation
{code}

Getting a list of nodes that was not updated 8ms after the removal seems to point to an issue in node processing caused by far to coarse locking. I am thus not so sure that we can attribute this to the same race condition (I know I am being really careful here). The log given above from timestamp 47:52,589 to 47:52,604 is complete no lines removed. 

BTW: this example might have been taken from a different application since I have seen this a number of times over the last weeks , Currently I didn't find other code path except YARN-3675, which can cause this issue.
bq.  there are multiple allocations happening in between the removal of the node and the allocation. These allocations happen on other nodes.
This won't help. We need find the log which shows the node removal and allocations on the same node.
To confirm this issue, we need find the logs which show what node container_1436927988321_1307950_01_000012 was allocated on and when that node used by container_1436927988321_1307950_01_000012 was removed., After discussing internally we have decided to close this jira for now since the real cause of the issue seems to be the same race as described in YARN-3675.

we'll reopen if the issue still occurs with that fix in place, I'm seeing a similar issue on what's roughly branch-2 (CDH 5.11.0), with the error being:

{code}
2017-06-27 16:32:39,381 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[Preemption Timer,5,main] threw an Exception.
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:687)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread$PreemptContainersTask.run(FSPreemptionThread.java:230)
        at java.util.TimerThread.mainLoop(Timer.java:555)
        at java.util.TimerThread.run(Timer.java:505)
{code}

This error, which causes the FSPreemptionThead to die, and thereby crashes the RM, seems to be correlated with NodeManagers being marked unhealthy due to lack of local disk space during large shuffles. I haven't confirmed, but presumably the unhealthy nodes are removed while we're waiting for the lock, and no longer exist when we call {{releaseContainer}}.

I'm curious as to whether others are seeing this as well on recent versions, in which case maybe this is worth reopening?, [~Steven Rand] looks like you are correct, lets re-open this issue.
The {{containerCompleted}} call we do just before we release the container on the node is synchronised which means that we could have been waiting there for just long enough to cause the node to become {{null}}
The patch will need a rebase due to all the pre-emption changes that have gone in and I'll upload a new patch soon., [~wilfreds], I can rebase the patch if you like. It seems to be working quite nicely by the way -- we applied it to a cluster which was periodically exhibiting this problem and haven't seen it since., Sorry, I was mistaken when I said the patch attached to this JIRA prevents the NPE. Unfortunately the FSPreemptionThread accesses nodes at multiple points, each of which is a new opportunity for the race condition to occur and cause an NPE. It seems impractical to wrap each node access in an {{if (node != null)}} block, though admittedly I don't have any better ideas right now. Are there alternate solutions that I'm failing to consider that would prevent the race condition from happening? Happy to submit a patch if anyone has ideas., Maybe we could have ClusterNodeTracker#getNode check to see if {{nodes.get(nodeId)}} returns null, and if it does, instead log a warning and return a special subclass of {{FSSchedulerNode}} that overrides all methods to be no-ops? I know it's not pretty, but the advantage is that we don't have to check for null in a bunch of different places.

Also, after looking more closely, the particular NPE that I'm seeing turns out to have been fixed by YARN-6432. However, I still think that we want a generic solution so as to be protected against access of unhealthy nodes going forward., I ran into this again and the current point of failure is still the same point in the code just a different code path to get there:
{code}
ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionhandler: Thread Thread[Preemption Timer,5,main] threw an Exception.
java.lang.NullPointerException
  at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:699)
  at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread$PreemptContainersTask.run(FSPreemptionThread.java:230)
  at java.util.TimerThread.mainLoop(Timer.java:555)
  at java.util.TimerThread.run(Timer.java:505)
{code}

In the log we also had the entry for an unknown host:
{code}
ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.VisitedResourceRequestTracker: Found ResourceRequest for a non-existant node/rack named <hostname>
{code}
Which shows that there was a node that used to be present but is no longer

[~Steven Rand]: The ClusterNodeTracker is used for all schedulers. We can not change what {{ClusterNodeTracker#getNode}} returns without impacting all schedulers and thus affecting a huge amount of code. Adding more checks to make sure the node is not null is not needed. This seems to be the last place in which we do not handle a removed node correctly.

Rebased the fix to trunk. , | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 23s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 33s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 26s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 27s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 60m 39s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}106m 59s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.TestOpportunisticContainerAllocatorAMService |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | YARN-4227 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12903016/YARN-4227.5.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 1ac53727bf56 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d62932c |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/18994/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/18994/testReport/ |
| Max. process+thread count | 835 (vs. ulimit of 5000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/18994/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Tests pass locally
Test failures also can not be related because they are hard coded to only test the capacity scheduler., Minor nit: The LOG.debug() calls for skipping containers aren't wrapped with LOG.isDebugEnabled()., Updated patch with the isDebugEnabled checks, | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  6m 39s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 20s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 22s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 31s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 60m  6s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 22s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}108m  0s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | YARN-4227 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12905009/YARN-4227.006.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux f7617b2b3e93 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 01f3f21 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/19138/testReport/ |
| Max. process+thread count | 836 (vs. ulimit of 5000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/19138/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, +1.  LGTM.

I'll commit this upstream soon., Committed to trunk and branch-2.

Thanks [~wilfreds] for the contribution!  Thanks [~adhoot] and [~zxu] for the earlier patch reviews and thanks [~Steven Rand] for your reviews of the final patch version!, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13463 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13463/])
YARN-4227. Ignore expired containers from removed nodes in (rchiang: rev 59ab5da0a0337c49a58bc9b2db9d1a89f4d5b9dd)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
]