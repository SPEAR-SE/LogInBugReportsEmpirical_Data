[During *RMAppRecoveredTransition* in RMAppImpl, may be we can check recovered app queue (can get this from submission context) is still a valid queue?
If this queue not present, recovery for that app can be made failed, and may be need to do some more RMApp clean up. Sounds doable?, I think it should doable, queue of application missing should not make RM failure to start., I am working on it, [~lichangleo], thanks for working on it!
Looking forward your patch., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12660878/jira2308.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4581//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4581//console

This message is automatically generated., [~lichangleo],
Thanks for working on this,
I took a quick scan at your patch, I think the general approach should be fine. Some minor suggestions:
1) 
{code}
+    if (application==null) {
+      LOG.info("can't retireve application attempt");
+      return;
+    }
{code}
Please leave a space before and after "==",
Use LOG.error instead of info

2) Test code
2.1
bq. +    System.out.println("testing queue change!!!");
Remove this plz,

2.2
{code}
+    conf.setBoolean(CapacitySchedulerConfiguration.ENABLE_USER_METRICS, true);
+    conf.set(CapacitySchedulerConfiguration.RESOURCE_CALCULATOR_CLASS,
{code}
We may not need this too

2.3
{code}
+    // clear queue metrics
+    rm1.clearQueueMetrics(app1);
{code}
Also this

2.4
It's better to wait and check for app state transition to Failed after it rejected

2.5
I think this test isn't work-preserving restart specific problem, it's better to place the test in TestRMRestart

Please let me know if you have any comment on them.

Thanks,
Wangda


, updated patch according to Wangda's advice, [~wangda]
I have updated my patch according to your suggestion. The patch is uploaded.

Thanks, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12661016/jira2308.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4590//console

This message is automatically generated., patch updated according to Wangda's suggestion, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12661044/jira2308.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4591//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4591//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12661066/jira2308.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4593//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4593//console

This message is automatically generated., [~lichangleo],
Thanks for updating, I think following line is not necessary
bq. +    conf.setBoolean(YarnConfiguration.RM_WORK_PRESERVING_RECOVERY_ENABLED, true);
I just tried in my local, remove it should be fine. Besides this, LGTM, +1.

[~zjshen], do you have take a look at this?

Thanks,
Wangda, [~wangda] 
I don't think this conf set should be necessary either, but if I don't include this my unit test will not fail on NPE somehow., Investigated into the problem: when submitting the app to a non-existing queue, the app is going to be rejected by CS. It works fine in a normal submission, because addAppAttempt happens after RMApp enters ACCEPTED, when addApp has already been executed successfully. However, in the recover mode, addAppAttempt is triggered independent of the result of addApp. At this moment, app doesn't exist in CS as it has been rejected, while addAppAttempt assumes it should exist, and result in NPE.

The fix makes sense to more. Some additional comments:

bq. + conf.setBoolean(YarnConfiguration.RM_WORK_PRESERVING_RECOVERY_ENABLED, true);

It should be true to imitate the failure case in the description, right? According AttemptRecoveredTransition, if isWorkPreservingRecoveryEnabled = true, AppAttemptAddedSchedulerEvent will not scheduled. However, whether AppAttemptAddedSchedulerEvent is scheduled or not, the app should get rejected finally, shouldn't it? What was the test failure when isWorkPreservingRecoveryEnabled = false?, Hi Zhijie,
Thanks for looking at this issue,
bq. It should be true to imitate the failure case in the description, right? According AttemptRecoveredTransition, if isWorkPreservingRecoveryEnabled = true, AppAttemptAddedSchedulerEvent will not scheduled. However, whether AppAttemptAddedSchedulerEvent is scheduled or not, the app should get rejected finally, shouldn't it? What was the test failure when isWorkPreservingRecoveryEnabled = false?
I revisit the code again, I found if we set RM_WORK_PRESERVING_RECOVERY_ENABLED=false, we should re-register AM to get this failure. I think set RM_WORK_PRESERVING_RECOVERY_ENABLED=true in test should be enough for this fix.

Wangda, Reject an app that's at ACCEPTED state doesn't seem semantically right to me.
{code}
+    .addTransition(RMAppState.ACCEPTED, RMAppState.FINAL_SAVING,
+        RMAppEventType.APP_REJECTED,
+        new FinalSavingTransition(new AppRejectedTransition(), RMAppState.FAILED))
{code}

I think we should catch exception in following code and return Failed directly.
{code}
      // Add application to scheduler synchronously to guarantee scheduler
      // knows applications before AM or NM re-registers.
      app.scheduler.handle(new AppAddedSchedulerEvent(app.applicationId,
        app.submissionContext.getQueue(), app.user, true));
{code}, Actually, app is rejected at AppAddedSchedulerEvent, but as I mentioned above AppAttemptAddedSchedulerEvent is scheduled regardless the app is added to CS or not. In fact, under the recover mode, RMApp will enter ACCEPTED regardless the app is added or not as well.

The thorough fix might be moving recovered APP to another state, and wait for the event from CS to move it ACCEPTED, and recover the attempts, including scheduling AppAttemptAddedSchedulerEvent. My feeling is that it is over-kill if we want to this single race condition. Thoughts?

bq.  I think set RM_WORK_PRESERVING_RECOVERY_ENABLED=true in test should be enough for this fix.

RM_WORK_PRESERVING_RECOVERY_ENABLED=true reflects the failure case in the description, but I'm wondering why RM_WORK_PRESERVING_RECOVERY_ENABLED=false, the test is going to fail. App will anyway be rejected, won't it?, bq. RMApp will enter ACCEPTED regardless the app is added or not as well.
That's what I meant. RMApp can choose to enter FAILED state directly and no need to add attempt any more., bq. I think we should catch exception in following code and return Failed directly.
Currently the CapacityScheduler will create AppRejectedEvent when found queue not existed while recovering or submit.
{code}
    if (queue == null) {
      String message = "Application " + applicationId + 
      " submitted by user " + user + " to unknown queue: " + queueName;
      this.rmContext.getDispatcher().getEventHandler()
          .handle(new RMAppRejectedEvent(applicationId, message));
      return;
    }
{code}
We cannot catch exception here, because now exception throw:
{code}
      // Add application to scheduler synchronously to guarantee scheduler
      // knows applications before AM or NM re-registers.
      app.scheduler.handle(new AppAddedSchedulerEvent(app.applicationId,
        app.submissionContext.getQueue(), app.user, true));
{code}

bq. That's what I meant. RMApp can choose to enter FAILED state directly and no need to add attempt any more.
It will not add attempt here, because it will get rejected directly

bq. RM_WORK_PRESERVING_RECOVERY_ENABLED=true reflects the failure case in the description, but I'm wondering why RM_WORK_PRESERVING_RECOVERY_ENABLED=false, the test is going to fail. App will anyway be rejected, won't it?
I've tried this in my local again, it can get passed. Set RM_WORK_PRESERVING_RECOVERY_ENABLED=false is enough to cover what we want to verify., Typo:
bq. We cannot catch exception here, because now exception throw:
Should be "We cannot catch exception here, because *no* exception throw:"
, bq. We cannot catch exception here, because no exception throw
Though we can throw exception if isAppRecovering is true, that'll a bit complicates the logic. we can add comments to the newly added transition to explain the scenario., Looked at this again, I think the solution mentioned by [~sunilg] is reasonable:
bq. During RMAppRecoveredTransition in RMAppImpl, may be we can check recovered app queue (can get this from submission context) is still a valid queue? If this queue not present, recovery for that app can be made failed, and may be need to do some more RMApp clean up. Sounds doable?
We can check if the queue exists on recovery. If not, directly return FAILED state and no need to add the attempts anymore.  Thoughts ?
, bq. We can check if the queue exists on recovery. If not, directly return FAILED state and no need to add the attempts anymore. Thoughts ?

If we are doing this, the RMAppAttempt will show the *in-correct* state in ApplicationHistoryStore, From the description, it seems like this is caused by removing some queues from Capacity Scheduler configuration file and then restarting the ResourceManager. Removing queues is *NOT* a supported feature yet in CS. So, we can do one of the following
 - Implement removal of a queue in CS separately
 - Or just do error handling so that on restart, scheduler can somehow validate that queues are removed and err-out.

Cancelling the patch., FTR, I propose we do the second option above for now here.., Thanks for [~vinodkv] for jumping in,
Rethink about this, I think as Vinod said, if you want to completely solve this issue, we should officially support removing queue in CS. 
+1 for RM better logging the error and exit for now.

Wangda, Agree, just returning application state to be FAILED doesn't completely solve the problem, as applications still keep running.  We can re-construct the list of queues from the submissionContext of each app saved in state-store.  And check whether the corresponding queue exists or not. If not, RM can error out and indicate user to add the missing queue. [~lichangleo], does this approach look good to you ?, +1 for this approach. I also feel that returning application state as
FAILED is not complete solution.


, thanks for the collective thought about this and all suggestions. I will improve my solution. , Here's a patch which does what I believe the consensus was - detects the condition and throws a descriptive exception which will not be caught / will result in rm exit, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12674250/YARN-2308.0.patch
  against trunk revision d3d3d47.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5367//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-YARN-Build/5367//artifact/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5367//console

This message is automatically generated., The release audit is not related to this change, it appears to be related to the yarn registry change., Add explicit logging before exception throw, just in case the exception message is somehow swallowed., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12674554/YARN-2308.1.patch
  against trunk revision 1770bb9.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5376//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5376//console

This message is automatically generated., The unit test failure does not appear to be related to the changes, I was able to run 

org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart

successfully on my box with the changes.  (And, I have noticed that this test seems to fail sporadically from time to time, having nothing to do with the change at hand).

[~jianhe] can you take a moment to review the patch?, patch looks good to me, [~lichangleo] , thanks for your previous work ! and thanks [~cwelch] !, just realized that we probably should do the same for FairScheduler too, [~kasha], do you think so ?, Thanks for the nudge, Jian. Filed YARN-2684 to unblock this JIRA. , thanks Karthik. committing this., FAILURE: Integrated in Hadoop-trunk-Commit #6253 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6253/])
YARN-2308. Changed CapacityScheduler to explicitly throw exception if the queue (jianhe: rev f9680d9a160ee527c8f2c1494584abf1a1f70f82)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java
Missing Changes.txt for YARN-2308 (jianhe: rev 178bc505da5d06d591a19aac13c040c6a9cf28ad)
* hadoop-yarn-project/CHANGES.txt
, Committed to trunk, branch-2, and branch-2.6.  thanks [~cwelch] and [~lichangleo] !, SUCCESS: Integrated in Hadoop-Yarn-trunk #711 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/711/])
YARN-2308. Changed CapacityScheduler to explicitly throw exception if the queue (jianhe: rev f9680d9a160ee527c8f2c1494584abf1a1f70f82)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java
Missing Changes.txt for YARN-2308 (jianhe: rev 178bc505da5d06d591a19aac13c040c6a9cf28ad)
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1901 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1901/])
YARN-2308. Changed CapacityScheduler to explicitly throw exception if the queue (jianhe: rev f9680d9a160ee527c8f2c1494584abf1a1f70f82)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java
Missing Changes.txt for YARN-2308 (jianhe: rev 178bc505da5d06d591a19aac13c040c6a9cf28ad)
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1926 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1926/])
YARN-2308. Changed CapacityScheduler to explicitly throw exception if the queue (jianhe: rev f9680d9a160ee527c8f2c1494584abf1a1f70f82)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java
Missing Changes.txt for YARN-2308 (jianhe: rev 178bc505da5d06d591a19aac13c040c6a9cf28ad)
* hadoop-yarn-project/CHANGES.txt
, Hi, Chang Li, as I went through the patches that you attached, previously these was 
+    if (application==null) {
+      LOG.info("can't retireve application attempt");
+      return;
+    }
but, finally, the patch merged does not have this modification. Is this updated on purpose?
What is the concern?
I am now facing one scenario, App status is Finished and AppAttempt status is null, this way when doing recover, application is null in CS and then NPE occur. I am thinking if condition "application==null" was there, the issue I meet will not occur., Hi, Chang Li, as I went through the patches that you attached, previously these was 
+    if (application==null) {
+      LOG.info("can't retireve application attempt");
+      return;
+    }
but, finally, the patch merged does not have this modification. Is this updated on purpose?
What is the concern?
I am now facing one scenario, App status is Finished and AppAttempt status is null, this way when doing recover, application is null in CS and then NPE occur. I am thinking if condition "application==null" was there, the issue I meet will not occur., Hi [~gu chi], this jira is intended to fix the NPE caused by a missing queue result from queue configuration during rm restart. I did some early work on this problem, and my initial approach is to do a null check in the exact place NPE happened in addApplicationAttempt. Craig Welch carried on with a different approach. The final patch is checking if the queue is removed and err out. I think your problem is worth firing a separate jira, also I'd like to take on the issue you mentioned. Thanks , [~gu chi], the issue you mentioned seems like already solved by YARN-2340. Could you please check?, Thx, I saw this and think not a same issue. YARN-2340 is triggered by queue stop, there will be clear clue of "Failed to submit application". My scenario is that ZK exception occurred and the appAttempt status update failed.]