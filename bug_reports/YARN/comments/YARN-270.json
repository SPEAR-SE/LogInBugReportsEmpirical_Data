[I was able to reproduce this on a smaller cluster by simulating 2800 nodes - had 720 node manager and made the heartbeat internal 250 ms (instead of 1s).  At about 400 applications the scheduler queue starts to grow.  I am still in the process of investigating what exactly is taking the time.

Note that for now we believe we have worked around it by increasing the nodemanager heartbeat interval from 1 second to 3 seconds., Thanks for filing this Thomas. IIRC, The event-handler's upper limit is about 0.6 million, somehow we only focus on number of nodes and never thought about the scaling issue with large number of applications. There are multiple solutions for this, in the order of importance:
 - Make NodeManagers to *NOT* blindly heartbeat irrespective of whether previous heartbeat is processed or not.
 - Figure out any obvious bottlenecks in the scheduling code.
 - When all else fails, try to parallelize the scheduler dispatcher., bq. Make NodeManagers to NOT blindly heartbeat irrespective of whether previous heartbeat is processed or not.
Filed YARN-275, Could we also add some additional flow control within the RM to prevent this work from getting into the event queues in the first place? Having the clients throttle on their end is important in the short term but in the long run we need a flow control strategy that can exert back pressure at all stages of the pipeline., Nathan, unfortunately, the dispatcher framework cannot exert back pressure in general, each event producer needs to control itself.

OTOH, YARN-275 is indeed a long term fix. NMs back off just like the TTs do in 1.*., It cannot exert back pressure currently, but I don't see any reason to think that it could not be added in the future.  Something as simple as setting a high water mark on the number of pending events and throttling events from incoming connections until the congestion subsides.

We have see a similar issue in the IPC layer on the AM when too many reducers were trying to download the mapper locations.  Granted this is not the same code, but it was caused by asynchronously handling events and buffering up the data so when we got behind we eventually got OOMs.  I think we will continue to see more issues as we scale up until we solve it generally, or every single client API call will have to be updated eventually to avoid overloading the system., I don't see it yet, but a comprehensive proposal can clarify what you have in mind I suppose. Please file a sub-ticket and propose what you think. Tx., break the sub-task 275 Make NodeManagers to NOT blindly heartbeat irrespective of whether previous heartbeat is processed or not. to smaller task.
1. Make RM provide heartbeat interval to NM
2. RM changes to handle NM heartbeat during overload., bq. When all else fails, try to parallelize the scheduler dispatcher

long term i think this should be the solution. we need ordering of events only for a given event type. so this should be doable and will give next level of scalability both for AM and RM, I agree that part of the fix needs to be making the scheduler parallel, but we also need a general way to apply back pressure otherwise there will always be a way to accidentally bring down the system with a DOS.  We recently saw what appears to be a very similar issue show up on an MRAppMaster.  We still don't understand exactly what triggered it, but a job that would typically take 5 to 10 mins to complete was still running 17 hours later because the queue filled up which caused the JVM to start garbage collecting like crazy which in turn made it so it could not process all of the events coming in, which made the queue fill up even more. We plan to address this in the short term by making the JVM OMM much sooner than is the default, but it is still just a band-aid on the underlying problem that unless there is back pressure there is always the possibility for incoming requests to overwhelm the system., changing this to not be a blocker since we have worked around and some of subtasks complete.]