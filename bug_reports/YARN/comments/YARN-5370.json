[It's expected behavior in the sense that the debug delay setting causes the NM to buffer every deletion task up to the specified amount of time.  100 days is a lot of time, so if there are many deletions within that period it will have to buffer a lot of tasks as you saw in the heap dump.

The debug delay is, as the name implies, for debugging.  If you set it to a very large value then, depending upon the amount of container churn on the cluster, a correspondingly large heap will be required given the way it works today.  It's not typical to set this to a very large value since it only needs to be large enough to give someone a chance to examine/copy off the requisite files after reproducing the issue.  Normally it doesn't take someone 100 days to get around to examining the files after a problem occurs. ;-)

Theoretically we could extend the functionality to spill tasks to disk or do something more clever with how they are stored to reduce the memory pressure, but I question the cost/benefit tradeoff.  Again this is a feature intended just for debugging.  I'm also not a big fan of putting in an arbitrary limit on the value.  If someone wants to store files for a few years and has the heap size and disk space to hold all that, who are we to stop them from trying?
, To solve this issue, we tried by setting yarn.nodemanager.delete.debug-delay-sec to very low value (zero second) assuming that it may clear off the existing scheduled deletion tasks. It didn't happen - basically it is not applied for the existing tasks which has been already scheduled. Then, we come to know that canRecover() method is getting called in service start, which is trying to pull the info from NM recovery directory (from local filesystem) and building this entire info in memory, which in turn, causing the problems in starting the services and consuming so much amount of memory. Then, we tried by moving the contents of NM recovery directory to some other place. From this points onwards, it was able to start smoothly and works as expected. I think showing some warnings about this high value (for ex, 100+ days) somewhere (for ex, in logs) indicating that it can cause potential crash can saving significant amount of time to troubleshoot this issue.]