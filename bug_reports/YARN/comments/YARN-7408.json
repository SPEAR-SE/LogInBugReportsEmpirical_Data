[Currently, the number of reserved containers for an application is increased proportionally according to how long it starves.
What about limiting maximum reserved resource, which an application can take.
Does this approach make sense? please correct me know if I am wrong.
Any comments or suggestions are welcome.
, I assume you are using CapacityScheduler?  The answers may change for FairScheduler as I am not as familiar with how it handles reservations.

Reservations can increase over time as the allocation request remains unsatisfied, but the amount of space that can be reserved is limited by the user and queue limits.  In other words, the user can't reserve the whole cluster unless they are allowed to use the whole cluster normally.

As for other container requests, it depends upon where these requests are coming from.  If these are other requests from the same application then the app needs to change the priority of the other requests.  The RM allocates containers in priority order, so it won't consider the other requests until the reservations are satisifed or the request is cancelled.  If the requests are coming from other apps then it could be the priority of the app relative to the other apps.  Apps ahead in the queue will get first crack at resources or we risk indefinite postponement.  Proposals to artificially limit reservations for an app also risk this same indefinite postponement if the scheduler happened to choose poorly where to place the limited number of reservations.  In a cluster with long running containers, this app may not ever run in a timely manner.

One way to achieve something close to what you are proposing is to have the problematic app run in a separate queue where you can explicitly cap the resources associated with that app, reserved or otherwise.  The app will only be able to reserve up to the queue's capacity at most.  This should work quite well, assuming the total resources required by the app is less than you are willing to allow it to reserve in its attempt to get containers.
, Hi [~jlowe].
Thank you for your comment and suggestion!
I have a question.

{quote}
Proposals to artificially limit reservations for an app also risk this same indefinite postponement if the scheduler happened to choose poorly where to place the limited number of reservations
{quote}

if reservation-continue-looking is enabled, does it try to allocate on another nodes within the limited number of reservations?
, Yes.  reservation-continue-looking simply means the scheduler will consider releasing a reservation in order to acquire free resources to satisfy a container request.  It solves this scenario:
# User has hit their user limit in active containers and container reservations
# A new node comes along with enough free space to satisfy an outstanding container request
# Normally the scheduler cannot grant this container because the user is at their user limit
# reservation-continue-looking will consider releasing existing reservations so the user remains within their limit when allocating the container
, Thanks to your comment, I was able to figure out that limiting reservation for an application is risky.
I had another thought about it.

Currently, the number of reserved containers increases proportionally depending on how much re-reservation has occurred.
The smaller the cluster size, the faster the capacity of the queue is occupied. However, there is no way for the user to control this.
all capacity of the queue has been occupied by reserved containers. since then, small container requests from the other user’s apps are delayed until reserved container is allocated. because, capacity-scheduler does not preempt resources from reserved containers within a queue.
It looks like this will be resolved in YARN-4945. as workaround, I would like to have an option to adjust the rate at which reserved containers grow. it prevent other apps from starvation even if the app requesting a large container is postponed.

Here is what i thought to achieve it. the following is part of shouldAllocOrReserveNewContainer
{code}
// Use percentage of node required to bias against large containers...
// Protect against corner case where you need the whole node with
// Math.min(nodeFactor, minimumAllocationFactor)
starvation = 
    (int)((application.getReReservations(priority) / (float)reservedContainers) * 
          (1.0f - (Math.min(nodeFactor, getMinimumAllocationFactor())))
         );
{code}
after starvation is calculated, adjust starvation value by "reservation-factor” which can be set
starvation = starvation * reservation-factor
the lower "reservation-factor", the slower the rate at which reserved containers grow.

Any comments are welcome.]