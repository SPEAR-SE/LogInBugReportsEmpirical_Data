[Exception on startup is the following, which leads to a general shutdown of the nodemanager

{noformat}
2012-08-15 20:56:33,506 ERROR service.CompositeService (CompositeService.java:start(72)) - Error starting services org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl
org.apache.hadoop.yarn.YarnException: Failed to check for existence of remoteLogDir [/tmp/logs]
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.verifyAndCreateRemoteLogDir(LogAggregationService.java:159)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.start(LogAggregationService.java:134)
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.start(ContainerManagerImpl.java:251)
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:178)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:270)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:286)
{noformat}

The nodemanager doesn't immediately fallover if the RM is unavailable, so it seems like we shouldn't if the NN is unavailable.  Maybe lazy intialization of the app log directory rather than forcing it to be there on startup?, Log aggregation is a criticial service when enabled, and so, yes, it was a conscious decision to shut-down NM if it can't reach NameNode.

But you are right, NN can go down anytime, so we should handle that case along with this. We need some way for apps to know that this NM couldn't upload logs in the past due to a downtime in DFS, have any ideas?, One thing we could consider is marking the node as UNHEALTHY if it encounters issues trying to create the initial app log directory or when it encounters issues trying to aggregate for a particular app.  That way we won't pile up more apps on a node that's already having issues trying to aggregate, and we're at least reporting on the cluster status page that the node needs someone to take a look at what's going on.

As for notifying an app that the log aggregation isn't quite complete, I'm not sure how best to handle that.  Since currently log aggregation is asynchronous from app execution, the app will often have exited before the aggregation completes even when there isn't an issue accessing the aggregation filesystem.  If we need to provide a way for apps to know for certain that all of their container logs have been aggregated then we'd need to have log aggregation support a notification service or minimally a way for AM's to query nodes to see if an aggregation of a container has completed.

Does it make sense to split this into two parts?  We can use this JIRA to have NMs become UNHEALTHY while they are having issues accessing the aggregation filesystem (and add retries in such cases), and file a separate JIRA to add the log aggregation status/notification feature.  The former would still be useful to have without the latter., I encountered this when trying to start a NM and a namenode at the same time.  The NM shut down because the namenode was in safe mode.  Having the NM die in this way introduces a dependency in the order that services are started.

Log aggregation is checked each time an app is run on a node, and the app is immediately killed if a log folder cannot be used for it.  Thus, merely removing the NM killing itself on startup doesn't introduce any correctness issues.  The worst that could happen is that time could be wasted by scheduling more containers on a node we already know has connection issues to the namenode.

Attached a patch that removes the NM killing itself on startup.  At initApp time, if verifyAndCreateRemoteLogDir has not been successfully completed, it is called again, and the app is failed if it fails.  If initApp fails five consecutive times, the NM sets its status to unhealthy.

I agree if an NM loses its ability to connect to the namenode after an app has started, it would be good for the NMs to report that they weren't able to write their logs, but my opinion is that that is a more difficult issue and does not need to be tied to this change. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12570539/YARN-24.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/419//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/419//console

This message is automatically generated., if we take the NM to unhealthy, then what will try to reset it back to healthy? I think the simplest way to handle this is to remove the creation of the dir on init and do that on app start time., Latest patch does what Alejandro requested, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12571952/YARN-24-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/462//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/462//console

This message is automatically generated., The approach looks good. Possible to have a testcase? Have you tests it in a cluster forcing the non-creation of the root log dir?, Updated patch includes a testcase that ensures that an application can still be successfully initialized after a previous one has failed., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12574270/YARN-24-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 tests included appear to have a timeout.{color}

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:red}-1 eclipse:eclipse{color}.  The patch failed to build with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/538//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/538//console

This message is automatically generated., I verified this on a pseudo-distributed cluster in the following way:
* Started up yarn expecting a namenode port of 7654.
* Started up HDFS with default namenode port of 9000.
* Ran a pi job.
* Verified that log aggregation failed because the nodemanager couldn't find the namenode.
* Restarted HDFS with the namenode port 7654.
* Ran another YARN job.
* Verified that the logs from the second job showed up in the UI and that the logs from the first job didn't., Thinking about this a little more, I didn't see a strong reason not to verify the root log dir each time.  This makes the log aggregation service resilient to the root directory being deleted or chmoded while a nodemanager is running.  Uploaded a new patch that does this., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12575571/YARN-24-3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:red}-1 eclipse:eclipse{color}.  The patch failed to build with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/612//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/612//console

This message is automatically generated., Verified the updated patch on a pseudo-distributed cluster as well., +1, Thanks Sandy. Committed to trunk and branch-2., Integrated in Hadoop-trunk-Commit #3537 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3537/])
    YARN-24. Nodemanager fails to start if log aggregation enabled and namenode unavailable. (sandyr via tucu) (Revision 1461891)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1461891
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
, Integrated in Hadoop-Yarn-trunk #169 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/169/])
    YARN-24. Nodemanager fails to start if log aggregation enabled and namenode unavailable. (sandyr via tucu) (Revision 1461891)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1461891
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
, Integrated in Hadoop-Hdfs-trunk #1358 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1358/])
    YARN-24. Nodemanager fails to start if log aggregation enabled and namenode unavailable. (sandyr via tucu) (Revision 1461891)

     Result = FAILURE
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1461891
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
, Integrated in Hadoop-Mapreduce-trunk #1386 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1386/])
    YARN-24. Nodemanager fails to start if log aggregation enabled and namenode unavailable. (sandyr via tucu) (Revision 1461891)

     Result = SUCCESS
tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1461891
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
]