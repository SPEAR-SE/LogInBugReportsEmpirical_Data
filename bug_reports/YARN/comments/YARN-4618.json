[Good catch [~bibinchundatt] !
I think {{org.apache.hadoop.yarn.api.records.Resource}} should have used *long* for memory atleast.
Though in the normal scenario we might not get to see such high #containers but definitely in the future we can see each container asking more  Memory (like 100GB or more) then it will get easily reproduced. , [~hex108], Whats the max number of containers run in your cluster ? i remember it to be a very large value, have you faced any problem like this any where ?, [~Naganarasimha]
Thanks for looking into the  issue .
For now changing resource to long impact will be huge (can will wait for limitation).Cluster memory we can have till 2100TB approx i feel its huge.

Pending containers can be high if so many application request with many containers pending in system. But RM getting stuck based on request is the problem for now. {{hasPendingResourceRequest}} we can have initial check based on pending container then pending resource will be a simpler solution for scheduler problem.
, hi [~Naganarasimha], we have met same problem in our cluster. We use FairScheduler. In FairScheduler#update(), we find some app's demand resource is negative, these apps are most HIVE SQL jobs, they requests so many containers that their demand resources overflow, then their queue overflows, the queue never gets resource. We are not sure whether it is HIVE SQL's bug, but we handle this problem in fair scheduler, in *updateDemand* we check whether demand resource is negative, and set it to *maxRes* if it is negative., Thanks [~bibinchundatt].
As [~Naganarasimha Garla]  mentioned, I also feel we can change the data type to long. Long run, this looks like a cleaner approach. Currently we can put a max check here and solve it. But some other places it may pop-up give your test case. , Nice catching [~bibinchundatt],

I'm not entirely sure if changing int to long is the only solution.

[~vvasudev] has a JIRA YARN-3926 to add resource type and unit to the "Resource" object. instead of using "MB", it can use "GB" for such a big cluster.,  [~leftnoteasy]/[~hex108]/[~Naganarasimha]

Thank you for looking and providing you inputs.

# Current solution can be limiting in {{AbstractCSQueue#incPendingResource}} and {{AbstractCSQueue#decPendingResource}}
# Changing to long compatibility issue might happen.
# Long term solution as [~leftnoteasy] pointed out could be YARN-3926.

Will wait for more discussion and inputs from all., [~leftnoteasy] 

Can we close this jira since YARN-4844  implemented the same]