[not all events are app related. some store secret key stores which cannot be ignored. 
what errors are we seeing in the store. if these are non-transient errors then the RM should probably stop. if these are transient errors then I remember discussing with [~vinodkv] and [~jianhe] about this offline. The summary is that the state store client (eg HDFS client) should retry enough times to cover cases of transient errors in the store.
With HA states now, we should ideally not kill the RM but just transitionToStandby()., We run into these when using the ZKRMStateStore. Below is a sample log. Just realized - ExitUtil.terminate doesn't log the cause. Created YARN-1616 to fix the logging issue. 
{code}
2014-01-18 05:18:47,955 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Creating password for identifier: owner=jenkins, renewer=oozie mr token, realUser=oozie, issueDate=1390051127955, maxDate=1390655927955, sequenceNumber=154, masterKeyId=178
2014-01-18 05:18:47,955 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing RMDelegation token with sequence number: 154
2014-01-18 05:18:47,973 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type STATE_STORE_OP_FAILED
2014-01-18 05:18:47,975 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
{code}

The error doesn't seem to be a transient one - looks the zk-store retries when it encounters a KeeperException different from the NoAuthException.

bq. With HA states now, we should ideally not kill the RM but just transitionToStandby().
True. Unfortunately, the second RM that takes over would try repeating the same operation and run into the same issue. Would it make sense to kill the application, and clear the store of the offending operations - store/update app-related information (including tokens).

Let me re-run with YARN-1616 fixed and get more information on this. , If its a non-transient error then the RMs should go into Standby. It might be good to the second RM to try the operation. In the worst case it will also go to standby. But if the misconfiguration is only on 1 RM locally then the cluster will continue running. If its a global issue then all RMs will be in standby and hopefully that will alert the admin. The RMs will stop touching the store and the admin can fix it. Then we can ask all RMs to transitionToActive and participate in leader election. Sounds good?, The above error happens only after running a number of Oozie jobs on the RM for a while - so, I don't think it is due to bad configuration. So, transitioning both RMs to Standby, would only result in alternating between the two RMs becoming the Active until the application gets killed because of exceeding the max-attempts. The only downside I see is the other applications might also be killed in the process.

bq. The RMs will stop touching the store and the admin can fix it.
The admin might be able to fix it by explicitly deleting some znodes from the store, but that would require understanding the store layout. 

Let me investigate more and see what the underlying cause for this issue is. May be, that would simplify what we should do in such cases.
, RM HA deployments have been working fairly well. Let us close this as "Won't Fix" for now and revisit it if need be. ]