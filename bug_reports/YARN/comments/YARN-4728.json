[Thanks for reporting this issue [~Silnov]! 
It looks like this issue is caused by the long timeout at two level. This issue is similar as YARN-3944, YARN-4414, YARN-3238 and YARN-3554. You may work around this issue by changing the configuration values: "ipc.client.connect.max.retries.on.timeouts" (default is 45),  "ipc.client.connect.timeout"(default is 20000ms) and "yarn.client.nodemanager-connect.max-wait-ms" (default is 900,000ms)., [~Silnov], in addition to above, can you check your AM logs and see if scenario is similar to MAPREDUCE-6513 ?
I suspect its same., zhihai xu,thanks for your response!
I will try to make some changes following your advice!, Varun Saxena,thanks for your response!
I have checked MAPREDUCE-6513. The scenario is similar to that as you said. 
I'll get some knowledge from it:) , Yes, MAPREDUCE-6513 is possible, but YARN-1680 may be more possible. Because blacklisted nodes can happen easier in your environment than MAPREDUCE-6513 especially with mapreduce.job.reduce.slowstart.completedmaps=1. To see whether it is MAPREDUCE-6513 or YARN-1680, you need check the log to see wether reduce task is preempted. If reduce task is preempted and map task still can't get resource, it is MAPREDUCE-6513/MAPREDUCE-6514. Otherwise, it is YARN-1680. Even YARN-1680 is fixed, which trigger the preemption, MAPREDUCE-6513 still will happen.]