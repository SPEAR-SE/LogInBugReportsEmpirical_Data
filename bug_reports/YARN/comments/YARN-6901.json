[Attached ver.001 patch for branch-2.8.

[~sunilg], could you help to take a look? [~jlowe]/[~eepayne], I'm not sure if this is found in your 2.8 environment, this looks like a easy-to-hit issue., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} branch-2.8 Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 48s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green} branch-2.8 passed with JDK v1.8.0_131 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green} branch-2.8 passed with JDK v1.7.0_131 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 19s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 10s{color} | {color:green} branch-2.8 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} branch-2.8 passed with JDK v1.8.0_131 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} branch-2.8 passed with JDK v1.7.0_131 {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 27s{color} | {color:green} the patch passed with JDK v1.8.0_131 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 32s{color} | {color:green} the patch passed with JDK v1.7.0_131 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 18s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 1 new + 88 unchanged - 1 fixed = 89 total (was 89) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 24s{color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} the patch passed with JDK v1.8.0_131 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} the patch passed with JDK v1.7.0_131 {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 78m 12s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.7.0_131. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}171m 38s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
|  |  org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue.getParent() is unsynchronized, org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue.setParent(CSQueue) is synchronized  At AbstractCSQueue.java:synchronized  At AbstractCSQueue.java:[line 186] |
| JDK v1.8.0_131 Failed junit tests | hadoop.yarn.server.resourcemanager.TestAMAuthorization |
|   | hadoop.yarn.server.resourcemanager.TestClientRMTokens |
| JDK v1.7.0_131 Failed junit tests | hadoop.yarn.server.resourcemanager.TestAMAuthorization |
|   | hadoop.yarn.server.resourcemanager.TestRMRestart |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSurgicalPreemption |
|   | hadoop.yarn.server.resourcemanager.TestClientRMTokens |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:d946387 |
| JIRA Issue | YARN-6901 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12879447/YARN-6901.branch-2.8.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 993f8195d1f9 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2.8 / a168287 |
| Default Java | 1.7.0_131 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_131 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_131 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/16603/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/16603/artifact/patchprocess/new-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/16603/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.7.0_131.txt |
| JDK v1.7.0_131  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/16603/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/16603/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks for the report and patch!  We have not seen this in practice on our 2.8 builds yet, and I have yet to verify it is susceptible to this deadlock given we have pulled back a few features from 2.9 that may have indirectly solved this.

However after a quick look at the proposed fix I'm wondering if [this comment|https://issues.apache.org/jira/browse/YARN-3137?focusedCommentId=14306040&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14306040] in YARN-3137 applies here.  I proposed to get rid of a lock in a similar manner but there was concern that the queue hierarchy could change during refreshQueues and end up having children that don't point back to their parents, etc.  As I commented in that other JIRA I'm not fully convinced it's a real issue, but I never got a complete answer to my questions there.  Seems like those same concerns people had could happen here with the proposed patch.
, [~leftnoteasy], thanks for the heads-up on this issue.
bq. I have yet to verify it is susceptible to this deadlock
I have compared and analyzed the code bases and I believe that we are still susceptible to this.

From the stack traces provided in the *Description* section, it looks like the deadlock would be between the assigncontainers thread and the preemption thread. We would definitely have noticed this if it were occurring, so, as [~jlowe] pointed out, I'm pretty sure that we have not seen this in practice (yet).

Since the deadlock involves the preemption thread, I do have a concern about the above unit test failure for {{TestCapacitySchedulerSurgicalPreemption}}. Can you comment?
{noformat}
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.fail(Assert.java:95)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerPreemptionTestBase.waitNumberOfLiveContainersFromApp(CapacitySchedulerPreemptionTestBase.java:110)
	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerSurgicalPreemption.testSimpleSurgicalPreemption(TestCapacitySchedulerSurgicalPreemption.java:143)
{noformat}, I suspect the test failure is a recurring one: YARN-5973, Thanks [~eepayne]/[~jlowe], 

I think the parent queue lock is to prevent move a sub queue to different hierarchy. However I think it is not allowed today:

CapacitySchedulerQueueManager.java:
{code}
if (!oldQueue.getQueuePath().equals(newQueue.getQueuePath())) {
          //Queue's cannot be moved from one hierarchy to other
          throw new IOException(queueName + " is moved from:"
              + oldQueue.getQueuePath() + " to:" + newQueue.getQueuePath()
              + " after refresh, which is not allowed.");
{code}

We should have this logic in branch-2.8 as well, since moving queue hierarchy causes many other issues. So add volatile to parent ref should be a simple fix we can do for now.

I haven't checked the preemption test failure yet, I can check it offline. The biggest issue I can see is, we should never do a reverse lock (e.g. lock app first, and before release app lock, lock leaf queue again). Currently preemption thread lock hierarchy in normal order (lock leaf queue then lock app), so we need to fix main scheduler logic accordingly., As I understand the stacktraces, we have the preemption monitor thread who has locked a leaf queue and is in the process of trying to lock the application within the queue.  Then we have the scheduling thread which is calling LeafQueue's assignContainers and ultimately locks an application then tries to lock the leaf queue for that application.  Is that correct?  If so I'm wondering how we're deadlocking since the thread calling LeafQueue#assignContainers should already have the lock on the leaf queue.  Maybe I missed a spot since I couldn't line up the line numbers in the stack trace with branch-2.8 source, but everywhere I saw LeafQueue's assignContainers calling the application's assignContainers method it was within the context of a {{synchronized(this)}} block.

Now I'm wondering how we did _not_ have the lock when trying to get the leaf queue path.  Are we somehow trying to examine an allocation for some other queue, or does the queue think this application is part of this queue but the app thinks it is part of some other queue?  I'm probably missing something basic, but I'm not sure why we don't already have the leaf queue locked when FiCaSchedulerApp#assignContainers is called.

On a related note, I found it odd that LeafQueue#assignContainers explicitly locks the app before callling assignContainers on it when it is trying to assign a reserved container but I didn't see a similar lock on the application before calling assignContainers for the non-reserved case.  Seems like we're either missing a lock we need or grabbing a lock unnecessarily., Thanks [~jlowe],

bq. Now I'm wondering how we did not have the lock when trying to get the leaf queue path...
This may not be possible, ideally moving an app from one queue to another need to lock queue, and assign container need to lock queue as well. It should be safe.

bq. On a related note, I found it odd that LeafQueue#assignContainers explicitly locks the app before callling assignContainers.
I just checked the code, assignContainers in app grabs synchronized lock, so I think we can remove the synchronize(app) from LeafQueue safely., bq. This may not be possible, ideally moving an app from one queue to another need to lock queue, and assign container need to lock queue as well. It should be safe.

I'm not sure I understand.  Could you elaborate on how the deadlock is happening?  Looking at the branch-2.8 code, I don't see how LeafQueue is calling assignContainers on the app without holding a lock on the queue.  Therefore I'm confused why the app's assignContainers is blocking on a queue lock it should already have., [~jlowe],

You're right, I checked the code of the problematic cluster, it is a little bit different from branch-2.8. When it allocates reserve container, leafqueue's synchronized lock isn't acquired properly. So I think the deadlock should not be existed in branch-2.8. 

However, I think the fix attached in the JIRA should be get into branch-2.8 in any case, since it fixes bad pattern which acquires lock of upper-level component while holding lock of lower-level component, this could likely cause deadlock in the future.

Downgrade priority to critical since this is more like a preventing fix.

, bq. I checked the code of the problematic cluster, it is a little bit different from branch-2.8.

OK, that explains why I couldn't line up the stacktrace with branch-2.8 code or recent ancestors to it and also why we've never seen this in practice.

I agree it would be nice if getQueuePath were lockless, although long-term I think something like YARN-6917 would be preferable to a volatile approach.  I'm OK with volatile in the short-term.

Why does the patch change the synchronization around the ordering policy?  That does not seem to have anything to do with reaching up the hierarchy.  It also looks like it introduces a bug if two threads try to call setOrderingPolicy at the same time, e.g.:
# Thread 1 notices the old ordering policy, policy A, is not null and begins to copy the old contents into its new policy, policy B
# Thread 2 notices the old ordering policy, policy A, is not null and begins to copy the old contents into its new policy, policy C
# Thread 1 sets the policy to policy B
# Thread 2 sets the policy to policy C

Now we are left with a policy that contains the entities from policy A and C and have lost the original entities from B, whereas the old code would result in a policy containing the entities of policy A, B, and C regardless of which thread won the race for the lock.  I think we can get rid of the lock on the getter, but I think it is necessary on the setter or we need to do CAS-like logic and loop back around if someone has swapped out the policy while we were copying the old one., bq. I agree it would be nice if getQueuePath were lockless, although long-term I think something like YARN-6917 would be preferable to a volatile approach. I'm OK with volatile in the short-term.
If there's no existing issue we can find, I agree to delay the fix to YARN-6917. 

bq. That does not seem to have anything to do with reaching up the hierarchy.
Inside AbstractContainerAllocator:
{code}
        application
            .getCSLeafQueue()
            .getOrderingPolicy()
{code}
To me grabbing lock of leaf queue while holding lock of app is also not a good practice. 

bq. It also looks like it introduces a bug if two threads try to call setOrderingPolicy at the same time 
Actually orderingPolicy can be set only when queue do reinitialize (which protected by queue's synchronize lock). Do you think does it still have the issue in the case?  , bq. If there's no existing issue we can find, I agree to delay the fix to YARN-6917.

If we're going to change the ordering policy locking then we should also change the getQueuePath locking here.  YARN-6917 may not get any traction in the short-term unless someone picks it up (unfortunately I probably won't get to it soon), and getQueuePath locking behavior is the cause of the deadlock on that 2.8-ish cluster.  I was simply pointing it out to raise awareness since that approach solves the same locking issue and also reduces the load of every allocate.  Totally agree we can defer to YARN-6917 if that is going in soon, but I also agree with your earlier assessment that we should address this sooner than later in case someone tries to call these app methods without holding the queue lock.

bq. Actually orderingPolicy can be set only when queue do reinitialize (which protected by queue's synchronize lock). Do you think does it still have the issue in the case? 

Yes because the way this is called may change in the future,  just like the non-issue we are fixing here.  ;-)  If setting the ordering policy is not a common occurrence then adding a lock there shouldn't be an issue, and it will always do something sane.  The important part is to keep it lock free on the getter.
, [~jlowe], make sense to me.

So just repeat my understanding:for this JIRA, we can do following two things:
1) Remove getParent synchronized lock and add volatile to parent ref.
2) Remove getOrderingPolicy synchronized lock and add volatile to orderingPolicy ref. 
In addition, keep synchronized lock for setOrderingPolicy. 

Please let me know if it is also your expectation., Yep, that sounds good to me.
, Doing an audit of 2.8.2 Blockers and Criticals.  Do we still feel that we need this for 2.8.2 or should we move it out?, [~jlowe], Since this is a preventive fix, I just downgraded priority and moved it out.]