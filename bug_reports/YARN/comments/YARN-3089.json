[The failures are unfortunately not present in the NM log due to bug YARN-3088 preventing the log message from being generated properly.  While debugging an instance of the nodemanager, I was able to see the error messages from the LCE executable, and they looked like the following:
{noformat}
Directory not found /somepath/application_1421927171686_0163/container_1421927171686_0163_01_000009/syslog/
rmdir of /somepath/application_1421927171686_0163/container_1421927171686_0163_01_000009/syslog/ failed - Permission denied
Directory not found /somepath/application_1421927171686_0163/container_1421927171686_0163_01_000009/stderr/
rmdir of /somepath/application_1421927171686_0163/container_1421927171686_0163_01_000009/stderr/ failed - Permission denied
Directory not found /somepath/application_1421927171686_0163/container_1421927171686_0163_01_000009/stdout/
rmdir of /somepath/application_1421927171686_0163/container_1421927171686_0163_01_000009/stdout/ failed - Permission denied
{noformat}, [~jlowe], can you please take a look?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12695421/YARN-3089.v1.txt
  against trunk revision f2c9109.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/6464//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-YARN-Build/6464//artifact/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6464//console

This message is automatically generated., Hi [~eepayne]

Thank you for bringing this up.. I have a comment on same.

{code}
int subDirEmptyStr = (subdir == NULL || subdir[0] == 0);
{code}
I think strlen(subdir) also has to be checked against 0, correct?



, Thank you, [~sunilg], for your review of this patch.

{quote}
{code}
int subDirEmptyStr = (subdir == NULL || subdir[0] == 0);
{code}
I think strlen(subdir) also has to be checked against 0, correct?
{quote}
The {{strlen}} function will do exactly the same thing that {{subdir[0] == 0}} does, which is check that the first byte in the string is 0. In {{strlen}}, it takes the form of {{*s == '\0'}}, but it amounts to the same thing. By checking for empty string as is done in the existing patch, it avoids the overhead of another function call., Thank You [~eepayne].

I have understood the intention here. Thank u for the detailed explanation. , Thanks for the patch, Eric!  Just a few nits:

We should probably use the S_ISREG macro to be consistent with the S_ISDIR macro usage already in this file.  There's also the question of whether the logic should be !S_ISDIR rather than S_ISREG.  Shouldn't matter in practice, but I'm wondering if someone later ends up creating a named-pipe, etc. that ends up being the target of this delete form.

"cannot contain subDir" should be "cannot contain subdir"., We should put this in a 2.6.1 release?

[~jlowe], so the issue now happens for logs of all applications (and not just those that leverage YARN-2468)? If that is the case, then that should be fixed too. YARN-2468 was supposed to be a compatible change - so no breakage is expected for non LRS apps. /cc [~xgong].
, Thanks for the review, [~jlowe]. I have made the suggested changes and retested on a secure cluster., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696282/YARN-3089.v2.txt
  against trunk revision 5bd9846.

    {color:red}-1 @author{color}.  The patch appears to contain  @author tags which the Hadoop community has agreed to not allow in code contributions.

    {color:green}+1 tests included{color}.  The patch appears to include  new or modified test files.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6496//console

This message is automatically generated., bq. so the issue now happens for logs of all applications (and not just those that leverage YARN-2468)? If that is the case, then that should be fixed too. YARN-2468 was supposed to be a compatible change - so no breakage is expected for non LRS apps. 

For the changes in YARN-2468, we only delete the very old logs from HDFS. We do not delete any logs from NM. But from the logs uploaded by Jason, this issue happened in NM. So, I do not think this is related to YARN-2468.

To answer this question "so no breakage is expected for non LRS apps ?" Yes, I do not think this will break any LRS apps., bq. the issue now happens for logs of all applications (and not just those that leverage YARN-2468)?

I think we're OK for normal apps.  It will try, and fail, to delete the individual log files, but then afterwards it issues a recursive delete on the log directory which does succeed.  We only found this problem because we saw delete tasks being leaked in the state store, and that's because errors from the container executor were escaping the try blocks.  See YARN-3090.

bq. For the changes in YARN-2468, we only delete the very old logs from HDFS. We do not delete any logs from NM.

The call of DeletionService.delete where basepaths are specified but subdir is empty/null is new with YARN-2468, specifically in this part of the code in AppLogAggregatorImpl.uploadLogsForContainers:

{code}
        this.delService.delete(this.userUgi.getShortUserName(), null,
          uploadedFilePathsInThisCycle
            .toArray(new Path[uploadedFilePathsInThisCycle.size()]));
{code}

The LCE wasn't properly handling the case when the basedirs arguments are _not_ directories.  (Based on the name of the argument, this isn't that surprising.)

The breakage we'll see is that on systems using the LCE the local delete triggered by the above code will not occur.  If the log directory is not going to be deleted shortly afterwards then we won't cleanup logs on the local disk in a timely manner after aggregation., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696282/YARN-3089.v2.txt
  against trunk revision ba58da2.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/6509//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6509//console

This message is automatically generated., Thanks for updating the patch, Eric!  I have one last nit that I should have caught last time:

The following code should just be replaced with a strdup call.
{code}
full_path = concatenate("%s", "user baseDir", 1, *ptr);
{code}

Other than that patch looks good., Thanks, [~jlowe]. I have updated the patch to reflect your additional suggestions., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696560/YARN-3089.v3.txt
  against trunk revision e04e8fa.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:red}-1 eclipse:eclipse{color}.  The patch failed to build with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/6513//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6513//console

This message is automatically generated., +1 latest patch lgtm.  The eclipse warnings don't appear related, and eclipse:eclipse builds pass for me locally with this patch.

[~vinodkv] do you still want this committed to 2.6.1 given it should only affect LRS apps?, bq. The call of DeletionService.delete where basepaths are specified but subdir is empty/null is new with YARN-2468, specifically in this part of the code in AppLogAggregatorImpl.uploadLogsForContainers:

Thanks for explanation. Jason. 

bq. do you still want this committed to 2.6.1 given it should only affect LRS apps?

For normal applications, the application logs directory will be deleted after the application is finished. As Jason mentioned, this action gets succeed.  
For the LRS applications, We deleted the logs which is already uploaded. 
{code}
this.delService.delete(this.userUgi.getShortUserName(), null,
          uploadedFilePathsInThisCycle
            .toArray(new Path[uploadedFilePathsInThisCycle.size()]));
{code}

So, whether it can affect the LRS apps, my answer is "depends on". It is users' responsibility to roll over their logs. YARN will upload and clean the logs which exist in the log directory.
Currently, it depends on how users want to roll-over their application logs and set up the LogAggregationContext. 
For example, If the users roll-over the logs with some pattern, say "log.1, log.2, ..., etc", and set up the includePattern in LogAggregationContext with something like "log.(\\d+)". So, YARN will only upload the logs (log.1, log.2), and delete the logs (log.1, log.2) afterward. In this case, *i do not think this LRS app is affected at all*.

But if the user does not set any includePattern/excludePattern in  LogAggregationContext, and does not want to roll-over the logs, all container logs will be written into the files with the same name(run the command, such as 1>>stdout, 2>>stderr ). After we aggregate the logs into HDFS, the stdout/stderr will be deleted. *In this case, this LRS app is affected.*, bq. But if the user does not set any includePattern/excludePattern in LogAggregationContext, and does not want to roll-over the logs, all container logs will be written into the files with the same name(run the command, such as 1>>stdout, 2>>stderr ). After we aggregate the logs into HDFS, the stdout/stderr will be deleted. In this case, this LRS app is affected.

This sounds pretty bad.  If the app doesn't roll its own logs but is normally not very chatty such that it isn't much of an issue, we're going to blow away the apps logs every log-aggregation.roll-monitoring-interval-seconds interval?  That interval isn't configurable per app, so I can see cases where someone puts an app on the cluster that isn't super long running, doesn't roll its own logs, but then this feature comes along and uploads a partial log and removes the active log files.  In other words, fixing this bug may actually start deleting logs we shouldn't delete on clusters using the LCE.  However that's a separate JIRA from this, as this is focusing on making the behavior of the LCE consistent with the default executor wrt. deletes., bq. That interval isn't configurable per app, so I can see cases where someone puts an app on the cluster that isn't super long running, doesn't roll its own logs, but then this feature comes along and uploads a partial log and removes the active log files. 

Yes, that is the issue that we have already realized. Currently, even we are running a MR job, it will upload the partial logs which does not sound right. And we need to fix it., Looking closer at AppLogAggregatorImpl's rolling interval support, I'm worried enabling the rolling interval on a cluster will easily lose logs.  For example if the cluster is configured with a log aggregation rolling interval of 24 hours, is it true that any MapReduce job or other typical job will have their partial logs uploaded and then removed from the local filesystem after 24 hours?  It seems like one would have to set the rolling interval to be larger than the worst-case runtime of any application that doesn't roll its own logs to avoid loss of logs., Ah, comment race.  Thanks for confirming Xuan.  To avoid another race, are you filing the log aggregation rolling JIRA or shall I?  Or is there already an existing JIRA?

Back to this JIRA, I'd like to commit this tomorrow if there aren't any objections., bq. is it true that any MapReduce job or other typical job will have their partial logs uploaded and then removed from the local filesystem after 24 hours? 

Yes,
* If the Application keeps running more than 24 hours
* The Application does not finish
* User does not set any roll-over
* User does not set anything in LogAggregationContext
* User does not config the proper value for yarn.nodemanager.delete.debug-delay-sec, bq. To avoid another race, are you filing the log aggregation rolling JIRA or shall I

I will do that., Thanks, Eric!  I committed this to trunk and branch-2.

Given the lack of proper deleting without this patch only affected LRS apps with a specified log aggregation context on clusters with log aggregation rolling intervals configured, I didn't commit this to 2.6.1., FAILURE: Integrated in Hadoop-trunk-Commit #7043 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7043/])
YARN-3089. LinuxContainerExecutor does not handle file arguments to deleteAsUser. Contributed by Eric Payne (jlowe: rev 4c484320b430950ce195cfad433a97099e117bad)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c
, bq. Currently, even we are running a MR job, it will upload the partial logs which does not sound right. And we need to fix it.
Wow, this is a huge blocker. We should fix it in 2.6.1. [~xgong], can you please file a ticket and link it here? Tx., created https://issues.apache.org/jira/browse/YARN-3154 for the MR job issue, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #97 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/97/])
YARN-3089. LinuxContainerExecutor does not handle file arguments to deleteAsUser. Contributed by Eric Payne (jlowe: rev 4c484320b430950ce195cfad433a97099e117bad)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c
, FAILURE: Integrated in Hadoop-Yarn-trunk #831 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/831/])
YARN-3089. LinuxContainerExecutor does not handle file arguments to deleteAsUser. Contributed by Eric Payne (jlowe: rev 4c484320b430950ce195cfad433a97099e117bad)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2029 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2029/])
YARN-3089. LinuxContainerExecutor does not handle file arguments to deleteAsUser. Contributed by Eric Payne (jlowe: rev 4c484320b430950ce195cfad433a97099e117bad)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #94 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/94/])
YARN-3089. LinuxContainerExecutor does not handle file arguments to deleteAsUser. Contributed by Eric Payne (jlowe: rev 4c484320b430950ce195cfad433a97099e117bad)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #98 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/98/])
YARN-3089. LinuxContainerExecutor does not handle file arguments to deleteAsUser. Contributed by Eric Payne (jlowe: rev 4c484320b430950ce195cfad433a97099e117bad)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2048 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2048/])
YARN-3089. LinuxContainerExecutor does not handle file arguments to deleteAsUser. Contributed by Eric Payne (jlowe: rev 4c484320b430950ce195cfad433a97099e117bad)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #832 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/832/])
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #98 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/98/])
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #2030 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2030/])
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #95 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/95/])
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2049 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2049/])
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #99 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/99/])
, Hi [~eepayne] and [~jlowe], does this fix need to be cherry-picked to branch-2.6?, As I mentioned in my last comment above, this shouldn't be needed in 2.6.  The problem is triggered by log rotation aggregation support which is only in 2.7.  It shouldn't hurt to have it in 2.6, but it's not necessary either.]