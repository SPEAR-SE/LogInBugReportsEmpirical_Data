[Thanks for the fix, patch looks good to me, pending on Jenkins., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 36s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 28m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 28s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 13s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 28s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 69m 42s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 29s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}132m 14s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:abb62dd |
| JIRA Issue | YARN-8505 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12930827/YARN-8505.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 1b5fab935d77 3.13.0-153-generic #203-Ubuntu SMP Thu Jun 14 08:52:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 43f7fe8 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_171 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/21186/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/21186/testReport/ |
| Max. process+thread count | 917 (vs. ulimit of 10000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/21186/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, [~cheersyang]/[~Tao Yang]

AMLimit is also used to limit the number of  application running in parallel in queue.
So skipping would allow unlimited number of application asking for containers for a queue rt ??, Hi [~bibinchundatt]

This patch only skips the check for unmanaged AMs, I don't think we should limit resources for such AMs as they are not launched or managed by YARN. Does it make sense?

[~Tao Yang], it looks like the UT failure was related, could you please take a look?, [~cheersyang]

As mentioned earlier,  limiting number of application requesting for containers get skipped after the patch if type is unmanaged . We can have unlimited number of unmanaged AM application requesting for containers,for a queue and will containers will get allocated by scheduler. 

Earlier i had similar discussion for container resource for Unmanaged AM.. IIRC currently for unmanaged AM the resource used is scheduler min allocation size rt ??.. Ideally limitation is not required.

Other impacts could be AMUsed resource shown in metrics could be more than the AM limits.

cc: [~leftnoteasy] / [~sunil.govind@gmail.com], Thanks [~bibinchundatt] for your mentions.  AMUsed resource metrics is a problem and should be handled together in the patch. Without AMLimit & userAMLimit limitations, unmanaged AM still can be limited by maxApplications and maxApplicationsPerUser. I think unmanaged AM can be activated because they doesn't taken resource from YARN, but their requests are still in control., {quote}
maxApplications and maxApplicationsPerUser. 
{quote}

Above properties are for total application in queue, not running application IIUC, {quote}
Above properties are for total application in queue, not running application IIUC
{quote}
There is a validation which is to make sure (numPendingApps+numActiveApps<=min(maxApplications, maxApplicationsPerUser) ) in LeafQueue#validateSubmitApplication, the submission of unmanaged App will be rejected if reached the limit. That is the limitation which I mean for unmanaged AM. , [~Tao Yang]

Thank you for clarification

{{numPendingApps+numActiveApps}} --> *Submitted application to queue*. So limitation will be only for submission of application to queue not for application *RUNNING application* (application in running state whose AM has started running).

Earlier we would have been able to limit RUNNING application based on AM LIMIT for unmanaged AM.

So this change will be a behaviour changed from old version too.

IIUC in case of federation application will be submitted to clusters as unmanaged AM in subcluster. So impact for should be evaluated for this change., [~bibinchundatt], Thanks for your replay.
Yes, this issue will change the old behavior. We just encounter a scene that only specified partitions(a/b) has NMs (None in default partition) in a cluster, our users want to submit an unmanaged AM which will request resources from partition a or b but it won't run because of the resource limitation in default partition. Users think that unmanaged AM's resource isn't got from YARN so that it should not be limited because of resource and feel puzzled at this limitation. Is current limitation reasonable?  I think we need a discuss for that. cc: [~leftnoteasy], [~cheersyang], [~sunilg], [~bibinchundatt]  / [~Tao Yang] / [~cheersyang], 

I would prefer to add a new limit to container #maximum-concurrently-activated-apps within a queue and skip updating AMLimit when a unmanaged AM is launched., [~leftnoteasy]

+1 for adding a new configuration limiting number of RUNNING application for queue. I was having the same idea too., Thanks [~leftnoteasy], [~bibinchundatt] for your suggestions! 
+1, LGTM.  I will upload a new patch for this solution later., Attached v2 patch for review. 
 Updates:
 (1) Skip checking amLimit/userAMLimit and skip updating amUsed metrics for unmanaged apps
 (2) Add maxActiveApplications/maxActiveApplicationsPerUser limitation in LeafQueue and expose them in WebUI and REST API. maxActiveApplications can be configured by yarn.scheduler.capacity.<queue-name>.maximum-active-applications and equal with maxApplications by default.

(3) Update UT cases which will be affected by this modification.

cc: [~bibinchundatt], [~leftnoteasy], [~cheersyang], [~Tao Yang] Thanks for the patch. Had a question/comment 

Shouldnt we skip dec/inc resource usage for unManagedAMs in FicaSchedulerApp as well?

{noformat}
 public void nodePartitionUpdated(RMContainer rmContainer, String oldPartition,
      String newPartition) {
    Resource containerResource = rmContainer.getAllocatedResource();
    this.attemptResourceUsage.decUsed(oldPartition, containerResource);
    this.attemptResourceUsage.incUsed(newPartition, containerResource);
    getCSLeafQueue().decUsedResource(oldPartition, containerResource, this);
    getCSLeafQueue().incUsedResource(newPartition, containerResource, this);

    // Update new partition name if container is AM and also update AM resource
    if (rmContainer.isAMContainer()) {
      setAppAMNodePartitionName(newPartition);
      this.attemptResourceUsage.decAMUsed(oldPartition, containerResource);
      this.attemptResourceUsage.incAMUsed(newPartition, containerResource);
      getCSLeafQueue().decAMUsedResource(oldPartition, containerResource, this);
      getCSLeafQueue().incAMUsedResource(newPartition, containerResource, this);
    }
{noformat}, Thanks [~suma.shivaprasad] for your comments. 
IIUC, unmanaged AM should not have a RMContainer instance, this patch can ignore these decAM/incAM handles for RMContainer because they are irrelated to unmanaged AM. Correct?, Thanks for the clarification [~Tao Yang] Makes sense.]