[Created YARN-5972 branch for this work, As per Subru's request in YARN-5292, I have moved this to an umbrella JIRA to foster further discussion as well as a place to post the design doc., [~subru], thanks for taking a look. w.r.t your ([comment|https://issues.apache.org/jira/browse/YARN-5292?focusedCommentId=15685939]) on YARN-5292, [~hrsharma], please feel free to chime in:
Based on my understanding of MAPREDUCE-4584, I feel that feature is actually quite orthogonal to this, and I don't think one approach is necessarily better/worse than the other. They can possibly be mixed and matched based on the use case.

While notifying an AM of containers that are about to be preempted does allow the AM to check-point work, it does imply, as you pointed out, that AMs be modified to act on this input and make some decisions based on it.

Container pausing/freezing on the other hand, given OS/VM level support (also exposed via Docker and LXC) to actually freeze a process (agreed, their definition of freeze might vary), is actually AM/application independent. This can be useful, for applications and deployments that do not really want to check-point on its own but at the same time like the idea of container preemption with work preservations.

Also, the NM container lifecycle and API changes in the Container Executor should not ideally take into account the execution type of the containers. The trigger can either be from the ContainerScheduler (in case of YARN-1011 and YARN-2877, when it decides resources are required for a guaranteed container) or from an AM (the AM wants to play nice and relinquish resources so that some opportunistic containers running on the node to run)

Even though this is currently targeted for opportunistic containers, I don't really see any problems exposing this to AMs via the ContainerManagementProtocol (though the devil is in the details)
bq. We cannot guarantee RESUME unless we block the allocation for the Container which IMHO defeats the purpose
Not sure I completely agree. If an AM pauses a guaranteed container, yes, the allocation is blocked, but this is no different from an AM starting a container running never-ending sleep job, except this has the advantage that the NM is aware of it and can use the relinquished resources to start any queued opportunistic containers. Since the container is guaranteed, resume is ensured, since any opportunistic container that was running due to graciousness of the AM would immediately be preempted. I do agree though, in the case of opportunistic containers, if an AM explicitly asks to pause it, it cannot expect the container to be resumed as soon as it asks.


, Hi folks, thanks for opening this JIRA and the feedback. Much appreciated. 

{quote}
While notifying an AM of containers that are about to be preempted does allow the AM to check-point work, it does imply, as you pointed out, that AMs be modified to act on this input and make some decisions based on it.

Container pausing/freezing on the other hand, given OS/VM level support (also exposed via Docker and LXC) to actually freeze a process (agreed, their definition of freeze might vary), is actually AM/application independent. This can be useful, for applications and deployments that do not really want to check-point on its own but at the same time like the idea of container preemption with work preservations.
{quote}

Agree with [~asuresh] here. What container pausing/freezing offers is an ability to delegate to the underlying OS how the resources used by a container should be reclaimed and when resources free up again then restart the container. The gains of doing so will vary based on the container executor implementation. That said it doesn't make the PAUSE/RESUME functionality to be the perfect solution for work preservation or substitute AM specific checkpointing.

[YARN-5292] adds PAUSE/RESUME for opportunistic containers and doesn't target guaranteed containers. I can think of scenarios where it is good to have this functionality in guaranteed containers but I would wait and see some need coming in the community.  

Allowing the ContainerManager to initiate a pause/resume on an opportunistic container was considered but we decided not to have that functionality. There are some edge cases around what happens if the CM initiates a RESUME on a paused container and the NM tries to PAUSE it ([YARN-5216]). I think [~subru] is also touching towards these edge cases.

Overall I feel that the current design of allowing PAUSE/RESUME on opportunistic containers is a good starting point and allows to PAUSE an opportunistic container in favor of a guaranteed one and when resources free up it gets RESUMED ([YARN-5216]). We should probably implement pauseContainer and resumeContainer for Docker based container executors as opportunistic containers running inside Docker containers can benefit from it. 

If the community feels then we can extend the functionality towards guaranteed containers. I personally think that may become more relevant as YARN containers become virtualized via Docker or virtual machines, but I would love to hear some scenarios before we do that., Moved design doc from YARN-5292 here.., My opinion is that PAUSED state should not be handled any differently from the current QUEUED state we already persist in the store, this implies YARN-6059 can probably be closed (We do need to fix the ContainerScheduler to populate it with the running containers though, but this is orthogonal to the paused/resume feature and should be handled as a separate JIRA).

If folks are fine, I was thinking of cherry-picking YARN-5292 and YARN-5216 to trunk. [~subru], [~kkaranasos], thoughts ?, bq. My opinion is that PAUSED state should not be handled any differently from the current QUEUED state we already persist in the store, this implies YARN-6059 can probably be closed (We do need to fix the ContainerScheduler to populate it with the running containers though, but this is orthogonal to the paused/resume feature and should be handled as a separate JIRA).
Regarding this, I just had an offline chat with [~asuresh]. It seems that it would be better to add a separate entry at the StateStore for the PAUSED containers. This way we can decide what to do with them at node recovery. For example, we might want to kill them to release node resources that they might be holding.

I will give a look at YARN-5292 and YARN-5216 next week., I was not sure if we needed a formal merge vote for this - given that the scope for this has been slightly reduced. The 3 sub-tasks under this umbrella all deal with opening up the interfaces and adding methods (which default to "feature not supported" exceptions) to the abstract {{ContainerExecutor}}.
Most of the changes are in the {{ContainerScheduler}} and some minor changes to the NM side Container state machines and the NM state store. The feature itself requires a ContainerExecutor implementation plugged in that can support Pausing and Thawing, and therefore is OFF by default. Support for the {{LinuxContainerExecutor}} is being tracked at YARN-6838 but I do not feel it should block merging this to trunk.
Given the above, I was wondering if it would be ok to just merge the 3 JIRAs into trunk and branch-2. Do let me know if anyone has any objections to doing so.
(cc [~jlowe] / [~jianhe] / [~chris.douglas]), I'm OK with this being added to trunk as separate JIRAs.  The entire YARN-6838 branch is currently a less than 80K patch against trunk, and I've seen far larger and scarier stuff go into trunk via a single JIRA.
, Thanks [~jlowe], cherry-picked the 3 JIRAs and committed]