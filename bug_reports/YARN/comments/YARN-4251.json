[scheduler port is hardcoded.Uploaded the patch to take random port..
{code}conf.set(YarnConfiguration.RM_SCHEDULER_ADDRESS, "0.0.0.0:9030");{code}

Kindly review..thanks.., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |   6m 18s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 58s | There were no new javac warning messages. |
| {color:red}-1{color} | release audit |   0m 17s | The applied patch generated 1 release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 26s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 29s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   0m 52s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:red}-1{color} | yarn tests |   7m  3s | Tests failed in hadoop-yarn-client. |
| | |  24m 59s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.yarn.client.api.impl.TestAMRMClientOnRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12765995/YARN-4251.patch |
| Optional Tests | javac unit findbugs checkstyle |
| git revision | trunk / 7e2c971 |
| Release Audit | https://builds.apache.org/job/PreCommit-YARN-Build/9399/artifact/patchprocess/patchReleaseAuditProblems.txt |
| hadoop-yarn-client test log | https://builds.apache.org/job/PreCommit-YARN-Build/9399/artifact/patchprocess/testrun_hadoop-yarn-client.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/9399/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/9399/console |


This message was automatically generated., {{Testcase}} failure and {{release audit warning}} is unrelated..Even I raised YARN-4250 for testcase failure..Kindly Review.., git history implies this went in with YARN-1366, This fix does not help. I still get the same message binding to 0.0.0.0:0. With whatever gets bunded with cloudera 5.4.2

at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:139)

{noformat}
at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.hadoop.ipc.Server.bind(Server.java:407)
	... 19 more
2015-10-04 19:31:10,567 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STARTED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:0] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException: Problem binding to [0.0.0.0:0] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl.getServer(RpcServerFactoryPBImpl.java:139)
	at org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC.getServer(HadoopYarnProtoRPC.java:65)
	at org.apache.hadoop.mapreduce.v2.app.client.MRClientService.serviceStart(MRClientService.java:119)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1084)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1500)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1496)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1429)
Caused by: java.net.BindException: Problem binding to [0.0.0.0:0] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
{noformat}
Can we re-open?, Does  ServerSocketUtil.getPort(45020, 10)); mean that we are using a range of 10 ports? That does not seem like many concurrent jobs?, To be clear I sometimes get that see the above error when running multiple concurrent map/reduce processes. , [~appodictic] do you mean, you applied this patch and still you are getting bind exception..? can you provide the logs such that I can look into your logs like why same port is got allocated..?

{quote}Does ServerSocketUtil.getPort(45020, 10)); mean that we are using a range of 10 ports? That does not seem like many concurrent jobs?{quote}

no, it will check whether 45020 port is free or not, If not free,it will retry 10 times(while retrying, try to get random free port)...

, I can see now that this patch is ONLY for a test case. So I am having the problem in production. I get this problem binding to 0.0.0,0:0.

Looking around I have found, which I think to be the related property:

<property>
  <name>yarn.app.mapreduce.am.job.client.port-range</name>
  <value></value>
  <description>Range of ports that the MapReduce AM can use when binding.
    Leave blank if you want all possible ports.  
    For example 50000-50050,50100-50200</description>
</property>

As far as I can tell it is set blank. so I do not understand why if has port conflicts. That should mean it can use ANY port., [~appodictic]  *As it's not related to this jira* , Please post this query in [user mailing list| http://hadoop.apache.org/mailing_lists.html] with logs,it can be analysed further., I did post this to the ML, and CDH user lists. I'm digging deeper because no one there answers. I don't know how any version of YARN could ship where launching 2-3 jobs at once causes prod issues without someone noticing it., Edward; not related to this JIRA. You can open anotherone saying "cannot bind [0.0.0.0:0] java.net.BindException: Address already in use;"

but if you do that, I'm going to say "have you followed the wiki link? Did you follow the steps to diagnose?". So do that first, OK?, Yes. I followed the wiki link. I know what a BindException is and how to troubleshoot it.

Possible Causes

{quote}
The port is in use (likeliest)
{quote}
Sure is but only hadoop runs on this cluster. Port 0 is not a port, and if it is supposed to pick dynamic ports I doubt they are ALL in use.

{quote}
If the port number is below 1024, the OS may be preventing your program from binding to a "trusted port"
{quote}
Why would hadoop by default bind to a trusted port?
{quote}
If the configuration is a hostname:port value, it may be that the hostname is wrong -or its IP address isn't one your machine has.
{quote}
Pretty sure 0.0.0.0 is not a wrong ip address
{quote}
There is an instance of the service already running.
{quote}
So something is running on PORT 0? Seems unlikely. Instance of what? Something that is trying to launch itself every job? I don't know? 

Also the dismissive nature of the wiki: 
"Finally, this is not a Hadoop problem, it is a host, network or Hadoop configuration problem. As it is your cluster, only you can find out and track down the problem.. Sorry"
Everything worked fine one day. I upgrade hadoop it stops working. The wiki ends with a bold claim that every bind exception that starts the day after upgrade is not a hadoop problem. , [~appodictic],
Well, MRClientService will bind to all the IP addresses on your machine i.e. 0.0.0.0 and will use port 0 if you do not specify one in the config above.
Hadoop code merely uses JAVA API to bind. So it is likely that all your ports are occupied for some reason.
This is not related to this JIRA hence as Brahma said kindly send a mail to the user mailing list so that some people can suggest what you can look at. There are commands which can tell you which process is occupying the ports., Port 0, is anywhere, so I doubt that's the problem. addr 0:0:0:0 is, "all hosts", isn't it? So there's something wrong there.

Like we say, please open a new JIRA for what is clearly a new issue, bq. Also the dismissive nature of the wiki: 
"Finally, this is not a Hadoop problem, it is a host, network or Hadoop configuration problem. As it is your cluster, only you can find out and track down the problem.. Sorry"
bq. Everything worked fine one day. I upgrade hadoop it stops working. The wiki ends with a bold claim that every bind exception that starts the day after upgrade is not a hadoop problem.

Edward, I an assure you that most of the JIRAs we get related to: ConnectionRefused, BindException, NoRouteToHostException,...etc are related to system configs. it is almost invariably some machine config issue, be it ubuntu mapping localhost to 127.0.1.1; a firewall in the way, rDNS broken, or tothers.  And we get so many complaining that the namenode is refusing connections, when either the firewall is up, the port settings for the client are wrong, the hostname is wrong or the NN isn't up. Same for BindException. 

We've gone to the effort of adding wrappers around all socket exceptions to add in hostnames and ports (the things people who understand networking need), and wiki entries to help people fend for themselves and not file Critical issues about problems that they generally have to fix for themselves. Yet even with those exceptions saying "look at the wiki" entry, we still get people not following the link, but going straight to JIRA: HADOOP-12391. 

if you look at the history of those wiki entries, you can see that they continually grow as we find new system setup issues which trigger the exception.  That's because I do hit problems, I do fix them myself, and whenever I do that, I add another line. If you've found a new way, once fixed, I encourage you add a new entry. And, at the same time, you are free to change that text at the end. , +1 about this patch. Checking this in., Committed this to trunk and branch-2. Thanks [~brahmareddy] for your contribution.

[~appodictic] do you mind opening new jira to address the problem you mentioned?, [~ozawa] thanks a lot for committing and reviewing this issue.., FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #593 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/593/])
YARN-4251. (ozawa: rev 9f4dfdf4eb60cc6b13da586dabcd95bd77fc783c)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientOnRMRestart.java
, FAILURE: Integrated in Hadoop-trunk-Commit #8718 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8718/])
YARN-4251. (ozawa: rev 9f4dfdf4eb60cc6b13da586dabcd95bd77fc783c)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientOnRMRestart.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2483 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2483/])
YARN-4251. (ozawa: rev 9f4dfdf4eb60cc6b13da586dabcd95bd77fc783c)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientOnRMRestart.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2537 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2537/])
YARN-4251. (ozawa: rev 9f4dfdf4eb60cc6b13da586dabcd95bd77fc783c)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientOnRMRestart.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk-Java8 #607 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/607/])
YARN-4251. (ozawa: rev 9f4dfdf4eb60cc6b13da586dabcd95bd77fc783c)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientOnRMRestart.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #1330 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/1330/])
YARN-4251. (ozawa: rev 9f4dfdf4eb60cc6b13da586dabcd95bd77fc783c)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientOnRMRestart.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #546 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/546/])
YARN-4251. (ozawa: rev 9f4dfdf4eb60cc6b13da586dabcd95bd77fc783c)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientOnRMRestart.java
]