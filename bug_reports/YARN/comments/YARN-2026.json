[Attached patch addresses this issue by setting fair share of inactive parent and leaf queues(queues which have no running apps) to zero.
Patch contains unit tests to illustrate the behavior. I manually tested in pseudo distributed cluster and verified that fair share are distributed to only active queues and found that preemption behavior/fairness is much better.
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12646588/YARN-2026-v1.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/3809//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3809//console

This message is automatically generated., The nice thing about fair share currently is that it's interpretable as an amount of resources that, as long as you stay under, you won't get preempted.   Changing it to depend on the running apps in the cluster severely complicates this.  It used to be that each app and queue's fair share was min'd with its resource usage+demand, which is sort of a continuous analog to what you're suggesting, but we moved to the current definition when we added multi-resource scheduling.

I'm wondering if the right way to solve this problem is to allow preemption to be triggered at higher levels in the queue hierarchy.  I.e. suppose we have the following situation:
* root has two children - parentA and parentB
* each of root's children has two children - childA1, childA2, childB1, and childB2
* the parent queues' minShares are each set to half of the cluster resources
* the child queue' minShares are each set to a quarter of the cluster resources 
* childA1 has a third of the cluster resources
* childB1 and childB2 each have a third of the cluster resources

Even though childA1 is above its fair/minShare, We would see that parentA is below its minShare, so we would preempt resources on its behalf.  Once we have YARN-596 in, these resources would end up coming from parentB, and end up going to childA1., Hi [~sandyr],
bq. We would see that parentA is below its minShare, so we would preempt resources on its behalf. 
minShare preemption at parent queue is not yet implemented ,FairScheduler.resToPreempt() is not recursive(YARN-596 doesn't address this).
I had created YARN-1961 for this purpose,which I plan to work on.
But yes you are right,if YARN-1961 is in place, we can set minShare and minShareTimeout at parentA,which would
reclaim resource from parentB.

This solves problem-1 in the description,but what about problem-2 ?
When we have many leaf queues under a parent,say using NestedUserQueue rule.
Eg.
 - parentA has 100 user queues under it
 - fair share of each user queue is 1% of parentA(assuming weight=1)
 - Say user queue parentA.user1 is taking up 100% of cluster since its the only active queue.
 - parentA.user2 which was inactive till now ,submits a job and needs say 20%.
 - parentA.user2 would get only 1% through preemption and parentA.user1 would have 99%.
  This seems unfair considering users have equal weight. Eventually,as user1 releases its containers,
  it would go to user2,but until that happens user1 can hog the cluster.

In our cluster we have about 200 users(so 200 user queues),but only about 20%(avg) are active
at a point in time. Fair share for each user becomes really low (1/200)*parent and can causes
this 'unfairness' mentioned in above example.
This can be solved by dividing fair share only to active queues.

How about this,can we have a new property say 'fairShareForActiveQueues' which turns on/off this feature,that way people
who need it can use it and other's can turn it off and would get the usual static fair share behavior.
Thoughts ?, Hi [~sandyr], did you have any comments ? 
basically in the above scenario fair share policy tends to look like fifo,
since the users who submitted apps first, hog the cluster, although all users have same fair share., Hi Ashwin, have been busy with other stuff and probably will be for the next week or two.  I see your point.  I need to think about it a little more - the main aim of preemption is to provide enforce guarantees for purposes like maintaining SLAs.  While converging towards fairness more quickly in user queues could be a nice property, it satisfies a slightly different goal., [~sandyr], 
Sure Sandy I'll patiently wait for your response. 
Also if you prefer ,please feel free to point me to some other committer who knows the FS code base well.
We are very interested to get this jira and YARN-1961 committed this month since its affecting our query cluster. , [~ashwinshankar77], can we implement this as a separate SchedulingPolicy with a different computeShares implementation?, [~sandyr], Sure. Just to be on the same page,I'd like to iron out design details upfront to avoid rework and minimize boilerplate code.
Here is my design proposal,feel free to change anything :
1. FairShareBase,DominantResourceFairnessBase would contain all the common code. Their parent class would be the existing SchedulingPolicy class.
2. FairShareBase would have two subclasses - FairSharePolicy(which is the existing one) and FairShareActiveQueuesPolicy.
The difference between the subclasses would be that they would be using different ComputeShare class.
3. Similarly DominantResourceFairnessBase would have two subclasses - DominantResourceFairnessPolicy(existing one) and DominantResourceActiveQueuesPolicy.
4. A new ComputeShareActiveQueues which computes fair share for active queues and used by FairShareActiveQueuesPolicy and DominantResourceActiveQueuesPolicy.

Thoughts ?, I think it might be simpler to just:
* Create FairShareActiveOnlyPolicy and DominantResourceFairnessActiveOnlyPolicy that extend FairSharePolicy and DominantResourceFairnessPolicy
* In those, override SchedulingPolicy.computeShares to set the fair shares for inactive apps to 0 and just call ComputeFairShares.computeShares on the active schedulables., [~sandyr], makes sense. I'll post a patch soon., Attached patch implements two new scheduling policies - FairShareActiveOnlyPolicy(fair-active) and DominantResourceFairnessActiveOnlyPolicy(drf-active). Added unit tests and updated the doc. Manually tested both the policies and could see queues converge towards fairness quickly through preemption., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12652693/YARN-2026-v2.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification
                  org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter
                  org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart
                  org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart
                  org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4099//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4099//console

This message is automatically generated., Test failures are unrelated to the patch.First two are known test failures - YARN-2158,YARN-2216.
Remaining three are intermittent(they pass on my laptop) and don't have jiras. 
Created jiras for those : YARN-2218, YARN-2219, YARN-2220
, [~sandyr], did you have any comments ?, I had a conversation with [~kkambatl] about this, and he convinced me that we should turn this on in all cases - i.e. modify FairSharePolicy and DominantResourceFairnessPolicy instead of creating additional policies.  Sorry to vacillate on this.

Some additional comments on the code:
{code}
+    return this.getNumRunnableApps() > 0;
{code}

{code}
+          || (sched instanceof FSQueue && ((FSQueue) sched).isActive())) {
{code}
Instead of using instanceof, can we add an isActive method to Schedulable, and always return true for it in AppSchedulable?

{code}
+    out.println("       <queue name=\"childA1\" />");
+    out.println("       <queue name=\"childA2\" />");
+    out.println("       <queue name=\"childA3\" />");
+    out.println("       <queue name=\"childA4\" />");
+    out.println("       <queue name=\"childA5\" />");
+    out.println("       <queue name=\"childA6\" />");
+    out.println("       <queue name=\"childA7\" />");
+    out.println("       <queue name=\"childA8\" />");
{code}
Do we need this many children?

{code}
+    out.println("</queue>");
+
+    out.println("</allocations>");
{code}
Unnecessary newline

{code}
+  public void testFairShareActiveOnly_ShareResetsToZeroWhenAppsComplete()
{code}
Take out underscore.

{code}
+  private void setupCluster(int mem, int vCores) throws IOException {
{code}
Give this method a name that's more descriptive of the kind of configuration it's setting up.

{code}
+  private void setupCluster(int nodeMem) throws IOException {
{code}
Can this call the setupCluster that takes two arguments?

To help with the fight against TestFairScheduler becoming a monstrosity, the tests should go into a new test file.  TestFairSchedulerPreemption is a good example of how to do this.

{code}
+    int nodeVcores = 10;
{code}
Nit: "nodeVCores", Yes. Per discussions with [~sandyr] and [~rkanter], it might make sense to have two notions of fairshare - (1) a dynamic fairshare that considers only Active queues to be used by the scheduler for allocating and preempting resources and a (2) static fairshare that considers all queues and is displayed to the user. This way, the values shown in the webui set the expectations for users, but the scheduler is more aggressive about allocations. , Can we follow up on the first patch in this JIRA, and work on adding a static fairshare in another? , [~sandyr],[~kasha]
Sure, we can incorporate fair share for active queues in FairSharePolicy and DominantResourceFairnessPolicy rather than creating new ones.

However regarding having two notions of fairness, I have couple of concerns :
1. Confusion/Debugging inconvenience : users looking at the UI would be confused since fair share preemption would be happening randomly(based on
internal dynamic fair share) rather than when usage is below "half fair share" shown on the UI. Also it might become a nightmare to debug when we have lots of preemption happening and we have two notions of fair share.
2. Code becomes kind of complicated - we would call ComputeFairShares twice at each node in the queue hierarchy,one with active child queues and other with all child queues. OR if you are thinking of changing ComputeFairShares itself rather than calling it twice, it still would look kinda messy with
setting dynamic/static fair shares inside it based on active queues.

I hope I'm making sense here. Thoughts ?
Is there a strong reason why we can't just show the dynamic fair share on the UI,which is the reality ?, bq. Confusion/Debugging inconvenience : users looking at the UI would be confused since fair share preemption would be happening randomly
Dynamic fairshare is always greater than or equal to Static fairshare. If we preempt resources from queues over their dynamic fairshare, they are strictly over their static fairshare as well. This shouldn't be confusing.

bq. Code becomes kind of complicated - we would call ComputeFairShares twice at each node in the queue hierarchy
ComputeFairShares should only compute the real/dynamic fairshare. Only the web ui should use the static fairshare. The static fairshare should change only when we add/remove queues or change the weights. 

bq. Is there a strong reason why we can't just show the dynamic fair share on the UI,which is the reality ?
The only reason is - the real fairshare keeps fluctuating and is hard to track. Looking at the real fairshare, the user can not predict a guaranteed finish time for his applications. 
, [~kasha],
I see your point. I had a quick chat with couple of ops and dev guys in my team and here is the concern they have with your proposal and they had a suggestion -
they want to be able to look at queues in the scheduler page, and be able to say that *queue X should kickoff preemption since its starving for fair share*.
We won't be able to infer that with your proposal since preemption is happening based on internal dynamic fair share, rather than what is displayed on the UI. 
Quick example describing the concern :
queue1 : static fair share=10% dynamic fair share=50% usage=20% demand=50%
queue2 : static fair share=10 % dynamic fair share=50% usage=80%
Both queues would appear ORANGE(above static fair share) on the scheduler page,which seems like preemption will not happen,but actually it would,
since queue1's usage is below half dynamic fair share. We couldn't have inferred this by looking at the UI. 

*Suggestion* : If we do not want to replace static fair share by dynamic on the UI,can we show both ?
basically there would be an extra line indicating dynamic fair share,apart from the existing dotted line
which indicates static fair share. Also the usage would turn ORANGE only when we go beyond dynamic fair share 
instead of static.

Thoughts ?, I see how showing both improves predictability, but I am slightly concerned about exposing another value adding more complexity to what the users see. 

Let us wait to hear from others. , I think Ashwin makes a good point.

I think displaying both is reasonable if we present it in a careful way.  For example, it might make sense to add tooltips that explain the difference., Thanks [~sandyr],[~kasha] - I'll create a separate jira to track the UI changes. I'll post a patch for this one soon., Incorporated [~kasha] suggestion of having two notions of fairness.
Also incorporated [~sandyr] unit test comments.
Please let me know if you have any other comments.

Created YARN-2360 to deal with UI changes to display dynamic fair share on scheduler page.
I've not added dynamic fair share in FSQueueMetrics . Could you please let me know how these metrics are used and if we want to add dynamic fair share to it ?, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12657909/YARN-2026-v3.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4438//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4438//console

This message is automatically generated., [~kasha],[~sandyr] , did you have any comments on the latest patch ?
I also made UI changes and attached screenshot which shows static/dynamic fair share in YARN-2360.
Can you please take a look at that also ?, Based on an offline discussion with [~kasha], the plan is to change the present fair share implementation
by making it dynamic i.e. calculate fair share only for active queues. We would implement static fair share in YARN-2393 and Web UI changes in YARN-2360.

[~kasha],
In my latest(v4) patch, you would find that I’ve abstracted out the fair share calculation
to a method ComputeFairShares#getWeightToResourceRatio and I invoke this with 
active schedulables. I’ve implemented it this way for two reasons :
1. When we do static fair share in YARN-2360, we can just reuse that method by passing all
schedulables.
2. It didn’t seem clean to me to mix “active queue” logic inside fair share computation., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12660479/YARN-2026-v4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4553//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4553//console

This message is automatically generated., Thanks for bearing with us on this JIRA, Ashwin. That patch looks mostly good. Minor comments:
# This is a very subjective opinion. In ComputeFairShares, would it be cleaner/simpler to rename existing {{public computeShares}} to {{private computeSharesInternal}}, and add a new {{public computeShares}} that calls the internal version only with active queues? 
# Thanks for adding a bunch of tests in TestFairSchedulerFairShare. Post YARN-1474, 
## setup() need not call {{scheduler.setRMContext(resourceManager.getRMContext());}}
## configureClusterWithQueuesAndOneNode need not call the following:
{code}
    scheduler.init(conf);
    scheduler.start();
    scheduler.reinitialize(conf, resourceManager.getRMContext());
{code}, Thanks [~kasha]. All comments addressed in v5 patch., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12660716/YARN-2026-v5.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4564//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4564//console

This message is automatically generated., +1. Checking this in.., Thanks for the contribution and your patience through reviews, Ashwin.

Just committed this to trunk and branch-2. , Thanks a lot [~kasha], [~sandyr] for reviewing and committing my patch !, FAILURE: Integrated in Hadoop-trunk-Commit #6041 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6041/])
YARN-2026. Fair scheduler: Consider only active queues for computing fairshare. (Ashwin Shankar via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1616915)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/ComputeFairShares.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerFairShare.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #639 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/639/])
YARN-2026. Fair scheduler: Consider only active queues for computing fairshare. (Ashwin Shankar via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1616915)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/ComputeFairShares.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerFairShare.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1832 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1832/])
YARN-2026. Fair scheduler: Consider only active queues for computing fairshare. (Ashwin Shankar via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1616915)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/ComputeFairShares.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerFairShare.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1858 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1858/])
YARN-2026. Fair scheduler: Consider only active queues for computing fairshare. (Ashwin Shankar via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1616915)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/Schedulable.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/ComputeFairShares.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerFairShare.java
]