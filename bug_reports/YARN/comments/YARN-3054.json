[Hi, [~peng.zhang]. Firstly, FairScheduler will check whether the usage is over fairness.
{code}
  private boolean preemptContainerPreCheck() {
    return parent.getPolicy().checkIfUsageOverFairShare(getResourceUsage(),
        getFairShare());
  }
{code}

bq. Mapreduce jobs can get additional resources when others are idle.
I'm not sure what your "idle" meaning here. But in YARN, one queue can take the over-fairshare resource, if the resources are not used by other queues. And in FairScheduler, each queue has "steady" fairshare and "dynamic" fairshare. For example, if we have two queues (Q1 and Q2), both with weight 1. So Q1's steady share is 50%, and Q2 is also 50%. Assume only Q1 has jobs and no job submitted to Q2, Q1's dynamic fairness is 100% and Q2 is 0. The dynamic fairshare calculation only considers "active" queues.

bq. Mapreduce jobs for one user in one queue can still progress with its min share when others preempt resources back.
As I said above, each queue is guaranted with minshare and fairshare. That means, some jobs can still move on. We cannot assign a minshare to each job. Otherwise, the job with multiple concurrent jobs may take over the cluster., [~peng.zhang] - the problem reported is not quite clear, mind elaborating it further? , Preemption happened on low priority container, and for MapReduce reduce task got higher priority than map task for scheduling first, but it has data dependency on map task. 
So preempt map task which has lower priority may cause job progress never proceed.

Detailed scenario described as below: 
1. assume 10 resources in cluster(map and reduce task request the same amount of memory and cpu, 1 resource per task), two queues(q1 and q2). 
2. q1 has one job and get all resources when q2 is idle.
3. job in q1 has 5 map tasks and 5 reduce tasks.
4. when q2 get new job, job in q1 will be preempted, and 5 containers will be preempted.
5. according to container preemption policy, all map tasks with lower priority will be preempted (all progress for these tasks are lost)
6. after container preemption, job in q1 get new resource headroom, and decide new ratio between map and reduce tasks, and then AM preempt reduce tasks for map tasks. so 5 reduce tasks are killed and new 5 map tasks start.
7. when q2 is idle, job in q1 will then get 5 extra resources and new 5 reduce tasks start. 
This is back to phase 3. and this may happens periodically(maybe because map tasks for job in q1 run for a long time), map tasks cannot finish before container is preempted. So job cannot make any progress., Thanks [~peng.zhang]. I understand it now. 

The elegant way of handling this would be to have a preemption priority or even a preemption cost per container, which is different from the priority that is used for allocation. That is a larger conversation to be had. Let us move this out of this umbrella and look at it for both schedulers together. 

That said, I would expect MapReduce to realize that pending mappers are blocked on waiting reducers and resolve this. MAPREDUCE-6302 and co. attempt to fix this, so you shouldn't see issues with job completion itself. , Also, the approach being proposed on YARN-4752 doesn't take container priority into consideration anymore. So, we shouldn't systemic bias towards preempting low priority containers. , Also, the approach being proposed on YARN-4752 doesn't take container priority into consideration anymore. So, we shouldn't systemic bias towards preempting low priority containers. , Thanks [~kasha]
I agreed with "have a preemption priority or even a preemption cost per container". 

And in my temporary fix, I preempt latest scheduled container instead of low or high priority containers. 
I think this will make containers for a mount of resources(at least steady fair share) steady. So MapReduce job progress will proceed. ]