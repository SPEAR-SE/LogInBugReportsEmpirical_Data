[Environment : HDP-2.4 (hadoop-2.7.1)

The usecase is as follows :- 

Cluster is of size 1200 nodes and on average around 15k jobs running per day. Hence keeping applogs in the same cluster adds too much pressure on NN because of small files problem. Around 5Million files created per day (normal load) leading to 10Million FS objects for keeping one day logs itself. The requirement is to maintain atleast 1 week of log and hence decided to move it to different cluster or different namespace (NN federation).

In these cases, expecting minimal latency on jobs if the other cluster is completely down (though configured with HA). In such situation, would want to have minimal impact on applications running in cluster. But currently it does 15 attempts {{dfs.client.failover.max.attempts}} to connect to NN before giving it up. Hence adding a latency of 2 to 2.5 mins on each container launch (per node manager) and hence affecting over all job completion time.

(Aware of YARN-2942 which is still in progress and MAPREDUCE-6415 is in 2.8.0)

Can we have a new config to pass it as {{dfs.client.failover.max.attempts}} while creating FileSystem instance in LogAggregationService so that we can configure it to fail fast? Or any configs already available to handle this case?, Attached a patch where LogAggregationService creates a new yarn config and overrides {{dfs.client.failover.max.attempts}} to create FileSystem instance.

There are two open points to discuss :

* "dfs.client.failover.max.attempts" is hardcoded in LogAggregationService because this config param is present in hadoop-hdfs (DFSConfigKeys.java) project. And hadoop-yarn-server-nodemanager (LogAggregationService) does not have dependency on hadoop-hdfs. "DFS_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY" to be moved from DFSConfigKeys.java (hadoop-hdfs) to CommonConfigurationKeys.java (hadoop-common). Any other way? 
* Only one new config is introduced {{yarn.nodemanager.remote-app-log-dfs-client-failover-max-attempts}} considering that Namenode is setup in HA mode. Need to check for other configs like "dfs.client.retry.max.attempts" for non-ha setup. 

Please check whether this is the correct way to handle? If not, please give suggestion. Thanks., Attached initial patch. Please review., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 16s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red} 0m 0s {color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 1m 1s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 8m 41s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 46s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 41s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 32s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 40s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 5s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 4s {color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 9s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 35s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 35s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 41s {color} | {color:red} hadoop-yarn-project/hadoop-yarn: The patch generated 13 new + 232 unchanged - 0 fixed = 245 total (was 232) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 5 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch 3 line(s) with tabs. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 2s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 55s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 59s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 26s {color} | {color:green} hadoop-yarn-api in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 2m 27s {color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 13m 34s {color} | {color:green} hadoop-yarn-server-nodemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 20s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 48m 0s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.logaggregation.TestAggregatedLogFormat |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12822026/YARN-5445-1.patch |
| JIRA Issue | YARN-5445 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 5d7a2e22061b 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 08e3338 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/12644/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn.txt |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/12644/artifact/patchprocess/whitespace-eol.txt |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/12644/artifact/patchprocess/whitespace-tabs.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/12644/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-YARN-Build/12644/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/12644/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/12644/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

, Moving {{DFS_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY}} to {{CommonConfigurationKeys}} doesn't sound like the right solution.  If the log aggregation code is going to depend on specific behavior of HDFS why shouldn't the project depend on hadoop-hdfs?

As you pointed out, there are other JIRAs that are attempting to resolve the base issue that is prompting your unusual cluster config.  I don't think introducing a new configuration parameter to deal with a temporary issue is a good idea.  Config params, like diamonds, are forever, and we already have entirely too many.

I haven't looked at the code for log aggregation.  What happens what the DFS connection fails?, [~templedf] Thanks for taking a look at this.

{quote}
Moving DFS_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY to CommonConfigurationKeys doesn't sound like the right solution. If the log aggregation code is going to depend on specific behavior of HDFS why shouldn't the project depend on hadoop-hdfs?
{quote}

Yes It makes sense. But I thought that adding dependency to hadoop-hdfs is a major change. If this can be done, then good.

{quote}
As you pointed out, there are other JIRAs that are attempting to resolve the base issue that is prompting your unusual cluster config. I don't think introducing a new configuration parameter to deal with a temporary issue is a good idea. Config params, like diamonds, are forever, and we already have entirely too many
{quote}

Yes, agreed on not introducing a new config for workaround. But recently we moved the applogs to different namenode (federated cluster) and seeing a benefit out of it as it reduces client/service rpc queue length and also rpc latencies. Also LogAggregationService is designed to write applogs to different filesystem. Hence thinking adding a config here might be worthy. But I am open to get input from you on this.

{quote}
I haven't looked at the code for log aggregation. What happens what the DFS connection fails?
{quote}

Seems this been handled in ConfiguredFailoverProxyProvider where {{ipc.client.connect.max.retries}} is set to 0 (default of {{dfs.client.failover.connection.retries}} . Please correct me If I am missing out something here.
{code:title=ConfiguredFailoverProxyProvider.java|borderStyle=solid}
public ConfiguredFailoverProxyProvider(Configuration conf, URI uri,
      Class<T> xface) {
    int maxRetries = this.conf.getInt(
        DFSConfigKeys.DFS_CLIENT_FAILOVER_CONNECTION_RETRIES_KEY,
        DFSConfigKeys.DFS_CLIENT_FAILOVER_CONNECTION_RETRIES_DEFAULT);
    this.conf.setInt(
        CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,
        maxRetries);
}
{code}
, Request to have a look at the patch and give suggestion. Thanks in advance.]