[I tried replicating this with a sleep job and manually killing the AM to force AM retries.  In this case the client reconnected to the new AM attempt and continued to show map/reduce progress for the new attempt., If we kill the AM, the client connects to new AM by getting the latest app report from RM, but if the AM attempt fails(Job fails), RM will start the new attempt and client shows the previous attempt status i.e Job Failed status. I think we need handle this case in the client side., That doesn't sound like something to fix on the client side.  If the AM told the client that the job failed then the job should have failed.  The fact that the attempt died between the time it told the client the job final status and the RM can happen, and IMHO we should fix things so the subsequent AM attempt doesn't retry the job but rather simply updates the RM with the failed status found from the previous attempt.  Otherwise we run into bad situations where we've already told the client the job failed, but the job subsequently retries (possibly from scratch, depending upon the output format support for recovery) and could succeed.  If the job has decided to fail and has already told the client, an AM attempt failure while trying to report that same decision to the RM shouldn't allow the job to subsequently succeed, IMHO., Filed MAPREDUCE-4819 to track the issue where the AM can re-run the job after reporting success to the client.  This could be particularly bad if the job succeeded but was rerun and re-committed its output just as another job was trying to consume it., Agree with Jason. We shouldn't workaround it on the client-side. 

I think we should close this as a duplicate of MAPREDUCE-4819., Closing as mentioned.]