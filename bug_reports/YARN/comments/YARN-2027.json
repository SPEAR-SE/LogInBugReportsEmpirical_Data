[Dug into this a bit more. Not entirely convinced that the TreeSet stuff is actually an issue anymore. RMContainerRequestor.makeRemoteRequest calls:

{code}
      allocateResponse = scheduler.allocate(allocateRequest);
{code}

If you drill down through the capacity scheduler, into SchedulerApplicationAttempt and AppSchedulingInfo, you'll eventually see that AppSchedulingInfo.updateResourceRequests simply adds the items in "ask" into a map based on priority. The order in which these asks come in seem to always be with ANY first (see above), so updatePendingResources will always be true, but this doesn't seem harmful.

Anyway, any ideas why YARN is ignoring host requests?, So, running this request with memMb=3584, cpuCores=1, containers=32:

{code}
  protected def requestContainers(memMb: Int, cpuCores: Int, containers: Int) {
    info("Requesting %d container(s) with %dmb of memory" format (containers, memMb)) 
    val capability = Records.newRecord(classOf[Resource]) 
    val priority = Records.newRecord(classOf[Priority])
    priority.setPriority(0)
    capability.setMemory(memMb)
    capability.setVirtualCores(cpuCores)
    def getHosts = {
      val hosts = getNextRoundRobinHosts
      System.err.println(hosts.toList)
      hosts 
    }
    (0 until containers).foreach(idx => amClient.addContainerRequest(new ContainerRequest(capability, getHosts, List("/default-rack").toArray[String], priority, false)))
  } 
{code}

Prints this in the AM logs:

{noformat}
List(eat1-app857, eat1-app873, eat1-app880)
List(eat1-app854, eat1-app864, eat1-app872)
List(eat1-app852, eat1-app873, eat1-app880)
List(eat1-app854, eat1-app880, eat1-app867)
List(eat1-app875, eat1-app852, eat1-app873)
List(eat1-app875, eat1-app852, eat1-app872)
List(eat1-app873, eat1-app859, eat1-app880)
List(eat1-app854, eat1-app873, eat1-app864)
List(eat1-app852, eat1-app874, eat1-app875)
List(eat1-app864, eat1-app859, eat1-app880)
List(eat1-app874, eat1-app872, eat1-app875)
List(eat1-app874, eat1-app873, eat1-app864)
List(eat1-app873, eat1-app859, eat1-app858)
List(eat1-app874, eat1-app873, eat1-app854)
List(eat1-app867, eat1-app880, eat1-app872)
List(eat1-app859, eat1-app875, eat1-app880)
List(eat1-app875, eat1-app872, eat1-app864)
List(eat1-app875, eat1-app867, eat1-app852)
List(eat1-app857, eat1-app852, eat1-app867)
List(eat1-app872, eat1-app854, eat1-app858)
List(eat1-app852, eat1-app872, eat1-app858)
List(eat1-app880, eat1-app873, eat1-app857)
List(eat1-app859, eat1-app871, eat1-app874)
List(eat1-app880, eat1-app874, eat1-app865)
List(eat1-app867, eat1-app873, eat1-app875)
List(eat1-app857, eat1-app858, eat1-app852)
List(eat1-app857, eat1-app867, eat1-app873)
List(eat1-app857, eat1-app871, eat1-app854)
List(eat1-app874, eat1-app865, eat1-app873)
List(eat1-app852, eat1-app880, eat1-app858)
List(eat1-app875, eat1-app873, eat1-app871)
List(eat1-app854, eat1-app880, eat1-app865)
{noformat}

With DEBUG logging in the RM logs (with no other job on the grid), I see:

{noformat}
21:18:02,958 DEBUG AppSchedulingInfo:135 - update: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: *, Relax Locality: false}
21:18:02,958 DEBUG ActiveUsersManager:68 - User my-job-name added to activeUsers, currently: 1
21:18:02,959 DEBUG CapacityScheduler:704 - allocate: post-update
21:18:02,959 DEBUG SchedulerApplicationAttempt:328 - showRequests: application=application_1399581102453_0003 headRoom=<memory:736256, vCores:0> currentConsumption=1024
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app875, Relax Locality: true}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app857, Relax Locality: true}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 11, Location: eat1-app880, Relax Locality: true}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app854, Relax Locality: true}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: /default-rack, Relax Locality: true}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app858, Relax Locality: true}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: *, Relax Locality: false}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app874, Relax Locality: true}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app872, Relax Locality: true}
21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app859, Relax Locality: true}
21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 13, Location: eat1-app873, Relax Locality: true}
21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app864, Relax Locality: true}
21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app865, Relax Locality: true}
21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app852, Relax Locality: true}
21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app867, Relax Locality: true}
21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app871, Relax Locality: true}
21:18:02,960 DEBUG CapacityScheduler:709 - allocate: applicationAttemptId=appattempt_1399581102453_0003_000001 #ask=16
21:18:03,009 DEBUG AsyncDispatcher:164 - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE
21:18:03,009 DEBUG RMNodeImpl:373 - Processing eat1-app863:35408 of type STATUS_UPDATE
21:18:03,009 DEBUG AsyncDispatcher:164 - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE
21:18:03,009 DEBUG CapacityScheduler:754 - nodeUpdate: eat1-app863:35408 clusterResources: <memory:737280, vCores:360>
21:18:03,009 DEBUG CapacityScheduler:785 - Node being looked for scheduling eat1-app863:35408 availableResource: <memory:49152, vCores:24>
21:18:03,009 DEBUG CapacityScheduler:828 - Trying to schedule on node: eat1-app863, available: <memory:49152, vCores:24>
21:18:03,009 DEBUG ParentQueue:559 - Trying to assign containers to child-queue of root
21:18:03,009 DEBUG ParentQueue:690 - printChildQueues - queue: root child-queues: root.default(0.0013888889),
21:18:03,009 DEBUG ParentQueue:652 - Trying to assign to queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:1024, vCores:1>usedCapacity=0.0013888889, absoluteUsedCapacity=0.0013888889, numApps=1, numContainers=1
21:18:03,009 DEBUG LeafQueue:807 - assignContainers: node=eat1-app863 #applications=1
21:18:03,009 DEBUG LeafQueue:826 - pre-assignContainers for application application_1399581102453_0003
21:18:03,009 DEBUG SchedulerApplicationAttempt:328 - showRequests: application=application_1399581102453_0003 headRoom=<memory:736256, vCores:0> currentConsumption=1024
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app875, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app857, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 11, Location: eat1-app880, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app854, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: /default-rack, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app858, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: *, Relax Locality: false}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app874, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app872, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app859, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 13, Location: eat1-app873, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app864, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app865, Relax Locality: true}
21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app852, Relax Locality: true}
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app867, Relax Locality: true}
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app871, Relax Locality: true}
21:18:03,011 DEBUG LeafQueue:1062 - User limit computation for my-job-name in queue default userLimit=100 userLimitFactor=1.0 required: <memory:3584, vCores:1> consumed: <memory:1024, vCores:1> limit: <memory:737280, vCores:1> queueCapacity: <memory:737280, vCores:1> qconsumed: <memory:1024, vCores:1> currentCapacity: <memory:737280, vCores:1> activeUsers: 1 clusterCapacity: <memory:737280, vCores:360>
21:18:03,011 DEBUG LeafQueue:995 - Headroom calculation for user my-job-name:  userLimit=<memory:737280, vCores:1> queueMaxCap=<memory:737280, vCores:1> consumed=<memory:1024, vCores:1> headroom=<memory:736256, vCores:0>
21:18:03,011 DEBUG LeafQueue:914 - post-assignContainers for application application_1399581102453_0003
21:18:03,011 DEBUG SchedulerApplicationAttempt:328 - showRequests: application=application_1399581102453_0003 headRoom=<memory:736256, vCores:0> currentConsumption=1024
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app875, Relax Locality: true}
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app857, Relax Locality: true}
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 11, Location: eat1-app880, Relax Locality: true}
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app854, Relax Locality: true}
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: /default-rack, Relax Locality: true}
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app858, Relax Locality: true}
21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: *, Relax Locality: false}
21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app872, Relax Locality: true}
21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app859, Relax Locality: true}
21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 13, Location: eat1-app873, Relax Locality: true}
21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app864, Relax Locality: true}
21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app865, Relax Locality: true}
21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app852, Relax Locality: true}
21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app867, Relax Locality: true}
21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app871, Relax Locality: true}
21:18:03,012 DEBUG ParentQueue:657 - Assigned to queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:1024, vCores:1>usedCapacity=0.0013888889, absoluteUsedCapacity=0.0013888889, numApps=1, numContainers=1 --> <memory:0, vCores:0>, NODE_LOCAL
{noformat}

The logs go on for quite a while like this, but this is the general gist of it. This is very confusing, since I made 32 single requests all with different random hosts, a hard-coded /default-rack, and relaxLocality set to false. I'm seeing all kinds of crazy requests in the logs.

I'm assuming the reason for the expansion in the number of requests is because of the AMRMClientImpl's expansion in addContainerRequest:

{code}
    if (req.getNodes() != null) {
      HashSet<String> dedupedNodes = new HashSet<String>(req.getNodes());
      if(dedupedNodes.size() != req.getNodes().size()) {
        Joiner joiner = Joiner.on(',');
        LOG.warn("ContainerRequest has duplicate nodes: "
            + joiner.join(req.getNodes()));        
      }
      for (String node : dedupedNodes) {
        addResourceRequest(req.getPriority(), node, req.getCapability(), req,
            true);
      }
    }

    for (String rack : dedupedRacks) {
      addResourceRequest(req.getPriority(), rack, req.getCapability(), req,
          true);
    }

    // Ensure node requests are accompanied by requests for
    // corresponding rack
    for (String rack : inferredRacks) {
      addResourceRequest(req.getPriority(), rack, req.getCapability(), req,
          req.getRelaxLocality());
    }

    // Off-switch
    addResourceRequest(req.getPriority(), ResourceRequest.ANY, 
                    req.getCapability(), req, req.getRelaxLocality());
{code}

Still, the first request that is logged from the RM is this one:

{noformat}
21:18:03,058 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app875, Relax Locality: true}
{noformat}

Sure enough, I see 9 containers running on eat1-app875. Why is the RM behaving in this way? I made 32 SINGLE requests, each with DIFFERENT hosts, with relaxLocality OFF, but it seems that either the client or the scheduler is bunching them together without my consent., Hi Chris

Take a look at YARN-1412 which describes related failures to allocate a container on a specific host. I've also had a very similar experience with host specific resource requests with relaxLocality set to false failing. I'm thinking of creating a new type of scheduler 'LocalityScheduler' which would allow specific hosts to be permanently reserved until released by the AM. Also take a look at YARN-371 - another instance of where host specific allocation fails and the related discussion., Thanks for pointing this out. Bummer! The LocalityScheduler idea crossed my mind last night, as well. It still seems to me that the correct solution is to properly patch the RM (or AMRMClient) to work., YARN doesn't guarantee any node-locality unless you specify strictLocality=true in your ContainerRequest.  The FIFO scheduler does not even make an attempt at node-locality.  For the Capacity Scheduler, you need to set yarn.scheduler.capacity.node-locality-delay, which I believes specifies a number of scheduling opportunities to pass on before accepting a non-local container.  Apparently it's not included in the Capacity Scheduler doc - http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html.  The Fair Scheduler equivalent is documented here, but it works a little bit differently - http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/FairScheduler.html., Was the relaxLocality flag set to false in order to make a hard constraint for the node? 
Or is the jira stating that even soft locality constraints (where YARN is allowed to relax the locality from node to rack to *) is also not working? Soft locality would need delay scheduling to be enabled and that needs the configs that Sandy mentioned., relaxLocality was set to false.

{noformat}
    (0 until containers).foreach(idx => amClient.addContainerRequest(new ContainerRequest(capability, getHosts, List("/default-rack").toArray[String], priority, false)))
{noformat}

The last false in that parameter list is relaxLocality., Including a rack in your request will allow containers to go anywhere on the rack, even when relaxLocality is set to false.

From the AMRMClient.ContainerRequest doc: "If locality relaxation is disabled, then only within the same request, a node and its rack may be specified together. This allows for a specific rack with a preference for a specific node within that rack."

So try passing in the rack list as null instead of List("/default-rack").toArray[String].
, Yes. If strict node locality is needed then the rack should not be specified. If the rack is specified then it will allow relaxing locality up to the rack but no further., I did it in YARN-1974 to specify nodes on which the containers should be allocated(for fair and capacity scheduler), and it works both in unit test and in our real cluster., K, feel free to close.

I'm fairly sure that I tried a host with a null rack during testing and it didn't work, but it might have been on the FIFO scheduler. Either way, we've figured out a workaround to our problem, and [~zhiguohong] has verified functionality on a real cluster, so I'm OK with closing this ticket out., I tried this on a 5 node cluster in AWS EC2 instances. I set relaxLocality to false. Hadoop-2.2. Capacity Scheudler. Did not work. I tried setting rack to null. I tried allocating 3 container request as well as just 1 container request.
3 container requests:
        containers += new ContainerRequest(capability, broker.nodes, null, pri, false)
        containers += new ContainerRequest(capability, null, Seq[String]("/default-rack").toArray[String], pri, false)
        containers += new ContainerRequest(capability, null, null, pri, false)

1 container request:
        containers += new ContainerRequest(capability, broker.nodes, null, pri, false)


where broker.nodes is an array of one FQDM host. Did not work. I can try Hong Zhiguo's distributed shell patch on the same AWS cluster and report findings. I'll run run hadoop-2.4 instead of hadoop-2.2.
]