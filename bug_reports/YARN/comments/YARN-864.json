[In the log, NodeManager.stop() is getting called. Know why this is happening? You can check the RM logs. Things have changed a bit from 2.0.5 to 2.1.0 so I've to look at the old code., Hey Vinod,

I have no idea why it would be called.

The ps tree shows the NM running since June 13 (this log trace is from the 19th).

{noformat}
$ ps -ef | grep Node
app      27915 27655  2 Jun13 ?        04:01:20 /export/apps/jdk/JDK-1_6_0_21/bin/java -Dproc_nodemanager...
{noformat}

Cheers,
Chris, BTW- my NM/RM are running on WARN right now, in logs. I'm going to switch to INFO and see if there's more detail., Attaching RM logs. They show that the NM was marked as lost.

09:36:05,614  INFO AbstractLivelinessMonitor:111 - Expired:my-host-name:45454 Timed out after 60 secs
09:36:05,616  INFO RMNodeImpl:493 - Deactivating Node my-host-name:45454 as it is now LOST
09:36:05,617  INFO RMNodeImpl:346 - my-host-name:45454 Node Transitioned from RUNNING to LOST
, Hey Vinod,

So here's what I know:

1. The RM is marking the NM as lost, and telling the NM to shut down.
2. The NM is trying to shut down, but appears unable to kill all containers:

{noformat}
09:37:46,132  INFO NodeStatusUpdaterImpl:365 - Node is out of sync with ResourceManager, hence rebooting.
09:37:47,932  INFO NodeManager:219 - Containers still running on shutdown: [container_1371756883564_0001_01_000003, container_1371756883564_0002_01_000002, container_1371756883564_0003_01_000005, container_1371756883564_0004_01_000001, container_1371756883564_0004_01_000002]
09:37:47,955  INFO NMAuditLogger:89 - USER=jhoman       IP=172.18.146.129       OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1371756883564_0004    CONTAINERID=container_1371756883564_0004_01_000001
09:37:47,970  INFO NodeManager:226 - Waiting for containers to be killed
09:37:47,970  WARN Server:997 - IPC Server Responder, call org.apache.hadoop.yarn.api.ContainerManagerPB.stopContainer from 172.18.146.129:46200: output error
09:37:47,973  INFO Container:835 - Container container_1371756883564_0004_01_000001 transitioned from RUNNING to KILLING
09:37:47,989  INFO ContainerLaunch:300 - Cleaning up container container_1371756883564_0004_01_000001
09:37:47,989  INFO Server:1797 - IPC Server handler 6 on 45454 caught an exception
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:126)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)
        at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2200)
        at org.apache.hadoop.ipc.Server.access$2000(Server.java:113)
        at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:946)
        at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1012)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1790)
09:37:48,837  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 29897 for container-id container_1371756883564_0003_01_000005: 4.2 GB of 6.1 GB physical memory used; 5.4 GB of 49 GB virtual memory used
09:37:48,863  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 28967 for container-id container_1371756883564_0004_01_000002: 16.4 GB of 19.3 GB physical memory used; 17.0 GB of 154 GB virtual memory used
09:37:48,904  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27555 for container-id container_1371756883564_0001_01_000003: 601.9 MB of 768 MB physical memory used; 1.1 GB of 6 GB virtual memory used
09:37:48,928  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27559 for container-id container_1371756883564_0002_01_000002: 731.7 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used
09:37:48,953  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 28890 for container-id container_1371756883564_0004_01_000001: 146.5 MB of 512 MB physical memory used; 726.6 MB of 4 GB virtual memory used
09:37:51,995  INFO NodeManager:242 - Done waiting for containers to be killed. Still alive: [container_1371756883564_0001_01_000003, container_1371756883564_0002_01_000002, container_1371756883564_0003_01_000005, container_1371756883564_0004_01_000001, container_1371756883564_0004_01_000002]
09:37:52,054  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 29897 for container-id container_1371756883564_0003_01_000005: 4.2 GB of 6.1 GB physical memory used; 5.4 GB of 49 GB virtual memory used
09:37:52,991  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl is stopped.
09:37:53,095  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 28967 for container-id container_1371756883564_0004_01_000002: 16.4 GB of 19.3 GB physical memory used; 17.0 GB of 154 GB virtual memory used
09:37:53,184  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27555 for container-id container_1371756883564_0001_01_000003: 602.0 MB of 768 MB physical memory used; 1.1 GB of 6 GB virtual memory used
09:37:53,271  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27559 for container-id container_1371756883564_0002_01_000002: 731.7 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used
09:37:53,310  WARN AsyncDispatcher:109 - Interrupted Exception while stopping
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Thread.join(Thread.java:1143)
        at java.lang.Thread.join(Thread.java:1196)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:107)
        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)
        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:209)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:336)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:61)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
        at java.lang.Thread.run(Thread.java:619)
09:37:53,318  INFO AbstractService:113 - Service:Dispatcher is stopped.
09:37:53,759  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 28890 for container-id container_1371756883564_0004_01_000001: 146.5 MB of 512 MB physical memory used; 726.6 MB of 4 GB virtual memory used
09:37:54,148  INFO log:67 - Stopped SelectChannelConnector@0.0.0.0:9999
09:37:54,250  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer is stopped.
09:37:54,250  INFO Server:2060 - Stopping server on 45454
09:37:54,452  INFO Server:654 - Stopping IPC Server listener on 45454
09:37:54,493  INFO Server:796 - Stopping IPC Server Responder
09:37:55,179  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler is stopped.
09:37:55,626  WARN LinuxContainerExecutor:245 - Exit code from container is : 137
09:37:55,987  INFO AbstractService:113 - Service:Dispatcher is stopped.
09:37:55,988  WARN ContainersMonitorImpl:463 - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.
09:37:55,988  INFO AbstractService:113 - Service:containers-monitor is stopped.
09:37:55,988  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices is stopped.
09:37:55,989  INFO AbstractService:113 - Service:containers-launcher is stopped.
09:37:55,989  INFO Server:2060 - Stopping server on 4344
09:37:56,028  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371756883564_0004_01_000002
09:37:56,028  WARN ContainerLaunch:247 - Failed to launch container.
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)
        at org.apache.hadoop.util.Shell.run(Shell.java:129)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
09:37:56,107  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371756883564_0003_01_000005
09:37:56,169  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371756883564_0001_01_000003
09:37:56,169  WARN ContainerLaunch:247 - Failed to launch container.
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)
        at org.apache.hadoop.util.Shell.run(Shell.java:129)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
09:37:56,107  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371756883564_0002_01_000002
09:37:56,169  WARN ContainerLaunch:247 - Failed to launch container.
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)
        at org.apache.hadoop.util.Shell.run(Shell.java:129)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
09:37:56,170  WARN ContainerLaunch:247 - Failed to launch container.
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)
        at org.apache.hadoop.util.Shell.run(Shell.java:129)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
09:37:56,176  INFO Server:654 - Stopping IPC Server listener on 4344
09:37:56,199  INFO Server:796 - Stopping IPC Server Responder
09:37:56,327  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker is stopped.
09:37:56,327  INFO ResourceLocalizationService:689 - Public cache exiting
09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService is stopped.
09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl is stopped.
09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is stopped.
09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService is stopped.
09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService is stopped.
09:37:56,440  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.
09:37:56,440  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeManager is stopped.
09:37:56,441  INFO MetricsSystemImpl:200 - Stopping NodeManager metrics system...
09:37:56,441  INFO MetricsSystemImpl:206 - NodeManager metrics system stopped.
09:37:56,442  INFO MetricsSystemImpl:572 - NodeManager metrics system shutdown complete.
{noformat}

3. The NM reboots itself:

09:37:56,441  INFO MetricsSystemImpl:206 - NodeManager metrics system stopped.
09:37:56,442  INFO MetricsSystemImpl:572 - NodeManager metrics system shutdown complete.
09:37:56,442  INFO NodeManager:304 - Rebooting the node manager.

, This might be related to YARN-688. 
, Hey Jian,

Interesting. Do you have a re-based patch for 2.0.5-alpha? I'd be willing to patch and try it out.

Cheers,
Chris, Hi Chris
Uploaded two patches there one for 2.0.5-alpha and one for latest trunk. Things have changed a lot since alpha, not sure that works on alpha. One big change is that NM reboot behavior is changed to resync., Hey Jian,

I'm testing YARN 2.0.5-alpha with YARN-799, YARN-600, and YARN-688. I'm going to let it run over the weekend. Failures normally start happening within 5-6 hours. I'll keep you posted.

Cheers,
Chris, Also it would be very useful if you can share NM logs around the time when container was lost., Hey Guys,

Container leaking still seems to be happening. [~ojoshi], here's the logs you asked for:

{noformat}
10:28:38,753  INFO NodeStatusUpdaterImpl:365 - Node is out of sync with ResourceManager, hence rebooting.
10:28:40,306  INFO NMAuditLogger:89 - USER=criccomi     IP=172.18.146.129       OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1371849977601_0001    CONTAINERID=container_1371849977601_0001_02_000001
10:28:40,345  INFO NodeManager:229 - Containers still running on shutdown: [container_1371849977601_0001_02_000001, container_1371849977601_0001_02_000003, container_1371849977601_0002_02_000003, container_1371849977601_0003_01_000004, container_1371849977601_0004_01_000002]
10:28:40,355  INFO Container:835 - Container container_1371849977601_0001_02_000001 transitioned from RUNNING to KILLING
10:28:40,375  INFO ContainerLaunch:300 - Cleaning up container container_1371849977601_0001_02_000001
10:28:40,376  INFO NodeManager:236 - Waiting for containers to be killed
10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diagnostics: "Container killed by the ApplicationMaster.\n", exit_status: -1000,
10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:41,378  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diagnostics: "Container killed by the ApplicationMaster.\n", exit_status: -1000,
10:28:41,378  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:41,378  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:41,378  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:41,379  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:41,555  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 4230 for container-id container_1371849977601_0001_02_000001: 161.0 MB of 512 MB physical memory used; 726.2 MB of 4 GB virtual memory used
10:28:41,802  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 4324 for container-id container_1371849977601_0001_02_000003: 522.9 MB of 768 MB physical memory used; 1.1 GB of 6 GB virtual memory used
10:28:41,844  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 5717 for container-id container_1371849977601_0002_02_000003: 608.3 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used10:28:41,869  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 26908 for container-id container_1371849977601_0004_01_000002: 16.4 GB of 19.3 GB physical memory used; 17.0 GB of 154 GB virtual memory used
10:28:41,896  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27868 for container-id container_1371849977601_0003_01_000004: 4.2 GB of 6.1 GB physical memory used; 5.4 GB of 49 GB virtual memory used
10:28:42,186  WARN LinuxContainerExecutor:245 - Exit code from container is : 137
10:28:42,382  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diagnostics: "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 137\n", exit_status: -1000,
10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diag
nostics: "", exit_status: -1000,
10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diag
nostics: "", exit_status: -1000,
10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diag
nostics: "", exit_status: -1000,
10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diag
nostics: "", exit_status: -1000,
10:28:43,389  INFO Container:835 - Container container_1371849977601_0001_02_000001 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diag
nostics: "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 137\n", exit_status: 137,
10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diag
nostics: "", exit_status: -1000,
10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diag
nostics: "", exit_status: -1000,
10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diag
nostics: "", exit_status: -1000,
10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diag
nostics: "", exit_status: -1000,
10:28:43,389  INFO Container:835 - Container container_1371849977601_0001_02_000001 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diag
nostics: "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 137\n", exit_status: 137,
10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diagnostics: "", exit_status: -1000,
10:28:43,448  INFO NMAuditLogger:89 - USER=criccomi     OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1371849977601_0001    CONTAINERID=container_1371849977601_0001_02_000001
10:28:43,468  INFO LinuxContainerExecutor:308 - Deleting absolute path : /path/to/yarn-data/usercache/criccomi/appcache/application_1371849977601_0001/container_1371849977601_0001_02_000001
10:28:43,481  INFO LinuxContainerExecutor:318 -  -- DEBUG -- deleteAsUser: [/path/to/yarn/i001/bin/container-executor, criccomi, 3, /path/to/yarn-data/usercache/criccomi/appcache/application_1371849977601_0001/container_1371849977601_0001_02_000001]
10:28:43,556  INFO Container:835 - Container container_1371849977601_0001_02_000001 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
10:28:43,666  INFO Application:321 - Removing container_1371849977601_0001_02_000001 from application application_1371849977601_0001
10:28:44,391  INFO NodeManager:253 - Done waiting for containers to be killed. Still alive: [container_1371849977601_0001_02_000001, container_1371849977601_0001_02_000003, container_1371849977601_0002_02_000003, container_1371849977601_0003_01_000004, container_1371849977601_0004_01_000002]
10:28:44,559  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl is stopped.
10:28:44,861  WARN AsyncDispatcher:109 - Interrupted Exception while stopping
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Thread.join(Thread.java:1143)
        at java.lang.Thread.join(Thread.java:1196)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:107)
        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)
        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:219)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:347)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:61)
				        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)
				        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
				        at java.lang.Thread.run(Thread.java:619)
10:28:45,162  INFO AbstractService:113 - Service:Dispatcher is stopped.
10:28:44,913  INFO ContainersMonitorImpl:347 - Stopping resource-monitoring for container_1371849977601_0001_02_000001
10:28:45,196  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 4324 for container-id container_1371849977601_0001_02_000003: 522.9 MB of 768 MB physical memory used; 1.1 GB of 6 GB virtual memory used
10:28:45,230  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 5717 for container-id container_1371849977601_0002_02_000003: 608.3 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used
10:28:45,266  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 26908 for container-id container_1371849977601_0004_01_000002: 16.4 GB of 19.3 GB physical memory used; 17.0 GB of 154 GB virtual memory used
10:28:45,291  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27868 for container-id container_1371849977601_0003_01_000004: 4.2 GB of 6.1 GB physical memory used; 5.4 GB of 49 GB virtual memory used
10:28:45,608  INFO log:67 - Stopped SelectChannelConnector@0.0.0.0:9999
10:28:45,709  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer is stopped.
10:28:45,709  INFO Server:2060 - Stopping server on 45454
10:28:45,718  INFO Server:654 - Stopping IPC Server listener on 45454
10:28:45,759  INFO Server:796 - Stopping IPC Server Responder
10:28:45,977  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler is stopped.
10:28:45,978  INFO AbstractService:113 - Service:Dispatcher is stopped.
10:28:45,978  WARN ContainersMonitorImpl:463 - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.
10:28:45,978  INFO AbstractService:113 - Service:containers-monitor is stopped.
10:28:45,978  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices is stopped.
10:28:45,979  INFO AbstractService:113 - Service:containers-launcher is stopped.
10:28:45,979  INFO Server:2060 - Stopping server on 4344
10:28:45,995  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371849977601_0002_02_000003
10:28:45,996  WARN ContainerLaunch:247 - Failed to launch container.
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)
        at org.apache.hadoop.util.Shell.run(Shell.java:129)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
10:28:46,076  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371849977601_0003_01_000004
10:28:46,076  WARN ContainerLaunch:247 - Failed to launch container.
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)
        at org.apache.hadoop.util.Shell.run(Shell.java:129)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
10:28:46,076  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371849977601_0004_01_000002
10:28:46,076  WARN ContainerLaunch:247 - Failed to launch container.
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)
        at org.apache.hadoop.util.Shell.run(Shell.java:129)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
10:28:46,109  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371849977601_0001_02_000003
10:28:46,109  WARN ContainerLaunch:247 - Failed to launch container.
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)
        at org.apache.hadoop.util.Shell.run(Shell.java:129)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)
        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
10:28:46,125  INFO Server:796 - Stopping IPC Server Responder
10:28:46,125  INFO Server:654 - Stopping IPC Server listener on 4344
10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker is stopped.
10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService is stopped.
10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl is stopped.
10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is stopped.
10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService is stopped.
10:28:46,384  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService is stopped.
10:28:46,384  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.
10:28:46,384  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeManager is stopped.
10:28:46,384  INFO MetricsSystemImpl:200 - Stopping NodeManager metrics system...
10:28:46,385  INFO MetricsSystemImpl:206 - NodeManager metrics system stopped.
10:28:46,385  INFO MetricsSystemImpl:572 - NodeManager metrics system shutdown complete.
10:28:46,385  INFO NodeManager:315 - Rebooting the node manager.
{noformat}

Cheers,
Chris, Wondering if this is because of YARN-495? I applied YARN-688, but YARN-495 didn't apply easily to the 2.0.5-alpha branch, so I didn't use it., patch for NM clean up containers on SHUTDOWN and REBOOT event., Hi Chris
Yes, the log shows its on REBOOT event. The earlier patch only takes care of SHUTDOWN event, uploaded a new patch for that., Hey Jian,

Awesome. I've patched and started the cluster with YARN-600, YARN-799, and YARN-864. I'll keep you posted.

Cheers,
Chris, Hey Jian,

With your patch applied, the new error (in the NM) is:

{noformat}
19:33:36,741  INFO NodeStatusUpdaterImpl:365 - Node is out of sync with ResourceManager, hence rebooting.
19:33:36,764  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 14751 for container-id container_1372091455469_0002_01_000002: 779.3 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used
19:33:37,239  INFO NodeManager:315 - Rebooting the node manager.
19:33:37,261  INFO NodeManager:229 - Containers still running on shutdown: [container_1372091455469_0002_01_000002]
19:33:37,278 FATAL AsyncDispatcher:137 - Error in dispatcher thread
org.apache.hadoop.metrics2.MetricsException: Metrics source JvmMetrics already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:126)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:107)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:217)
	at org.apache.hadoop.metrics2.source.JvmMetrics.create(JvmMetrics.java:79)
	at org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics.create(NodeManagerMetrics.java:49)
	at org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics.create(NodeManagerMetrics.java:45)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.<init>(NodeManager.java:75)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.createNewNodeManager(NodeManager.java:357)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.reboot(NodeManager.java:316)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:348)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:61)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
	at java.lang.Thread.run(Thread.java:619)
{noformat}

For the record, you can reproduce this yourself by:

1. Start a YARN RM and NM.
2. Run a YARN job on the cluster that uses at least one container.
3. Run kill -STOP <NM PID> on the NM.
4. Wait 65 seconds (enough for the NM to time out).
5. Run kill -CONT <NM PID>

You will see the NM trigger a reboot since it's out of sync with the RM., Hi Chris 
that failure was due to reboot starts even before stop fully completes.
Uploaded a new patch, tested locally. let me know if that works, thx, Hey Jian,

I re-deployed my test cluster with YARN-600, YARN-799, and your latest patch (.2.patch) from YARN-864. I simulated the timeout using kill -STOP (as described above), and your patch worked! :)

I'm going to let the cluster run for 24h before declaring victory, but this looks promising. I'll follow up tomorrow, when I know more.

Cheers,
Chris, Hey Guys,

Thanks for all the help. The cluster has been stable after applying YARN-864.2.patch.

Jian, is this a patch that's already incorporated into 2.1.0-beta, or has it not yet been applied to anything yet?

Cheers,
Chris, Hi Chris
YARN-495 was already committed which deals with containers cleanup in RESYNC case (REBOOT is changed to RESYNC in that patch).
YARN-688 which takes care of the SHUTDOWN event still needs sometime to get in., Given Jian's update, I'm closing this as duplicate of YARN-688.]