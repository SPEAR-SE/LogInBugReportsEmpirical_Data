[[~kasha], it seems similar situation as mentioned in YARN-5636.
Should we have a common mechanism that supports "reserving certain resource on a certain node for a certain app for a while" ?, [~Tao Jie], I like the idea that you described in YARN-5636.

I did some testing and prototyping on this Jira. I am open to a wider solution as well.

1. What I have learned so far is that preemption has a conflict with the following code:
{code}
isReservable(capability) &&
        reserve(pendingAsk.getPerAllocationResource(), node, reservedContainer,
            type, schedulerKey)
{code}
Basically the preempted application will have an excess demand and it will reserve the lost resources. They will be assigned back to it, when they become free.
2. We also need to be careful with prioritizing reservations. For example how it works now is that a reservation takes priority before any other request.
What happens, if I have a preemption from a lower priority request but there is a demand from a higher priority application?
3. It is a great idea to have a timeout. We also need to take into consideration proper release of the reservation. If the app is killed, all reservations should be released.
I do not see such code in the current reservation code for FS.

, Thank you [~miklos.szegedi@cloudera.com] for sharing your thought.
1, It is easy to confuse the reservation we are taking about with the current reservation mechanism in scheduler. IIRC, the purpose of current reservation is to prevent starvation of request with large resource. And our reservation here is to assign container on node to one exact application.
2, I feel both OK about 1)reuse/extend current reservation mechanism or 2)add another logic to handle the reservation for preemption.  If is 2), it's better to find another name to avoid naming confusion.
3,
{quote}
2. We also need to be careful with prioritizing reservations. For example how it works now is that a reservation takes priority before any other request.
What happens, if I have a preemption from a lower priority request but there is a demand from a higher priority application?
{quote}
In my opinion, the reservation for preemption should have higher priority than current reservation in allocation. If starved application that triggers preempting is not satisfied as soon as possible, it will still in starvation and try to preempt more containers. However a normal application has reservation container on nodes would wait for a while since the resource is allocated to another starved application, it makes sense that application would get higher priority when itself becomes a starved application. , Thank you, [~Tao Jie] for the reply.
1, I suggest turning off large resource starvation, if preemption is enabled. They do not work well together. In my situation A was preempted by B with equal demand but A kept reserving the preempted containers to itself.
2, I will share the patch soon for this Jira. From there we can decide, whether it is worth to expand to a complete solution or keep it just for preemption.
3, I agree, I also lean towards reservation for preemption being the highest priority allocation. The rationality behind it is that the preemption decision was made previously, not in the current situation. However, this might not be the opinion of others. I would be glad to see confirmation from the community., GitHub user szegedim opened a pull request:

    https://github.com/apache/hadoop/pull/201

    YARN-5829 Reserve containers for preemptors

    YARN-5829 Reserve containers for preemptors.
    The change does the following.
    1. When a container is selected for preemption, it stores the preemptor next to the container.
    2. When a container is finally completed it creates a reservation for the preemptor application.
    3. On the next node update we check first reservations, then preemption reservations. They are satisfied before any other request, even if those have higher priorities. This is used tho avoid stealing preempted containers.
    4. It also disables standard reservations because I noticed that the preempted applications reserve nodes for themselves stealing back the preempted resources.
    5. If a preemptor is deleted before the preemption or after the preemption, the assignment won't happen at node update.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/szegedim/hadoop YARN-5829

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hadoop/pull/201.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #201
    
----
commit 07acc98178ea88bdd1621dc39e2aa28354be7f63
Author: Miklos Szegedi <miklos.szegedi@cloudera.com>
Date:   2017-03-10T04:50:08Z

    YARN-5829 Initial suggestion. Track preempted containers and create reservation, when they free up

commit 7009aac04f0eb9301868b50c6e0c1720db3b92e2
Author: Miklos Szegedi <miklos.szegedi@cloudera.com>
Date:   2017-03-11T01:00:28Z

    YARN-5829 Added unit test, fixed concurrent exception bug with multiple preempted applications.

commit bbaa3a5980d1e25d2788c5f80539dfcdff500184
Author: Miklos Szegedi <miklos.szegedi@cloudera.com>
Date:   2017-03-11T01:42:15Z

    YARN-5829 Make sure deleted applications do not satisfy reservations.

----
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105787743
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -118,23 +178,56 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
        *
        * @param containers container to mark
        */
    -  void addContainersForPreemption(Collection<RMContainer> containers) {
    -    containersForPreemption.addAll(containers);
    +  void addContainersForPreemption(Collection<RMContainer> containers,
    +                                  FSAppAttempt appAttempt) {
    +    for(RMContainer container : containers) {
    +      containersForPreemption.put(container, appAttempt);
    --- End diff --
    
    Actually, if we do that, the first list of containers need not be a map. It could continue to be a set. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105792954
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,360 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.Container;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.ExecutionType;
    +import org.apache.hadoop.yarn.api.records.Resource;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    --- End diff --
    
    Should the tests related to preemptions be in TestFairSchedulerPreemption instead? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105792834
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,6 +132,44 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * @return if any resources were allocated
    +   */
    +  synchronized boolean assignContainersToPreemptionReservees() {
    --- End diff --
    
    I am slightly concerned about this splitting the container allocation code across too many places. The alternative is to do this in FairScheduler but maintain the map here, which is also ugly. Would it make any sense to hold this information in FSContext? 
    
    Just calling out we need to think through this carefully. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105785747
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java ---
    @@ -831,7 +831,11 @@ private Resource assignContainer(
         }
     
         // The desired container won't fit here, so reserve
    -    if (isReservable(capability) &&
    +    // Important: Do not enable this kind of reservation in case of preemption,
    +    // since the preempted application will re-reserve the preempted resources
    +    // and prevent the starving application to get them
    +    if (!scheduler.getConf().getPreemptionEnabled() &&
    --- End diff --
    
    This seems very bad. What happens if preemption is enabled, but no containers are preempted. However, if there are apps requesting large containers that don't fit in the free resources created by smaller containers finishing, these apps end up being starved forever. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105786285
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,30 +18,52 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.util.Collection;
    +import java.util.Comparator;
    +import java.util.Iterator;
    +import java.util.LinkedHashMap;
    +import java.util.Map;
     import java.util.Set;
    -import java.util.concurrent.ConcurrentSkipListSet;
    +import java.util.concurrent.ConcurrentSkipListMap;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    -
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Map<RMContainer, FSAppAttempt> containersForPreemption =
    +      new ConcurrentSkipListMap<>(comparator);
    +
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  Map<FSAppAttempt, Resource> reservedApp =
    --- End diff --
    
    This map can be final as well. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105790059
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -118,23 +178,56 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
        *
        * @param containers container to mark
        */
    -  void addContainersForPreemption(Collection<RMContainer> containers) {
    -    containersForPreemption.addAll(containers);
    +  void addContainersForPreemption(Collection<RMContainer> containers,
    +                                  FSAppAttempt appAttempt) {
    +    for(RMContainer container : containers) {
    +      containersForPreemption.put(container, appAttempt);
    +    }
       }
     
       /**
        * @return set of containers marked for preemption.
        */
       Set<RMContainer> getContainersForPreemption() {
    -    return containersForPreemption;
    +    return containersForPreemption.keySet();
       }
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
    -  void removeContainerForPreemption(RMContainer container) {
    +  private synchronized void removeContainerForPreemption(RMContainer container) {
    --- End diff --
    
    If we add the entry to the map when we add container, this method doesn't need to be changed. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105787301
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -118,23 +178,56 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
        *
        * @param containers container to mark
        */
    -  void addContainersForPreemption(Collection<RMContainer> containers) {
    -    containersForPreemption.addAll(containers);
    +  void addContainersForPreemption(Collection<RMContainer> containers,
    +                                  FSAppAttempt appAttempt) {
    +    for(RMContainer container : containers) {
    +      containersForPreemption.put(container, appAttempt);
    --- End diff --
    
    We should add the entry to the other map also here. That way, we count allocations prior to the preemptions also against the same app. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105786158
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,30 +18,52 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.util.Collection;
    +import java.util.Comparator;
    +import java.util.Iterator;
    +import java.util.LinkedHashMap;
    +import java.util.Map;
     import java.util.Set;
    -import java.util.concurrent.ConcurrentSkipListSet;
    +import java.util.concurrent.ConcurrentSkipListMap;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    -
    +  private static final Comparator<RMContainer> comparator =
    --- End diff --
    
    Why do we need this comparator?
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105786728
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,30 +18,52 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.util.Collection;
    +import java.util.Comparator;
    +import java.util.Iterator;
    +import java.util.LinkedHashMap;
    +import java.util.Map;
     import java.util.Set;
    -import java.util.concurrent.ConcurrentSkipListSet;
    +import java.util.concurrent.ConcurrentSkipListMap;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    -
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Map<RMContainer, FSAppAttempt> containersForPreemption =
    +      new ConcurrentSkipListMap<>(comparator);
    +
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  Map<FSAppAttempt, Resource> reservedApp =
    --- End diff --
    
    Instead of calling this reservedApp which can be confused with reservedAppSchedulable, how about we call this resourcesPreemptedPerApp?
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105790122
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -118,23 +178,56 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
        *
        * @param containers container to mark
        */
    -  void addContainersForPreemption(Collection<RMContainer> containers) {
    -    containersForPreemption.addAll(containers);
    +  void addContainersForPreemption(Collection<RMContainer> containers,
    +                                  FSAppAttempt appAttempt) {
    +    for(RMContainer container : containers) {
    +      containersForPreemption.put(container, appAttempt);
    +    }
       }
     
       /**
        * @return set of containers marked for preemption.
        */
       Set<RMContainer> getContainersForPreemption() {
    -    return containersForPreemption;
    +    return containersForPreemption.keySet();
       }
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
    -  void removeContainerForPreemption(RMContainer container) {
    +  private synchronized void removeContainerForPreemption(RMContainer container) {
    +    FSAppAttempt app = containersForPreemption.get(container);
    +    if (app != null) {
    +      Resource containerSize =
    +          Resources.clone(container.getAllocatedResource());
    +      if (!reservedApp.containsKey(app)) {
    +        reservedApp.put(app, containerSize);
    +      } else {
    +        Resources.addTo(reservedApp.get(app),
    +            Resources.clone(containerSize));
    +      }
    +    }
         containersForPreemption.remove(container);
       }
    +
    +  /**
    +   * Release an allocated container on this node.
    +   * It also releases from the reservation list to trigger preemption
    +   * allocations.
    +   * @param containerId ID of container to be released.
    +   * @param releasedByNode whether the release originates from a node update.
    +   */
    +  @Override
    +  public synchronized void releaseContainer(ContainerId containerId,
    +                                            boolean releasedByNode) {
    +    RMContainer container = getContainer(containerId);
    +    super.releaseContainer(containerId, releasedByNode);
    +    if (container != null) {
    +      removeContainerForPreemption(container);
    --- End diff --
    
    This is nice to see. We should have done this to begin with. 
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105794343
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,30 +18,52 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.util.Collection;
    +import java.util.Comparator;
    +import java.util.Iterator;
    +import java.util.LinkedHashMap;
    +import java.util.Map;
     import java.util.Set;
    -import java.util.concurrent.ConcurrentSkipListSet;
    +import java.util.concurrent.ConcurrentSkipListMap;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    -
    +  private static final Comparator<RMContainer> comparator =
    --- End diff --
    
    The unit test failed without it.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105795629
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,30 +18,52 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.util.Collection;
    +import java.util.Comparator;
    +import java.util.Iterator;
    +import java.util.LinkedHashMap;
    +import java.util.Map;
     import java.util.Set;
    -import java.util.concurrent.ConcurrentSkipListSet;
    +import java.util.concurrent.ConcurrentSkipListMap;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    -
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Map<RMContainer, FSAppAttempt> containersForPreemption =
    +      new ConcurrentSkipListMap<>(comparator);
    +
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  Map<FSAppAttempt, Resource> reservedApp =
    --- End diff --
    
    Sure, done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r105796619
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,6 +132,44 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * @return if any resources were allocated
    +   */
    +  synchronized boolean assignContainersToPreemptionReservees() {
    --- End diff --
    
    The reason I brought it here is that we need to run it synchronized to make sure the list does not change. The preemption reservation data belongs here I think. What I could do is that do the assignContainersToPreemptionReservees tasks in fair scheduler and expose the lock to that function.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r106237085
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,30 +18,52 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.util.Collection;
    +import java.util.Comparator;
    +import java.util.Iterator;
    +import java.util.LinkedHashMap;
    +import java.util.Map;
     import java.util.Set;
    -import java.util.concurrent.ConcurrentSkipListSet;
    +import java.util.concurrent.ConcurrentSkipListMap;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    -
    +  private static final Comparator<RMContainer> comparator =
    --- End diff --
    
    java.lang.ClassCastException: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer$$EnhancerByMockitoWithCGLIB$$4607190f cannot be cast to java.lang.Comparable
    
    	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655)
    	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:794)
    	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546)
    	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode.removeContainerForPreemption(FSSchedulerNode.java:203)
    	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode.releaseContainer(FSSchedulerNode.java:230)
    	at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSSchedulerNode.testPreemptionToCompletedApp(TestFSSchedulerNode.java:302)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
    	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
    	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
    	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
    	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
    	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
    	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
    	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    	at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
    	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
    	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:51)
    	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:237)
    	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r106266603
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,360 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.Container;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.ExecutionType;
    +import org.apache.hadoop.yarn.api.records.Resource;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    --- End diff --
    
    Only, if all the preemption classes are used by the test. This is a real unit test for the class FSSchedulerNode. In this case the unit test is restricted to mocks and only FSSchedulerNode is a real class. All in all, I see this as a test of a limited subset of preemption functionality restricted to FSSchedulerNode, so it should be in a class named TestFSSchedulerNode. If you want me to add additional functional tests to TestFairSchedulerPreemption, I am happy to do that.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r106269289
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -118,23 +178,56 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
        *
        * @param containers container to mark
        */
    -  void addContainersForPreemption(Collection<RMContainer> containers) {
    -    containersForPreemption.addAll(containers);
    +  void addContainersForPreemption(Collection<RMContainer> containers,
    +                                  FSAppAttempt appAttempt) {
    +    for(RMContainer container : containers) {
    +      containersForPreemption.put(container, appAttempt);
    +    }
       }
     
       /**
        * @return set of containers marked for preemption.
        */
       Set<RMContainer> getContainersForPreemption() {
    -    return containersForPreemption;
    +    return containersForPreemption.keySet();
       }
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
    -  void removeContainerForPreemption(RMContainer container) {
    +  private synchronized void removeContainerForPreemption(RMContainer container) {
    +    FSAppAttempt app = containersForPreemption.get(container);
    +    if (app != null) {
    +      Resource containerSize =
    +          Resources.clone(container.getAllocatedResource());
    +      if (!reservedApp.containsKey(app)) {
    +        reservedApp.put(app, containerSize);
    +      } else {
    +        Resources.addTo(reservedApp.get(app),
    +            Resources.clone(containerSize));
    +      }
    +    }
         containersForPreemption.remove(container);
       }
    +
    +  /**
    +   * Release an allocated container on this node.
    +   * It also releases from the reservation list to trigger preemption
    +   * allocations.
    +   * @param containerId ID of container to be released.
    +   * @param releasedByNode whether the release originates from a node update.
    +   */
    +  @Override
    +  public synchronized void releaseContainer(ContainerId containerId,
    +                                            boolean releasedByNode) {
    +    RMContainer container = getContainer(containerId);
    +    super.releaseContainer(containerId, releasedByNode);
    +    if (container != null) {
    +      removeContainerForPreemption(container);
    --- End diff --
    
    Thanks!
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r106269431
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -118,23 +178,56 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
        *
        * @param containers container to mark
        */
    -  void addContainersForPreemption(Collection<RMContainer> containers) {
    -    containersForPreemption.addAll(containers);
    +  void addContainersForPreemption(Collection<RMContainer> containers,
    +                                  FSAppAttempt appAttempt) {
    +    for(RMContainer container : containers) {
    +      containersForPreemption.put(container, appAttempt);
    +    }
       }
     
       /**
        * @return set of containers marked for preemption.
        */
       Set<RMContainer> getContainersForPreemption() {
    -    return containersForPreemption;
    +    return containersForPreemption.keySet();
       }
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
    -  void removeContainerForPreemption(RMContainer container) {
    +  private synchronized void removeContainerForPreemption(RMContainer container) {
    --- End diff --
    
    Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r106269539
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -118,23 +178,56 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
        *
        * @param containers container to mark
        */
    -  void addContainersForPreemption(Collection<RMContainer> containers) {
    -    containersForPreemption.addAll(containers);
    +  void addContainersForPreemption(Collection<RMContainer> containers,
    +                                  FSAppAttempt appAttempt) {
    +    for(RMContainer container : containers) {
    +      containersForPreemption.put(container, appAttempt);
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r106274089
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java ---
    @@ -831,7 +831,11 @@ private Resource assignContainer(
         }
     
         // The desired container won't fit here, so reserve
    -    if (isReservable(capability) &&
    +    // Important: Do not enable this kind of reservation in case of preemption,
    +    // since the preempted application will re-reserve the preempted resources
    +    // and prevent the starving application to get them
    +    if (!scheduler.getConf().getPreemptionEnabled() &&
    --- End diff --
    
    I improved this by giving higher priority to preempted containers than reservation. Child queues may pass containers to each other this way resolving fairness starvation. Please review the updated change.
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107801140
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -131,10 +203,58 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
       void removeContainerForPreemption(RMContainer container) {
         containersForPreemption.remove(container);
       }
    +
    +  /**
    +   * The Scheduler has allocated containers on this node to the given
    +   * application.
    +   * @param rmContainer Allocated container
    +   * @param launchedOnNode True if the container has been launched
    +   */
    +  @Override
    +  protected synchronized void allocateContainer(RMContainer rmContainer,
    +                                                boolean launchedOnNode) {
    +    super.allocateContainer(rmContainer, launchedOnNode);
    +    Resource allocated = rmContainer.getAllocatedResource();
    +    if (!Resources.isNone(allocated)) {
    +      for (FSAppAttempt app: resourcesPreemptedPerApp.keySet()) {
    +        if (app.getApplicationAttemptId().equals(
    +            rmContainer.getApplicationAttemptId())) {
    +          Resource reserved = resourcesPreemptedPerApp.get(app);
    +          Resource fulfilled = Resources.componentwiseMin(reserved, allocated);
    --- End diff --
    
    This is not necessary if we use subtractFromNonNegative in the following two statements. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107779617
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedPerApp = new LinkedHashMap<>();
    +  Resource totalResourcesPreempted = Resource.newInstance(0, 0);
     
       public FSSchedulerNode(RMNode node, boolean usePortForNodeName) {
         super(node, usePortForNodeName);
       }
     
    +  /**
    +   * Total amount of reserved resources including reservations and preempted
    +   * containers.
    +   * @return total resources reserved
    +   */
    +  Resource getTotalReserved() {
    +    Resource totalReserved = getReservedContainer() != null ?
    +        Resources.clone(getReservedContainer().getAllocatedResource()) :
    --- End diff --
    
    My personal preference is for the "?" to go on the same line as the corresponding value, and the same for ":".
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107778404
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedPerApp = new LinkedHashMap<>();
    --- End diff --
    
    The variable name could be construed two ways: (1) resources being preempted **from** an app and (2) resources being preempted **for** an app. It would be nice to pick a name that avoids this ambiguity. 
    
    Also, should the associated comment be updated? Let us keep the comment as simple as possible. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107774052
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSPreemptionThread.java ---
    @@ -113,11 +113,6 @@ public void run() {
             List<FSSchedulerNode> potentialNodes = scheduler.getNodeTracker()
                 .getNodesByResourceName(rr.getResourceName());
             for (FSSchedulerNode node : potentialNodes) {
    -          // TODO (YARN-5829): Attempt to reserve the node for starved app.
    -          if (isNodeAlreadyReserved(node, starvedApp)) {
    -            continue;
    -          }
    -
    --- End diff --
    
    Observation: We aren't checking for the node being reserved because the reservation is only for larger containers. Even if a node is reserved, we can preempt running containers to fit in a new app. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107779451
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedPerApp = new LinkedHashMap<>();
    +  Resource totalResourcesPreempted = Resource.newInstance(0, 0);
     
       public FSSchedulerNode(RMNode node, boolean usePortForNodeName) {
         super(node, usePortForNodeName);
       }
     
    +  /**
    +   * Total amount of reserved resources including reservations and preempted
    +   * containers.
    +   * @return total resources reserved
    +   */
    +  Resource getTotalReserved() {
    +    Resource totalReserved = getReservedContainer() != null ?
    +        Resources.clone(getReservedContainer().getAllocatedResource()) :
    +        Resource.newInstance(0, 0);
    --- End diff --
    
    Use Resources.clone(Resources.none()). That way, you could do 
    
    Resources.clone(
    getReservedContainer() != null ? getReservedContainer.getAllocatedResource() : Resources.none())
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107777379
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    --- End diff --
    
    How about having RMContainer implement Comparable instead? 
    
    RMContainerImpl already has an implementation that IMO should just be in RMContainer. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107778744
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedPerApp = new LinkedHashMap<>();
    +  Resource totalResourcesPreempted = Resource.newInstance(0, 0);
    --- End diff --
    
    This variable is tracking total resources that are slated for preemption. Should we update the name to reflect that and maybe also add a comment? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107772839
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java ---
    @@ -841,7 +841,11 @@ private Resource assignContainer(
         }
     
         // The desired container won't fit here, so reserve
    -    if (isReservable(capability) &&
    +    boolean reservedToOtherApp = (reservedContainer != null &&
    +        reservedContainer.getId().getApplicationAttemptId().
    +            equals(getApplicationAttemptId()));
    +    if (reservedToOtherApp &&
    --- End diff --
    
    This if-condition is getting a little too complex. Can we move these checks to reserve() itself? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107776870
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
    --- End diff --
    
    We avoid wildcard imports in YARN land. Can we just include the necessary imports? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107801908
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -987,9 +978,19 @@ void attemptScheduling(FSSchedulerNode node) {
           }
     
           // Assign new containers...
    -      // 1. Check for reserved applications
    -      // 2. Schedule if there are no reservations
    -
    +      // 1. Ensure containers are assigned to the apps that preempted
    +      // 2. Check for reserved applications
    +      // 3. Schedule if there are no reservations
    +
    +      // Apps may wait for preempted containers
    +      // We have to satisfy these first to avoid cases, when we preempt
    +      // a container for A from B and C gets the preempted containers,
    +      // when C does not qualify for preemption itself.
    +      for (FSAppAttempt app : node.getPreemptionList()) {
    --- End diff --
    
    We don't seem to handle the case where an app is to be allocated multiple containers from based on the preempted resources.
    
    Would it help to store the apps in a TreeMap in FSSchedulerNode and have a method that returns the next preempted app to allocate to? If none exist, return null? Also, if this were to work, we might have to revert back to the behavior in an earlier patch: add app/resources to the map only after the container is actually preempted? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107777757
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    --- End diff --
    
    Rephrase to say "Stores list of containers still to be preempted"
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107801268
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -93,15 +91,8 @@
     import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.io.IOException;
    -import java.util.ArrayList;
    -import java.util.Collection;
    -import java.util.Comparator;
    -import java.util.EnumSet;
    -import java.util.HashSet;
    -import java.util.List;
    -import java.util.Map;
    +import java.util.*;
    --- End diff --
    
    Same comment as above. Let us avoid using wildcard imports
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107799584
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -131,10 +203,58 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
       void removeContainerForPreemption(RMContainer container) {
         containersForPreemption.remove(container);
       }
    +
    +  /**
    +   * The Scheduler has allocated containers on this node to the given
    +   * application.
    +   * @param rmContainer Allocated container
    +   * @param launchedOnNode True if the container has been launched
    +   */
    +  @Override
    +  protected synchronized void allocateContainer(RMContainer rmContainer,
    +                                                boolean launchedOnNode) {
    +    super.allocateContainer(rmContainer, launchedOnNode);
    +    Resource allocated = rmContainer.getAllocatedResource();
    +    if (!Resources.isNone(allocated)) {
    +      for (FSAppAttempt app: resourcesPreemptedPerApp.keySet()) {
    --- End diff --
    
    Given this is a map, why aren't we just doing a map.get()? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107776685
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSPreemptionThread.java ---
    @@ -164,7 +160,10 @@ private PreemptableContainers identifyContainersToPreemptOnNode(
         containersToCheck.removeAll(node.getContainersForPreemption());
     
         // Initialize potential with unallocated resources
    -    Resource potential = Resources.clone(node.getUnallocatedResource());
    +    Resource potential = Resources.clone(
    +        Resources.componentwiseMax(Resources.none(),
    --- End diff --
    
    Would using Resources.subtractFromNonNegative be better here for one fewer Resource object creation.
    
    If we decide to keep this, the comment above needs to be changed accordingly. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107802366
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -987,9 +978,19 @@ void attemptScheduling(FSSchedulerNode node) {
           }
     
           // Assign new containers...
    -      // 1. Check for reserved applications
    -      // 2. Schedule if there are no reservations
    -
    +      // 1. Ensure containers are assigned to the apps that preempted
    +      // 2. Check for reserved applications
    +      // 3. Schedule if there are no reservations
    +
    +      // Apps may wait for preempted containers
    +      // We have to satisfy these first to avoid cases, when we preempt
    +      // a container for A from B and C gets the preempted containers,
    +      // when C does not qualify for preemption itself.
    +      for (FSAppAttempt app : node.getPreemptionList()) {
    --- End diff --
    
    Also, we are not counting allocated resources/containers against the assignMultiple. We probably should so that we limit the number of non-preemption, non-reserved containers/resources appropriately. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107780786
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -131,10 +203,58 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
       void removeContainerForPreemption(RMContainer container) {
    --- End diff --
    
    This is a one-liner method that is only used in the same class. Can we get rid of the method? 
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107804587
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSPreemptionThread.java ---
    @@ -113,11 +113,6 @@ public void run() {
             List<FSSchedulerNode> potentialNodes = scheduler.getNodeTracker()
                 .getNodesByResourceName(rr.getResourceName());
             for (FSSchedulerNode node : potentialNodes) {
    -          // TODO (YARN-5829): Attempt to reserve the node for starved app.
    -          if (isNodeAlreadyReserved(node, starvedApp)) {
    -            continue;
    -          }
    -
    --- End diff --
    
    Indeed, preemption reservation has higher priority.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107804983
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSPreemptionThread.java ---
    @@ -164,7 +160,10 @@ private PreemptableContainers identifyContainersToPreemptOnNode(
         containersToCheck.removeAll(node.getContainersForPreemption());
     
         // Initialize potential with unallocated resources
    -    Resource potential = Resources.clone(node.getUnallocatedResource());
    +    Resource potential = Resources.clone(
    +        Resources.componentwiseMax(Resources.none(),
    --- End diff --
    
    Yes that is nicer. Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107805522
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
    --- End diff --
    
    Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107805618
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    --- End diff --
    
    RMContainer is an interface, it cannot implement a method.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107805823
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    --- End diff --
    
    All right.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107806119
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedPerApp = new LinkedHashMap<>();
    --- End diff --
    
    All right.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107806509
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedPerApp = new LinkedHashMap<>();
    +  Resource totalResourcesPreempted = Resource.newInstance(0, 0);
    --- End diff --
    
    All right.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107806639
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedPerApp = new LinkedHashMap<>();
    +  Resource totalResourcesPreempted = Resource.newInstance(0, 0);
     
       public FSSchedulerNode(RMNode node, boolean usePortForNodeName) {
         super(node, usePortForNodeName);
       }
     
    +  /**
    +   * Total amount of reserved resources including reservations and preempted
    +   * containers.
    +   * @return total resources reserved
    +   */
    +  Resource getTotalReserved() {
    +    Resource totalReserved = getReservedContainer() != null ?
    +        Resources.clone(getReservedContainer().getAllocatedResource()) :
    --- End diff --
    
    All right.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107806808
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,35 +18,66 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
    -import java.util.Collection;
    -import java.util.Set;
    +import java.util.*;
     import java.util.concurrent.ConcurrentSkipListSet;
     
     @Private
     @Unstable
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores preemption list until the container is completed
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores preemption list after the container is completed before assigned
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedPerApp = new LinkedHashMap<>();
    +  Resource totalResourcesPreempted = Resource.newInstance(0, 0);
     
       public FSSchedulerNode(RMNode node, boolean usePortForNodeName) {
         super(node, usePortForNodeName);
       }
     
    +  /**
    +   * Total amount of reserved resources including reservations and preempted
    +   * containers.
    +   * @return total resources reserved
    +   */
    +  Resource getTotalReserved() {
    +    Resource totalReserved = getReservedContainer() != null ?
    +        Resources.clone(getReservedContainer().getAllocatedResource()) :
    +        Resource.newInstance(0, 0);
    --- End diff --
    
    Sorry, what is the point cloning none? I think the current one is more straightforward.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107806990
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -131,10 +203,58 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
       void removeContainerForPreemption(RMContainer container) {
    --- End diff --
    
    Good catch. Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107807194
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -131,10 +203,58 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
       void removeContainerForPreemption(RMContainer container) {
         containersForPreemption.remove(container);
       }
    +
    +  /**
    +   * The Scheduler has allocated containers on this node to the given
    +   * application.
    +   * @param rmContainer Allocated container
    +   * @param launchedOnNode True if the container has been launched
    +   */
    +  @Override
    +  protected synchronized void allocateContainer(RMContainer rmContainer,
    +                                                boolean launchedOnNode) {
    +    super.allocateContainer(rmContainer, launchedOnNode);
    +    Resource allocated = rmContainer.getAllocatedResource();
    +    if (!Resources.isNone(allocated)) {
    +      for (FSAppAttempt app: resourcesPreemptedPerApp.keySet()) {
    --- End diff --
    
    It is not indexed by ID but the app.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107807464
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -131,10 +203,58 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
       void removeContainerForPreemption(RMContainer container) {
         containersForPreemption.remove(container);
       }
    +
    +  /**
    +   * The Scheduler has allocated containers on this node to the given
    +   * application.
    +   * @param rmContainer Allocated container
    +   * @param launchedOnNode True if the container has been launched
    +   */
    +  @Override
    +  protected synchronized void allocateContainer(RMContainer rmContainer,
    +                                                boolean launchedOnNode) {
    +    super.allocateContainer(rmContainer, launchedOnNode);
    +    Resource allocated = rmContainer.getAllocatedResource();
    +    if (!Resources.isNone(allocated)) {
    +      for (FSAppAttempt app: resourcesPreemptedPerApp.keySet()) {
    +        if (app.getApplicationAttemptId().equals(
    +            rmContainer.getApplicationAttemptId())) {
    +          Resource reserved = resourcesPreemptedPerApp.get(app);
    +          Resource fulfilled = Resources.componentwiseMin(reserved, allocated);
    --- End diff --
    
    Not really, because we would subtract a bigger value from total than from reserved, leaving the state of the map and it's totals inconsistent.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107808165
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -93,15 +91,8 @@
     import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.io.IOException;
    -import java.util.ArrayList;
    -import java.util.Collection;
    -import java.util.Comparator;
    -import java.util.EnumSet;
    -import java.util.HashSet;
    -import java.util.List;
    -import java.util.Map;
    +import java.util.*;
    --- End diff --
    
    All right.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r107820670
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -987,9 +978,19 @@ void attemptScheduling(FSSchedulerNode node) {
           }
     
           // Assign new containers...
    -      // 1. Check for reserved applications
    -      // 2. Schedule if there are no reservations
    -
    +      // 1. Ensure containers are assigned to the apps that preempted
    +      // 2. Check for reserved applications
    +      // 3. Schedule if there are no reservations
    +
    +      // Apps may wait for preempted containers
    +      // We have to satisfy these first to avoid cases, when we preempt
    +      // a container for A from B and C gets the preempted containers,
    +      // when C does not qualify for preemption itself.
    +      for (FSAppAttempt app : node.getPreemptionList()) {
    --- End diff --
    
    > Would it help to store the apps in a TreeMap in FSSchedulerNode and have a method that > returns the next preempted app to allocate to? If none exist, return null? Also, if this were > to work, we might have to revert back to the behavior in an earlier patch: add app/resources to the map only after the container is actually preempted?
    This approach has a clear advantage, that it allocates any newly appeared container to the preemptors. TreeMap might slow down search a little bit. Let's try to continue with this approach but multiple allocations enabled.
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020252
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -131,10 +203,58 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
       void removeContainerForPreemption(RMContainer container) {
         containersForPreemption.remove(container);
       }
    +
    +  /**
    +   * The Scheduler has allocated containers on this node to the given
    +   * application.
    +   * @param rmContainer Allocated container
    +   * @param launchedOnNode True if the container has been launched
    +   */
    +  @Override
    +  protected synchronized void allocateContainer(RMContainer rmContainer,
    +                                                boolean launchedOnNode) {
    +    super.allocateContainer(rmContainer, launchedOnNode);
    +    Resource allocated = rmContainer.getAllocatedResource();
    +    if (!Resources.isNone(allocated)) {
    +      for (FSAppAttempt app: resourcesPreemptedPerApp.keySet()) {
    --- End diff --
    
    A for loop seems a little wasteful. Would it make any sense to keep another map so this can just be a lookup? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108021344
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -968,6 +966,32 @@ private boolean shouldContinueAssigning(int containers,
         }
       }
     
    +  /**
    +   * Assign preempted containers to the applications that have reserved
    +   * resources for preempted containers.
    +   * @param node Node to check
    +   * @return assignment has occurred
    +   */
    +  static boolean assignPreemptedContainers(FSSchedulerNode node ) {
    +    boolean assignedAny = false;
    +    node.cleanupPreemptionList();
    +    for (Entry<FSAppAttempt, Resource> entry :
    +        node.getPreemptionList().entrySet()) {
    +      FSAppAttempt app = entry.getKey();
    +      Resource reserved = Resources.clone(entry.getValue());
    +      while (!app.isStopped() && !Resources.isNone(reserved)) {
    +        Resource assigned = app.assignContainer(node);
    +        if (Resources.isNone(assigned)) {
    +          break;
    --- End diff --
    
    Add a comment as to why we are breaking here. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020399
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,18 +18,27 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.util.Collection;
    +import java.util.Comparator;
    +import java.util.Iterator;
    +import java.util.LinkedHashMap;
    +import java.util.LinkedHashSet;
    --- End diff --
    
    Don't think we are using this import anymore. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108021321
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -968,6 +966,32 @@ private boolean shouldContinueAssigning(int containers,
         }
       }
     
    +  /**
    +   * Assign preempted containers to the applications that have reserved
    +   * resources for preempted containers.
    +   * @param node Node to check
    +   * @return assignment has occurred
    +   */
    +  static boolean assignPreemptedContainers(FSSchedulerNode node ) {
    +    boolean assignedAny = false;
    +    node.cleanupPreemptionList();
    +    for (Entry<FSAppAttempt, Resource> entry :
    +        node.getPreemptionList().entrySet()) {
    +      FSAppAttempt app = entry.getKey();
    +      Resource reserved = Resources.clone(entry.getValue());
    --- End diff --
    
    Can we call this variable pendingPreempted or some such. I would like us to avoid calling this reserved :)
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023329
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    --- End diff --
    
    Also, numContainers might be a better name. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108021362
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -968,6 +966,32 @@ private boolean shouldContinueAssigning(int containers,
         }
       }
     
    +  /**
    +   * Assign preempted containers to the applications that have reserved
    +   * resources for preempted containers.
    +   * @param node Node to check
    +   * @return assignment has occurred
    +   */
    +  static boolean assignPreemptedContainers(FSSchedulerNode node ) {
    +    boolean assignedAny = false;
    +    node.cleanupPreemptionList();
    +    for (Entry<FSAppAttempt, Resource> entry :
    +        node.getPreemptionList().entrySet()) {
    +      FSAppAttempt app = entry.getKey();
    +      Resource reserved = Resources.clone(entry.getValue());
    +      while (!app.isStopped() && !Resources.isNone(reserved)) {
    +        Resource assigned = app.assignContainer(node);
    +        if (Resources.isNone(assigned)) {
    +          break;
    +        }
    +        assignedAny = true;
    +        Resources.subtractFromNonNegative(reserved, assigned);
    +      }
    +    }
    +    node.cleanupPreemptionList();
    --- End diff --
    
    Again, let us avoid cleaning up explicitly. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023073
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    +    Iterator<FSAppAttempt> iterator =
    +        resourcesPreemptedForApp.keySet().iterator();
    +    while (iterator.hasNext()) {
    +      FSAppAttempt app = iterator.next();
    +      boolean removeApp = false;
    +      if (app.isStopped() || Resources.equals(
    --- End diff --
    
    Actually, we probably need to check if the app is starved instead of if it having pending demand. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023563
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    +      schedulerNode.containerStarted(containers.get((int)containerNum - 1).
    +          getContainerId());
    +    }
    +  }
    +
    +  private FSAppAttempt createStarvingApp(FSSchedulerNode schedulerNode,
    +                                         Resource request) {
    +    FSAppAttempt starvingApp = mock(FSAppAttempt.class);
    +    ApplicationAttemptId appAttemptId =
    +        mock(ApplicationAttemptId.class);
    +    when(starvingApp.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(starvingApp.assignContainer(schedulerNode)).thenAnswer(
    +        new Answer<Resource>() {
    +          @Override
    +          public Resource answer(InvocationOnMock invocationOnMock)
    +              throws Throwable {
    +            Resource response = Resource.newInstance(0, 0);
    +            while (!Resources.isNone(request) &&
    +                !Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +              RMContainer container = createContainer(request, appAttemptId);
    +              schedulerNode.allocateContainer(container);
    +              Resources.addTo(response, container.getAllocatedResource());
    +              Resources.subtractFrom(request,
    +                  container.getAllocatedResource());
    +            }
    +            return response;
    +          }
    +        });
    +    when(starvingApp.getPendingDemand()).thenReturn(request);
    +    return starvingApp;
    +  }
    +
    +  private void finalValidation(FSSchedulerNode schedulerNode) {
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    assertTrue("No containers should be reserved for preemption",
    +        schedulerNode.containersForPreemption.isEmpty());
    +    assertTrue("No resources should be reserved for preemptees",
    +        schedulerNode.resourcesPreemptedForApp.isEmpty());
    +    assertEquals(
    +        "No amount of resource should be reserved for preemptees",
    +        Resources.none(),
    +        schedulerNode.getTotalReserved());
    +  }
    +
    +  private void allocateContainers(FSSchedulerNode schedulerNode) {
    +    FairScheduler.assignPreemptedContainers(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimpleAllocation() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    assertEquals("Container should be allocated",
    +        containers.get(0).getContainer().getResource(),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +
    +    // Check that we are error prone
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release three containers with launch.
    +   */
    +  @Test
    +  public void testMultipleAllocations() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    schedulerNode.containerStarted(containers.get(0).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(1));
    +    schedulerNode.containerStarted(containers.get(1).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(2));
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(), 3.0),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(1).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(2).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimplePreemption() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    // Launch containers and saturate the cluster
    +    saturateCluster(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(),
    +            containerNum),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Request preemption
    +    FSAppAttempt starvingApp = createStarvingApp(schedulerNode,
    +        Resource.newInstance(1024, 1));
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(0)), starvingApp);
    +    assertEquals(
    +        "No resource amount should be reserved for preemptees",
    +        containers.get(0).getAllocatedResource(),
    +        schedulerNode.getTotalReserved());
    +
    +    // Preemption occurs
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    allocateContainers(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        schedulerNode.getTotalResource(),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Release all containers
    +    for (int i = 1; i < containerNum; ++i) {
    +      schedulerNode.releaseContainer(containers.get(i).getContainerId(), true);
    +    }
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release three containers requested by two apps.
    +   */
    +  @Test
    +  public void testComplexPreemption() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    // Launch containers and saturate the cluster
    +    saturateCluster(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(),
    +            containerNum),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Preempt a container
    +    FSAppAttempt starvingApp1 = createStarvingApp(schedulerNode,
    +        Resource.newInstance(2048, 2));
    +    FSAppAttempt starvingApp2 = createStarvingApp(schedulerNode,
    +        Resource.newInstance(1024, 1));
    +
    +    // Preemption thread kicks in
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(0)), starvingApp1);
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(1)), starvingApp1);
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(2)), starvingApp2);
    +
    +    // Preemption happens
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(2).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(1).getContainerId(), true);
    +
    +    allocateContainers(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        schedulerNode.getTotalResource(),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Release all containers
    +    for (int i = 3; i < containerNum; ++i) {
    +      schedulerNode.releaseContainer(containers.get(i).getContainerId(), true);
    --- End diff --
    
    Same comment as above. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023245
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    --- End diff --
    
    execution type is repeated. Also, do we need to add a when for all these? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108021737
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java ---
    @@ -646,7 +646,6 @@ private Container createContainer(FSSchedulerNode node, Resource capability,
       private boolean reserve(Resource perAllocationResource, FSSchedulerNode node,
           Container reservedContainer, NodeType type,
           SchedulerRequestKey schedulerKey) {
    -
    --- End diff --
    
    Spurious change. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023542
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    --- End diff --
    
    Actually, can't you just use containers.size() instead? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023320
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    --- End diff --
    
    You don't need to typecast if you change it to int. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023612
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    +      schedulerNode.containerStarted(containers.get((int)containerNum - 1).
    +          getContainerId());
    +    }
    +  }
    +
    +  private FSAppAttempt createStarvingApp(FSSchedulerNode schedulerNode,
    +                                         Resource request) {
    +    FSAppAttempt starvingApp = mock(FSAppAttempt.class);
    +    ApplicationAttemptId appAttemptId =
    +        mock(ApplicationAttemptId.class);
    +    when(starvingApp.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(starvingApp.assignContainer(schedulerNode)).thenAnswer(
    +        new Answer<Resource>() {
    +          @Override
    +          public Resource answer(InvocationOnMock invocationOnMock)
    +              throws Throwable {
    +            Resource response = Resource.newInstance(0, 0);
    +            while (!Resources.isNone(request) &&
    +                !Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +              RMContainer container = createContainer(request, appAttemptId);
    +              schedulerNode.allocateContainer(container);
    +              Resources.addTo(response, container.getAllocatedResource());
    +              Resources.subtractFrom(request,
    +                  container.getAllocatedResource());
    +            }
    +            return response;
    +          }
    +        });
    +    when(starvingApp.getPendingDemand()).thenReturn(request);
    +    return starvingApp;
    +  }
    +
    +  private void finalValidation(FSSchedulerNode schedulerNode) {
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    assertTrue("No containers should be reserved for preemption",
    +        schedulerNode.containersForPreemption.isEmpty());
    +    assertTrue("No resources should be reserved for preemptees",
    +        schedulerNode.resourcesPreemptedForApp.isEmpty());
    +    assertEquals(
    +        "No amount of resource should be reserved for preemptees",
    +        Resources.none(),
    +        schedulerNode.getTotalReserved());
    +  }
    +
    +  private void allocateContainers(FSSchedulerNode schedulerNode) {
    +    FairScheduler.assignPreemptedContainers(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimpleAllocation() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    assertEquals("Container should be allocated",
    +        containers.get(0).getContainer().getResource(),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +
    +    // Check that we are error prone
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release three containers with launch.
    +   */
    +  @Test
    +  public void testMultipleAllocations() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    schedulerNode.containerStarted(containers.get(0).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(1));
    +    schedulerNode.containerStarted(containers.get(1).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(2));
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(), 3.0),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(1).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(2).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimplePreemption() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    // Launch containers and saturate the cluster
    +    saturateCluster(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(),
    +            containerNum),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Request preemption
    +    FSAppAttempt starvingApp = createStarvingApp(schedulerNode,
    +        Resource.newInstance(1024, 1));
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(0)), starvingApp);
    +    assertEquals(
    +        "No resource amount should be reserved for preemptees",
    +        containers.get(0).getAllocatedResource(),
    +        schedulerNode.getTotalReserved());
    +
    +    // Preemption occurs
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    allocateContainers(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        schedulerNode.getTotalResource(),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Release all containers
    +    for (int i = 1; i < containerNum; ++i) {
    +      schedulerNode.releaseContainer(containers.get(i).getContainerId(), true);
    --- End diff --
    
    This seems to be in many tests. How about a helper to release all containers? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023440
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    +      schedulerNode.containerStarted(containers.get((int)containerNum - 1).
    +          getContainerId());
    +    }
    +  }
    +
    +  private FSAppAttempt createStarvingApp(FSSchedulerNode schedulerNode,
    +                                         Resource request) {
    +    FSAppAttempt starvingApp = mock(FSAppAttempt.class);
    +    ApplicationAttemptId appAttemptId =
    +        mock(ApplicationAttemptId.class);
    +    when(starvingApp.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(starvingApp.assignContainer(schedulerNode)).thenAnswer(
    +        new Answer<Resource>() {
    +          @Override
    +          public Resource answer(InvocationOnMock invocationOnMock)
    +              throws Throwable {
    +            Resource response = Resource.newInstance(0, 0);
    +            while (!Resources.isNone(request) &&
    +                !Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +              RMContainer container = createContainer(request, appAttemptId);
    +              schedulerNode.allocateContainer(container);
    +              Resources.addTo(response, container.getAllocatedResource());
    +              Resources.subtractFrom(request,
    +                  container.getAllocatedResource());
    +            }
    +            return response;
    +          }
    +        });
    +    when(starvingApp.getPendingDemand()).thenReturn(request);
    +    return starvingApp;
    +  }
    +
    +  private void finalValidation(FSSchedulerNode schedulerNode) {
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    assertTrue("No containers should be reserved for preemption",
    +        schedulerNode.containersForPreemption.isEmpty());
    +    assertTrue("No resources should be reserved for preemptees",
    --- End diff --
    
    s/preemptee/preemptor
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023020
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    +    Iterator<FSAppAttempt> iterator =
    +        resourcesPreemptedForApp.keySet().iterator();
    +    while (iterator.hasNext()) {
    +      FSAppAttempt app = iterator.next();
    +      boolean removeApp = false;
    --- End diff --
    
    This variable is not used. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023650
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerPreemption.java ---
    @@ -294,11 +294,29 @@ private void verifyPreemption(int numStarvedAppContainers)
             8 - 2 * numStarvedAppContainers,
             greedyApp.getQueue().getMetrics().getAggregatePreemptedContainers());
     
    +    // Verify the node is reserved for the starvingApp
    +    for (RMNode rmNode : rmNodes) {
    +      FSSchedulerNode node = (FSSchedulerNode)
    +          scheduler.getNodeTracker().getNode(rmNode.getNodeID());
    +      if (node.getContainersForPreemption().size() > 0) {
    +        assertTrue(node.getPreemptionList().keySet().contains(starvingApp));
    +      }
    +    }
    +
         sendEnoughNodeUpdatesToAssignFully();
     
         // Verify the preempted containers are assigned to starvingApp
         assertEquals("Starved app is not assigned the right # of containers",
             numStarvedAppContainers, starvingApp.getLiveContainers().size());
    +
    +    // Verify the node is not reserved for the starvingApp anymore
    +    for (RMNode rmNode : rmNodes) {
    +      FSSchedulerNode node = (FSSchedulerNode)
    +          scheduler.getNodeTracker().getNode(rmNode.getNodeID());
    +      if (node.getContainersForPreemption().size() > 0) {
    +        assertTrue(!node.getPreemptionList().keySet().contains(starvingApp));
    --- End diff --
    
    assertFalse?
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020432
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -38,15 +47,45 @@
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    --- End diff --
    
    If you have RMContainer extend Comparable, you don't need this comparator. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023665
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerPreemption.java ---
    @@ -294,11 +294,29 @@ private void verifyPreemption(int numStarvedAppContainers)
             8 - 2 * numStarvedAppContainers,
             greedyApp.getQueue().getMetrics().getAggregatePreemptedContainers());
     
    +    // Verify the node is reserved for the starvingApp
    +    for (RMNode rmNode : rmNodes) {
    +      FSSchedulerNode node = (FSSchedulerNode)
    +          scheduler.getNodeTracker().getNode(rmNode.getNodeID());
    +      if (node.getContainersForPreemption().size() > 0) {
    +        assertTrue(node.getPreemptionList().keySet().contains(starvingApp));
    --- End diff --
    
    assert message. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020477
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -38,15 +47,45 @@
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores list of containers still to be preempted
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores amount of respources preempted and reserved for each app
    --- End diff --
    
    Typo in resources
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020752
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    --- End diff --
    
    Having to call cleanup before get seems like a bug-magnet. 
    
    Since get already calls cleanup, I recommend not suggesting that callers call cleanup. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108021228
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -968,6 +966,32 @@ private boolean shouldContinueAssigning(int containers,
         }
       }
     
    +  /**
    +   * Assign preempted containers to the applications that have reserved
    +   * resources for preempted containers.
    +   * @param node Node to check
    +   * @return assignment has occurred
    +   */
    +  static boolean assignPreemptedContainers(FSSchedulerNode node ) {
    +    boolean assignedAny = false;
    +    node.cleanupPreemptionList();
    --- End diff --
    
    Let us rely on the call to getPreemptionList to do cleanup instead. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023522
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    +      schedulerNode.containerStarted(containers.get((int)containerNum - 1).
    +          getContainerId());
    +    }
    +  }
    +
    +  private FSAppAttempt createStarvingApp(FSSchedulerNode schedulerNode,
    +                                         Resource request) {
    +    FSAppAttempt starvingApp = mock(FSAppAttempt.class);
    +    ApplicationAttemptId appAttemptId =
    +        mock(ApplicationAttemptId.class);
    +    when(starvingApp.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(starvingApp.assignContainer(schedulerNode)).thenAnswer(
    +        new Answer<Resource>() {
    +          @Override
    +          public Resource answer(InvocationOnMock invocationOnMock)
    +              throws Throwable {
    +            Resource response = Resource.newInstance(0, 0);
    +            while (!Resources.isNone(request) &&
    +                !Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +              RMContainer container = createContainer(request, appAttemptId);
    +              schedulerNode.allocateContainer(container);
    +              Resources.addTo(response, container.getAllocatedResource());
    +              Resources.subtractFrom(request,
    +                  container.getAllocatedResource());
    +            }
    +            return response;
    +          }
    +        });
    +    when(starvingApp.getPendingDemand()).thenReturn(request);
    +    return starvingApp;
    +  }
    +
    +  private void finalValidation(FSSchedulerNode schedulerNode) {
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    assertTrue("No containers should be reserved for preemption",
    +        schedulerNode.containersForPreemption.isEmpty());
    +    assertTrue("No resources should be reserved for preemptees",
    +        schedulerNode.resourcesPreemptedForApp.isEmpty());
    +    assertEquals(
    +        "No amount of resource should be reserved for preemptees",
    +        Resources.none(),
    +        schedulerNode.getTotalReserved());
    +  }
    +
    +  private void allocateContainers(FSSchedulerNode schedulerNode) {
    +    FairScheduler.assignPreemptedContainers(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimpleAllocation() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    assertEquals("Container should be allocated",
    +        containers.get(0).getContainer().getResource(),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +
    +    // Check that we are error prone
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release three containers with launch.
    +   */
    +  @Test
    +  public void testMultipleAllocations() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    schedulerNode.containerStarted(containers.get(0).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(1));
    +    schedulerNode.containerStarted(containers.get(1).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(2));
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(), 3.0),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(1).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(2).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimplePreemption() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    // Launch containers and saturate the cluster
    +    saturateCluster(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(),
    +            containerNum),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Request preemption
    +    FSAppAttempt starvingApp = createStarvingApp(schedulerNode,
    +        Resource.newInstance(1024, 1));
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(0)), starvingApp);
    +    assertEquals(
    +        "No resource amount should be reserved for preemptees",
    +        containers.get(0).getAllocatedResource(),
    +        schedulerNode.getTotalReserved());
    +
    +    // Preemption occurs
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    allocateContainers(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        schedulerNode.getTotalResource(),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Release all containers
    +    for (int i = 1; i < containerNum; ++i) {
    +      schedulerNode.releaseContainer(containers.get(i).getContainerId(), true);
    --- End diff --
    
    Can't you go through the container list for this? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020976
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    +    Iterator<FSAppAttempt> iterator =
    +        resourcesPreemptedForApp.keySet().iterator();
    +    while (iterator.hasNext()) {
    +      FSAppAttempt app = iterator.next();
    +      boolean removeApp = false;
    +      if (app.isStopped() || Resources.equals(
    +          app.getPendingDemand(), Resources.none())) {
    +        // App does not need more resources
    +        Resources.subtractFrom(totalResourcesPreempted,
    +            resourcesPreemptedForApp.get(app));
    +        iterator.remove();
    +      }
    +    }
    +  }
    +
    +  /**
        * Mark {@code containers} as being considered for preemption so they are
        * not considered again. A call to this requires a corresponding call to
    -   * {@link #removeContainerForPreemption} to ensure we do not mark a
    +   * removeContainer for preemption to ensure we do not mark a
    --- End diff --
    
    Needs javadoc fixing.
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020913
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    +    Iterator<FSAppAttempt> iterator =
    +        resourcesPreemptedForApp.keySet().iterator();
    +    while (iterator.hasNext()) {
    +      FSAppAttempt app = iterator.next();
    +      boolean removeApp = false;
    +      if (app.isStopped() || Resources.equals(
    --- End diff --
    
    Use Resources.isNone instead.
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023588
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    +      schedulerNode.containerStarted(containers.get((int)containerNum - 1).
    +          getContainerId());
    +    }
    +  }
    +
    +  private FSAppAttempt createStarvingApp(FSSchedulerNode schedulerNode,
    +                                         Resource request) {
    +    FSAppAttempt starvingApp = mock(FSAppAttempt.class);
    +    ApplicationAttemptId appAttemptId =
    +        mock(ApplicationAttemptId.class);
    +    when(starvingApp.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(starvingApp.assignContainer(schedulerNode)).thenAnswer(
    +        new Answer<Resource>() {
    +          @Override
    +          public Resource answer(InvocationOnMock invocationOnMock)
    +              throws Throwable {
    +            Resource response = Resource.newInstance(0, 0);
    +            while (!Resources.isNone(request) &&
    +                !Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +              RMContainer container = createContainer(request, appAttemptId);
    +              schedulerNode.allocateContainer(container);
    +              Resources.addTo(response, container.getAllocatedResource());
    +              Resources.subtractFrom(request,
    +                  container.getAllocatedResource());
    +            }
    +            return response;
    +          }
    +        });
    +    when(starvingApp.getPendingDemand()).thenReturn(request);
    +    return starvingApp;
    +  }
    +
    +  private void finalValidation(FSSchedulerNode schedulerNode) {
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    assertTrue("No containers should be reserved for preemption",
    +        schedulerNode.containersForPreemption.isEmpty());
    +    assertTrue("No resources should be reserved for preemptees",
    +        schedulerNode.resourcesPreemptedForApp.isEmpty());
    +    assertEquals(
    +        "No amount of resource should be reserved for preemptees",
    +        Resources.none(),
    +        schedulerNode.getTotalReserved());
    +  }
    +
    +  private void allocateContainers(FSSchedulerNode schedulerNode) {
    +    FairScheduler.assignPreemptedContainers(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimpleAllocation() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    assertEquals("Container should be allocated",
    +        containers.get(0).getContainer().getResource(),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +
    +    // Check that we are error prone
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release three containers with launch.
    +   */
    +  @Test
    +  public void testMultipleAllocations() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    schedulerNode.containerStarted(containers.get(0).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(1));
    +    schedulerNode.containerStarted(containers.get(1).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(2));
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(), 3.0),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(1).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(2).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimplePreemption() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    // Launch containers and saturate the cluster
    +    saturateCluster(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(),
    +            containerNum),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Request preemption
    +    FSAppAttempt starvingApp = createStarvingApp(schedulerNode,
    +        Resource.newInstance(1024, 1));
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(0)), starvingApp);
    +    assertEquals(
    +        "No resource amount should be reserved for preemptees",
    +        containers.get(0).getAllocatedResource(),
    +        schedulerNode.getTotalReserved());
    +
    +    // Preemption occurs
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    allocateContainers(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        schedulerNode.getTotalResource(),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Release all containers
    +    for (int i = 1; i < containerNum; ++i) {
    +      schedulerNode.releaseContainer(containers.get(i).getContainerId(), true);
    --- End diff --
    
    And, shouldn't you start i with 0? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020673
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -38,15 +47,45 @@
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores list of containers still to be preempted
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores amount of respources preempted and reserved for each app
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedForApp = new LinkedHashMap<>();
    +  // Sum of resourcesPreemptedForApp values, total resources that are
    +  // slated for preemption
    +  Resource totalResourcesPreempted = Resource.newInstance(0, 0);
     
       public FSSchedulerNode(RMNode node, boolean usePortForNodeName) {
         super(node, usePortForNodeName);
       }
     
    +  /**
    +   * Total amount of reserved resources including reservations and preempted
    +   * containers.
    +   * @return total resources reserved
    +   */
    +  Resource getTotalReserved() {
    +    Resource totalReserved = getReservedContainer() != null
    +        ? Resources.clone(getReservedContainer().getAllocatedResource())
    +        : Resource.newInstance(0, 0);
    --- End diff --
    
    One of the reasons I feel cloning none is better is when we add more resources (for example through ResourceTypes). 
    
    An alternative would be to actually add a newInstance method that doesn't take any arguments and initializes all resources (including future ones) to zero. This could be done in a later JIRA.
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023192
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    --- End diff --
    
    For this test, do you really need a long? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108020845
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    --- End diff --
    
    This method should be private. Offline, you mentioned this is called in tests. I would also see if that can be avoided. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023660
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerPreemption.java ---
    @@ -294,11 +294,29 @@ private void verifyPreemption(int numStarvedAppContainers)
             8 - 2 * numStarvedAppContainers,
             greedyApp.getQueue().getMetrics().getAggregatePreemptedContainers());
     
    +    // Verify the node is reserved for the starvingApp
    +    for (RMNode rmNode : rmNodes) {
    +      FSSchedulerNode node = (FSSchedulerNode)
    +          scheduler.getNodeTracker().getNode(rmNode.getNodeID());
    +      if (node.getContainersForPreemption().size() > 0) {
    +        assertTrue(node.getPreemptionList().keySet().contains(starvingApp));
    +      }
    +    }
    +
         sendEnoughNodeUpdatesToAssignFully();
     
         // Verify the preempted containers are assigned to starvingApp
         assertEquals("Starved app is not assigned the right # of containers",
             numStarvedAppContainers, starvingApp.getLiveContainers().size());
    +
    +    // Verify the node is not reserved for the starvingApp anymore
    +    for (RMNode rmNode : rmNodes) {
    +      FSSchedulerNode node = (FSSchedulerNode)
    +          scheduler.getNodeTracker().getNode(rmNode.getNodeID());
    +      if (node.getContainersForPreemption().size() > 0) {
    +        assertTrue(!node.getPreemptionList().keySet().contains(starvingApp));
    --- End diff --
    
    assert message. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108023213
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    --- End diff --
    
    Should the arraylist be final? 
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024058
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -131,10 +203,58 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
     
       /**
        * Remove container from the set of containers marked for preemption.
    +   * Reserve the preempted resources for the app that requested
    +   * the preemption.
        *
        * @param container container to remove
        */
       void removeContainerForPreemption(RMContainer container) {
         containersForPreemption.remove(container);
       }
    +
    +  /**
    +   * The Scheduler has allocated containers on this node to the given
    +   * application.
    +   * @param rmContainer Allocated container
    +   * @param launchedOnNode True if the container has been launched
    +   */
    +  @Override
    +  protected synchronized void allocateContainer(RMContainer rmContainer,
    +                                                boolean launchedOnNode) {
    +    super.allocateContainer(rmContainer, launchedOnNode);
    +    Resource allocated = rmContainer.getAllocatedResource();
    +    if (!Resources.isNone(allocated)) {
    +      for (FSAppAttempt app: resourcesPreemptedPerApp.keySet()) {
    --- End diff --
    
    All right. Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024135
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -38,15 +47,45 @@
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024138
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -18,18 +18,27 @@
     
     package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
     
    +import com.google.common.annotations.VisibleForTesting;
     import org.apache.commons.logging.Log;
     import org.apache.commons.logging.LogFactory;
     import org.apache.hadoop.classification.InterfaceAudience.Private;
     import org.apache.hadoop.classification.InterfaceStability.Unstable;
     import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
    +import org.apache.hadoop.yarn.api.records.ContainerId;
    +import org.apache.hadoop.yarn.api.records.Resource;
     import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
     import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;
     import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;
     import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
     
     import java.util.Collection;
    +import java.util.Comparator;
    +import java.util.Iterator;
    +import java.util.LinkedHashMap;
    +import java.util.LinkedHashSet;
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024599
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -38,15 +47,45 @@
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores list of containers still to be preempted
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores amount of respources preempted and reserved for each app
    --- End diff --
    
    Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024673
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -38,15 +47,45 @@
     public class FSSchedulerNode extends SchedulerNode {
     
       private static final Log LOG = LogFactory.getLog(FSSchedulerNode.class);
    +  private static final Comparator<RMContainer> comparator =
    +      new Comparator<RMContainer>() {
    +    @Override
    +    public int compare(RMContainer o1, RMContainer o2) {
    +      return Long.compare(o1.getContainerId().getContainerId(),
    +          o2.getContainerId().getContainerId());
    +    }
    +  };
     
       private FSAppAttempt reservedAppSchedulable;
    -  private final Set<RMContainer> containersForPreemption =
    -      new ConcurrentSkipListSet<>();
    +  // Stores list of containers still to be preempted
    +  @VisibleForTesting
    +  final Set<RMContainer> containersForPreemption =
    +      new ConcurrentSkipListSet<>(comparator);
    +  // Stores amount of respources preempted and reserved for each app
    +  @VisibleForTesting
    +  final Map<FSAppAttempt, Resource>
    +      resourcesPreemptedForApp = new LinkedHashMap<>();
    +  // Sum of resourcesPreemptedForApp values, total resources that are
    +  // slated for preemption
    +  Resource totalResourcesPreempted = Resource.newInstance(0, 0);
     
       public FSSchedulerNode(RMNode node, boolean usePortForNodeName) {
         super(node, usePortForNodeName);
       }
     
    +  /**
    +   * Total amount of reserved resources including reservations and preempted
    +   * containers.
    +   * @return total resources reserved
    +   */
    +  Resource getTotalReserved() {
    +    Resource totalReserved = getReservedContainer() != null
    +        ? Resources.clone(getReservedContainer().getAllocatedResource())
    +        : Resource.newInstance(0, 0);
    --- End diff --
    
    Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024698
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    --- End diff --
    
    Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024703
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024707
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    +    Iterator<FSAppAttempt> iterator =
    +        resourcesPreemptedForApp.keySet().iterator();
    +    while (iterator.hasNext()) {
    +      FSAppAttempt app = iterator.next();
    +      boolean removeApp = false;
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024723
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    +    Iterator<FSAppAttempt> iterator =
    +        resourcesPreemptedForApp.keySet().iterator();
    +    while (iterator.hasNext()) {
    +      FSAppAttempt app = iterator.next();
    +      boolean removeApp = false;
    +      if (app.isStopped() || Resources.equals(
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024779
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +149,57 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * It is a good practice to call {@link #cleanupPreemptionList()}
    +   * after this call
    +   * @return if any resources were allocated
    +   */
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  synchronized void cleanupPreemptionList() {
    +    Iterator<FSAppAttempt> iterator =
    +        resourcesPreemptedForApp.keySet().iterator();
    +    while (iterator.hasNext()) {
    +      FSAppAttempt app = iterator.next();
    +      boolean removeApp = false;
    +      if (app.isStopped() || Resources.equals(
    +          app.getPendingDemand(), Resources.none())) {
    +        // App does not need more resources
    +        Resources.subtractFrom(totalResourcesPreempted,
    +            resourcesPreemptedForApp.get(app));
    +        iterator.remove();
    +      }
    +    }
    +  }
    +
    +  /**
        * Mark {@code containers} as being considered for preemption so they are
        * not considered again. A call to this requires a corresponding call to
    -   * {@link #removeContainerForPreemption} to ensure we do not mark a
    +   * removeContainer for preemption to ensure we do not mark a
    --- End diff --
    
    Fixed.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024812
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -968,6 +966,32 @@ private boolean shouldContinueAssigning(int containers,
         }
       }
     
    +  /**
    +   * Assign preempted containers to the applications that have reserved
    +   * resources for preempted containers.
    +   * @param node Node to check
    +   * @return assignment has occurred
    +   */
    +  static boolean assignPreemptedContainers(FSSchedulerNode node ) {
    +    boolean assignedAny = false;
    +    node.cleanupPreemptionList();
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024815
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -968,6 +966,32 @@ private boolean shouldContinueAssigning(int containers,
         }
       }
     
    +  /**
    +   * Assign preempted containers to the applications that have reserved
    +   * resources for preempted containers.
    +   * @param node Node to check
    +   * @return assignment has occurred
    +   */
    +  static boolean assignPreemptedContainers(FSSchedulerNode node ) {
    +    boolean assignedAny = false;
    +    node.cleanupPreemptionList();
    +    for (Entry<FSAppAttempt, Resource> entry :
    +        node.getPreemptionList().entrySet()) {
    +      FSAppAttempt app = entry.getKey();
    +      Resource reserved = Resources.clone(entry.getValue());
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024823
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -968,6 +966,32 @@ private boolean shouldContinueAssigning(int containers,
         }
       }
     
    +  /**
    +   * Assign preempted containers to the applications that have reserved
    +   * resources for preempted containers.
    +   * @param node Node to check
    +   * @return assignment has occurred
    +   */
    +  static boolean assignPreemptedContainers(FSSchedulerNode node ) {
    +    boolean assignedAny = false;
    +    node.cleanupPreemptionList();
    +    for (Entry<FSAppAttempt, Resource> entry :
    +        node.getPreemptionList().entrySet()) {
    +      FSAppAttempt app = entry.getKey();
    +      Resource reserved = Resources.clone(entry.getValue());
    +      while (!app.isStopped() && !Resources.isNone(reserved)) {
    +        Resource assigned = app.assignContainer(node);
    +        if (Resources.isNone(assigned)) {
    +          break;
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024837
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -968,6 +966,32 @@ private boolean shouldContinueAssigning(int containers,
         }
       }
     
    +  /**
    +   * Assign preempted containers to the applications that have reserved
    +   * resources for preempted containers.
    +   * @param node Node to check
    +   * @return assignment has occurred
    +   */
    +  static boolean assignPreemptedContainers(FSSchedulerNode node ) {
    +    boolean assignedAny = false;
    +    node.cleanupPreemptionList();
    +    for (Entry<FSAppAttempt, Resource> entry :
    +        node.getPreemptionList().entrySet()) {
    +      FSAppAttempt app = entry.getKey();
    +      Resource reserved = Resources.clone(entry.getValue());
    +      while (!app.isStopped() && !Resources.isNone(reserved)) {
    +        Resource assigned = app.assignContainer(node);
    +        if (Resources.isNone(assigned)) {
    +          break;
    +        }
    +        assignedAny = true;
    +        Resources.subtractFromNonNegative(reserved, assigned);
    +      }
    +    }
    +    node.cleanupPreemptionList();
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108024844
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java ---
    @@ -646,7 +646,6 @@ private Container createContainer(FSSchedulerNode node, Resource capability,
       private boolean reserve(Resource perAllocationResource, FSSchedulerNode node,
           Container reservedContainer, NodeType type,
           SchedulerRequestKey schedulerKey) {
    -
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025184
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025201
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025247
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    --- End diff --
    
    That is for the inner class. Sorry, what do you mean? When?
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025251
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    --- End diff --
    
    I do have a when in all these cases.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025258
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    --- End diff --
    
    I changed to int.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025333
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    +      schedulerNode.containerStarted(containers.get((int)containerNum - 1).
    +          getContainerId());
    +    }
    +  }
    +
    +  private FSAppAttempt createStarvingApp(FSSchedulerNode schedulerNode,
    +                                         Resource request) {
    +    FSAppAttempt starvingApp = mock(FSAppAttempt.class);
    +    ApplicationAttemptId appAttemptId =
    +        mock(ApplicationAttemptId.class);
    +    when(starvingApp.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(starvingApp.assignContainer(schedulerNode)).thenAnswer(
    +        new Answer<Resource>() {
    +          @Override
    +          public Resource answer(InvocationOnMock invocationOnMock)
    +              throws Throwable {
    +            Resource response = Resource.newInstance(0, 0);
    +            while (!Resources.isNone(request) &&
    +                !Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +              RMContainer container = createContainer(request, appAttemptId);
    +              schedulerNode.allocateContainer(container);
    +              Resources.addTo(response, container.getAllocatedResource());
    +              Resources.subtractFrom(request,
    +                  container.getAllocatedResource());
    +            }
    +            return response;
    +          }
    +        });
    +    when(starvingApp.getPendingDemand()).thenReturn(request);
    +    return starvingApp;
    +  }
    +
    +  private void finalValidation(FSSchedulerNode schedulerNode) {
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    assertTrue("No containers should be reserved for preemption",
    +        schedulerNode.containersForPreemption.isEmpty());
    +    assertTrue("No resources should be reserved for preemptees",
    +        schedulerNode.resourcesPreemptedForApp.isEmpty());
    +    assertEquals(
    +        "No amount of resource should be reserved for preemptees",
    +        Resources.none(),
    +        schedulerNode.getTotalReserved());
    +  }
    +
    +  private void allocateContainers(FSSchedulerNode schedulerNode) {
    +    FairScheduler.assignPreemptedContainers(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimpleAllocation() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    assertEquals("Container should be allocated",
    +        containers.get(0).getContainer().getResource(),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +
    +    // Check that we are error prone
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release three containers with launch.
    +   */
    +  @Test
    +  public void testMultipleAllocations() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    schedulerNode.containerStarted(containers.get(0).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(1));
    +    schedulerNode.containerStarted(containers.get(1).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(2));
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(), 3.0),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(1).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(2).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimplePreemption() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    // Launch containers and saturate the cluster
    +    saturateCluster(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(),
    +            containerNum),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Request preemption
    +    FSAppAttempt starvingApp = createStarvingApp(schedulerNode,
    +        Resource.newInstance(1024, 1));
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(0)), starvingApp);
    +    assertEquals(
    +        "No resource amount should be reserved for preemptees",
    +        containers.get(0).getAllocatedResource(),
    +        schedulerNode.getTotalReserved());
    +
    +    // Preemption occurs
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    allocateContainers(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        schedulerNode.getTotalResource(),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Release all containers
    +    for (int i = 1; i < containerNum; ++i) {
    +      schedulerNode.releaseContainer(containers.get(i).getContainerId(), true);
    --- End diff --
    
    It is not 0 because I release the rest of the containers that were not preempted. That is, I would like to see the actual schedulerNode.releaseContainer call next to the preemption in the same function to see what is actually happening. Another function would hide the ability to see all releases together.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025337
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSSchedulerNode.java ---
    @@ -0,0 +1,376 @@
    +package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;
    +
    +import org.apache.hadoop.yarn.api.records.*;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;
    +import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;
    +import org.apache.hadoop.yarn.util.resource.Resources;
    +import org.junit.Test;
    +import org.mockito.invocation.InvocationOnMock;
    +import org.mockito.stubbing.Answer;
    +
    +import java.util.ArrayList;
    +import java.util.Collections;
    +import java.util.Map;
    +
    +import static org.junit.Assert.assertEquals;
    +import static org.junit.Assert.assertNotEquals;
    +import static org.junit.Assert.assertTrue;
    +import static org.mockito.Mockito.mock;
    +import static org.mockito.Mockito.when;
    +
    +/**
    + * Test scheduler node, especially preemption reservations.
    + */
    +public class TestFSSchedulerNode {
    +  private long containerNum = 0;
    +  private ArrayList<RMContainer> containers = new ArrayList<>();
    +
    +  private RMNode createNode() {
    +    RMNode node = mock(RMNode.class);
    +    when(node.getTotalCapability()).thenReturn(Resource.newInstance(8192, 8));
    +    when(node.getHostName()).thenReturn("host.domain.com");
    +    return node;
    +  }
    +
    +  private RMContainer createDefaultContainer() {
    +    return createContainer(Resource.newInstance(1024, 1), null);
    +  }
    +
    +  private RMContainer createContainer(
    +      Resource request, ApplicationAttemptId appAttemptId) {
    +    RMContainer container = mock(RMContainer.class);
    +    Container containerInner = mock(Container.class);
    +    ContainerId id = mock(ContainerId.class);
    +    when(id.getContainerId()).thenReturn(containerNum);
    +    when(containerInner.getResource()).
    +        thenReturn(Resources.clone(request));
    +    when(containerInner.getId()).thenReturn(id);
    +    when(containerInner.getExecutionType()).
    +        thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(container.getContainerId()).thenReturn(id);
    +    when(container.getContainer()).thenReturn(containerInner);
    +    when(container.getExecutionType()).thenReturn(ExecutionType.GUARANTEED);
    +    when(container.getAllocatedResource()).
    +        thenReturn(Resources.clone(request));
    +    containers.add(container);
    +    containerNum++;
    +    return container;
    +  }
    +
    +  private void saturateCluster(FSSchedulerNode schedulerNode) {
    +    while (!Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +      createDefaultContainer();
    +      schedulerNode.allocateContainer(containers.get((int)containerNum - 1));
    +      schedulerNode.containerStarted(containers.get((int)containerNum - 1).
    +          getContainerId());
    +    }
    +  }
    +
    +  private FSAppAttempt createStarvingApp(FSSchedulerNode schedulerNode,
    +                                         Resource request) {
    +    FSAppAttempt starvingApp = mock(FSAppAttempt.class);
    +    ApplicationAttemptId appAttemptId =
    +        mock(ApplicationAttemptId.class);
    +    when(starvingApp.getApplicationAttemptId()).thenReturn(appAttemptId);
    +    when(starvingApp.assignContainer(schedulerNode)).thenAnswer(
    +        new Answer<Resource>() {
    +          @Override
    +          public Resource answer(InvocationOnMock invocationOnMock)
    +              throws Throwable {
    +            Resource response = Resource.newInstance(0, 0);
    +            while (!Resources.isNone(request) &&
    +                !Resources.isNone(schedulerNode.getUnallocatedResource())) {
    +              RMContainer container = createContainer(request, appAttemptId);
    +              schedulerNode.allocateContainer(container);
    +              Resources.addTo(response, container.getAllocatedResource());
    +              Resources.subtractFrom(request,
    +                  container.getAllocatedResource());
    +            }
    +            return response;
    +          }
    +        });
    +    when(starvingApp.getPendingDemand()).thenReturn(request);
    +    return starvingApp;
    +  }
    +
    +  private void finalValidation(FSSchedulerNode schedulerNode) {
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    assertTrue("No containers should be reserved for preemption",
    +        schedulerNode.containersForPreemption.isEmpty());
    +    assertTrue("No resources should be reserved for preemptees",
    +        schedulerNode.resourcesPreemptedForApp.isEmpty());
    +    assertEquals(
    +        "No amount of resource should be reserved for preemptees",
    +        Resources.none(),
    +        schedulerNode.getTotalReserved());
    +  }
    +
    +  private void allocateContainers(FSSchedulerNode schedulerNode) {
    +    FairScheduler.assignPreemptedContainers(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimpleAllocation() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    assertEquals("Container should be allocated",
    +        containers.get(0).getContainer().getResource(),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    assertEquals("Everything should have been released",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +
    +    // Check that we are error prone
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release three containers with launch.
    +   */
    +  @Test
    +  public void testMultipleAllocations() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    createDefaultContainer();
    +    assertEquals("Nothing should have been allocated, yet",
    +        Resources.none(), schedulerNode.getAllocatedResource());
    +    schedulerNode.allocateContainer(containers.get(0));
    +    schedulerNode.containerStarted(containers.get(0).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(1));
    +    schedulerNode.containerStarted(containers.get(1).getContainerId());
    +    schedulerNode.allocateContainer(containers.get(2));
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(), 3.0),
    +        schedulerNode.getAllocatedResource());
    +    schedulerNode.releaseContainer(containers.get(1).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(2).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release a single container.
    +   */
    +  @Test
    +  public void testSimplePreemption() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    // Launch containers and saturate the cluster
    +    saturateCluster(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(),
    +            containerNum),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Request preemption
    +    FSAppAttempt starvingApp = createStarvingApp(schedulerNode,
    +        Resource.newInstance(1024, 1));
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(0)), starvingApp);
    +    assertEquals(
    +        "No resource amount should be reserved for preemptees",
    +        containers.get(0).getAllocatedResource(),
    +        schedulerNode.getTotalReserved());
    +
    +    // Preemption occurs
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    allocateContainers(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        schedulerNode.getTotalResource(),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Release all containers
    +    for (int i = 1; i < containerNum; ++i) {
    +      schedulerNode.releaseContainer(containers.get(i).getContainerId(), true);
    +    }
    +    finalValidation(schedulerNode);
    +  }
    +
    +  /**
    +   * Allocate and release three containers requested by two apps.
    +   */
    +  @Test
    +  public void testComplexPreemption() {
    +    RMNode node = createNode();
    +    FSSchedulerNode schedulerNode = new FSSchedulerNode(node, false);
    +
    +    // Launch containers and saturate the cluster
    +    saturateCluster(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        Resources.multiply(containers.get(0).getContainer().getResource(),
    +            containerNum),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Preempt a container
    +    FSAppAttempt starvingApp1 = createStarvingApp(schedulerNode,
    +        Resource.newInstance(2048, 2));
    +    FSAppAttempt starvingApp2 = createStarvingApp(schedulerNode,
    +        Resource.newInstance(1024, 1));
    +
    +    // Preemption thread kicks in
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(0)), starvingApp1);
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(1)), starvingApp1);
    +    schedulerNode.addContainersForPreemption(
    +        Collections.singletonList(containers.get(2)), starvingApp2);
    +
    +    // Preemption happens
    +    schedulerNode.releaseContainer(containers.get(0).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(2).getContainerId(), true);
    +    schedulerNode.releaseContainer(containers.get(1).getContainerId(), true);
    +
    +    allocateContainers(schedulerNode);
    +    assertEquals("Container should be allocated",
    +        schedulerNode.getTotalResource(),
    +        schedulerNode.getAllocatedResource());
    +
    +    // Release all containers
    +    for (int i = 3; i < containerNum; ++i) {
    +      schedulerNode.releaseContainer(containers.get(i).getContainerId(), true);
    --- End diff --
    
    Same response :-)
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025370
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerPreemption.java ---
    @@ -294,11 +294,29 @@ private void verifyPreemption(int numStarvedAppContainers)
             8 - 2 * numStarvedAppContainers,
             greedyApp.getQueue().getMetrics().getAggregatePreemptedContainers());
     
    +    // Verify the node is reserved for the starvingApp
    +    for (RMNode rmNode : rmNodes) {
    +      FSSchedulerNode node = (FSSchedulerNode)
    +          scheduler.getNodeTracker().getNode(rmNode.getNodeID());
    +      if (node.getContainersForPreemption().size() > 0) {
    +        assertTrue(node.getPreemptionList().keySet().contains(starvingApp));
    --- End diff --
    
    Done.
, Github user szegedim commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108025378
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairSchedulerPreemption.java ---
    @@ -294,11 +294,29 @@ private void verifyPreemption(int numStarvedAppContainers)
             8 - 2 * numStarvedAppContainers,
             greedyApp.getQueue().getMetrics().getAggregatePreemptedContainers());
     
    +    // Verify the node is reserved for the starvingApp
    +    for (RMNode rmNode : rmNodes) {
    +      FSSchedulerNode node = (FSSchedulerNode)
    +          scheduler.getNodeTracker().getNode(rmNode.getNodeID());
    +      if (node.getContainersForPreemption().size() > 0) {
    +        assertTrue(node.getPreemptionList().keySet().contains(starvingApp));
    +      }
    +    }
    +
         sendEnoughNodeUpdatesToAssignFully();
     
         // Verify the preempted containers are assigned to starvingApp
         assertEquals("Starved app is not assigned the right # of containers",
             numStarvedAppContainers, starvingApp.getLiveContainers().size());
    +
    +    // Verify the node is not reserved for the starvingApp anymore
    +    for (RMNode rmNode : rmNodes) {
    +      FSSchedulerNode node = (FSSchedulerNode)
    +          scheduler.getNodeTracker().getNode(rmNode.getNodeID());
    +      if (node.getContainersForPreemption().size() > 0) {
    +        assertTrue(!node.getPreemptionList().keySet().contains(starvingApp));
    --- End diff --
    
    Done.

, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108520022
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -130,11 +200,47 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
       }
     
       /**
    -   * Remove container from the set of containers marked for preemption.
    -   *
    -   * @param container container to remove
    +   * The Scheduler has allocated containers on this node to the given
    +   * application.
    +   * @param rmContainer Allocated container
    +   * @param launchedOnNode True if the container has been launched
    +   */
    +  @Override
    +  protected synchronized void allocateContainer(RMContainer rmContainer,
    +                                                boolean launchedOnNode) {
    +    super.allocateContainer(rmContainer, launchedOnNode);
    +    Resource allocated = rmContainer.getAllocatedResource();
    +    if (!Resources.isNone(allocated)) {
    +      FSAppAttempt app =
    +          appIdToAppMap.get(rmContainer.getApplicationAttemptId());
    +      if (app != null) {
    --- End diff --
    
    For readability, can we can add a comment either before/after the null check or in the else block. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108521170
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -987,12 +1010,18 @@ void attemptScheduling(FSSchedulerNode node) {
           }
     
           // Assign new containers...
    -      // 1. Check for reserved applications
    -      // 2. Schedule if there are no reservations
    -
    -      boolean validReservation = false;
    +      // 1. Ensure containers are assigned to the apps that preempted
    +      // 2. Check for reserved applications
    +      // 3. Schedule if there are no reservations
    +
    +      // Apps may wait for preempted containers
    +      // We have to satisfy these first to avoid cases, when we preempt
    +      // a container for A from B and C gets the preempted containers,
    +      // when C does not qualify for preemption itself.
    +      boolean validReservation;
    +      validReservation = assignPreemptedContainers(node);
           FSAppAttempt reservedAppSchedulable = node.getReservedAppSchedulable();
    -      if (reservedAppSchedulable != null) {
    +      if (!validReservation && reservedAppSchedulable != null) {
    --- End diff --
    
    In lieu of future changes around assigning as many containers as directed by assign-multiple settings, can we not gate further allocations on the result of assignPreemptedContainers. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108518561
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -110,16 +140,56 @@ synchronized FSAppAttempt getReservedAppSchedulable() {
       }
     
       /**
    +   * List reserved resources after preemption and assign them to the
    +   * appropriate applications in a FIFO order.
    +   * @return if any resources were allocated
    +   */
    +  @VisibleForTesting
    +  synchronized LinkedHashMap<FSAppAttempt, Resource> getPreemptionList() {
    +    cleanupPreemptionList();
    +    return new LinkedHashMap<>(resourcesPreemptedForApp);
    +  }
    +
    +  /**
    +   * Remove apps that have their preemption requests fulfilled
    +   */
    +  private synchronized void cleanupPreemptionList() {
    +    Iterator<FSAppAttempt> iterator =
    +        resourcesPreemptedForApp.keySet().iterator();
    +    while (iterator.hasNext()) {
    +      FSAppAttempt app = iterator.next();
    +      if (app.isStopped() || !app.isStarved()) {
    +        // App does not need more resources
    +        Resources.subtractFrom(totalResourcesPreempted,
    +            resourcesPreemptedForApp.get(app));
    +        appIdToAppMap.remove(app.getApplicationAttemptId());
    +        iterator.remove();
    +      }
    +    }
    +  }
    +
    +  /**
        * Mark {@code containers} as being considered for preemption so they are
        * not considered again. A call to this requires a corresponding call to
    -   * {@link #removeContainerForPreemption} to ensure we do not mark a
    +   * {@code releaseContainer} for preemption to ensure we do not mark a
    --- End diff --
    
    Should we drop "for preemption" altogether? 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108518121
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainer.java ---
    @@ -42,7 +42,8 @@
      * when resources are being reserved to fill space for a future container 
      * allocation.
      */
    -public interface RMContainer extends EventHandler<RMContainerEvent> {
    +public interface RMContainer extends EventHandler<RMContainerEvent>,
    --- End diff --
    
    We need a corresponding change in RMContainerImpl. It should suffice to implement only RMContainer now. .
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r108519744
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSSchedulerNode.java ---
    @@ -130,11 +200,47 @@ void addContainersForPreemption(Collection<RMContainer> containers) {
       }
     
       /**
    -   * Remove container from the set of containers marked for preemption.
    -   *
    -   * @param container container to remove
    +   * The Scheduler has allocated containers on this node to the given
    +   * application.
    +   * @param rmContainer Allocated container
    +   * @param launchedOnNode True if the container has been launched
    +   */
    +  @Override
    +  protected synchronized void allocateContainer(RMContainer rmContainer,
    +                                                boolean launchedOnNode) {
    +    super.allocateContainer(rmContainer, launchedOnNode);
    +    Resource allocated = rmContainer.getAllocatedResource();
    +    if (!Resources.isNone(allocated)) {
    --- End diff --
    
    Will we have an empty allocation? If not, should we log an error when we do. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r109071718
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java ---
    @@ -987,10 +1010,16 @@ void attemptScheduling(FSSchedulerNode node) {
           }
     
           // Assign new containers...
    -      // 1. Check for reserved applications
    -      // 2. Schedule if there are no reservations
    -
    +      // 1. Ensure containers are assigned to the apps that preempted
    +      // 2. Check for reserved applications
    +      // 3. Schedule if there are no reservations
    +
    +      // Apps may wait for preempted containers
    +      // We have to satisfy these first to avoid cases, when we preempt
    +      // a container for A from B and C gets the preempted containers,
    +      // when C does not qualify for preemption itself.
           boolean validReservation = false;
    +      assignPreemptedContainers(node);
    --- End diff --
    
    Call to assignPreemptedContainers should go before declaring validReservation. 
, Github user kambatla commented on a diff in the pull request:

    https://github.com/apache/hadoop/pull/201#discussion_r109071512
  
    --- Diff: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java ---
    @@ -647,7 +647,12 @@ private boolean reserve(Resource perAllocationResource, FSSchedulerNode node,
           Container reservedContainer, NodeType type,
           SchedulerRequestKey schedulerKey) {
     
    -    if (!reservationExceedsThreshold(node, type)) {
    +    RMContainer nodeReservedContainer = node.getReservedContainer();
    +    boolean reservableForThisApp = nodeReservedContainer == null ||
    +        nodeReservedContainer.getApplicationAttemptId()
    +            .equals(getApplicationAttemptId());
    +    if (!reservationExceedsThreshold(node, type) &&
    +        reservableForThisApp) {
    --- End diff --
    
    Let us flip the order here, because reservableForThisApp is cheaper to check. 
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 29s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 47s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  0m 19s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  2m 39s{color} | {color:red} root in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  2m 39s{color} | {color:red} root in the patch failed. {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 39s{color} | {color:orange} root: The patch generated 13 new + 94 unchanged - 3 fixed = 107 total (was 97) {color} |
| {color:red}-1{color} | {color:red} mvnsite {color} | {color:red}  0m 19s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  3s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 19s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 27s{color} | {color:red} hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common generated 4 new + 4575 unchanged - 0 fixed = 4579 total (was 4575) {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 10s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  0m 20s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 39m 17s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 21s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}110m  2s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.TestRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5829 |
| GITHUB PR | https://github.com/apache/hadoop/pull/201 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  xml  findbugs  checkstyle  |
| uname | Linux bce91ad714f7 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 73835c7 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| mvninstall | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| compile | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/patch-compile-root.txt |
| javac | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/patch-compile-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/diff-checkstyle-root.txt |
| mvnsite | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/patch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/diff-javadoc-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/15465/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-YARN-Build/15465/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/15465/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, I think Jenkins got confused on the pull request. The patch does not even contain a TotalResourceMapDecorator class., Retrying the same patch. Hopefully it gets selected this time., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 48s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 23s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 16s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  0m 20s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  2m 37s{color} | {color:red} root in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  2m 37s{color} | {color:red} root in the patch failed. {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 52s{color} | {color:orange} root: The patch generated 13 new + 94 unchanged - 3 fixed = 107 total (was 97) {color} |
| {color:red}-1{color} | {color:red} mvnsite {color} | {color:red}  0m 23s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  3s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: hadoop-project {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 22s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 33s{color} | {color:red} hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common generated 4 new + 4575 unchanged - 0 fixed = 4579 total (was 4575) {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 14s{color} | {color:green} hadoop-project in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  0m 23s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 39m 18s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 27s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}115m 33s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5829 |
| GITHUB PR | https://github.com/apache/hadoop/pull/201 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  xml  findbugs  checkstyle  |
| uname | Linux d5b13b8efe35 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / bbd6847 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| mvninstall | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| compile | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/patch-compile-root.txt |
| javac | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/patch-compile-root.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/diff-checkstyle-root.txt |
| mvnsite | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/patch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/diff-javadoc-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/15489/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-YARN-Build/15489/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C: hadoop-project hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: . |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/15489/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Github user szegedim closed the pull request at:

    https://github.com/apache/hadoop/pull/201
, Looks like Miklos created YARN-6432 to have Jenkins run against the patch instead of the pull request. 

Closing this as a duplicate. ]