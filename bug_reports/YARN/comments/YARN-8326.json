[This appears to be introduced by YARN-5662 by turning on container monitor from false to true.  This feature is used by opportunistic container scheduling and pre-emption to gather statics of the containers to make scheduling decisions.  You can disable this feature by:

{code}
    <property>
      <name>yarn.nodemanager.container-monitor.enabled</name>
      <value>false</value>
    </property>
{code}

Or reduce the stats collection time from 3 seconds to 300 milliseconds (use more system resources, but faster scheduling):

{code}
    <property>
      <name>yarn.nodemanager.container-monitor.interval-ms</name>
      <value>300</value>
    </property>
{code}

Timer optimization might be possible to the work done in YARN-2883.  The queuing and scheduling of containers is based on monitoring thread information.  If it takes several seconds to wait for information to become available before next container is scheduled, then it can introduced artificial delay to rapidly launching containers.  The timer value can not be smaller than certain value otherwise monitoring/container forking both will tax cpu resources too much.  If your workloads take less time than container scheduling/launch, then you might need to revisit how to decrease the containers to launch, and increase the work to run in containers.  [~hlhuang@us.ibm.com] Can you confirm that those settings changes the benchmark result?, [~hlhuang@us.ibm.com] Does the same log entries show up?, Hi  [~eyang]

   I ran the sample job,

{color:#14892c}time hadoop jar /usr/hdp/3.0.0.0-829/hadoop-yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.3.0.0.0-829.jar Client -classpath simple-yarn-app-1.1.0.jar -cmd "java com.hortonworks.simpleyarnapp.ApplicationMaster /bin/date 8"{color}

with the changed settings, it still ran 15 seconds compared to 6 or 7 seconds in 2.6 environment.  So I am not sure if the significant performance role that these two  monitoring setting would play in this. The major issue could still be in the exiting container that in 3.0 environment is much slower than 2.6 environment.  Can someone from yarn team look into this? This is a general yarn application performance issue in 3.0. 

 , Here is more detail information from node manager log that compares between Hadoop 3.0 and 2.6.  They are both running on 4 node cluster with 3 data nodes with same machine power/cpu/memory and same type of job.   I picked only one node to compare the container cycle. 

*1. On 3.0.*  when I request 8 containers to run on 3 data nodes,  I picked the second node to examine the log:

this job used 2 containers in this node:

 

container *container_e04_1527109836290_0004_01_000002*  on application application_1527109836290_0004  (from container succeeded to Stopping container (from blue to red line) took about *4 seconds*)

 

152231 2018-05-23 15:04:45,541 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(1059)) - Start request for container_e04_1527109836290_0004_01_000002 by user hlhuang

152232 2018-05-23 15:04:45,657 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(1127)) - Creating a new application reference for app application_1527109836290_0004

152233 2018-05-23 15:04:45,658 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(632)) - Application application_1527109836290_0004 transitioned from NEW to INITING

152234 2018-05-23 15:04:45,658 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(446)) - Adding container_e04_1527109836290_0004_01_000002 to application application_1527109836290_0004

152235 2018-05-23 15:04:45,658 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(632)) - Application application_1527109836290_0004 transitioned from INITING to RUNNING

152236 2018-05-23 15:04:45,659 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000002 transitioned from NEW to SCHEDULED

152237 2018-05-23 15:04:45,659 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_INIT for appId application_1527109836290_0004

152238 2018-05-23 15:04:45,659 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e04_1527109836290_0004_01_000002

152239 2018-05-23 15:04:45,660 INFO  scheduler.ContainerScheduler (ContainerScheduler.java:startContainer(503)) - Starting container [container_e04_1527109836290_0004_01_000002]

152246 2018-05-23 15:04:45,965 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000002 transitioned from SCHEDULED to RUNNING

152247 2018-05-23 15:04:45,965 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStartMonitoringContainer(941)) - Starting resource-monitoring for container_e04_1527109836290_0004_01_000002

{color:#205081}152250 2018-05-23 15:04:46,002 INFO  launcher.ContainerLaunch (ContainerLaunch.java:handleContainerExitCode(512)) - Container container_e04_1527109836290_0004_01_000002 succeeded{color}

 

152251 2018-05-23 15:04:46,003 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS

152252 2018-05-23 15:04:46,003 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1527109836290_0004_01_000002

152254 2018-05-23 15:04:48,132 INFO  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(794)) - Deleting absolute path : /hadoop/yarn/local/usercache/hlhuang/appcache/application_1527109836290_0004/container_e04_1527109836290_0004_01_000002

152256 2018-05-23 15:04:48,133 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE

152258 2018-05-23 15:04:49,171 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:removeOrTrackCompletedContainersFromContext(682)) - Removed completed containers from NM context: [container_e04_1527109836290_0004_01_000002]

152260 2018-05-23 15:04:50,289 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(489)) - Removing container_e04_1527109836290_0004_01_000002 from application application_1527109836290_0004

{color:#d04437}152261 2018-05-23 15:04:50,290 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStopMonitoringContainer(932)) - Stopping resource-monitoring for container_e04_1527109836290_0004_01_000002{color}

152263 2018-05-23 15:04:50,290 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e04_1527109836290_0004_01_000002

152262 2018-05-23 15:04:50,290 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_STOP for appId application_1527109836290_0004

 

 

container *container_e04_1527109836290_0004_01_000006* on application application_1527109836290_0004   (from container succeeded to Stopping container (from blue to red line) took about 4 seconds)

 

152240 2018-05-23 15:04:45,876 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(1059)) - Start request for container_e04_1527109836290_0004_01_000006 by user hlhuang

152241 2018-05-23 15:04:45,879 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(446)) - Adding container_e04_1527109836290_0004_01_000006 to application application_1527109836290_0004

152242 2018-05-23 15:04:45,879 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000006 transitioned from NEW to SCHEDULED

152243 2018-05-23 15:04:45,882 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_INIT for appId application_1527109836290_0004

152244 2018-05-23 15:04:45,883 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e04_1527109836290_0004_01_000006

152245 2018-05-23 15:04:45,883 INFO  scheduler.ContainerScheduler (ContainerScheduler.java:startContainer(503)) - Starting container [container_e04_1527109836290_0004_01_000006]

152248 2018-05-23 15:04:45,985 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000006 transitioned from SCHEDULED to RUNNING

152249 2018-05-23 15:04:45,985 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStartMonitoringContainer(941)) - Starting resource-monitoring for container_e04_1527109836290_0004_01_000006

{color:#205081}152253 2018-05-23 15:04:46,013 INFO  launcher.ContainerLaunch (ContainerLaunch.java:handleContainerExitCode(512)) - Container container_e04_1527109836290_0004_01_000006 succeeded{color}

 

152255 2018-05-23 15:04:48,133 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000006 transitioned from RUNNING to EXITED_WITH_SUCCESS

152257 2018-05-23 15:04:48,133 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1527109836290_0004_01_000006

152259 2018-05-23 15:04:50,289 INFO  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(794)) - Deleting absolute path : /hadoop/yarn/local/usercache/hlhuang/appcache/application_1527109836290_0004/container_e04_1527109836290_0004_01_000006

152264 2018-05-23 15:04:50,290 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000006 transitioned from EXITED_WITH_SUCCESS to DONE

152265 2018-05-23 15:04:50,291 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(489)) - Removing container_e04_1527109836290_0004_01_000006 from application application_1527109836290_0004

152266 2018-05-23 15:04:50,291 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStopMonitoringContainer(932)) - Stopping resource-monitoring for container_e04_1527109836290_0004_01_000006

152267 2018-05-23 15:04:50,291 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_STOP for appId application_1527109836290_0004

{color:#d04437}152268 2018-05-23 15:04:50,291 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e04_1527109836290_0004_01_000006{color}

 

{color:#14892c}Below is something that probably needs to look at too.  You can see the previous line is 152269, and next line is 152270, and in between there is a 5 seconds gap!!!  i.e. after containers are removed till application transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP  took 5 seconds!{color}

 

*152269 2018-05-23 15:04:51,301* INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:removeOrTrackCompletedContainersFromContext(682)) - Removed completed containers from NM context: [container_e04_1527109836290_0004_01_000006]

*152270 2018-05-23 15:04:56,315* INFO  application.ApplicationImpl (ApplicationImpl.java:handle(632)) - Application application_1527109836290_0004 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP

152271 2018-05-23 15:04:56,316 INFO  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(794)) - Deleting absolute path : /hadoop/yarn/local/usercache/hlhuang/appcache/application_1527109836290_0004

152272 2018-05-23 15:04:56,316 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event APPLICATION_STOP for appId application_1527109836290_0004

152273 2018-05-23 15:04:56,316 INFO  shuffle.ExternalShuffleBlockResolver (ExternalShuffleBlockResolver.java:applicationRemoved(186)) - Application application_1527109836290_0004 removed, cleanupLocalDirs = false

152274 2018-05-23 15:04:56,320 INFO  impl.TimelineV2ClientImpl (TimelineV2ClientImpl.java:stop(542)) - Stopping TimelineClient.

 

 

*2.  On 2.6.*  when I request 8 containers to run on 3 data nodes,  I picked the second node to examine the log:

this job used 3 containers in this node:

 

Application application_1526961340393_0012,  container *container_e11_1526961340393_0012_01_000003* (from container succeeded to Stopping container (from blue to red line) took less than *1 seconds*)

 

991154 2018-05-23 15:41:26,904 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(810)) - Start request for container_e11_1526961340393_0012_01_000003 by user hlhuang

991155 2018-05-23 15:41:26,908 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(850)) - Creating a new application reference for app application_1526961340393_0012

991156 2018-05-23 15:41:26,908 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1526961340393_0012 transitioned from NEW to INITING

991157 2018-05-23 15:41:26,909 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(304)) - Adding container_e11_1526961340393_0012_01_000003 to application application_1526961340393_0012

 

991158 2018-05-23 15:41:26,914 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1526961340393_0012 transitioned from INITING to RUNNING

991159 2018-05-23 15:41:26,915 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000003 transitioned from NEW to LOCALIZED

991160 2018-05-23 15:41:26,915 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_INIT for appId application_1526961340393_0012

991161 2018-05-23 15:41:26,915 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(192)) - Initializing container container_e11_1526961340393_0012_01_000003

991162 2018-05-23 15:41:26,915 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e11_1526961340393_0012_01_000003

991163 2018-05-23 15:41:26,968 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000003 transitioned from LOCALIZED to RUNNING

991164 2018-05-23 15:41:26,968 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

991165 2018-05-23 15:41:26,968 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread

{color:#205081}991166 2018-05-23 15:41:26,993 INFO  launcher.ContainerLaunch (ContainerLaunch.java:call(384)) - Container container_e11_1526961340393_0012_01_000003 succeeded{color}

 

991167 2018-05-23 15:41:26,994 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS

991168 2018-05-23 15:41:26,994 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(541)) - Cleaning up container container_e11_1526961340393_0012_01_000003

991169 2018-05-23 15:41:26,995 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

991170 2018-05-23 15:41:27,021 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE

991171 2018-05-23 15:41:27,021 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(347)) - Removing container_e11_1526961340393_0012_01_000003 from application application_1526961340393_0012

991172 2018-05-23 15:41:27,021 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread

991173 2018-05-23 15:41:27,035 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_STOP for appId application_1526961340393_0012

991174 2018-05-23 15:41:27,035 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(198)) - Stopping container container_e11_1526961340393_0012_01_000003

{color:#d04437}991175 2018-05-23 15:41:27,036 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e11_1526961340393_0012_01_000003{color}

 

Application application_1526961340393_0012,  container *container_e11_1526961340393_0012_01_000005* (from container succeeded to Stopping container (from blue to red line) took less than *1 seconds*)

 

991176 2018-05-23 15:41:27,152 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(810)) - Start request for container_e11_1526961340393_0012_01_000005 by user hlhuang

991177 2018-05-23 15:41:27,154 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(304)) - Adding container_e11_1526961340393_0012_01_000005 to application application_1526961340393_0012

 

991178 2018-05-23 15:41:27,154 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000005 transitioned from NEW to LOCALIZED

991179 2018-05-23 15:41:27,155 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_INIT for appId application_1526961340393_0012

991180 2018-05-23 15:41:27,155 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(192)) - Initializing container container_e11_1526961340393_0012_01_000005

991181 2018-05-23 15:41:27,155 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e11_1526961340393_0012_01_000005

991182 2018-05-23 15:41:27,205 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000005 transitioned from LOCALIZED to RUNNING

991183 2018-05-23 15:41:27,205 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread

991184 2018-05-23 15:41:27,205 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

{color:#205081}991185 2018-05-23 15:41:27,227 INFO  launcher.ContainerLaunch (ContainerLaunch.java:call(384)) - Container container_e11_1526961340393_0012_01_000005 succeeded{color}

 

991186 2018-05-23 15:41:27,228 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000005 transitioned from RUNNING to EXITED_WITH_SUCCESS

991187 2018-05-23 15:41:27,228 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(541)) - Cleaning up container container_e11_1526961340393_0012_01_000005

991188 2018-05-23 15:41:27,228 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

991189 2018-05-23 15:41:27,258 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

991190 2018-05-23 15:41:27,268 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000005 transitioned from EXITED_WITH_SUCCESS to DONE

991191 2018-05-23 15:41:27,268 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(347)) - Removing container_e11_1526961340393_0012_01_000005 from application application_1526961340393_0012

991192 2018-05-23 15:41:27,268 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread

991193 2018-05-23 15:41:27,268 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_STOP for appId application_1526961340393_0012

991194 2018-05-23 15:41:27,268 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(198)) - Stopping container container_e11_1526961340393_0012_01_000005

{color:#d04437}991195 2018-05-23 15:41:27,268 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e11_1526961340393_0012_01_000005{color}

 

Application application_1526961340393_0012,  *container container_e11_1526961340393_0012_01_000008* (from container succeeded to Stopping container (from blue to red line) took less than *1 seconds*)

991196 2018-05-23 15:41:27,340 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(810)) - Start request for container_e11_1526961340393_0012_01_000008 by user hlhuang

991197 2018-05-23 15:41:27,350 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(304)) - Adding container_e11_1526961340393_0012_01_000008 to application application_1526961340393_0012

991198 2018-05-23 15:41:27,351 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000008 transitioned from NEW to LOCALIZED

991199 2018-05-23 15:41:27,351 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_INIT for appId application_1526961340393_0012

991200 2018-05-23 15:41:27,351 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(192)) - Initializing container container_e11_1526961340393_0012_01_000008

991201 2018-05-23 15:41:27,352 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e11_1526961340393_0012_01_000008

991202 2018-05-23 15:41:27,397 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000008 transitioned from LOCALIZED to RUNNING

991203 2018-05-23 15:41:27,397 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread

991204 2018-05-23 15:41:27,397 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

{color:#205081}991205 2018-05-23 15:41:27,440 INFO  launcher.ContainerLaunch (ContainerLaunch.java:call(384)) - Container container_e11_1526961340393_0012_01_000008 succeeded{color}

 

991206 2018-05-23 15:41:27,440 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000008 transitioned from RUNNING to EXITED_WITH_SUCCESS

991207 2018-05-23 15:41:27,440 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(541)) - Cleaning up container container_e11_1526961340393_0012_01_000008

991208 2018-05-23 15:41:27,441 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

991209 2018-05-23 15:41:27,471 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000008 transitioned from EXITED_WITH_SUCCESS to DONE

991210 2018-05-23 15:41:27,471 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(347)) - Removing container_e11_1526961340393_0012_01_000008 from application application_1526961340393_0012

991211 2018-05-23 15:41:27,471 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread

991212 2018-05-23 15:41:27,471 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_STOP for appId application_1526961340393_0012

991213 2018-05-23 15:41:27,471 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(198)) - Stopping container container_e11_1526961340393_0012_01_000008

{color:#d04437}991214 2018-05-23 15:41:27,471 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e11_1526961340393_0012_01_000008{color}

{color:#d04437} {color}

 

{color:#14892c}Now compare with 3.0 above,  from Removed completed containers (line 991215) to transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP (line 991218), it only took 1 seconds for 2.6.{color}

{color:#14892c} {color}

 

*991215 2018-05-23 15:41:27,472* INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:removeOrTrackCompletedContainersFromContext(553)) - Removed completed containers from NM context: [container_e11_1526961340393_0012_01_000005, container_e11_1526961340393_0012_01_000003]

991216 2018-05-23 15:41:27,485 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

991217 2018-05-23 15:41:27,699 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime

*991218 2018-05-23 15:41:28,473* INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1526961340393_0012 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP

991219 2018-05-23 15:41:28,474 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event APPLICATION_STOP for appId application_1526961340393_0012

991220 2018-05-23 15:41:28,474 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopApplication(179)) - Stopping application application_1526961340393_0012

991221 2018-05-23 15:41:28,475 INFO  shuffle.ExternalShuffleBlockResolver (ExternalShuffleBlockResolver.java:applicationRemoved(206)) - Application application_1526961340393_0012 removed, cleanupLocalDirs = false

991222 2018-05-23 15:41:28,475 INFO  shuffle.ExternalShuffleBlockResolver (ExternalShuffleBlockResolver.java:applicationRemoved(186)) - Application application_1526961340393_0012 removed, cleanupLocalDirs = false

991223 2018-05-23 15:41:28,475 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1526961340393_0012 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED

991224 2018-05-23 15:41:28,475 INFO  loghandler.NonAggregatingLogHandler (NonAggregatingLogHandler.java:handle(167)) - Scheduling Log Deletion for application: application_1526961340393_0012, with delay of 604800 seconds

  

Above logs are from changing the yarn.nodemanager.container-monitor.interval-ms to 300 only.  I removed the monitoring to false property.

 

To summarize,   I believe there are two places that need to be investigated:

1. on each container, between container succeeded to stopping container, it takes 4 seconds for 3.0, and only 1 second for 2.6

2. from 'Removed completed containers' to 'application transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP', 3.0 takes a wopping 5 seconds and only 1 second for 2.6.

 

 , The root cause for the stopping delay coming from clean up containers:

{code}
    final int sleepMsec = 100;
    int msecLeft = 2000;
    if (pidFilePath != null) {
      File file = new File(getExitCodeFile(pidFilePath.toString()));
      while (!file.exists() && msecLeft >= 0) {
        try {
          Thread.sleep(sleepMsec);
        } catch (InterruptedException e) {
        }
        msecLeft -= sleepMsec;
      }
      if (msecLeft < 0) {
        if (LOG.isDebugEnabled()) {
          LOG.debug("Timeout while waiting for the exit code file:  "
              + file.getAbsolutePath());
        }
      }
    }
{code}

This was committed as part of YARN-5366 for making sure that exit code file exists, or we waited sufficient amount of time for the file to appear on disk before clean up operation happens.  [~shanekumpf@gmail.com] is there a better way to solve this problem without using the waiting logic?  Should this part of logic apply to non-docker containers?
, That code should only come into play if the exit code file isn't written and at most would add 2 seconds. Any idea what is delaying the exit code file? Do debug logs show the timeout message? (e.g. Timeout while waiting for the exit code file..), [~shanekumpf@gmail.com] In line [802|https://github.com/apache/hadoop/blob/fba9d7cd746cd7b659d2fd9d2bfa23266be9009b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java#L804] of ContainerLaunch.java, the exit_code file is deleted.  By the time that YARN-5366 sleeping loop is encountered, the file does not exist.  This is the reason that 2 second sleep always happens.  I mentioned in YARN-5366, I did not find sleep and retry docker inspect logic in YARN-5366, and I added in YARN-7654 line [1671|https://github.com/apache/hadoop/blame/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c#L1671] to have the retry logic for entry_point based docker container.  I think the two seconds sleep logic can be removed from ContainerLaunch.java. Thoughts?, Thanks for the analysis, [~eyang]. Based on your findings, it does appear this should be removed., [~shanekumpf@gmail.com]  When do you think you can have the patch uploaded?  This is causing a big slow down for 3.0 Yarn applications., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 22s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 23m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 29s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 42s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  9s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 18m  0s{color} | {color:green} hadoop-yarn-server-nodemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 21s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 68m  8s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:abb62dd |
| JIRA Issue | YARN-8326 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12928790/YARN-8326.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux cd2f8bf00ab6 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 55fad6a |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_171 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/21079/testReport/ |
| Max. process+thread count | 459 (vs. ulimit of 10000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/21079/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thank you for the patch, [~shanekumpf@gmail.com].  I committed this to branch-3.1 and trunk., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14469 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14469/])
YARN-8326.  Removed exit code file check for launched container.         (eyang: rev 8a32bc39eb210fca8052c472601e24c2446b4cc2)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java
, Thank you for reporting the issue, [~hlhuang@us.ibm.com], and thanks for the analysis and commit, [~eyang]!, FAILURE: Integrated in Jenkins build Hadoop-precommit-ozone-acceptance #20 (See [https://builds.apache.org/job/Hadoop-precommit-ozone-acceptance/20/])
YARN-8326.  Removed exit code file check for launched container.         (eyang: [https://github.com/apache/hadoop/commit/8a32bc39eb210fca8052c472601e24c2446b4cc2])
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java
, Thanks for fixing this! We at BellSoft also noticed major regression between Hadoop 3.0.1 and 3.1 and tracked that down to YARN-5366. Below there are some details someone may find interesting.

We run benchmarks like Terasort and DFSIO on a setup that uses Yarn but not Docker. Mainly on ARM64 machines. For example it is terasort of 32 M records which is relatively short running (minute scale). So on Cavium ThunderX2 the slowdown is ~2x, on ThunderX it's ~1.5 and also we made x86 setup which also shown ~1.5x slowdown. It happened with JDK 8,9,10 and 11.

We plot running JVM processes on a single timeline and take into account their CPU utilization. Consider plots before and after the change:

Before
https://raw.githubusercontent.com/dchuyko/hadoop/86bdc68a142e19c8eff083daa2de0d11257aae12/terasort-32M-good.png

After
https://raw.githubusercontent.com/dchuyko/hadoop/86bdc68a142e19c8eff083daa2de0d11257aae12/terasort-32M-bad.png

In the new gap between actual MR work all processes that are alive take <1% cpu, but then we sample stacks and it turns out that reaper and launcher logic at NodeManager take the time
https://raw.githubusercontent.com/dchuyko/hadoop/86bdc68a142e19c8eff083daa2de0d11257aae12/terasort-32M-bad-gap-stacks.png

And now the issue is gone.]