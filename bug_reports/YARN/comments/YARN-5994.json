[This test is racy and so the issue with it only manifests sometimes. YARN-5773 put in a change so that {{verifyAMLimitForLeafQueue}} registers a 2GB node. {{queueA.getAMResourceLimit}} should return 1024 for memory, but is racy and almost always gets set to 0 for memory. So when the test submits the app with {{amResource2}}, that resource is set to 2048, instead of 3072. In the instances that it gets correctly set to 3072, the test errors out that it is trying to use more than the max memory available. 

The bandaid fix would be to increase the node size to 4GB, but that doesn't fix the race in why {{queueA.getAMResourceLimit}} isn't getting updated before it is being accessed in the test. 

[~wangda], [~varun_saxena], [~bibinchundatt], [~rohithsharma], [~Naganarasimha], [~sunilg]: Does this analysis make sense? Should we just add a {{waitFor}} to wait until {{queueA.getAMResourceLimit}} isn't 0,0? , [~ebadger]
Thanks for reporting. Could you please point to complete log of this failed test case? If its jenkins failure, you can share the link.
Ideally if {{app shouldn't be null}} is happening, there should be some more information in the log which can help to us get real root cause. , [~ebadger]
As [~sunilg] mentioned app shouldnt be null after submit. As part of YARN-5773 only change is node addition and the events should have got drained.
To recheck the issue tried running locally on *trunk*  about 50-70 times. Didnt encounter the issue.
{noformat}
for i in {1..100}; do mvn test -Dtest=TestCapacityScheduler#testAMLimitUsage | grep BUILD; done
{noformat}
Could you please upload the logs.
Was the test failure on trunk ?, Uploading a log of the test failing in trunk. This is a smaller race condition that I haven't seen fail on an unloaded machine. But we've seen it internally twice in the last month or so. The way I forced the test to fail was by adding a {{Thread.sleep(10000)}} immediately before {{amResourceLimit}} is set. 

{noformat}
@@ -3280,6 +3280,7 @@ private void verifyAMLimitForLeafQueue(CapacitySchedulerConfiguration config)
     ResourceScheduler scheduler = rm.getRMContext().getScheduler();
     LeafQueue queueA =
         (LeafQueue) ((CapacityScheduler) scheduler).getQueue(queueName);
+    Thread.sleep(10000);
     Resource amResourceLimit = queueA.getAMResourceLimit();
{noformat}, Turns out the problem was on the scheduler side. {{rm.submitApp()}} was being called before the max capacity on the scheduler was being updated. So in most normal cases (i.e. not a slow machine) the max would still be set to the default (8092) when it checked to see if the request was over capacity. In the case of the main thread getting slowed for whatever reason, the max capacity would update to 2048, which is what we set in {{rm.registerNode()}} early in the test. Then when {{rm.submitApp()}} was called it would see that the request of 3072 > 2048 and fail. Putting a {{Thread.sleep(10000)}} anywhere before {{rm.submitApp()}} causes the test to fail in this way.

Putting up a patch that increases the node memory to 4GB and puts in a {{waitFor()}} to wait for the max capacity to be updated. , | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 13s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  3s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 20s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 1 new + 189 unchanged - 0 fixed = 190 total (was 189) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  1s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 39m 24s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 61m 29s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5994 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12848073/YARN-5994.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux e9df1a48f32d 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 72054a8 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14692/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/14692/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14692/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14692/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, [~bibinchundatt], [~sunilg], could you please review the latest patch? Thanks!, Thanks [~ebadger]

Ideally this make sense. Since this test is only caring about application activation, we dont need an NM to be registered. However for internal cluster resource and other data  structures, we need some resource in cluster. 

For eg: *scheduler.getMaximumResourceCapability()* is calculated as
{{Math.min(configuredMaxAllocation.getMemorySize(), maxNodeMemory)}}, we consider 8GB (max-allocation-mb) or max-memory across all nodes. 
Here max NM memory is 2GB. Hence {{scheduler.getMaximumResourceCapability()}} will become 2GB. This will cause *RMAppManager#validateAndCreateResourceRequest* to fail the app as AM resource requests is for 3GB which is greater than {{scheduler.getMaximumResourceCapability()}}.

So bringing NM memory to 4G is fine. [~bibinchundatt], any comments on this?, Thanks [~ebadger] for reporting and fixing this issue.

The only thing I found is that the patch does not build in branch-2 or branch-2.8 because the anonymous inner class returned by {{GenericTestUtils.waitFor}} in {{TestCapacityScheduler#verifyAMLimitForLeafQueue}} uses both of the {{nodeMemory}} and {{scheduler}} local variables. Since both branch-2 and branch-2.8 are compiling with java 1.7, these variables need to be final.

The patch looks good to me. I give my +1. I will commit this soon if there are no objections, and I will fix the {{final}} issues as part of the backport to branch-2 and branch-2.8., | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 25s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 1 new + 195 unchanged - 0 fixed = 196 total (was 195) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 39m 19s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 62m 27s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:612578f |
| JIRA Issue | YARN-5994 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12848073/YARN-5994.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux ff757257d637 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 0cab572 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/15639/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/15639/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/15639/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11585 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11585/])
YARN-5994. TestCapacityScheduler.testAMLimitUsage fails intermittently. (epayne: rev a41f8dd58e27d8835fbb64eeaba5d7416df0499d)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11591 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11591/])
YARN-5994. TestCapacityScheduler.testAMLimitUsage fails intermittently. (epayne: rev a41f8dd58e27d8835fbb64eeaba5d7416df0499d)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java
, 2.8.1 became a security release. Moving fix-version to 2.8.2 after the fact.]