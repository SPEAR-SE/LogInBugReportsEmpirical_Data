[hi all，
     Here's other phenomena :
     1.       If some one submit job, the resourcemanager accepts it, but the job doesn’t run. In the meantime, the resourcemanager print a lot logs like “2013-11-27 14:27:02,258 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Request for appInfo of unknown attemptappattempt_1384743376038_1121_000001”, and the fairscheduler doesn’t print hearbeat log to ${HADOOP_HOME}/logs/fairscheduler/hadoop-{user}-fairscheduler.log
     2.       The fairscheduler ui can’t be opened and response 500 error.

And here's resourcemanager log when this error appearing:
 Here's  normal logs:
2013-11-27 14:25:36,515 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1120 submitted by user root
2013-11-27 14:25:36,515 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1384743376038_1120
2013-11-27 14:25:36,515 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root     IP=192.168.24.101       OPERATION=Submit Application Request    TARGET=ClientRMService  RESULT=SUCCESS  APPID=application_1384743376038_1120
2013-11-27 14:25:36,515 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1384743376038_1120 State change from NEW to NEW_SAVING
2013-11-27 14:25:36,515 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1384743376038_1120
2013-11-27 14:25:36,516 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1384743376038_1120 State change from NEW_SAVING to SUBMITTED
2013-11-27 14:25:36,516 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1384743376038_1120_000001
2013-11-27 14:25:36,516 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1384743376038_1120_000001 State change from NEW to SUBMITTED
2013-11-27 14:25:36,516 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application Submission: appattempt_1384743376038_1120_000001, user: root, currently active: 2
2013-11-27 14:25:36,516 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1384743376038_1120_000001 State change from SUBMITTED to SCHEDULED
2013-11-27 14:25:36,516 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1384743376038_1120 State change from SUBMITTED to ACCEPTED
2013-11-27 14:25:36,816 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable: Node offered to app: application_1384743376038_1120 reserved: false
 
 
Abnormal logs:  these logs doesn’t contain the log like :
 2013-11-27 14:25:36,516 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application Submission: appattempt_1384743376038_1120_000001, user: root, currently active: 2
 
Here is abnormal logs:
2013-11-27 14:27:01,391 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1122
2013-11-27 14:27:01,391 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1121
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1121 submitted by user yangping.wu
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1122 submitted by user yangping.wu
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yangping.wu      IP=192.168.24.101 OPERATION=Submit Application Request    TARGET=ClientRMService  RESULT=SUCCESS  APPID=application_1384743376038_1121
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1384743376038_1122
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yangping.wu      IP=192.168.24.101
       OPERATION=Submit Application Request    TARGET=ClientRMService  RESULT=SUCCESS  APPID=application_1384743376038_1122
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1384743376038_1122 State change from NEW to NEW_SAVING
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1384743376038_1121
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1384743376038_1122
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1384743376038_1121 State change from NEW to NEW_SAVING
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1384743376038_1121
2013-11-27 14:27:02,252 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1384743376038_1122 State change from NEW_SAVING to SUBMITTED
2013-11-27 14:27:02,253 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1384743376038_1121 State change from NEW_SAVING to SUBMITTED
2013-11-27 14:27:02,253 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1384743376038_1122_000001
2013-11-27 14:27:02,253 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1384743376038_1122_000001 State change from NEW to SUBMITTED
2013-11-27 14:27:02,253 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1384743376038_1121_000001
2013-11-27 14:27:02,253 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1384743376038_1121_000001 State change from NEW to SUBMITTED
2013-11-27 14:27:02,258 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Request for appInfo of unknown attemptappattempt_1384743376038_1122_000001
2013-11-27 14:27:02,258 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Request for appInfo of unknown attemptappattempt_1384743376038_1121_000001

, In the jstack, the Fair Scheduler update thread, which runs periodically, and the event processor thread are competing to synchronize on the FairScheduler lock. The jstack is not suspicious by itself, but perhaps the update thread is taking unusually long to complete recalculating shares and release the lock.  The next time it occurs would you be able to collect a few jstacks spaced a second or less apart and we could see whether this is happening?, Hi Sandy,
    Thank you for your attention.
    We collected the jstacks stutus spaced few minutes When that phenomenon happening. We get lots of tmp file of the jstacks status,but the ResourceManager Event Processor still blocked. And few hours later, it still blocked. Finally, we restart the resourcemanager., Hi Sandy,
     Just now, we print some mark in the FairScheduler.getAppWeight() method. Fortunately , we maked the error appears again. We found that the FairScheduler.getAppWeight() method was called repeat frequently. The lock of FairScheduler object was occupied by the  FairScheduler.getAppWeight(), it seams like the  ResourceManager Event Processor didn't have opportunity to get the lock. So, it's blocked., About how many apps are running / submitted when this occurs?, @Sandy， 10~20 apps are running / submitting when this occurs., I read the related code, and I find the code below may be the reason that the fairscheduler object lock is always held.   
In some exceptional case(not verified, just guess), some abnormal  jobs are contained, which may cause the endless loop(while loop) , so the lock is not released. 

//file: org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares: line 102 
while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type) < totalResource) {
      rMax *= 2.0;
}

I try to modify the code . I add a counter of the loop and break the loop when the counter is up to a threshold(here is 30 ).
The example code is below: 

int count = 0;
while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type) < totalResource and count < 30) {
      rMax *= 2.0;
      ++count;
}

The correctness of code modification is not verified and need some experiments and tests., sorry, there is a syntax error in the modification code, which should be as follows: 
int count = 0;
while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type) < totalResource && count < 30)
{ rMax *= 2.0; ++count; }, I tested the code //file: org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares: line 102 , found that when the error happed the return value of resourceUsedWithWeightToResourceRatio(rMax, schedulables, type) always was 0, totalResource was 1000 and the Collection<? extends Schedulable> schedulables.size() is 1. So, the endless loop happened here, We will find out the reason of leading to that endless loop. But the error is hard to reappear, we are trying to submit jobs and restart the resouremanager to guide the error., Oh, it is good news. It verified my guess. Although we haven't known why the exceptional case to be happened yet, that while loop code segment is very danger to be a endless loop. Anyway, that code segment should be modified to become more robust. , Thanks for tracking this down.  That makes a lot of sense.  While the change you propose may ultimately be what we choose fix this, I'd like to understand why this is happening.  I.e. resourceUsedWithWeightToResourceRatio should never return 0, so we should find out what's causing it to return that.

For resourceUsedWithWeightToResourceRatio to return 0, computeShare must be returning 0 for the schedulable.  Here is computeShare:
{code}
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
{code}
For computeShare to return 0, either the AppSchedulable's weight must be 0 or the max share must be 0.  An app's weight is 1, unless size based weight is turned on or you've provided a custom weight adjuster.  An app's max share is always Integer.MAX_VALUE. Have you turned on size based weight or provided a custom weight adjuster?, Hi Sandy, we have turned on size based weight but haven't provided a custom weight adjuster., Cool, in that case I think I understand the problem.
{code}
      if (sizeBasedWeight) {
        // Set weight based on current memory demand
        weight = Math.log1p(app.getDemand().getMemory()) / Math.log(2);
      }
{code}
If size based weight is turned on, the weight is calculated as log(1 + demand), which will be 0 if demand is 0.  An app that has not yet submitted resource requests or otherwise has 0 demand will thus cause the weight to be 0, which, as discussed above, causes the update thread to loop indefinitely.

The best solution is probably to handle 0 values of resourceUsedWithWeightToResourceRatio in ComputeFairShares#computeShares.  If it returns 0 we should just set the fair shares of all the considered schedulables to 0.

If one of you who uncovered this wants to post a patch, I can review it.  Otherwise I'll post a patch sometime later this week., Thank you firstly. That's right. We found that thing too， and we found the app.getDemand().getMemory() always return 0，that's confirmed your point.  We suspect the app hasn't called AppSchedulable.updateDemand() method, but the FairScheduler.UpdateThread().run() called the FairScheduler.getAppWeight() method, so FairScheduler get in the endless loop.
We will post a patch laterly, and it's greate honour that you helping review it.
Thanks again.
, We have test our suspicious of points, and it doesn't work. We will  focus on handle 0 values of ComputeFairShares#computeShares returnning.  How about if it return 0 we just count it's weight just as the situation that sizebasedweight is true.  That's mean, if the it return 0, we can set it's weight 1., If size based weight is turned on and an app has 0 demand, I think giving it 0 fair share is the correct thing to do.  I.e., if there are two apps and one has 0 demand, the other app should get the entire share.  We just need to handle the special case where all apps in a queue have 0 weight and make it so that this does not result in an infinite loop in the computeShares method., Thanks Sandy. We were confused by your point that "  If it returns 0 we should just set the fair shares of all the considered schedulables to 0.". In our understanding, you suggested to set all app's weight to 0 when one app's weight is 0. So we proposed the idea above. 
But now we agree with the point that "If size based weight is turned on and an app has 0 demand, I think giving it 0 fair share is the correct thing to do.".  It's more precise to the principle of FairShare., In the Fair Scheduler, if size based weight is turned on, it will lead to endless loop in "ComputeFairShares.computeShares (ComputeFairShares.java:102)" that if all app's require resource in one queue is 0. 
This patch deals with that situation, we let the program jump out of the loop if all app's require resource of one queue is 0. That means set that queue's require resource to 0., In the Fair Scheduler, if size based weight is turned on, it will lead to endless loop in "ComputeFairShares.computeShares (ComputeFairShares.java:102)" that if all app's require resource in one queue is 0.
This patch deals with that situation, we let the program jump out of the loop if all app's require resource of one queue is 0. That means set that queue's require resource to 0, Hi Sandy，
We have submitted the patch: YARN-1458.patch . Please help us reviewing it.
Nice to work with you. Thank you so much!, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12616677/YARN-1458.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2583//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2583//console

This message is automatically generated., Guys, What is the status of this issue now?
We met this issue in our production cluster recently.
Now, we have to disable sizebasedweight. 

This patch looks good to me. But there is 1 core UT failure. And now, jenkins has rotated the test results. Can we trigger the test again and see what is wrong?

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12616677/YARN-1458.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4610//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4610//console

This message is automatically generated., The regression is org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler.testIsStarvedForFairShare.

I applied the patch to the latest trunk code, ran this UT in my local laptop. The UT always succeeds.
I've also check the code, but could not figure out why the UT fails.
Can anyone help?

Thanks., Sorry about my last response.  My code was not the latest.

Just updated the trunk code to the latest. The regression happened.
I'll look into this issue., The patch didn't consider type conversion from double to integer in computeShare will lose precision. So "break when zero" will cause all  Schedulable's FairShare to be zero if all Schedulable's Weight and MinShare are less than 1. In the unit test, the queues' Weight are 0.25 and 0.75, the queues' MinShare are Resources.none().
I will create a new patch., I uploaded a new patch YARN-1458.001.patch, which will avoid losing precision for type conversion from double to integer.
[~sandyr], Could you review it? thanks, George Wong ,
You can try our YARN-1458.patch and it is easy to understand,but the issue is still unresolved. You can consult to the corresponding code in later hadoop version such as 2.2.1, 2.3.x, 2.4.x


zhihai xu,
It seams your thinking is more rigorous than our patch., I added a test case "testFairShareWithZeroWeight" in new patch "YARN-1458.002.patch" to verify the patch can work with zero weight.
Without the patch, "testFairShareWithZeroWeight" will run forever., [~shurong.mai], YARN-1458.patch will cause regression. It won't work if all the weight and MinShare in the active queues are less than 1.
The type conversion from double to int in computeShare loses precision.
{code}
private static int computeShare(Schedulable sched, double w2rRatio,
      ResourceType type) {
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
  }
{code}
In above code, the initial value w2rRatio is 1.0. If weight and MinShare are less than 1, computeShare will return 0.
resourceUsedWithWeightToResourceRatio will return the sum of all these return values from computeShare(after lose precision).
It will be zero if all the weight and MinShare in the active queues are less than 1. Then YARN-1458.patch will exit the loop earlier with
"rMax" value 1.0. Then "right" variable will be less than "rMax"(1.0). Then all queues' fair share will be set to 0 in the following code.
{code}
    for (Schedulable sched : schedulables) {
      setResourceValue(computeShare(sched, right, type), sched.getFairShare(), type);
    }
{code}

This is the reason why the TestFairScheduler is failed at line 1049.
testIsStarvedForFairShare configure the queueA weight 0.25 and queueB weight 0.75 and total node resource 4 * 1024.
It creates two applications: one is assigned to queueA and the other is assigned to queueB.
After FaiScheduler(update) calculated the fair share,  queueA fair share should be 1 * 1024 and queueB fair share should be 3 * 1024.
but with YARN-1458.patch, both queueA fair share and queueB fair share are set to 0,
It is because in this test there are two active queues:queueA  and queueB, both weights are less than 1(0.25 and 0.75), MinShare(minResources) in queueA  and queueB are not configured, both MinShare use default value(0)., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12663617/YARN-1458.002.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4695//console

This message is automatically generated., I uploaded a new patch "YARN-1458.003.patch" to resolve merge conflict after rebase to latest code., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12663743/YARN-1458.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4698//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4698//console

This message is automatically generated., I uploaded a new patch "YARN-1458.004.patch" to fix the test failure.
The test failure is the following:
Parent Queue: "root.parentB" have one Vcore steady fair share.
But root.parentB have two child queues:root.parentB.childB1 and root.parentB.childB2. we can't split one Vcore to two child queues.
The new patch will calculate conservatively to assign 0 Vcore to both child queues.
The old code will assign 1 Vcore to both child queues, which will be over total resource limit., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12663814/YARN-1458.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4702//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4702//console

This message is automatically generated., The test failure is not related to my change.
TestAMRestart is passed in my local build.


 T E S T S
-------------------------------------------------------
Running org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 89.639 sec - in org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart

Results :

Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, If we don't want to change the old way to calculate the fair share, I uploaded an alternative patch "YARN-1458.alternative0.patch",
This patch filtered all the Schedulable/queues which has zero weight before calculate the fair share.
It set these zero weight Schedulable/queues fair share to 0 and removes these Schedulable/queues from the list.
This patch will be conservative without affecting the old tests.
But the old code will allocate fair share more than total resource sometimes., I just found another corner case, which can cause loop forever, this corner case is if weight is 0 but minShare is not 0.
For example, we have tow active queues:queueA and queueB,
queueA 's weight is 0, queueA's minShare is 1.
queueB 's weight is 0, queueB's minShare is 1.
and total resource is 1024.
computeShare for both queueA and queueB will return 1.
So resourceUsedWithWeightToResourceRatio will always return 2, no matter what  w2rRatio will be.
Then it will loop forever. Check zero("break when zero") won't be enough.
In this case, Modifying the first solution is very difficult to fix this case,
it will make more sense to modify the alternative solution., I uploaded a new patch "YARN-1458.alternative1.patch" which modify the alternative solution to fix both corner cases.
I also added a new test case "testFairShareWithZeroWeightNoneZeroMinRes" in the new patch.

By the way, about "allocate fair share more than total resource sometimes" issue,
It is shown in tests: testSimpleFairShareCalculation and testFairShareWithDRFMultipleActiveQueuesUnderDifferentParent.
I am not sure whether it is intended, if it is not intended behavior in fair scheduler, we should create a separate JIRA to address it. The easy fix can be using "left" instead of "right" to do computeShare. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12663982/YARN-1458.alternative1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4711//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4711//console

This message is automatically generated., Thanks Zhihai for working on this. I like the first approach: uploading a patch with minor nit fixes. Let me know if this looks good to you. , By the way, I like the first approach mainly because of its simplicity and readability. 

In the while loop that was running forever, we could optionally keep track of the resource-usage from the previous iteration and see if we are making progress. , Hi [~kasha], thanks for the review, The first approach has simplicity and readability advantage but it can't cover all the corner cases.
the alternative approach can fix zero weight with non-zero minShare but the first approach can't. 
Both approach can fix zero weight with zero minShare. We may have limitation to keep track of the resource-usage from the previous iteration and see if we are making progress, For example for a very small weight, there may be 0 value return from resourceUsedWithWeightToResourceRatio  after multiple iteration.
thanks
zhihai, bq. the alternative approach can fix zero weight with non-zero minShare but the first approach can't
I see. Good point. I was wondering if there were cases we might want to check for {{if (currentRU - previousRU < epsilon || currentRU > totalResource)}}. The zero weight and non-zero minshare should be handled by such a check, no? , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667252/yarn-1458-5.patch
  against trunk revision d989ac0.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4849//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4849//console

This message is automatically generated., Yes, it works, it can fix the zero weight with non-zero minShare if we compare with previous result.
But the alternative approach will be a little faster compare to the first approach(less computation and less schedulables in the calculation after filtering fixed shared schedulables). Either approach is ok for me.
I will submit a patch on the first approach to compare with previous result., Hi [~kasha], I just found an example to prove the first approach doesn't work when minShare is not zero(all queues have none zero minShare).
The following is the example:
We have 4 queues A,B,C and D: each have 0.25 weight, each have minShare 1024,
The cluster have resource 6144(6*1024)
using the first approach to compare with previous result, we will exit early in the loop with each Queue's fair share is 1024.
The reason is that computeShare will return minShare value 1024 when rMax <=2048 in the following code:
{code}
private static int computeShare(Schedulable sched, double w2rRatio,
      ResourceType type) {
    double share = sched.getWeights().getWeight(type) * w2rRatio;
    share = Math.max(share, getResourceValue(sched.getMinShare(), type));
    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));
    return (int) share;
  }
{code}
So for the first 12 iterations, the currentRU is not changed which is sum of all queues' minShare(4096).
If we use second approach, we will get the correct result: each Queue's fair share is 1536.
In this case, the second approach is definitely better than the first approach,
the first approach can't handle the case:all queues have none zero minShare.

I will create a new test case in the second approach patch., I uploaded a new patch "YARN-1458.alternative2.patch" which add a new test case:all queues have none zero minShare:
queueA and queueB each have eight 0.5 and minShare 1024,
the cluster have resource 8192. so each queue should have 4096 fair share., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667336/YARN-1458.alternative2.patch
  against trunk revision 7498dd7.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4854//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4854//console

This message is automatically generated., I uploaded a patch YARN-1458.006.patch for the first approach:
This patch compare with previous result in the loop to fix "the zero weight with non-zero minShare" issue and calculate the start point for rMax using the minimum ratio of minShare/weight to fix "all queues have none zero minShare" issue.
Either approach is ok for me. but the second approach is a little simpler and faster than the first approach.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667451/YARN-1458.006.patch
  against trunk revision 90c8ece.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The test build failed in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager 

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4859//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4859//console

This message is automatically generated., Thanks Zhihai. 

I see the advantage of the second approach. My main concern is readability of the approach. I have taken a stab at making it more readable/maintainable through only cosmetic changes. Can you please take a look and see if these cosmetic changes make sense to you. , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667535/yarn-1458-7.patch
  against trunk revision 28d99db.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4865//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4865//console

This message is automatically generated., Hi [~kasha], Your change makes the code much easier to read and maintain.
I uploaded a new patch "yarn-1458-8.patch" with two minor changes based on your patch:
use Math.max instead of Math.abs and check schedulables.isEmpty() after handleFixedFairShares.
Please review it.
thanks
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667572/yarn-1458-8.patch
  against trunk revision 0de563a.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4869//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4869//console

This message is automatically generated., +1. Committing version 8. , Thanks Zhihai and [~qingwu.fu] for working on this, and Sandy for the reviews. 

Just committed this to trunk and branch-2. , SUCCESS: Integrated in Hadoop-Yarn-trunk #677 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/677/])
YARN-1458. FairScheduler: Zero weight can lead to livelock. (Zhihai Xu via kasha) (kasha: rev 3072c83b38fd87318d502a7d1bc518963b5ccdf7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/ComputeFairShares.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1893 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1893/])
YARN-1458. FairScheduler: Zero weight can lead to livelock. (Zhihai Xu via kasha) (kasha: rev 3072c83b38fd87318d502a7d1bc518963b5ccdf7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/ComputeFairShares.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* hadoop-yarn-project/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1868 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1868/])
YARN-1458. FairScheduler: Zero weight can lead to livelock. (Zhihai Xu via kasha) (kasha: rev 3072c83b38fd87318d502a7d1bc518963b5ccdf7)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/ComputeFairShares.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
, [~kkambatl], do you mind backporting the patch for branch-2.5? It looks critical problem since 2.2.0., I am open to backporting this to branch-2.5, but we don't have a 2.5.2 release planned yet. We should probably discuss 2.5.2 and the need for it on the dev lists. , Sure, thanks for reply :-), Seems to me that this is not fixed in 2.6.0. We've hit this bug with CDH 5.4.4 which is shipped with patched hadoop 2.6.0

{noformat}
"FairSchedulerUpdateThread" daemon prio=10 tid=0x00007f550c0f5800 nid=0x1155 runnable [0x00007f54fdf4d000]
  java.lang.Thread.State: RUNNABLE
       at java.lang.StrictMath.log1p(Native Method)
       at java.lang.Math.log1p(Math.java:1236)
       at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getAppWeight(FairScheduler.java:510)
       - locked <0x00000007a58d9430> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)
       at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getWeights(FSAppAttempt.java:749)
       at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShare(ComputeFairShares.java:191)
{noformat}

disabling size-based weights helped immediately (apps were running again), Hi [~dwatzke], thanks for reporting this issue, I double check the code, I find one corner case which can cause this issue. Hopefully this is the only case which isn't handled.
The corner case is when current memory demand for the app is Integer Overflow. If that happen, the weight will become [NaN|https://docs.oracle.com/javase/7/docs/api/java/lang/Math.html#log1p(double)] because current memory demand is a negative value.
{code}
weight = Math.log1p(app.getDemand().getMemory()) / Math.log(2); 
{code}
{{getFairShareIfFixed}} will treat NaN weight same as positive weight.
{{computeShare}} will always return 0 if the weight is NaN because {{share}} is NaN and {{(int)NaN}} is 0.
I attached a addendum patch YARN-1458.addendum.patch, Could you verify whether this patch can fix your issue?
thanks

, I think FSAppAttempt may add duplicate ResourceRequest to demand, which may cause current memory demand Integer Overflow. I created YARN-4979 to fix the wrong demand calculation issue for FSAppAttempt. The root cause may be YARN-4979., [~zxu] Thanks very much for looking into this, however I won't be able to test this anytime soon - it only happened once on our production cluster under a heavy load and we can't afford any planned outage there right now., Ok, no problem, you can try it at your convenience. thanks for finding this issue!]