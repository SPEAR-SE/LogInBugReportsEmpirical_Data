[YARN-3593 In this issue if can be sure <DEFAULT_PARTITION> is stored as <ECLUSIVE_TRUE> in RMNodeLabelManager?, Hi [~Feng Yuan], 
bq.  think it means if queue A`s partition x is used up,app can get containers from DEFAULT_PARTITION? Do i misstake?
i think your understanding here is wrong, apps with requests on DEFAULT_PARTITION can make use of resources in NonExclusive partitions only when Nonexclusive parttion resources are free. But resource requests for specific partition cannot be allocated to Default Partition. And neither in 2.6 we supported it !
, bq. "All queues can access to nodes without label"
This means no need to configure the "DEFAULT_PARTITION" explicitly for a queue, by default it will be available!, thanks for your quickly reply,i agree with your explain about the "All queues can access to nodes without label". Along this line of thought:
If i set queue A "yarn.scheduler.capacity.root.A.accessible-node-labels"=x  --means it can access x and default.
and i unset "yarn.scheduler.capacity.root.A.default-node-label-expression" as doc say "By default, this is empty, so application will get containers from nodes without label." if my app can get containers from "default" firstly and when default used up gets containers from x?

In fact, my demand is jobs in queue A(batch data work) can use one partition as 'MR' and another partition as 'SPARK'(high memory) such as 100%*MR and 30%*SPARK.
and jobs in queue B(ML jobs) can use 70%*SPARK. could i achieve this by now(2.8)?, bq.  "By default, this is empty, so application will get containers from nodes without label." if my app can get containers from "default" firstly and when default used up gets containers from x?
Its basically waits till certain number of heartbeats before allocating in the other Non Exclusive Partition. So more or less we can say that default gets usedup then if node of partition X is free then it will assign in it.
bq. In fact, my demand is jobs in queue A(batch data work) can use one partition as 'MR' and another partition as 'SPARK'(high memory) such as 100%*MR and 30%*SPARK. and jobs in queue B(ML jobs) can use 70%*SPARK. could i achieve this by now(2.8)?
this can be acheived in any version but sharing between *Spark -> default partition* or *MR -> default partition* depends on the type of SPARK and MR Partition . And this sharing is supported for Non Exclusive Partition from 2.8 only !
, Hi all above is my configs,i can run a job which can get containers from partition x and partition default with this configs in hdp 2.7.2.
I setup two nodes:
                  node "196" have 6GB with label "default".
                  node "197" have 6GB with label "x".
                  in my configs,queue A have 100% resource of them two.
                  and queue A`s default-node-label-expression is "x".(default empty dont work like below.)
                  When i submit job in queue A,containers(one allcate 2GB) spread over 196 and 197 as the picture show.
                  I think this  phenomenon  contrarys to what [~Naganarasimha]     says.
                 And i cant run this scene in 2.8.
                 How could i explain this?, {quote}
this can be acheived in any version but sharing between Spark -> default partition or MR -> default partition depends on the type of SPARK and MR Partition . And this sharing is supported for Non Exclusive Partition from 2.8 only !
{quote}
I think i understand your words,you mean set SPARK "Non Exclusive",sharing SPARK to default? But i finded job with "No Label" cant control the usage of SPARK.For example when default used up and SPARK is idel fully,resources will be bleed white by this job,i cant control x% the job can use.(example:i set yarn.scheduler.capacity.root.A.accessible-node-labels.x.capacity to 10% didnt work when x is "Non Exclusive") 
So i want to know if i can get this scene,job get share from default to spark,so that i can control usage of spark and default both by set capacity(%) and no label is "Non Exclusive".I do this in 2.7.2 as above comment.
Thanks [~Naganarasimha] for your detailed reply.
Have a good day!, [~Feng Yuan]
bq. .For example when default used up and SPARK is idel fully,resources will be bleed white by this job,i cant control x% the job can use.(example:i set yarn.scheduler.capacity.root.A.accessible-node-labels.x.capacity to 10% didnt work when x is "Non Exclusive") 
May be i did not get you completely but to summarize :
Currently NonExclusive Partitions are only in 2.8 sharing is based on this feature.
And sharing is only from {{Non Exclusive to Default Partition}} only. , [~Feng Yuan], I'm also confused.

bq. Labeled apps cant use <DEFAULT_PARTITION> resources
This is expected behavior. Shareable partition works in a reserved way: non-labeled app can use idle resource from shareable partition.
, Hi [~wangda],could you spend little time to act the example i attached above,because in my environment i really get the sharing from default to Exclusive labeled partition.
So i am confused of the rule among 2.7.2 and 2.8.
Thanks a lot!, Hi [~Naganarasimha] Because *IGNORE_PARTITION_EXCLUSIVITY* does not regard the partition max capacity,so sharing from Non Exclusive to default is uncontrolled,right?, Hi [~Feng Yuan]

From 2.8, we will only be able to share resources from a non-exclusive label (exclusivity=false) to "default" label. Its will not be possible other way around. So any apps which are submitted without any labels, could be able to share/use resource from a non-exclusive label. So I suggest you could try defining your system with this node-label model., Yes while in *IGNORE_PARTITION_EXCLUSIVITY* it will not regard whether queue has access to the NonExclusive partition which is trying to share. And it tries to share the Max capacity of resources, Do 2.8 support the *x or y* expression in the yarn.scheduler.capacity.root.A.default-node-label-expression property? 
I wanna a  queue to get containers from both x and y together., Its not yet supported !, Close this issue since it`s normal behavior in 2.8. Thanks for your replys.]