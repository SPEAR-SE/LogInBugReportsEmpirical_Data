[Extracted thread dump from NodeManager is 

{noformat}
"Node Status Updater" prio=10 tid=0x00000000414dc000 nid=0x1d754 in Object.wait() [0x00007fefa2dec000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1231)
	- locked <0x00000000def4f158> (a org.apache.hadoop.ipc.Client$Call)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
	at $Proxy28.nodeHeartbeat(Unknown Source)
	at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.nodeHeartbeat(ResourceTrackerPBClientImpl.java:70)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at $Proxy30.nodeHeartbeat(Unknown Source)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:348)
{noformat}, Are you able to reproduce this scenario? Can you please enable DEBUG (HADOOP_ROOT_LOGGER & YARN_ROOT_LOGGER) logs and attach them to this jira? How big is your cluster? what is the frequency at which nodemanagers are heartbeating? Can you also attach yarn-site.xml? which version are you using?, Actual issue I got in 5 node cluster (1 RM and 5 NM).It is hard to recure scenario for resourcemanager is hang up state in real cluster. 

The same scenario can be simulated manually bringing resourcemanager to hang up state with help of linux command "KILL -STOP <RM_PID>". All the NM->RM call wait indefinitely. Another case where we can observer indefinite wait is "Add new NodeManager when ResouceMangaer is hang up state".

    , How can NM wait infinitely? I mean what is your connection timeout set to? can you add below parameters to your log4j.properties and see if actually times out or wait infinitely for RM... Also can attach those logs once you simulate it?
{code}
log4j.logger.org.apache.hadoop.ipc.Server=DEBUG
log4j.logger.org.apache.hadoop.ipc.Client=DEBUG
{code}

Also helpful configurations from *CommonConfigurationKeysPublic*
{code}
  public static final String  IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY =
    "ipc.client.connection.maxidletime";
  /** Default value for IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY */
  public static final int     IPC_CLIENT_CONNECTION_MAXIDLETIME_DEFAULT = 10000; // 10s
  /** See <a href="{@docRoot}/../core-default.html">core-default.xml</a> */
  public static final String  IPC_CLIENT_CONNECT_TIMEOUT_KEY =
    "ipc.client.connect.timeout";
  /** Default value for IPC_CLIENT_CONNECT_TIMEOUT_KEY */
  public static final int     IPC_CLIENT_CONNECT_TIMEOUT_DEFAULT = 20000; // 20s
{code}
, I added all the ipc configurations to log4j.properities file, stil same issue recured.

bq. How can NM wait infinitely? I mean what is your connection timeout set to? 
When I debug the issue , found that it is an issue with IPC layer. This problem ocure in DataNode to NameNode communication also.

When process is in T state(for running process, state is S1. This can be seen by "ps -p <pid> -o pid,stat" ) i.e process is stopped using "kill -stop <pid>" , ipc proxy does not throw any timeout exception.
This is becaue , during proxy creation RPC timetime out is set to Zero(hardcoded) at RPC.waitForProtocolProxy method. Settiing rpc timeout to Zero makes ipc call does not throw any exception.Always ipc call(client) retry for sendPing to server(RM).
This can be seen in Client.handleTimeout method
{noformat}
      private void handleTimeout(SocketTimeoutException e) throws IOException {
        if (shouldCloseConnection.get() || !running.get() || rpcTimeout > 0) {
          throw e;
        } else {
          sendPing();
        }
      }
{noformat}

I think RPC timeout should be taken from configurations instead of hardcoding to 0., This is a dupe from YARN-2578. Writes do not time out and they should., [~wilfreds] Thanks for pointing out issue in RMHA. Yes this is same as YARN-2578. This issue was got when RM HA was not implemented but now this become very serious problem in RM HA cases.
, Close this as duplicate. Let us track the issue from YARN-2578]