[We've noticed container release is particularly painful as well, although we haven't seen it deadlock.

Whether we do this asynchronously or not, one issue is that releasing a bunch of containers requires grabbing a highly-contended lock for every container released.  Do this in a loop and it ends up taking a long time since getting the lock is not cheap.  Async scheduling helps since we can wait in some other thread rather than in the AM handler threads or scheduler dispatcher thread, but it will still take a long time looping through all those events.  I think it would be a lot better if there was a bulk-release interface so we could grab the critical lock once.  We can put a limit on how many we do per batch if we're worried it will hold that lock for too long, but I don't think it's so much the actual work per container as it is the time spent waiting for the lock that makes this so painful.
, I like Jason's idea. , The only potential issue I can see is: prior to this change, AM can assume containers are released by RM once allocate() returns. In the new world, AM has to check completed container list in AllocateResponse to make sure containers are released. It may not be a big issue though since I don't think we guarantee this in API description.

Beyond that, I like Jason's idea as well, share one fact: When I was doing async scheduling test in YARN-5139, I found resource commit phase (acquires write lock, check and update scheduler internal state such as resource usages, etc.) only takes less than 6% time, most of the time are consumed by {{CapacityScheduler#allocateContainersToNode}}. I suspect container release take the similar amount of time (around 6%)., Thanks for chiming in folks.
And yes, I agree with [~jlowe] too. To move forward, and if everyone if fine with the approach, I will post a patch that does the following:
* Introduce a *RELEASE_CONTAINERS* scheduler event : will refactor the existing RELEASE_CONTAINER event to take multiple containers.
* Will expose an aysnc release method in the AbstractYarnScheduler that takes a list of containers, will split the list into some (configured ?) max containers released at a time, and will send an event for each the sub-list.
* Route all calls to release containers from both the scheduler to the new API. Currently, the problematic ones are during app attempt complete, node removed and the schedulers's handling of AM's explicit release containers., Assigning to [~manirajv06@gmail.com], since he's kindly agreed to take this up.., Thanks [~asuresh]

Attached .001 patch for early review. It has changes as described in https://issues.apache.org/jira/browse/YARN-7086?focusedCommentId=16140295&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16140295.

[~jlowe]

{quote}I think it would be a lot better if there was a bulk-release interface so we could grab the critical lock once.{quote}

I assume you are referring the lock inside LeafQueue#completedContainer(). If answer is yes, one approach would be doing changes in Scheduler#completedContainer(), Scheduler#completedContainerInternal() and LeafQueue#completedContainer() to accept list of containers and process accordingly as opposed to accepting single container. Currently, All these methods accepts single RMContainer and do the operation with respect to that. With this new approach, We will need to see how we can able to accept list and traverse accordingly. Can you please confirm this?, bq. I assume you are referring the lock inside LeafQueue#completedContainer().

I was referring to the scheduler back in the 2.7/2.8 code which has changed considerably in trunk from that.  Back in 2.7 releasing a container required the highly-contended CapacityScheduler lock to be obtained, separately, for every container released.  When releasing a lot of containers in a single AM heartbeat, this caused a long backup as the highly-contended lock needed to be reacquired for every released container.  It would have been far more efficient to just grab the lock once and release all the containers with the lock held the entire time.

The big CapacityScheduler lock appears to be gone in trunk, so I would expect the next level of locking bottleneck to be the LeafQueue lock., [~jlowe] Thanks for sharing the background details.

Attached .002 patch to contain the changes required to acquire LeafQueue lock only once to release set of containers. Introduced wrapper methods on top of existing methods to re use the functionality wherever possible. On the flip side, ended up in traversing same set of containers for processing few more times. Please review and share your comments. If approach is fine, can drill down more to see for any further improvements.]