[The more stack trace is here:  this is reproducible.

---
2015-03-26 20:04:43,690 FATAL org.apache.hadoop.conf.Configuration: error parsing conf mapred-site.xml
org.xml.sax.SAXParseException; systemId: file:/etc/hadoop/conf/mapred-site.xml; lineNumber: 316; columnNumber: 3; The element type "property" must be terminated by the matching end-tag "</property>".
        at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:257)
        at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)
        at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2183)
        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2171)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2242)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2195)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2112)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:858)
        at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:877)
        at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1278)
        at org.apache.hadoop.io.compress.zlib.ZlibFactory.isNativeZlibLoaded(ZlibFactory.java:65)
        at org.apache.hadoop.io.compress.zlib.ZlibFactory.getZlibCompressorType(ZlibFactory.java:82)
        at org.apache.hadoop.io.compress.DefaultCodec.getCompressorType(DefaultCodec.java:74)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)
        at org.apache.hadoop.io.file.tfile.Compression$Algorithm.getCompressor(Compression.java:274)
        at org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:129)
        at org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:430)
        at org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642)
        at org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)
        at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.writeVersion(AggregatedLogFormat.java:276)
        at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:272)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainer(AppLogAggregatorImpl.java:108)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:166)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:140)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$2.run(LogAggregationService.java:354)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
2015-03-26 20:04:43,691 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl: Aggregation did not complete for application application_1426202183036_103251
2015-03-26 20:04:43,691 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[LogAggregationService #2,5,main] threw an Throwable, but we are shutting down, so ignoring this
java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/etc/hadoop/conf/mapred-site.xml; lineNumber: 316; columnNumber: 3; The element type "property" must be terminated by the matching end-tag "</property>".
--, Hi [~mnikhil],  which version are you testing with ?, Hi [~Naganarasimha] this is with apache hadoop 2.5.1, A possible solution is to cache configuration, avoid reading new properties and continue to use old ones in cache if fails to load resource. If cache is empty when a service failed to load resource, then shutdown the JVM. The problem is, does it worth the effort ? , I think the real issue here is the nodemanager is not supposed to be constantly re-reading the configuration files.  Normally an explicit refresh admin command or restart of the daemon is expected to be performed before daemons recognize updated configs.  The problem seems to stem from the fact that BCFile.WBlockState is creating a compressor without providing the configuration.  When the codec pool gets invoked without a conf it ends up creating its own which causes the confs to be read from disk each time., Has something changed in 2.6 w.r.t this jira?  I notice that I am using 2.6.4 and this issue is not happening, xml-syntax wrong config files do not cause nodemanager to go down. Can someone please confirm this ?]