[This situation can happen under the following conditions:

- All of the resources in the cluster are being used
- {{QueueA}} is preemptable and over its absolute capacity (AKA guaranteed capacity)
- {{QueueB}} is not preemptable, over its absolute capacity, also over its absolute max capacity (which can happen), and asking for more resources

In the above scenario, {{ProportionalCapacityPreemptionPolicy}} will subtract {{QueuB}}'s ideal assigned value from its absolute max capacity value and get a negative number, which will adjust it's ideally assigned resources downwards by that amount, which will result in that amount getting preempted.

Regardless of the reason, if a queue is marked as unpreemptable, resources should never be preempted from that queue.
, bq. also over its absolute max capacity (which can happen)
This shouldn't happen?, [~vinodkv], thank you for your comment.
{quote}
bq. also over its absolute max capacity (which can happen)
This shouldn't happen?
{quote}
No, I don't think going over the absolute max capacity should happen, in one case, we saw a small queue in a very busy and large cluster go over by 5%, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12701454/YARN-3275.v1.txt
  against trunk revision 8719cdd.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 5 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/6787//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/6787//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6787//console

This message is automatically generated., Actually, go over max capacity is possible, when a cluster with resource = 1000G, and a queue reaches its max capacity, after the cluster resource goes down to 100G, it can over max capacity. , In addition, parent queue can go beyond max capacity as described in YARN-3243 no matter if cluster resource changed or not. But child queue can only go beyond max capacity when cluster resource reduced.

Eric, could you verify is this any of cases I mentioned above?
bq. No, I don't think going over the absolute max capacity should happen, in one case, we saw a small queue in a very busy and large cluster go over by 5%

I feel maybe we need preempt container in your described case https://issues.apache.org/jira/browse/YARN-3275?focusedCommentId=14340393&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14340393, capacity and disable-preemption are just two options, we haven't defined "disable-preemption" is more important than "max-capacity". IMO, if we should do this JIRA or not is still discussable.

Thanks,
Wangda, Thanks very much, [~leftnoteasy], for reviewing this issue.
{quote}
Actually, go over max capacity is possible, when a cluster with resource = 1000G, and a queue reaches its max capacity, after the cluster resource goes down to 100G, it can over max capacity.
n addition, parent queue can go beyond max capacity as described in YARN-3243 no matter if cluster resource changed or not. But child queue can only go beyond max capacity when cluster resource reduced.
{quote}
It is possible that the total available capacity of the cluster dropped by some percentage, causing the leaf node to go over its abs max cap by 5%. The cluster has a large number of nodes and memory, and that value is always changing slightly as nodes are lost and re-register. This may not account for the 5% overage we saw on the small leaf queue, because that total memory number isn't varying by 5%.
{quote}
we haven't defined "disable-preemption" is more important than "max-capacity". IMO, if we should do this JIRA or not is still discussable.
{quote}
I see your point. In other words, it could be argued that the preemption monitor is doing the right thing. That is, when it sees that the queue is over its absolute max capacity (which should not happen), the preemption monitor is moving those resources back into the usable pool.

However, the expectation of our users is that if they are running a job on a non-preemptable queue, their containers should never be preempted. From their point of view, it doesn't matter what the reason is, they are expecting the RM to obey the contract that says it will not preempt their resources.
, Thanks for the patch, Eric!

bq. the expectation of our users is that if they are running a job on a non-preemptable queue, their containers should never be preempted.

I completely agree with this.  IMHO the whole point of the preemption disable feature is to guarantee a queue marked as such will never be preempted.  It's as if the entire preemption feature was turned off from that queue's perspective.

Looking at the patch, I'm a bit worried about this part:

{code}
+      Resource absMaxCapIdealAssignedDelta = Resource.newInstance(0, 0);
+      if (Resources.greaterThanOrEqual(
+                        rc, clusterResource, maxCapacity, idealAssigned)) {
+        absMaxCapIdealAssignedDelta = Resources.subtract(maxCapacity, idealAssigned);
+      }
{code}

If the intent of this calculation is to guarantee none of the components of absMaxCapIdealAssignedDelta are negative then I don't believe this accomplishes that goal.  I believe it's possible for Resources.greaterThanOrEqual to return true yet subtracting the values will result in one of the components to be negative.  For example, what if both resources are memory dominant, maxCapacity has more memory than idealAssigned, but the opposite is true for vcores?  Subtracting idealAssigned from maxCapacity will result in a positive memory component but a negative vcore component.  If we need to make sure neither component goes negative then I think we need to do a component-wise max with the zero resource rather than a comparision.

Also one style nit: we normally don't do one-liner conditionals without braces, so I'd like to see the continue explicitly put in a block.  It might be useful to put a debug log statement with the continue to note that we wanted to preempt this queue for some reason (and by how much) but it was marked with preemption disabled., [~jlowe] and [~leftnoteasy], thank you for the reviews.

Attached is an updated patch (v2) with your suggested changes., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12702623/YARN-3275.v2.txt
  against trunk revision ed70fa1.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 7 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/6850//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/6850//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/6850//console

This message is automatically generated., +1 lgtm.  Findbug warnings appear to be unrelated.  Will commit this tomorrow if there are no further comments or objections., Thanks to Eric for the contribution and to Wangda for additional review!  I committed this to trunk and branch-2., FAILURE: Integrated in Hadoop-trunk-Commit #7276 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7276/])
YARN-3275. CapacityScheduler: Preemption happening on non-preemptable queues. Contributed by Eric Payne (jlowe: rev 27e8ea820fab8dce59f4db9814e73bd60c1d4ef1)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #125 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/125/])
YARN-3275. CapacityScheduler: Preemption happening on non-preemptable queues. Contributed by Eric Payne (jlowe: rev 27e8ea820fab8dce59f4db9814e73bd60c1d4ef1)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #859 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/859/])
YARN-3275. CapacityScheduler: Preemption happening on non-preemptable queues. Contributed by Eric Payne (jlowe: rev 27e8ea820fab8dce59f4db9814e73bd60c1d4ef1)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2057 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2057/])
YARN-3275. CapacityScheduler: Preemption happening on non-preemptable queues. Contributed by Eric Payne (jlowe: rev 27e8ea820fab8dce59f4db9814e73bd60c1d4ef1)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk-Java8 #116 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/116/])
YARN-3275. CapacityScheduler: Preemption happening on non-preemptable queues. Contributed by Eric Payne (jlowe: rev 27e8ea820fab8dce59f4db9814e73bd60c1d4ef1)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #125 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/125/])
YARN-3275. CapacityScheduler: Preemption happening on non-preemptable queues. Contributed by Eric Payne (jlowe: rev 27e8ea820fab8dce59f4db9814e73bd60c1d4ef1)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #2075 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2075/])
YARN-3275. CapacityScheduler: Preemption happening on non-preemptable queues. Contributed by Eric Payne (jlowe: rev 27e8ea820fab8dce59f4db9814e73bd60c1d4ef1)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/resource/Resources.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java
]