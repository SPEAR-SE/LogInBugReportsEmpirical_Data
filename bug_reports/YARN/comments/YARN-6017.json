[Can you share the details of 
{noformat}
jmap -heap <pid>
{noformat}

Can you also get the heapdump if the proc is still alive?

{noformat}
jmap -dump:format=b,file=/tmp/nm.hprof <PID>
{noformat}

, $ jmap -heap 31169
Attaching to process ID 31169, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 24.65-b04

using parallel threads in the new generation.
using thread-local object allocation.
Concurrent Mark-Sweep GC

Heap Configuration:
   MinHeapFreeRatio = 40
   MaxHeapFreeRatio = 70
   MaxHeapSize      = 2147483648 (2048.0MB)
   NewSize          = 805306368 (768.0MB)
   MaxNewSize       = 805306368 (768.0MB)
   OldSize          = 5439488 (5.1875MB)
   NewRatio         = 2
   SurvivorRatio    = 8
   PermSize         = 134217728 (128.0MB)
   MaxPermSize      = 268435456 (256.0MB)
   G1HeapRegionSize = 0 (0.0MB)

Heap Usage:
New Generation (Eden + 1 Survivor Space):
   capacity = 724828160 (691.25MB)
   used     = 220609280 (210.389404296875MB)
   free     = 504218880 (480.860595703125MB)
   30.43608018761302% used
Eden Space:
   capacity = 644349952 (614.5MB)
   used     = 197290320 (188.1507110595703MB)
   free     = 447059632 (426.3492889404297MB)
   30.61850464761112% used
From Space:
   capacity = 80478208 (76.75MB)
   used     = 23318960 (22.238693237304688MB)
   free     = 57159248 (54.51130676269531MB)
   28.97549607466409% used
To Space:
   capacity = 80478208 (76.75MB)
   used     = 0 (0.0MB)
   free     = 80478208 (76.75MB)
   0.0% used
concurrent mark-sweep generation:
   capacity = 1342177280 (1280.0MB)
   used     = 596879392 (569.2285461425781MB)
   free     = 745297888 (710.7714538574219MB)
   44.470980167388916% used
Perm Generation:
   capacity = 134217728 (128.0MB)
   used     = 48817648 (46.55613708496094MB)
   free     = 85400080 (81.44386291503906MB)
   36.37198209762573% used

15971 interned Strings occupying 1520304 bytes., Is this NodeManager running with work preserving restart i.e NM is configured with *yarn.nodemanager.recovery.enabled* set to true? , Usage seems to be from native side. Can you also post "/proc/31169/smaps" ?, we hadn't set "yarn.nodemanager.recovery.enabled" in our config files., smaps of process 31169, thank you for reply, i already post,please check the attach file., "10193716 in [heap]" in smaps indicates that it is from native side., you are right.  but native memory should be limited by MaxDirectSize which the default value is equal to Xmx( in our case should be 2g),but actually it reached to 10g,so i confused by this and NM had used DirectByteBuffer to allocate native memory and forgot to release them?, No. from JVM's accounting perspective it is still at 2048 as per the logs. But need to check if it is anything to do with JVM's internal code itself or netty.  Have you tried with other JDK versions?, we haven't try another jvm version as currently. but we had set the jvm initial parameter -XX:MaxDirectMemorySize to 1G.
but up to now, it seems the problem still exist.

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
13465 data      20   0 3249m 2.1g  17m S 17.0  2.3 877:16.22 java, we need relative long time to obsever this problem., As my suggestion in YARN-6062 also, can we use MAT or similar tools to analysis the heap dump to see what's classes in memory get leaked? If it is due to metrics (most likely), we already have a fix in HADOOP-13362., To clear my understanding, as per the smaps and jmap output, it looks like java heap *capacity = 724828160 (691.25MB)* is well within limits of 2GB
The [heap] shown in smaps output is not same as java heap, right.? i just ran a shell program and its smaps detail also contained detail about [heap]. The java heap is part of OS memory heap. So i feel this is the OS heap size *10193716*  what we are talking about. Correct.?, The [heap] shown in smaps output is not same as java heap, right.?   

====

yes

The java heap is part of OS memory heap. So i feel this is the OS heap size 10193716 what we are talking about. Correct.?

yes, I think this problem is not the same with HADOOP-13362.  because this node manager's gc is normal., [~chenrongwei]

This could be because of JDK bug. Found the following link [JDK-8054841|http://bugs.java.com/bugdatabase/view_bug.do?bug_id=8054841] during reading.
As per the defect description {{ProcessBuilder leaks native memory}} . {{Shell.java}} do use {{ProcessBuilder}} for all child process launch
{quote}
For the Oracle JDK, it does not appear to be present in 7u45, but does appear to be present in 7u55.
{quote}
Looks like the issue is available in 7u65 . If possible could you try changing JRE version. 

[GITHUB|https://github.com/cprice404/jvm-processbuilder-leak/blob/master/README.md]
{quote}
For Oracle JDK, the leak does not appear to be present in 7u45, but does appear to be present in 7u55. (I believe, though I have less data, that the leak is not present in Oracle 7u51. The leak definitely appears to be present in Oracle 7u65 and 7u67 as well.) For OpenJDK, the leak does not appear to be present in 7u55, but does appear to be present in 7u65.
{quote}, [~chenrongwei]
Any update on issue after trying with new JRE version, Thanks for Bibin A Chundatt 's answer. we used jdk 1.7u80 to run node manager, and fixed this problem. So this bug is caused by jdk indeed., Closing issue as not a problem since its jdk issue. 
Please feel free to reopen if it happens again. Thanks!, We are seeing a similar issue  with yarn node managers. We have enabled  yarn.nodemanager.recovery.enabled and stored in leveldb
 
java version: java8u25
hadoop version: 2.7.1
node manager xmx: 4096m
 
  PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+  COMMAND                                                                                                                
15599 yarn      20   0 13.2g  10g 4176 S     3 20.3  31961:10 java  

, This doesn't seem like a java issue  as we had updated java8u25 to java8u90.
[~bibinchundatt] would be great if you could help.]