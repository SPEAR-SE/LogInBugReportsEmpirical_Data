[@Krishna, could you provide more information:
   - What scheduler are you using? 
   - Could you attach the application logs as well as the RM's logs. , Hi Hitesh,

 Thanks for the reply. I have actually changed my code currently to send
new requests for the remaining of required containers, so I am not seeing
this error. I shall revert my changes and try to reproduce the error and
send you the logs in 1 or 2 days.

Thanks,
Kishore



, Hi Hitesh,

  I am attaching the logs for AM, RM, and NM. I have an application being
run in a loop, which requires 5 containers. The 8th run has failed with
this issue of getAllocatedContainers(). The Application Master couldn't get
all the 5 containers it required, the getAllocatedContainers() method
returned only 4. The RM's log is saying that the 5th container is also
allocated thro' the message,

2013-04-16 03:32:54,701 INFO  [ResourceManager Event Processor]
rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(220)) -
container_1366096597608_0008_01_000006 Container Transitioned from NEW to
ALLOCATED

In RM's log, you can see that this kind of for the remaining 4 containers
also, i.e. container_1366096597608_0008_01_000002 to
container_1366096597608_0008_01_000005.

Also, as I said before this issue is seen randomly.

Thanks,
Kishore




, Hi Hitesh,

  I am very curious to know if you could reproduce and resolve this issue.

Thanks,
Kishore


On Tue, Apr 16, 2013 at 1:35 PM, Krishna Kishore Bonagiri <

, [~write2kishore] I just took a look at nm logs and I can see that "container_1366096597608_0001_01_000006" container was allocated by RM and AM made a start container request for it on NM. I think there is some problem in the AM logs. Can you take a look at your AM code again? Looks like something is getting missed there.. If it is still occurring then can you print the logs when AM makes a start container request to NM?? probably something is getting missed there..

{code}
2013-04-16 03:29:57,681 INFO  [IPC Server handler 4 on 34660] containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainer(402)) - Start request for container_1366096597608_0001_01_000006 by user dsadm
2013-04-16 03:29:57,684 INFO  [IPC Server handler 4 on 34660] nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=dsadm	IP=127.0.1.1	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1366096597608_0001	CONTAINERID=container_1366096597608_0001_01_000006
2013-04-16 03:29:57,687 INFO  [AsyncDispatcher event handler] application.Application (ApplicationImpl.java:transition(255)) - Adding container_1366096597608_0001_01_000006 to application application_1366096597608_0001
2013-04-16 03:29:57,689 INFO  [AsyncDispatcher event handler] container.Container (ContainerImpl.java:handle(835)) - Container container_1366096597608_0001_01_000006 transitioned from NEW to LOCALIZED
2013-04-16 03:29:57,952 INFO  [AsyncDispatcher event handler] container.Container (ContainerImpl.java:handle(835)) - Container container_1366096597608_0001_01_000006 transitioned from LOCALIZED to RUNNING
2013-04-16 03:29:58,475 INFO  [Node Status Updater] nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:getNodeStatus(249)) - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1366096597608, }, attemptId: 1, }, id: 1, }, state: C_RUNNING, diagnostics: "", exit_status: -1000, 
2013-04-16 03:29:58,478 INFO  [Node Status Updater] nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:getNodeStatus(249)) - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1366096597608, }, attemptId: 1, }, id: 5, }, state: C_RUNNING, diagnostics: "", exit_status: -1000, 
2013-04-16 03:29:58,481 INFO  [Node Status Updater] nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:getNodeStatus(249)) - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1366096597608, }, attemptId: 1, }, id: 6, }, state: C_RUNNING, diagnostics: "", exit_status: -1000, 
2013-04-16 03:29:58,489 INFO  [ContainersLauncher #2] nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:launchContainer(113)) - launchContainer: [bash, /tmp/nm-local-dir/usercache/dsadm/appcache/application_1366096597608_0001/container_1366096597608_0001_01_000006/default_container_executor.sh]
2013-04-16 03:29:58,638 INFO  [ContainersLauncher #1] launcher.ContainerLaunch (ContainerLaunch.java:call(282)) - Container container_1366096597608_0001_01_000005 succeeded 
2013-04-16 03:29:58,639 INFO  [ContainersLauncher #2] launcher.ContainerLaunch (ContainerLaunch.java:call(282)) - Container container_1366096597608_0001_01_000006 succeeded 
2013-04-16 03:29:58,643 INFO  [AsyncDispatcher event handler] container.Container (ContainerImpl.java:handle(835)) - Container container_1366096597608_0001_01_000005 transitioned from RUNNING to EXITED_WITH_SUCCESS
2013-04-16 03:29:58,644 INFO  [AsyncDispatcher event handler] container.Container (ContainerImpl.java:handle(835)) - Container container_1366096597608_0001_01_000006 transitioned from RUNNING to EXITED_WITH_SUCCESS
2013-04-16 03:29:58,644 INFO  [AsyncDispatcher event handler] launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(300)) - Cleaning up container container_1366096597608_0001_01_000005
2013-04-16 03:29:58,693 INFO  [AsyncDispatcher event handler] launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(300)) - Cleaning up container container_1366096597608_0001_01_000006
{code}, I am closing this as invalid... please reopen if you still see the issue is there..., [~ojoshi] [~write2kishore] I think [~bikassaha] discovered a race condition in the AMRMClient that may be causing this., I shall try to get you the logs you needed today or as soon as possible and
reopen it.


On Fri, Jul 12, 2013 at 5:49 AM, Omkar Vinit Joshi (JIRA)

, [~write2kishore] if you plan to re-run this to get new logs, could you please run the RM and NM with DEBUG log level. Thanks., Likewise have the AM also run with the debug log level if possible. , Hitesh,
  How can I do  that?



, export HADOOP_ROOT_LOGGER="DEBUG,RFA"
export YARN_ROOT_LOGGER="DEBUG,RFA"
when starting the RM and NM. 

For the DSShell, you can use --log_properties and pass in a log4j.properties which has a hardcoded DEBUG level for the root logger. However, based on what I can see, the DS Shell AM at DEBUG level may not be necessary.
, [~write2kishore] your issue seems to be valid... I missed the fact that you are trying to launch applications one after the other... for "container_1366096597608_0008_01_000006" it definitely failed... I realized after [~hitesh] pointed it out .. can you please post debug logs as per [~hitesh] comment?..sorry for missing it, Attaching patch that fixes the race condition. No test since I cannot think of a way to repro this., +1. Looks fine. We should ideally look at creating a ResourceRequest#clone function that could do a more optimal copy/clone of the object., Thanks! YARN-921 created to track clone methods., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12592039/YARN-541.1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/1470//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/1470//console

This message is automatically generated., SUCCESS: Integrated in Hadoop-trunk-Commit #4079 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4079/])
YARN-541. getAllocatedContainers() is not returning all the allocated containers (bikas) (bikas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1502906)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java
, Committed to trunk, branch-2 and branch-2.1-beta, FAILURE: Integrated in Hadoop-Yarn-trunk #270 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/270/])
YARN-541. getAllocatedContainers() is not returning all the allocated containers (bikas) (bikas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1502906)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1460 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1460/])
YARN-541. getAllocatedContainers() is not returning all the allocated containers (bikas) (bikas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1502906)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1487 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1487/])
YARN-541. getAllocatedContainers() is not returning all the allocated containers (bikas) (bikas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1502906)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java
, Hi Hudson & Hitesh,

  Does this mean this issue is fixed? Do you guys expect reproduce the
issue still and give you logs? I am sorry for the delay, I have been busy
with other things at work.

Thanks,
Kishore



, [~write2kishore] Logs, if you get a chance, would help to confirm if the issue that you saw was the same as the one that was reproduced by Bikas when providing the fix. If you can take the patch attached to the jira and try it out to see if it addresses the issue in your env, that would be great too. 



 ]