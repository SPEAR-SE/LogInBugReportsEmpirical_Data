[are you running MapReduce? MapReduce doesn't support AM restart with running containers., yes, i'm running mapreduce with the am restart.  So AM restart is supported only for other applications except MR? Whether it's feasible to support AM restart in MR?, The MapReduce AM supports recovery of completed tasks upon restart, but it does not support reacquiring active containers.  Doing so would require the tasks to determine the new address of the subsequent AM attempt so the umbilical connection can be re-established., yeah, i'm trying to find a way to change the umbilical connection in the fly. But before i tackle the real problem, this error comes out, which seems the NMToken are missed for the new appattempt. When i disable the function of keepContainers, there is no problem for the am to restart.  maybe there still exist some problem for previous solution? Is there anybody tried that before in a real cluster?, I have found the cause for this error: the new launched appattempt will transfer the old containers from previous attempts, so the Nodeset in NMTokenSecretManagerInRM.java will be filled. When the new appattempt to get the allocated containers via  pullNewlyAllocatedContainersAndNMTokens(), it will get "null" nmToken because of the full Nodeset in createAndGetNMToken(). The Null nmToken will be returned to the ContainerLauncher, so the new container will fail in the launch. What i have done is clear the nodeset in  pullNewlyAllocatedContainersAndNMTokens() before the creation of container and node tokens. 

   public synchronized ContainersAndNMTokensAllocation
  438       pullNewlyAllocatedContainersAndNMTokens() {                                                          
  439     List<Container> returnContainerList =
  440         new ArrayList<Container>(newlyAllocatedContainers.size());
  441     List<NMToken> nmTokens = new ArrayList<NMToken>();
+ 442     // clear the nodeset for NMTokens
+ 443     rmContext.getNMTokenSecretManager().clearNodeSetForAttempt(getApplicationAttemptId());
  444     for (Iterator<RMContainer> i = newlyAllocatedContainers.iterator(); i
  445       .hasNext();) {
  446       RMContainer rmContainer = i.next();
  447       Container container = rmContainer.getContainer();
  448       try {
  449         // create container token and NMToken altogether.
  450         container.setContainerToken(rmContext.getContainerTokenSecretManager()
  451           .createContainerToken(container.getId(), container.getNodeId(),
  452             getUser(), container.getResource(), container.getPriority(),
  453             rmContainer.getCreationTime(), this.logAggregationContext));
  454         NMToken nmToken =
  455             rmContext.getNMTokenSecretManager().createAndGetNMToken(getUser(),
  456               getApplicationAttemptId(), container);
+ 457         //check whether nmtoken is null
+ 458         LOG.info("[hchen]NMToken for container "+container.getId()+" NMToken:"+nmToken);
  459         if (nmToken != null) {
  460           nmTokens.add(nmToken);
  461         }
  462       } catch (IllegalArgumentException e) {
  463         // DNS might be down, skip returning this container.
  464         LOG.error("Error trying to assign container token and NM token to" +
  465             " an allocated container " + container.getId(), e);
  466         continue;
  467       }
  468       returnContainerList.add(container);
  469       i.remove();
  470       rmContainer.handle(new RMContainerEvent(rmContainer.getContainerId(),
  471         RMContainerEventType.ACQUIRED));
  472     }
  473     return new ContainersAndNMTokensAllocation(returnContainerList, nmTokens);
  474   }, [~xtchenhui],  thanks for you investigation and fix! 

I get a similar issue when I kill AM process by {noformat}kill -9 process_id{noformat} and RM recovers it. Not sure that I'm dealing with the same problem (root cause), but your fix helps me too. However, I would like to clarify one important thing. According to the *"Apache Hadoop YARN: Moving beyond MapReduce and Batch Processing with Apache Hadoop 2"*: 

{quote}
As for network optimization, NMTokens are not sent to the ApplicationMasters for each and every allocated container, but only for the first time or if NMTokens have to be invalidated due to the rollover of the underlying master key
{quote}

If you clear node set in _"pullNewlyAllocatedContainersAndNMTokens"_ then RM generates new tokens for every allocated container. As for me, the fix may cause a regression for network optimization. What do you think about it? 

I'm going to investigate the issue too. I will update the Jira if I find something interesting.]