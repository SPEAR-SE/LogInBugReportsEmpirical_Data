[Submit application handled as below. Max application is handled based on capacity of queue capacity of default partition.
{{LeafQueue#submitApplication}}
{code}
      // Check submission limits for queues
      if (getNumApplications() >= getMaxApplications()) {
        String msg = "Queue " + getQueuePath() + 
        " already has " + getNumApplications() + " applications," +
        " cannot accept submission of application: " + applicationId;
        LOG.info(msg);
        throw new AccessControlException(msg);
      }
{code}
 
{{LeafQueue#setupQueueConfigs}} max application is set based in default partition absolute capacity if max application per queue is not set.
{code}
    maxApplications = conf.getMaximumApplicationsPerQueue(getQueuePath());
    if (maxApplications < 0) {
      int maxSystemApps = conf.getMaximumSystemApplications();
      maxApplications =
          (int) (maxSystemApps * queueCapacities.getAbsoluteCapacity());
    }
{code}

We should consider max of absolute capacity of all partition in this case.

Any thoughts??, Thanks [~bibinchundatt] for reporting this. 

We need config for maximum application per queue per label if we need to solve the problem cleanly. For long term,  this may be better. With this,  we might also need to rebook on metrics, UI etc too. 
Otherwise we need to introduce few hacks when default cap is not configured. 
I prefer first option. Thoughts.?, Thank you [~sunilg] for looking into issue. Had an offline discussion with [~Naganarasimha Garla] also. 

Its always better to handle the limits of application on overall partition 
# Application submitted can ask AM  resource from one partition and other resource from another partition. So limit should be on queue level
# User/tenant level limit for application should be based on queue.
# The configuration (maxclusterapplication) is cluster limit of application and when we partition to lower level its should be based on queue

*Approach*
Consider average of absolute percentage of all partition,but not average of absolute percentage per partition b ,Label 1 can be of 10% of 20 GB and default partition can be of 50% of 100GB.

# Get percentage capacity of queue as [ sum of resource of queue A all partition (*X*) / Total cluster resource n cluster (*Y*) ]= absolute percentage overall cluster (*Z*).
# max application of queue = *Z* * *maxclusterapplication*
# Have to update the max application always with NODE registration and removal.


, Thanks [~bibinchundatt] for looking into the issue, 
bq. Get percentage capacity of queue as [ sum of resource of queue A all partition (X) / Total cluster resource n cluster (Y) ]= absolute percentage overall cluster (Z).
+1 for the above approach but just ensure that calculations are optimized and doesnt happen too frequent..., Hi [~bibinchundatt], we have been tried to use NodeLabels a while ago, and have gone through the major JIRAs related. Thought this issue had already been solved by YARN-3216. Would you please elaborate more on what's the difference here and why it hasn't been covered by YARN-3216? Thanks very much., Hi [~Ying Zhang],
Difference between  this jira and YARN-3216 is one is related to limiting number of applications and other is related to limiting quantity of cluster resources for AM. Later is more obvious case which gets easily reproduced !, I am still not very much sure about the issue with per-label-per-queue max application.

With the current approach, I can see a small problem. Since queues can have heterogeneous apps in terms of its resource consumption, I will try to show a scenario. 
queueA can run 1000apps taking 1GB of memory. queueB can run may be 10apps which take 100GB.  If queueA and queueB are given with 50% capacity, still more apps can run in queueA. So if we directly go with clusterwise apps and share it with per label, could it affect some queue's which has more apps to run. This is not a very likely case, still I would like to point out the same., Assume max_apps = 1000, labelX=100GB, labelY=50GB, default=20GB

||queueA||queueB||
|labelX = 30% |labelX = 70%|
|labelY = 100% |labelY = 0|
|default = 50% |default = 50%|

For queueA,  is *Z* is like {{(0.3 * 100GB + 1 * 50Gb + 0.5 * 20GB)/(100GB+50Gb+20GB) = 0.53}}? Pls let me know whether I understood the calculation. If we use this, i think we are considering resource of different label in queue to one single %, weightage is not considered. I think, with above table, if we have max_apps_labelX=100, and similarly, will it make it simple? But i agree that we need to consider user level calculations also under label if we do this. , Thanks for your feeback [~sunilg], Idea of considering overall resource is we do not want to limit number of applications based on partition as AM can be submitted in a partition with higher resource and request for partions resources which are still limited, so we thought limiting overall number of apps per leaf queue is better.
bq. I think, with above table, if we have max_apps_labelX=100, and similarly, will it make it simple? But i agree that we need to consider user level calculations also under label if we do this.
Yes it makes it simple but IMO doesnt solve anything as explained earlier.
bq. still more apps can run in queueA. So if we directly go with clusterwise apps and share it with per label, could it affect some queue's which has more apps to run. 
Anyway max number of apps per queue can also be defined {{yarn.scheduler.capacity.<queue-path>.maximum-applications}} which can still behave in the same way i.e. max number of apps per queue = max number of apps per queue across any partition.
, 

Thank you [~sunilg] and [~Naganarasimha Garla] for feedback.
 As already mentioned .The issue is when queue level max apps are not configured.
Then {{int maxSystemApps = conf.getMaximumSystemApplications();}} apps limits is considered and  for each queue absolute percentage wise apps are distributed.

{quote}
The configuration (maxclusterapplication) is cluster limit of application and when we partition to lower level its should be based on queue
{quote}
System level app limit should be considered.

If we implement as per our approach :

From [~sunilg] example queue A will have {{53%*10000=5300}} as app limit and queueB will have {{47%*10000=4700}} as limit  when {{yarn.scheduler.capacity.maximum-applications}} is {{5300+4700=10000}} as cluster limit. And when labels not available then will get same behavior as old.

User level calculation are already based in max application in a queue. User level should be only based on overall queue shouldnot consider label thts my understanding.

{code}
    maxApplicationsPerUser = Math.min(maxApplications,
        (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor));
 {code}, [~leftnoteasy] Could you please share your thoughts., [~sunilg]/ [~Naganarasimha Garla]
# Solution based in resource usage is having issue during startup when none of the node managers are registered.Resource will be zero and application can get rejected.
# Attaching patch based on {{yarn.scheduler.capacity.maximum-applications}} to be run on partition for all queues. For each label we can configure as {{yarn.scheduler.capacity.maximum-applications.accessible-node-labels.<label>}}
# During application limit check will considered applications than run on complete cluster for  leaf queue (of all partitions).
# When property is not configured default value of {{yarn.scheduler.capacity.maximum-applications}} is considered for partition queues

If max-application for queue is configured then {{yarn.scheduler.capacity.maximum-applications}} will not be considered.

Attaching first patch for the same., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 18s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 34s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 34s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 22s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 39s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 4s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 33s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 32s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 32s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 20s {color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 6 new + 123 unchanged - 2 fixed = 129 total (was 125) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 38s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 8s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 19s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 38m 39s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 16s {color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 54m 32s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12826489/YARN-5545.0001.patch |
| JIRA Issue | YARN-5545 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 3628e2179fc0 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 01721dd |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/12973/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/12973/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-YARN-Build/12973/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/12973/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-YARN-Build/12973/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/12973/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

, Attaching patch after handling checkstyle and fixing testcase failure, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 19s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 47s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 33s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 38s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 56s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 21s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 19s {color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 2 new + 216 unchanged - 2 fixed = 218 total (was 218) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 35s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 2s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 18s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 39m 26s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 16s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 54m 5s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.TestRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12827633/YARN-5545.0002.patch |
| JIRA Issue | YARN-5545 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux bbc76c507adb 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 401db4f |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/13050/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/13050/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-YARN-Build/13050/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/13050/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/13050/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

, jira YARN-5548 exists for testcase failure, | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 16s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 8m 0s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 39s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 40s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 57s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 20s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 34s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 34s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 21s {color} | {color:green} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 0 new + 216 unchanged - 2 fixed = 216 total (was 218) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 43s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 16s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 18s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 19s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 40m 45s {color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 16s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 57m 18s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12827648/YARN-5545.0003.patch |
| JIRA Issue | YARN-5545 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 43d78b37e973 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b6d839a |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/13053/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/13053/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

, Thanks [~bibinchundatt], for the patch .
Few points to discuss on the approach
# Would it be good to have a separate queue partition based max application limit similar to {{yarn.scheduler.capacity.<queue-path>.maximum-applications}} so that there is finer control on logical partitions similar to default partition ?
# Would it be better to set the default value of {{yarn.scheduler.capacity.maximum-applications.accessible-node-labels.<label>}} to that of {{yarn.scheduler.capacity.maximum-applications}}, it will make the work of the admin much easier. similarly we can decide the same for the previous point if we plan to adopt it.
# IIUC you seem to adopt the approach little different than what you mention in your [comment|https://issues.apache.org/jira/browse/YARN-5545?focusedCommentId=15453163&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15453163], though we are having per partition level max app limit, we just sum up max limits of all partitions under a queue and check against {{ApplicationLimit.getAllMaxApplication()}}. If we were to not actually validate against per Queue's PartitionLevelMaxApps then why need to come up with a new configuration? Also consider the cases when the accessibility is * and new partitions are added {{without refreshing}}, this configuration will be wrong as its static.
# Need to take care of documentation which i think is missed for *MaximumAMResourcePercentPerPartition* too, May be can be handled in a different jira , [~Naganarasimha Garla] and [~bibinchundatt]

I earlier suggested to have "maximum-applications" per label. And as mentioned by Naga in the last summary, it is one of the option to control apps for labels. 
However it may be an added hurdle for admins to set it correctly per-label. Also I had discussed with [~leftnoteasy] earlier.I think its better if we have  {{maximum-applications per <label> }} in cluster-wise (as mentioned in option2 with slight difference) to that of {{yarn.scheduler.capacity.maximum-applications}}. May be we need not have to expose this as a new config. Rather we can adopt it from {{yarn.scheduler.capacity.maximum-applications}} itself. It could be documented to explain this. Thoughts?, [~Naganarasimha Garla] and [~sunilg]

{quote}
 Also consider the cases when the accessibility is * and new partitions are added without refreshing, this configuration will be wrong as its static.
{quote}
Thank you for pointing out will check the same. But [~Naganarasimha Garla]  when ever we reconfigure capacity scheduler xml this limits also will get refreshed.

{quote}
Would it be better to set the default value of yarn.scheduler.capacity.maximum-applications.accessible-node-labels.<label> to that of yarn.scheduler.capacity.maximum-applications
{quote}
Will use  {{yarn.scheduler.capacity.maximum-applications}} itself.

{quote}
IIUC you seem to adopt the approach little different than what you mention in your comment, though we are having per partition level max app limit, we just sum up max limits of all partitions under a queue and check against ApplicationLimit.getAllMaxApplication()
{quote}
This was added since application per partition we can't consider for app limit IIUC we have to check max apps to queue from all partitions.

Documentation will add for the same.



, [~bibinchundatt], [~sunilg], [~Naganarasimha].

Thanks for discussion,

I think for this issue, what we should do:
- Don't split maximum-application-number to per-partition, as we already have am-resource-percent-per-partition, adding more per-partition configuration will confuse user
- And also, you cannot say one app belongs to one partition, you can only say one AM belongs to one partition
- So queue will split maximum-application-number according to ratio of their total configured resource across partitions. For example, 
{code}
Cluster maximum-application = 100, 
queueA configured partitionX = 10G, partitionY = 20G; 
queueB configured partitionX = 20G, partitionY = 50G;
{code}
So queueA 's maximum-application is 100 * (10 + 20) / (10 + 20 + 20 + 50)  = 30
And queueB's maximum-application is 100 * (20 + 50) / (10 + 20 + 20 + 50)  = 70 
- Please note that, the maximum-applications of queues will be updated when CS configuration updated (refresh queue), and cluster resource updated, so we need to update it inside CSQueue#updateClusterResource .

Thoughts?, Thank you [~leftnoteasy] for looking  into issue.

{quote}
So queue will split maximum-application-number according to ratio of their total configured resource across partitions
{quote}

{noformat}
Approach
Consider average of absolute percentage of all partition,but not average of absolute percentage per partition b ,Label 1 can be of 10% of 20 GB and default partition can be of 50% of 100GB.

    Get percentage capacity of queue as [ sum of resource of queue A all partition (X) / Total cluster resource n cluster (Y) ]= absolute percentage overall cluster (Z).
    max application of queue = Z * maxclusterapplication
    Have to update the max application always with NODE registration and removal.

{noformat}
This was the initial approach we thought about, during discussion we came across scenario when rm is restarted and NM is not registered
might get rejected any thoughts on that.

 Any thoughts on above scenarios how we should handle ??

, [~bibinchundatt], [~wangda],
Bibin's last approach seems to be very complicated for admin to understand the intent of it, IMO Bibin's initial approach which matches the wangda's last comment is probably the best approach and its just that we need to solve for this one scenario, so my thoughts on that scenario :
# Do not allow any apps to be submitted. I feel no harm in it either its momentary situation(in case of fail over) or something seriously wrong which admin needs to take care of.
#  Allow certain number of apps which should also consider already running apps, say 10% of max running apps can be considered.
Thoughts?
, [~bibinchundatt], [~Naganarasimha Garla]

Had an offline discussion with [~leftnoteasy] on this. Summary is as follows

If we are opting for Approach1 (Consider average of absolute percentage of all partition,but not average of absolute percentage per partition), we may have issues like [~bibinchundatt] came across,
 - If there are no nodes in cluster
 - During RM restart

and apps will be rejected. We could come up with work around here, but code will not be that clean in a critical code patch of scheduler.

So one of the suggestion is, 
we could only keep only {{yarn.scheduler.capacity.maximum-applications}} at system level. We could avoid configuring {{maximum-applications}} per queue. Yes, its a behavioral change. Still in current use cases, this configuration per-queue is not very strict configuration. This will be usually configured with a very big value, and {{max-am-resource-percent}} is playing a crucial role in limitting applications (max AM containers) running in a queue.

Current code in LeafQueue:
{code}
    maxApplications = conf.getMaximumApplicationsPerQueue(getQueuePath());
    if (maxApplications < 0) {
      int maxSystemApps = conf.getMaximumSystemApplications();
      maxApplications =
          (int) (maxSystemApps * queueCapacities.getAbsoluteCapacity());
    }
{code}

So this could be changed to {{maxApplications = conf.getMaximumSystemApplications();}}. This value could be configured higher in case where more labels are available in system. Thoughts?

Looping [~jlowe]. Pls share your thoughts., {quote}
we could only keep only yarn.scheduler.capacity.maximum-applications at system level. We could avoid configuring maximum-applications per queue. 
{quote}
We should not remove maximum-applications per queue . Only when max application per queue is not configured the system level application has impact as per current implementation no need to change the same., The problem with changing queues to use the max apps conf directly is that it becomes more difficult for admins to control the overall memory pressure on the RM from pending apps.  IIUC after that change each queue would be able to hold up to the system max-apps number of apps.  So each time an admin adds a queue it piles on another potential max-apps amount of apps the RM could be tracking in total.  Or if the admin increases the max-apps number by X it actually increases the total RM app storage by Q*X, where Q is the number of leaf queues.

That's quite different than what happens today and is a significant behavior change.  If we go down this route then I think we should have a separate top-level config that, when set, specifies the default max-apps per queue explicitly rather than having them try to derive it based on relative capacities.  We can then debate whether that also overrides the system-wide setting or if we still respect the system-wide limit (i.e.: queue may reject an app submission not because it hit the queue's max apps limit but because the RM hit the system-wide apps limit).  Going with a separate, new config means we can preserve backwards compatibility for those who have become accustomed to the existing behavior and no surprises when admins use their old configs on the new software.

I think max-am-resource-percent is a red herring with respect to the max apps discussion.  max-am-resource-percent only controls how many _active_ applications there are in a queue, and max apps is controlling the total number of apps in the queue.  In fact I wouldn't be surprised if the code doesn't check and an admin could configure the RM to allow more active apps than the total number of apps the queue is allowed to have at any time.
, Thank you very much [~jlowe] for pitching in and sharing thoughts. Makes sense to me overall.

bq.if we go down this route then I think we should have a separate top-level config that, when set, specifies the default max-apps per queue explicitly rather than having them try to derive it based on relative capacities. We can then debate whether that also overrides the system-wide setting or if we still respect the system-wide limit.

IIUC, existing {{yarn.scheduler.capacity.maximum-applications}} can be used for system level max-limit for apps, and proposing new config like {{yarn.scheduler.capacity.global.queue-level-maximum-applications}}. This could be configured to set max-apps per queue level  in cluster level (queue won’t override this). So if I set this config as 10k, then any queue in best case could atmost submit 10k apps. And this will also work along with system-wide app limit. Hence if we configure system-wide app limit as 50k, and assuming we have 10queues (10k each limit), we will not end up in having 100k apps in cluster. Rather we will hit system-wide limit of 50k.

As more queues are added to system, admin can decrease global queue max-app-limit for better fine tuning if needed. If we are tending to use global queue max-app-limit as a relaxed boundary, then strict actions (reject an app) can be done based on system-wide limit. But if we are configuring this limit more judiciously, we can think of making same queue max-app-limit also as strict limit to reject apps for a queue.
I see only one problem for this now. If  we are not making {{Q * X ~ Y}} (where Q is number of queues, X is global per-queue limit and Y is system-wide max-app limit) as a strict rule, then we have 2 possibilities {{Q * X > Y}} and {{Q * X < Y}}.  I think mostly admin prefer to use former approach, where system-wide limit will be stricter and a relaxed limit for per-queue limit. But if we use latter, then we may reject apps even though system-wide limit is still not met. This may or may not be fine. I think with more discussion we can come to a common consensus here. , bq. This could be configured to set max-apps per queue level in cluster level (queue won’t override this).

A queue-level max-app setting should always override the system-level setting.  If a user explicitly sets the max-apps setting for a particular queue then we cannot ignore that.  We already have setups today where max-apps is being tuned at the queue-level for some queues.

Today if users set a queue-level max app limit then it overrides any system-level limit.  That means even today users are allowed to configure RMs that can accept over the system-level app limit by explicitly overriding the derived queue limits with specific limits that are larger.  Therefore I'm tempted to have the global queue config completely override the old system-level max-apps config because it's akin to setting the max-apps level for each queue explicitly.  That means we operate in one of two modes: if global queue max-apps is not set then we do what we do today and derive the max-apps based on relative capacities.  Queues that override max-apps at their level continue to behave as they do today and get the override setting.  If the global queue max-apps is set then yarn.scheduler.capacity.maximum-applications is completely ignored.  Queues that override max-apps at their level continue to behave as they do today and get the override setting.  Queues that do not override get the global queue setting as their max apps setting.

This preserves existing behavior if the queue is not set and is likely the least surprising behavior when the new setting is used, especially if we document for both the old system max-apps and global queue max-apps configs that the latter always overrides the former when set.

, Thank you very much [~jlowe] for sharing use case and detailed analysis.

I think i understood now the intend here. We will be sticking with the existing configuration set here, and introducing a much more flexible global queue max-apps. So for those queues which are not configured per-queue level, and do not have any capacity configured (in case of node labels and the the problem mentioned in this jira) will be set to this new config (global queue max-apps).

So I think more or less, we could have below pseudo code to represent this behavior.
{code}
    maxApplications = conf.getMaximumApplicationsPerQueue(getQueuePath());
    if (maxApplications < 0) {
      int maxGlobalPerQueueApps = conf.getGlobalMaximumApplicationsPerQueue();
      if(maxGlobalPerQueueApps > 0) {
         maxApplications = maxGlobalPerQueueApps;
      } else  {
        int maxSystemApps = conf.getMaximumSystemApplications();
        maxApplications =
          (int) (maxSystemApps * queueCapacities.getAbsoluteCapacity());
      }
    }
{code}

So in cases where there are no capacity configured for some labels in a queue, we could make use of global queue max-apps configurations., Yes, that's essentially the idea.  Users can work around the issue initially reported today by setting a queue-specific max apps setting.  All the new global queue max apps setting does is allow users to easily specify a default max apps value for all queues that don't have a specific setting rather than manually set it themselves on each one., Thanks [~jlowe], [~sunilg] for suggestions.

I generally agree with approach at https://issues.apache.org/jira/browse/YARN-5545?focusedCommentId=15494147&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15494147.

Since the maximum-application is major used to cap memory consumed by apps in RM. So I think at least in a follow up JIRA, system-level maximum applications should be enforced. We should not allow pending + running apps number beyond system-level maximum applications. Without this, it gonna be hard to estimate how many apps in RM.

Thoughts?, {quote}
Since the maximum-application is major used to cap memory consumed by apps in RM. So I think at least in a follow up JIRA, system-level maximum applications should be enforced.
{quote}
+1 for the same. similar to cgroups we can add configuration for strict mode to be enable., Thanks [~jlowe] for the valuable thoughts and suggestions.

Thanks [~leftnoteasy]. It makes sense for me. [~bibinchundatt], However I think we do not need another config to enforce strict checking. It can be done in todays form.

I will file a followup jira for same. IN that, we can check and reject app submission to any queue, if system-wide limit is met. Thoughts?, [~sunilg]
{quote}
However I think we do not need another config to enforce strict checking. It can be done in todays form.
{quote}
To keep the old behavior we can keep the value as false by default. 
, Thanks [~sunilg],[~wangda] & [~jlowe], for taking the discussion forward.
I had few queries still
# GlobalMaximumApplicationsPerQueue doesnt have any default set right ? if set then there is no need for {{maxSystemApps * queueCapacities.getAbsoluteCapacity()}} as it will never reach
# IMO approach which was captured by Sunil in his earlier [comment|https://issues.apache.org/jira/browse/YARN-5545?focusedCommentId=15494147&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15494147] is not solving the base problem completely. Problem started with {{maxSystemApps * queueCapacities.getAbsoluteCapacity()}}, which partition's absolute capacity needs to be considered when for a given queue is not overriding max applications and default capacity of the queue is zero. So based on your approach only way to avoid it is to set {{GlobalMaximumApplicationsPerQueue}} so this would imply that for all the queues this value will be taken and earlier approach of {{maxSystemApps * queueCapacities.getAbsoluteCapacity()}} will not be considered. 
# I feel that {{enforce strict checking}} should have been implicit requirement with the assumption that the admin would have not configured in a way that queue max apps exceeds system max apps. And we need not validate the configuration that all queue's max apps is not greater than  system max apps but just validate that while submitting the app first the system level max apps are not getting violated and then queue level max app is not getting violated.
Thoughts ?, Thanks Naga for the points.
Basically we will be giving precedence for any queue specific max-apps config (overriding). This is existing behavior. So we are only looking for cases where this queue specific config for max-apps are not present.

bq.if set then there is no need for maxSystemApps * queueCapacities.getAbsoluteCapacity() as it will never reach
There will not be any defaults for global max-apps config. Hence if its not set specifically by admin, then we will consider existing way of calculating max-apps for a queue from system level max-apps w.r.t capacity of queue.

So the code will be reachable if user is not specifying global max-apps config, thus complying to backward compatibility.

bq.Problem started with maxSystemApps * queueCapacities.getAbsoluteCapacity(), which partition's absolute capacity needs to be considered when for a given queue is not overriding max applications and default capacity of the queue is zero. 

I think its a choice from admin side. For scenarios like default label capacity is not configured, and there are no queue level overriding for max-apps, a possible solution is to configure global max-apps config. Still if any queue is using its own override, that will be considered. This can solve the pblm here.
But One case to point out here is that, is there any use case by which customer is expecting to divide max-apps by capacity (no queue override) and do not want a default global max-apps?. If so, then we can add few more tuning configs to forcefully enable capacity division of max-apps in each queue level over default global max-app. Does this make sense?

bq.I feel that enforce strict checking should have been implicit requirement
As mentioned in earlier comment, we can add strict check for max-apps w.r.t to system-wide max-apps limit. And it can be implicit and we can reject apps if its hit. As pointed out by Bibin, I do not feel we need a config for that. , Thank you [~sunilg]/[~naganarasimha_gr@apache.org]/[~jlowe] for discussion and comments

Attaching patch based on discussion. 
# Added new configuration {{yarn.scheduler.capacity.global-queue-max-application}} property 
# Testcase added for application submit for handling user limit and application limit for queue, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 15s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 38s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 32s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 23s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 37s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 0m 56s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 20s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 35s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 2s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 18s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 36m 26s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 15s {color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 50m 43s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.TestRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12835090/YARN-5545.004.patch |
| JIRA Issue | YARN-5545 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 6de9ebf193ce 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / dbd2057 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/13498/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-YARN-Build/13498/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/13498/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/13498/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

, Testcase failure is not related to patch attached. YARN-5548 is already available to track the same. 
[~varun_saxena] can u have at look at YARN-5548 too., Extremely for the comment. I mistyped in a wrong Jira. Pls discard below comment

.....
Currently we are trying to invoke activateApplications while recovering each application. Yes, as of now nodes are getting registered later in the flow. But for scheduler, we need not have to consider such timing cases from RMAppManager/RM end. Being said that, its important to separate 2 issues out here
......, All other things are fine just for the caveat that you mentioned.
bq. But One case to point out here is that, is there any use case by which customer is expecting to divide max-apps by capacity (no queue override) and do not want a default global max-apps?. If so, then we can add few more tuning configs to forcefully enable capacity division of max-apps in each queue level over default global max-app. Does this make sense?

So based on the consensus from [~jlowe],[~sunilg] & [~wangda], may be we can conclude to currently go ahead with the same approach and if required to add additional configs in future., [~sunilg]
Could you please review current implementation, Hi [~bibinchundatt],
Seems like there is no thoughts from others on this yet, so i think we can go ahead with the existing approach which has a drawback that we will not be able to set global-default-max for only few queues (having default Queue partition cap = 0) but will get enforced to all. If required we can introduce some new config later.
Additionally as we were discussing earlier can you put a check to ensure that total number of applications do not exceed cluster max applications ? , Hi [~bibinchundatt] and [~Naganarasimha Garla]

Current approach in the patch looks fine. I also think that a cluster max check can be added to protect system from over shooting max-applications. I have not looked patch in detail, will do that today., Attaching patch after handling system level limits too., | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 38m 34s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 16s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 53m 48s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Issue | YARN-5545 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12837644/YARN-5545.0005.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 3eeeb13a7305 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d8bab3d |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/13798/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/13798/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks for the patch [~bibinchundatt],
{{createAndPopulateNewRMApp}} is used in the recover flow and will be called for the finished apps too so i think checking for it here would not be the right location. Other than other parts of the patch is fine !
, Thank you [~naganarasimha_gr@apache.org] for comments.IIUC the finished application never gets to scheduler. From NEW state to FINISHED the transition will be complete. But for pending cases might cause a problem.
Adding handling for isRecovery should be enough. If we add the check in current location we have the additional benefit of not creating apps and attempts when not necessary.
{code}
    // Check system level max application limit is reached
    if (!isRecovery && scheduler instanceof CapacityScheduler) {
      if (((CapacityScheduler) scheduler).isSystemAppsLimitReached()) {
        String message =
            "Cluster level application limit reached,rejecting application";
        throw new YarnException(message);
      }
    }
{code}, [~bibinchundatt], 
bq. IIUC the finished application never gets to scheduler. From NEW state to FINISHED the transition will be complete. But for pending cases might cause a problem. ...  If we add the check in current location we have the additional benefit of not creating apps and attempts when not necessary.
I could not get you completely but additionally adding a check {{!isRecovery}} like you mentioned should be sufficient, earlier had not seen this argument & just wanted to say that finished app also goes through this call and then moves to the state *FINISHED* so recover flow would fail.
[~sunilg], any other comments on the latest patch ? if the above mentioned issue is fixed would it be sufficient to go on ?
, [~sunilg] & [~bibinchundatt],
Was wondering whether to type cast would be the right approach or to introduce an api in YarnScheduler to validate whether application can be accepted or event better to do it in CapacityScheduler.addApplication which call leafQueue.submitApplication(which currently does the queue level validation for max apps)  ? As in future there can be similar checks for other schedulers too and not good to have specific scheduler checks in the main RM flow, [~Naganarasimha]/[~sunilg]

We willadd interface in {{YarnScheduler}} to check app can be submitted. So that each scheduler we can implement as per the needs.
Probably the access check in {{RMAppManager}} we can move to the same., Hi [~naganarasimha_gr@apache.org] and [~bibinchundatt]

I also have a similar opinion with [~naganarasimha_gr@apache.org]. We are introducing lot of *instanceof* checks, which is not so clean.

Couple of options I feel
- We could have this check inside {{CS#addApplication}}, and we could raise APP_REJECTED event back of the limit is met.
- As suggested by Naga, We can also try to have a interface in {{YarnScheduler}} and then create a dummy implementation in {{AbstractYarnScheduler}}. Then in CS, we could have checks as mentioned in the patch.

I feel option 1 is slightly simple if we could achieve the same. Thoughts?, [~sunilg]
Will update patch  with solution 2., [~bibinchundatt]
Is there any potential problem or feasibility issue with Option1?, {quote}
Is there any potential problem or feasibility issue with Option1?
{quote}
Both solution are ok. But if we implement second its does avoid app add event to scheduler. Also later any schedulers can implement and use the same.
We have to just finalize whether scheduler and history should know whether application was submitted and rejected due to application limit.
, Thanks [~bibinchundatt]

Option2 will introduce a new api, and such an api could potentially grab some lock in scheduler later (eventhough this patch may not intend to do so), and that would not be very good since its a direct api call from client end. I think such a provision needs to be discussed more and may need more visibility. Also as per this patch, this important change will go here as a sub-part. I think lets move this change to another ticket and we can discuss about the same there. So current patch regarding label can go here and most of us has consensus for same already. Thoughts?

, Thanks for sharing thoughts and discussing on this [~sunilg] & [~bibinchundatt],
Earlier thought there are multiple places we are typecasting scheduler and checking for accepting apps, hence thought adding to interface.
But seems like its not been done multiple places hence i am fine with approach 1 and keep it simple and if we require later on we can create similar to option 2
, Attaching patch based after moving system max application limit to scheduler., Thanks [~bibinchundatt], 
Overall the approach seems to be fine just small nits :
# In {{TestApplicationLimits.testApplicationLimitSubmit}} we have not captured any error messages for *assert*'s
# May be we can statically bind so that we can directly use {{assertEquals}} instead of {{Assert.assertEquals}}, | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 41m 14s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 59m 12s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:e809691 |
| JIRA Issue | YARN-5545 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12837810/YARN-5545.0006.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 8ac4d3e0f03b 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / acd509d |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/13809/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/13809/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Any updates on this?, Attaching patch handling comments from naga, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 20s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 19s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 41m 20s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 16s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 57m 18s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:e809691 |
| JIRA Issue | YARN-5545 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12838291/YARN-5545.0007.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 7b3d10941fc9 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 71adf44 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/13850/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/13850/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/13850/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, [~bibinchundatt]
In {{isSystemAppsLimitReached}}, less than or equal to check is used. Hence I feel one more extra  applications can get submitted. Could you please help to confirm that., Thank you [~sunilg] for review.
Attaching patch after handling both testcase fix and condition check, | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 42m 39s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 16s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 59m  2s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:e809691 |
| JIRA Issue | YARN-5545 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12838310/YARN-5545.0008.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 3e8ae235e1f0 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / c8bc7a8 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/13854/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/13854/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks [~bibinchundatt], +1, Latest patch looks good to me, if no further comments will commit it later today., +1, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #10820 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/10820/])
YARN-5545. Fix issues related to Max App in capacity scheduler. (naganarasimha_gr: rev 503e73e849cbdd1194cc0d16b4969c60929aca11)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacitySchedulerConfiguration.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
, Thanks [~bibinchundatt], Latest patch seems to be failing in compilation in branch-2. Can you please check and provide a patch for branch-2 ?
{code}[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /opt/git/commit/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java:[790,16] local variable mgr is accessed from within inner class; needs to be declared final
[INFO] 1 error
{code}
, It was simple test case fix to make the local variable mgr as final in the testcase, hence have fixed it and committed the patch ! Thanks for the contribution [~bibinchundatt] and additional reviews from [~sunilg] & [~jlowe], Committed the branch to trunk and branch-2]