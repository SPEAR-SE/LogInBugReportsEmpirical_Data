[I looked at AMRMClientImpl, I see that the addContainerRequest(T) method is setting true for dedupedNodes and dedupedRacks in Line 361 and 367., Can you please report if the same is observed when no rack is set. ie. only specific node is requested without any fallback.
Can you please attach RM logs along with container id of the container returned by the RM that did not match expectations. Were , Here is the synopsis of the various combinations

Node_Set	 Rack_Set	Relax locality	
Yes	         No	                FALSE	      I  get back on the node, but then fallback doesn't work
Yes	         No	                TRUE	I don't get back the correct node
Yes	         Yes	                T/F   	I don't get back the correct node


I am attaching the logs when Node is Yes and Rack is False and Relax is true. The containers for which it is not working is container_1384534729839_0001_01_000002 and container_1384534729839_0001_01_000004

2013-11-15 09:00:38,116 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(335)) - showRequests: application=application_1384534729839_0001 headRoom=<memory:9091072, vCores:0> currentConsumption=2048
2013-11-15 09:00:38,116 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 0, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: /default-rack, Relax Locality: true}
2013-11-15 09:00:38,116 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 0, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true}
2013-11-15 09:00:38,116 IPC Server handler 43 on 8031 DEBUG security.UserGroupInformation (UserGroupInformation.java:logPrivilegedAction(1513)) - PrivilegedAction as:hadoop (auth:SIMPLE) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2042)
2013-11-15 09:00:38,116 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 0, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: node10.morado.com, Relax Locality: true}
2013-11-15 09:00:38,117 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(335)) - showRequests: application=application_1384534729839_0001 headRoom=<memory:9091072, vCores:0> currentConsumption=2048
2013-11-15 09:00:38,117 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 1, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true}
2013-11-15 09:00:38,117 AsyncDispatcher event handler DEBUG event.AsyncDispatcher (AsyncDispatcher.java:dispatch(125)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE
2013-11-15 09:00:38,117 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(335)) - showRequests: application=application_1384534729839_0001 headRoom=<memory:9091072, vCores:0> currentConsumption=2048
2013-11-15 09:00:38,117 AsyncDispatcher event handler DEBUG rmnode.RMNodeImpl (RMNodeImpl.java:handle(354)) - Processing node6.morado.com:39327 of type STATUS_UPDATE
2013-11-15 09:00:38,117 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 2, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: /default-rack, Relax Locality: true}
2013-11-15 09:00:38,117 AsyncDispatcher event handler DEBUG event.AsyncDispatcher (AsyncDispatcher.java:dispatch(125)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE
2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 2, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true}
2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 2, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: node18.morado.com, Relax Locality: true}
2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:computeUserLimit(1056)) - User limit computation for gaurav in queue default userLimit=100 userLimitFactor=1.0 required: <memory:8192, vCores:1> consumed: <memory:2048, vCores:1> limit: <memory:9093120, vCores:1> queueCapacity: <memory:9093120, vCores:1> qconsumed: <memory:2048, vCores:1> currentCapacity: <memory:9093120, vCores:1> activeUsers: 1 clusterCapacity: <memory:9093120, vCores:296>
2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:computeUserLimitAndSetHeadroom(989)) - Headroom calculation for user gaurav:  userLimit=<memory:9093120, vCores:1> queueMaxCap=<memory:9093120, vCores:1> consumed=<memory:2048, vCores:1> headroom=<memory:9091072, vCores:0>
2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:assignContainer(1306)) - assignContainers: node=node8.morado.com application=1 priority=0 request={Priority: 0, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true} type=OFF_SWITCH
2013-11-15 09:00:38,119 ResourceManager Event Processor DEBUG security.BaseContainerTokenSecretManager (BaseContainerTokenSecretManager.java:createPassword(90)) - Creating password for container_1384534729839_0001_01_000002 for user container_1384534729839_0001_01_000002 (auth:SIMPLE) to be run on NM node8.morado.com:51530
2013-11-15 09:00:38,119 ResourceManager Event Processor DEBUG security.ContainerTokenIdentifier (ContainerTokenIdentifier.java:write(112)) - Writing ContainerTokenIdentifier to RPC layer: org.apache.hadoop.yarn.security.ContainerTokenIdentifier@77c5b2de
2013-11-15 09:00:38,120 ResourceManager Event Processor DEBUG security.ContainerTokenIdentifier (ContainerTokenIdentifier.java:write(112)) - Writing ContainerTokenIdentifier to RPC layer: org.apache.hadoop.yarn.security.ContainerTokenIdentifier@77c5b2de
2013-11-15 09:00:38,120 ResourceManager Event Processor DEBUG scheduler.AppSchedulingInfo (AppSchedulingInfo.java:allocate(377)) - allocate: applicationId=application_1384534729839_0001 container=container_1384534729839_0001_01_000002 host=node8.morado.com:51530
2013-11-15 09:00:38,120 ResourceManager Event Processor DEBUG scheduler.AppSchedulingInfo (AppSchedulingInfo.java:allocate(265)) - allocate: user: gaurav, memory: <memory:8192, vCores:1>
2013-11-15 09:00:38,120 ResourceManager Event Processor DEBUG rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(208)) - Processing container_1384534729839_0001_01_000002 of type START
2013-11-15 09:00:38,120 ResourceManager Event Processor INFO  rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(220)) - container_1384534729839_0001_01_000002 Container Transitioned from NEW to ALLOCATED
2013-11-15 09:00:38,146 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:computeUserLimit(1056)) - User limit computation for gaurav in queue default userLimit=100 userLimitFactor=1.0 required: <memory:8192, vCores:1> consumed: <memory:18432, vCores:3> limit: <memory:9093120, vCores:1> queueCapacity: <memory:9093120, vCores:1> qconsumed: <memory:18432, vCores:3> currentCapacity: <memory:9093120, vCores:1> activeUsers: 1 clusterCapacity: <memory:9093120, vCores:296>
2013-11-15 09:00:38,146 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:computeUserLimitAndSetHeadroom(989)) - Headroom calculation for user gaurav:  userLimit=<memory:9093120, vCores:1> queueMaxCap=<memory:9093120, vCores:1> consumed=<memory:18432, vCores:3> headroom=<memory:9074688, vCores:-2>
2013-11-15 09:00:38,146 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:assignContainer(1306)) - assignContainers: node=node7.morado.com application=1 priority=2 request={Priority: 2, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true} type=OFF_SWITCH
2013-11-15 09:00:38,147 ResourceManager Event Processor DEBUG security.BaseContainerTokenSecretManager (BaseContainerTokenSecretManager.java:createPassword(90)) - Creating password for container_1384534729839_0001_01_000004 for user container_1384534729839_0001_01_000004 (auth:SIMPLE) to be run on NM node7.morado.com:36087
2013-11-15 09:00:38,147 ResourceManager Event Processor DEBUG security.ContainerTokenIdentifier (ContainerTokenIdentifier.java:write(112)) - Writing ContainerTokenIdentifier to RPC layer: org.apache.hadoop.yarn.security.ContainerTokenIdentifier@2f566b7d
2013-11-15 09:00:38,147 ResourceManager Event Processor DEBUG security.ContainerTokenIdentifier (ContainerTokenIdentifier.java:write(112)) - Writing ContainerTokenIdentifier to RPC layer: org.apache.hadoop.yarn.security.ContainerTokenIdentifier@2f566b7d
2013-11-15 09:00:38,147 ResourceManager Event Processor DEBUG scheduler.AppSchedulingInfo (AppSchedulingInfo.java:allocate(377)) - allocate: applicationId=application_1384534729839_0001 container=container_1384534729839_0001_01_000004 host=node7.morado.com:36087
2013-11-15 09:00:38,148 ResourceManager Event Processor DEBUG scheduler.ActiveUsersManager (ActiveUsersManager.java:deactivateApplication(94)) - User gaurav removed from activeUsers, currently: 0
2013-11-15 09:00:38,148 ResourceManager Event Processor DEBUG scheduler.AppSchedulingInfo (AppSchedulingInfo.java:allocate(265)) - allocate: user: gaurav, memory: <memory:8192, vCores:1>
2013-11-15 09:00:38,148 ResourceManager Event Processor DEBUG rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(208)) - Processing container_1384534729839_0001_01_000004 of type START
2013-11-15 09:00:38,148 ResourceManager Event Processor INFO  rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(220)) - container_1384534729839_0001_01_000004 Container Transitioned from NEW to ALLOCATED



, What is the value of the following configuration? 
yarn.scheduler.capacity.node-locality-delay

It looks like you are being hit by a bug that will happen with small number of container requests.

LeafQueue.assignContainersOnNode()
Looks like if rackLocalityDelay is not met, even then the scheduler falls back to off-switch assignment. The delay calculation for off-switch assignment is basically (#different-locations/#nodes-in-cluster)*#containers < #node-heartbeats-without-assignment. In your case, if you have 20 nodes in all, (2/20)*1 == 0.1. So the moment we skip 1 node (waiting for locality delay) we end up assigning an off-switch container to the request.

Try the following, set the node locality delay mentioned at the beginning to the number of nodes in the cluster. Then instead of asking for 1 container at pri 0, ask for 20 containers, each for a specific node, rack=false, relax=true. The above off-switch locality delay will become 20/20*1 == 20 missed assignments.
If you see correct assignments then the above theory is correct about the bug.

Btw, what you are trying to do (node=specific, rack=null and relaxLocality=true) is the default behavior of existing schedulers. They will always try to relax locality to rack and then off-switch by default. So you dont need to explicitly code for it. ,  yarn.scheduler.capacity.node-locality-delay is set to 50 and the size of cluster is 36. 
Since this property affects the cluster, not sure if it is right thing to depend on this property for container allocation

For the nature of our application we are requesting containers with incremental priorities so we have 1 container per priority, so we can't request for multiple containers at pri 0 and there are some applications were total number of containers are less than the cluster size. 

We want what you mentioned "Btw, what you are trying to do (node=specific, rack=null and relaxLocality=true) is the default behavior of existing schedulers. They will always try to relax locality to rack and then off-switch by default. So you dont need to explicitly code for it. "... But since it is not working we are trying to code it., I mentioned allocating multiple containers at the same priority as an experiment to check if the above theory is correct or not.

bq. But since it is not working we are trying to code it.
I am saying that your code (yes, null, true) is the same as the default. The behavior that you will get (correct or buggy as it is now) will be the same in both cases.

bq. yarn.scheduler.capacity.node-locality-delay is set to 50 and the size of cluster is 36. 
The max value of this should be the number of nodes in the cluster. Higher than that has no effect.

Btw, are all nodes in the same rack?, Yes All the nodes are on the same Rack.

Here is the experiment that I did to verify the theory
1. Cluster size: 36 nodes
2. yarn.scheduler.capacity.node-locality-delay is set to 36
3. Asked for 36 containers with priority 0
4. I requested containers with (node=yes, rack=yes,relax-locality=true)

But I still see that the containers are allocated on different nodes.
, Any updates on this?, We implemented it in the AM, tracking resource requests made for a specific host with relaxLocality=false and then, if they are not filled by the scheduler after n heartbeats, dropping host constraint and switching to relaxLocality=true. We would prefer to leave this to YARN with the combination of specific host and relaxLocality=true, but it does not work.

The requirement is not unique to our application, and instead of handling it in user land it would be great to see this working as expected in future YARN versions.
, Seeing this as well (YARN-2027).]