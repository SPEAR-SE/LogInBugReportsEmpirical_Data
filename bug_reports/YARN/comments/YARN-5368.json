[hi [~wyukawa], 
5th column of the top output which you are pointing to refers to the {{Virtual Memory Size}} which includes all code, data and shared libraries plus pages that have been swapped out and pages that have been mapped but not used. So its not precisely using 25GB of ram.
Are you facing any particular impact from this and also it would be helpful if you shares how many Open files are present for the timeline service process.
, Thank you for comment.

I point out 6th column(RSS), not 5th column VSZ(28.4g) .

RSS is 25GB.

>Are you facing any particular impact from this
now, nothing.
but I'm afraid memory shortage.

lsof -p `pgrep -f timeline` is the following.

https://gist.github.com/wyukawa/36ccfde48f118bb1e8e4f2564d4ecc2a

thanks, [~wyukawa] I believe, this issue with level-db with {{cent-OS 6.7}} only.. Did you get any workaround for this..?

Recently I noticed same issue with {{NodeManger}} when recovery is enabled.NM RES is keep on growing which leads {{ResourceLocalization}} slow.

*When NM RES Memory  more ResourceLocalization took ~3 mins* 
{noformat}
2016-10-21 13:48:14,481 | INFO  | LocalizerRunner for container_e08_1476679121221_34954_01_000005 | Writing credentials to the nmPrivate file /srv/BigData/data12/yarn/localdir/nmPrivate/container_e08_1476679121221_34954_01_000005.tokens. Credentials list:  | ResourceLocalizationService.java:1238
 2016-10-21 13:48:14,487  | INFO  | LocalizerRunner for container_e08_1476679121221_34954_01_000006 | Writing credentials to the nmPrivate file /srv/BigData/data5/yarn/localdir/nmPrivate/container_e08_1476679121221_34954_01_000006.tokens. Credentials list:  | ResourceLocalizationService.java:1238
 2016-10-21 13:51:40,382  | INFO  | IPC Server handler 3 on 26007 | Resource hdfs://hacluster/tmp/hadoop-yarn/staging/IOCLG/.staging/job_1476679121221_34954/libjars/hbase-server-1.0.2.jar(->/srv/BigData/data22/yarn/localdir/usercache/IOCLG/filecache/557841/hbase-server-1.0.2.jar) transitioned from DOWNLOADING to LOCALIZED | LocalizedResource.java:203
{noformat}

*When normal ResourceLocalization time* 
{noformat}
2016-10-21 14:19:05,600 | INFO  | LocalizerRunner for container_e10_1477030404479_0013_01_000006 | Writing credentials to the nmPrivate file /srv/BigData/data6/yarn/localdir/nmPrivate/container_e10_1477030404479_0013_01_000006.tokens. Credentials list:  | ResourceLocalizationService.java:1238
 2016-10-21 14:19:05,600  | INFO  | LocalizerRunner for container_e10_1477030404479_0013_01_000005 | Writing credentials to the nmPrivate file /srv/BigData/data15/yarn/localdir/nmPrivate/container_e10_1477030404479_0013_01_000005.tokens. Credentials list:  | ResourceLocalizationService.java:1238
 2016-10-21 14:19:07,860  | INFO  | IPC Server handler 2 on 26007 | Resource hdfs://hacluster/tmp/hadoop-yarn/staging/IOCLG/.staging/job_1477030404479_0013/libjars/hbase-server-1.0.2.jar(->/srv/BigData/data15/yarn/localdir/usercache/IOCLG/filecache/558308/hbase-server-1.0.2.jar) transitioned from DOWNLOADING to LOCALIZED | LocalizedResource.java:203
2016-10-21 14:19:07,898 | INFO  | IPC Server handler 3 on 26007 | Resource hdfs://hacluster/tmp/hadoop-yarn/staging/IOCLG/.staging/job_1477030404479_0013/libjars/hbase-client-1.0.2.jar(->/srv/BigData/data19/yarn/localdir/usercache/IOCLG/filecache/558312/hbase-client-1.0.2.jar) transitioned from DOWNLOADING to LOCALIZED | LocalizedResource.java:203
{noformat}

I looked at level-db community and did not find any memory leak issue handled after {{1.8}} release. 

[~jlowe] any thoughts on this..? Thanks., bq. Recently I noticed same issue with NodeManger when recovery is enabled.NM RES is keep on growing which leads ResourceLocalization slow.

We have not seen that on our clusters.  Three minutes is a _really_ long time.  Do you have gc logging enabled for the nodemanager JVM?  It would be interesting to know if it was trying to run one or more GC cycles during that time.  If it wasn't GC cycles then I'm not sure how increased off-heap memory would directly contribute to slower resource localization unless the machine was near or at the point where it started swapping.

As for the timeline server memory usage, it looks like the rolling level db instances are starting to pile up, accumulating a lot of off-heap memory.  Pinging [~jeagles] since I vaguely remember something like this occurring in the past, and there may be a known fix for that issue., The Rolling Level DB does roll by default every hour (configurable). Memory in leveldb is retained until the file is closed and the amount of memory retained by default is a max (read-buffer-size + write-buffer-size) * 2 per hour. Upon startup, historical leveldb are lazily loaded so they don't incur the memory hit until the data in the hour is referenced. What I am wondering is how long the retention period is set for the data. The default time to live (_yarn.timeline-service.ttl-ms_) is 7 days. If you want to reduce memory usage drastically

 - reduce the number of leveldb file retained in memory per day:  perhaps set _yarn.timeline-service.rolling-period_ to daily (there is only limited support for changing periods) 
 - reduce retention time to meet requirements perhaps set _yarn.timeline-service.ttl-ms_ to a lower number
 - reduce the read cache size: _yarn.timeline-service.leveldb-timeline-store.read-cache-size_ perhaps to 4MB (4194304)
 - reduce the write cache size: _yarn.timeline-service.leveldb-timeline-store.write-buffer-size_ perhaps to 4MB (4194304)
 - implement a yarn patch for rolling leveldb LRU cache so that only a fixed number are in memory at a time and the others are purged until needed 

There is also a _yarn.timeline-service.leveldb-timeline-store.max-open-files_ configuration, but I have limited understanding if that is a global setting or a per rolling instance setting.
, [~jlowe]  and [~jeagles] thanks for your inputs.

bq. Do you have gc logging enabled for the nodemanager JVM? It would be interesting to know if it was trying to run one or more GC cycles during that time. If it wasn't GC cycles then I'm not sure how increased off-heap memory would directly contribute to slower resource localization unless the machine was near or at the point where it started swapping.

GC looks normal,and after applying the YARN-3491,ResourceLocalization is also normal. But for RES memory increase , need to try with [~jeagles] options. 

But I doubt {{db.compactRange(null, null);}}, file number is not decreasing after this. if we close and open the db then files are got deleted( not from the memory).
, Forgot to update here.. Upon investigation, there was internal change which caused the leveldb leak..:(.. , >Did you get any workaround for this..?
I restart timeline server., We cross verified whether similar leak existed in the existing ATS code (level DB iterator was not being closed after usage) but was not able to find anywhere else so hope issue did not reoccur to you !, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 31s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 11s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice: The patch generated 1 new + 5 unchanged - 1 fixed = 6 total (was 6) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  2m 46s{color} | {color:red} hadoop-yarn-server-applicationhistoryservice in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 16s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 21m 28s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.timeline.webapp.TestTimelineWebServices |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5368 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12859755/YARN-5368.1.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux e0e79f287c6a 3.13.0-106-generic #153-Ubuntu SMP Tue Dec 6 15:44:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2841666 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/15343/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-applicationhistoryservice.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/15343/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-applicationhistoryservice.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/15343/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/15343/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Unit test failure in TestTimelineWebServices is covered by YARN-5934. Checkstyle method length was pre-existing., [~Naganarasimha], I have posted a patch that addresses the missing iterator.close() leak. Let me know if you are ok with the try-with-resources approach to take care of this case, [~jeagles], nice catch.
Closing of only the last iterator in the loop must be the reason for leak. 
The leak we got in NM, albeit due to our private code, was also due to DBIterator not being closed.

Using try-with-resources approach for DBIterator should be fine. 
How about using try-with-resources for DBIterator elsewhere in the RollingLevelDBTimelineStore class i.e. where it's not used in the loop, just to make the code consistent., Thanks [~jeagles], Sorry missed your comment earlier. Thanks for identifying the cause.
Similar to it i was able to find Db Iterator initialized within the loop @ {{RollingLevelDBTimelineStore.getEntityByTime, ln no: 718 - 819}} and closing the last one of the loop only.
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 22s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 13s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice: The patch generated 1 new + 5 unchanged - 1 fixed = 6 total (was 6) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m  8s{color} | {color:green} hadoop-yarn-server-applicationhistoryservice in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 25m 21s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5368 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12860727/YARN-5368.2.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 3d33552d8645 3.13.0-105-generic #152-Ubuntu SMP Fri Dec 2 15:37:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / cd014d5 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/15399/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-applicationhistoryservice.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/15399/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/15399/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Checkstyle warning is pre-existing. [~varun_saxena], tried to make the entire file consistent. There is one case left over that I was not able to find a way to convert easily without method redesign. Let me know what is left for this patch., Thanks [~jeagles] for the updated patch fixing the comments.

bq. There is one case left over that I was not able to find a way to convert easily without method redesign. 
That should be fine.

+1. The changes LGTM.
I will wait for a day and then commit unless there are opposite opinions., +1. The changes LGTM.
, This patch should apply cleanly to branch-2 and branch-2.8. Let me know if there are difficulties. Since this is an important bug fix, I think it is work going into 2.8 so it can reach 2.8.1., Yeah will get this in branch-2.8 too., Committed to trunk, branch-2 and branch-2.8
Thanks [~jeagles] for finding the root cause and fixing it.
Thanks [~wyukawa] for reporting the issue and Naga for additional review., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11483 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11483/])
YARN-5368. Memory leak in timeline server (Jonathan Eagles via Varun (varunsaxena: rev 01aca54a22c8586d232a8f79fe9977aeb8d09b83)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java
, 2.8.1 became a security release. Moving fix-version to 2.8.2 after the fact.]