[[~rchiang] thank you for reporting this. I think  the root cause is:

In {{updateMetricsForDeactivatedNode}}, the re-addition of a node does not decrement the Decommissioned node count as expected. It looks at previous state and there is no switch case for decommissioned nodes., Spoke too soon. on trunk {{updateMetricsForRejoinedNode}} should handle that., Thanks for letting me know.   I'll take a look at that., Looking at the {{NodesListManager}}, it looks to me like {{ClusterMetrics.getMetrics().getDecomissionedNMs()}} is set to the size of the excludes list, but {{getUnusableNodes()}} returns the list of nodes that have been decommissioned since the last reboot., So, would the right solution be that getUnuableNodes() should be "excludes list" aware?, Hi [~rchiang] & [~kshukla]
YARN-3102 also is for the same issue, earlier had stopped working on this because i was skeptical of YARN-914 (or its subjira's ) might have impact or take care of this issue. If you guys have already narrowed down on the cause feel free to assign YARN-3102 and close this issue., Thank you [~Naganarasimha]. Asking [~rchiang] if its alright for me to work on it. I am currently working in that code base for YARN-4311., Now that I've had a chance to look at the web UI code, I see that my theory was close, but not quite.  The number of decommissioned nodes is taken from {{ClusterMetrics.getMetrics().getDecomissionedNMs()}}, which is just the count of nodes in the excludes list.  The list of decommissioned nodes comes from {{ResourceManager.getRMContext().getInactiveRMNodes()}}, which contains only nodes that have been decommissioned since the last restart., Thanks [~Naganarasimha].  I'll close up this JIRA as a duplicate.

As for fixing it, I'll leave that up to you and [~templedf].  It looks like you two are further ahead than I am., YARN-3226 which is a subtask of YARN-914 will be splitting cluster metrics in to two TABLES (Node metrics table)  as we have to show Decommissioning nodes too. 
Patch is given there already for same. However this particular case s not handled there. Mostly as progress is made for this,  please also see the progress in YARN-3226.

, Yes that is right, the issue is present on trunk. We could during {{serviceInit}} populate this metric to the number of decommissioned nodes in the inactive list, since we don't care about nodes that were decommissioned before last restart AFAIK. 

At present:
{code}
  private void setDecomissionedNMsMetrics() {
    Set<String> excludeList = hostsReader.getExcludedHosts();
    ClusterMetrics.getMetrics().setDecommisionedNMs(excludeList.size());
  }
{code}

To:
{code}
  private void setDecomissionedNMsMetrics() {
    int numDecommissioned = 0;
    for(RMNode rmNode : rmContext.getInactiveRMNodes().values()) {
      if (rmNode.getState() == NodeState.DECOMMISSIONED) {
        numDecommissioned++;
      }
    }
    ClusterMetrics.getMetrics().setDecommisionedNMs(numDecommissioned);
  }
{code}
, That looks good to me., That's the simplest resolution, but I was actually leaning the other direction: making the list of decommissioned nodes include the full excludes list.  I guess it comes down to how we define decommissioned in the UI.  I interpret the excludes list as the canonical list of decommissioned nodes., I agree. I was thinking about that too. During {{registerwithRM()}} we throw a YarnException while on the ResourceTrackerService side we just send NodeAction as SHUTDOWN. We could in fact update InactiveRMNode list with this node, so that it is consistent. Let me know what you think. I will put up a patch soon.]