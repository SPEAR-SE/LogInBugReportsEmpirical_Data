[The possible solution is to make proxyserver configurable to disable AmFilterInitializer when proxyserver is running as embedded web application on resource manager.
[~daryn] Any thoughts?, [~kbadani], please leave the fix-version empty. In Hadoop projects, we set it only at commit time. Tx., Here is the stack trace to better illustrate the problem:
{code}
2018-04-16 22:56:34,208 WARN org.apache.hadoop.security.authentication.server.AuthenticationFilter: error at: {}
org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:303)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:413)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:536)
        at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.doFilter(RMAuthenticationFilter.java:82)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:645)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:304)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)
        at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.doFilter(RMAuthenticationFilter.java:82)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1601)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
        at org.eclipse.jetty.server.Server.handle(Server.java:534)
        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
        at java.lang.Thread.run(Thread.java:748)
Caused by: GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))
        at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:856)
        at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
        at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.runWithPrincipal(KerberosAuthenticationHandler.java:329)
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.access$000(KerberosAuthenticationHandler.java:64)
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:295)
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:292)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:291)
        ... 36 more
Caused by: KrbException: Request is a replay (34)
        at sun.security.krb5.internal.rcache.AuthList.put(AuthList.java:83)
        at sun.security.krb5.internal.rcache.MemoryCache.checkAndStore(MemoryCache.java:71)
        at sun.security.krb5.KrbApReq.authenticate(KrbApReq.java:323)
        at sun.security.krb5.KrbApReq.<init>(KrbApReq.java:149)
        at sun.security.jgss.krb5.InitSecContextToken.<init>(InitSecContextToken.java:108)
        at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:829)
        ... 45 more
{code}

RMAuthenticationFilter line 82 is invoked twice in the filtering chain.

One instance of RMAuthenticationFilter is registered for resource manager web filter:
{code}
        at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilterInitializer.initFilter(RMAuthenticationFilterInitializer.java:102)
        at org.apache.hadoop.http.HttpServer2.initializeWebServer(HttpServer2.java:587)
        at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:537)
        at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)
        at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1189)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1299)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1495)
{code}

The second instance of RMAuthenticationFilter is registered by proxyserver initialization when it tries to setup logs, static:
{code}
    934   @Override
    935   public void addFilter(String name, String classname,
    936       Map<String, String> parameters) {
    937 
    938     FilterHolder filterHolder = getFilterHolder(name, classname, parameters);
    939     final String[] USER_FACING_URLS = { "*.html", "*.jsp" };
    940     FilterMapping fmap = getFilterMapping(name, USER_FACING_URLS);
    941     defineFilter(webAppContext, filterHolder, fmap);
    942     LOG.info(
    943         "Added filter " + name + " (class=" + classname + ") to context "
    944             + webAppContext.getDisplayName());
    945     final String[] ALL_URLS = { "/*" };
    946     fmap = getFilterMapping(name, ALL_URLS);
    947     for (Map.Entry<ServletContextHandler, Boolean> e
    948         : defaultContexts.entrySet()) {
    949       if (e.getValue()) {
    950         ServletContextHandler ctx = e.getKey();
    951         defineFilter(ctx, filterHolder, fmap);
    952         LOG.info("Added filter " + name + " (class=" + classname
    953             + ") to context " + ctx.getDisplayName());
    954       }
    955     }
    956     filterNames.add(name);
    957   }
{code}

Note in line 945, the filter mapping is applied to all URLs, this creates a problem that the same kerberos token is being checked twice, which leads to the request is a replay error.  One possible solution is to make sure the addFilter have ability to fine tune the URL prefix to avoid overlaps that caused the same AuthenticationFilter to be trigged twice., Analysis looks sound.  Agree each servlet should scope filters for itself, not globally.

I'm surprised this hasn't been found before.  Is this specific to 3.x?  Or does it exist in 2.x?  (I guess we haven't see this bug due to an alternate auth for the RM), [~daryn] Hadoop 2.7.5 is unaffected by this bug, but it could be a result of masked by Jetty difference.  In HttpServer2, there is a section of code that does universal initialization of AuthenticationFilter, if Kerberos is enabled:  https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java#L1123

RMAuthenticationFilter extends DelegationTokenAuthenticationFilter which also extends from AuthenticationFilter.  Multiple copies of AuthenticationFilter conflicts with each other with different initialization settings.  One possible solution is to ensure that DelegationTokenAuthenticationFilter is chained after SpnegoFilter, and not extended from AuthenticationFilter.  This allows issue of DT for authorized user without recheck kerberos ticket in the same request., Patch 001 is a brute force approach to blanket the entire RM server with RMAuthenticationFilter.  This seems to work and not trigger code path registered by proxyserver.
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 53s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 26m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 37s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 46s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  2m 17s{color} | {color:green} hadoop-yarn-server-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 22s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 59m 57s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |
| JIRA Issue | YARN-8108 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12919508/YARN-8108.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux cdcaff5e65e5 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / e4313e7 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_162 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/20380/testReport/ |
| Max. process+thread count | 334 (vs. ulimit of 10000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/20380/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, bq. This seems to work and not trigger code path registered by proxyserver.

Please elaborate:
# Why do we want to bypass the code registered by the proxyserver?
# Should the proxy service even be using the RM's auth filter?
# How/why does changing addFilter to addGlobalFilter fix the problem?  Adding the filter to every context (even those explicitly registered to not be filtered) seems counterintuitive.

I think we also need to root cause exactly what change caused the RM auth filter to be double registered so we can ensure we've correctly fixed the bug., {quote}Why do we want to bypass the code registered by the proxyserver?{quote}

1.  proxyserver is a default registration to have spnego filter applied to logs, and static directory.  The same protection can be covered by RMAuthenticationFilter.  It doesn't seem to cause harm when this is going through RMAuthenticationFilter path.  I am not sure if proxyserver should or should not issue DT when it is embedded on RM.  Thoughts?

{quote}Should the proxy service even be using the RM's auth filter?{quote}
2.  Probably for embedded case.  Both /proxy and /cluster can be configured to two different HTTP principals.  This might be problematic when checking if TGS session key is granted properly because the service principal names are the same on the same service port, but it appears there is two TGS session key initialized by Hadoop code to cause this problem.

{quote}How/why does changing addFilter to addGlobalFilter fix the problem? Adding the filter to every context (even those explicitly registered to not be filtered) seems counterintuitive.{quote}
3.  The global coverage ensures that all entry points are served by the same TGS session key.  Hence, it doesn't confuse Kerberos that there are two kerberos sessions initialized.

{quote}I think we also need to root cause exactly what change caused the RM auth filter to be double registered so we can ensure we've correctly fixed the bug.{quote}

I tried to peel the code to see if we can eliminate one of the registration, but it appears that we have spotty kerberos coverage when that happens.  The amount of code reuse make it impossible to decouple the threads and regroup them in the timely manner.  The trade off between refactoring httpServer2 code to get filter initialization configuration correct for special case vs global coverage of the same TGS session key.  I chose to be conservative in code changes without causing regression.  This is the reason that global filter approach was implemented.  Base on my testing, nothing is broken when global filter is applied.

I found the following stack trace that might be able to help people who like to try to improve filter configuration in httpServer2.  However, the init code is all over the place, but going down the same path, it is not easy to untangle them.

First instance (RMAuthenticationFilter applies to /cluster/*, /ws/*, /app/*) is configured at:
{code}
        at org.apache.hadoop.http.HttpServer2.defineFilter(HttpServer2.java:998)
        at org.apache.hadoop.http.HttpServer2.addFilter(HttpServer2.java:962)
        at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilterInitializer.initFilter(RMAuthenticationFilterInitializer.java:111)
        at org.apache.hadoop.http.HttpServer2.initializeWebServer(HttpServer2.java:587)
        at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:537)
        at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)
        at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1189)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1299)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1495)
{code}

Second instance (SpnegoFilter applies to /logs, /static/*, /proxy/*) is configured at:
{code}
        at org.apache.hadoop.http.HttpServer2.defineFilter(HttpServer2.java:998)
        at org.apache.hadoop.http.HttpServer2.defineFilter(HttpServer2.java:989)
        at org.apache.hadoop.http.HttpServer2.initSpnego(HttpServer2.java:1141)
        at org.apache.hadoop.http.HttpServer2.access$200(HttpServer2.java:117)
        at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:424)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)
        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1189)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1299)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1495)
{code}

Line number might be slightly off because I added some debug statement to gather the stack trace., The TGS issues are purely caused by the double registration of the RMAuthenticationFilter for the /proxy path, so I don't think the SpnegoFilter init is involved.  Please clarify the relevance?

Silently ignoring the explicit configuration for the proxyserver when it's internal may have security ramifications.  An admin may want more or less restrictive auth for the two services.

I'm a bit uneasy with rationalizing how to fix an issue, with an unknown root cause, with a not well understood fix.  Please track down the Jira that introduced the regression/incompatibility so we can correctly assess the problem., There are 3 registrations of RMAuthenticationFilter to context cluster, logs, and static.  There is 1 registration of SpnegoFilter to logs, static.  /ws, /proxy, /app path spec are blanket by the default SpnegoFilter when embedded proxyserver is enabled (because proxyserver filter is initialized after cluster context).   I tried to reduce RMAuthenticationFilter to 1, and discover that it still has conflict between /proxy and /cluster.  I tried to disable SpnegoFilter, then /proxy become insecure.

In YARN-1553, webproxy was converted to use HttpServer2.Builder.  This change picked up webproxy initSpnego filter.  
In YARN-1482, code was modified to allow WebAppProxy to run in RM.  In HADOOP-10075 + HADOOP-10703 are written to apply handlers > context > filter > servlet globally.  Existing code which doesn't have handler applied, defineFilter applies to context.  This is all working fine, if there is only one context that enclosing all hadoop logic.  However, if multiple webapps are put on the same server, older code written pre-dated HADOOP-10703 may have separated context initialized with separated AuthenticationFilter that would result in request is a replay error.  

A couple problems with Hadoop code misuse of web application:
- Servlet code are setup to be context.  Context should be RM, webproxy, timelineserver.  Today, it is cluster, logs, static.
- YARN servlet logic are written as Filter.
- Handler logic are written as Filter.
- Filter are applied to wildcard path and assuming it is applied globally, which may not be true.

This problem appears to have existed since creation of HttpServer2 and getting more complex to manage with introduction of jetty 9 upgrade.  A few JIRA like YARN-2397 are attempts to band-aid the general code reuse and specialized AuthenticationFilter.  Each of the pieces were evolved independently, and there was no conflicts until everything is put into RM.  One possible solution is to rewrite RMAuthenticationFilter, and AuthenticationFilter to become AuthenticationHandler, and it is applied globally.  This would be the same as turning RMAuthenticationFilter into a global Filter.

I am not 100% sure if RMAuthenticationFilter should oversee all Kerberos login activity when proxyserver is in embedded mode, but I am more inclined to make it so after analyze the code.  I post here to see if anyone who are more familiar with YARN code base can shed some lights on best approach to address this issue.


, I took a look into the issue and am feeling okay about the conservative fix of making RMAuthenticationFilter global whenever it is enabled. Does anyone else have additional comments?, [~daryn], any additional thoughts?, bq. I took a look into the issue and am feeling okay about the conservative fix of making RMAuthenticationFilter global whenever it is enabled.

While that would "work", isn't it be a regression?  An admin that specifically configured those filters, perhaps with different principals as Eric previously mentioned, would be quite surprised to discover that the configuration is now silently ignored.

Per earlier comments, the issue is apparently not present through at least 2.7.5.  Most of the referenced jiras are up to 5 years old.  We still need to identity which (recent-ish) jira caused the regression to understand the problem., [~daryn] This issue doesn't present in Hadoop 2.7.5, does not mean it was done properly.  It is not possible to configure different HTTP principal for RM and Proxy Server on the same host/port, and it was only half working.  This is because Hadoop only have yarn.resourcemanager.webapp.spnego-keytab-file and yarn.resourcemanager.webapp.spnego-principal setting to define HTTP principal to use on RM server.  It does not have yarn.web-proxy.webapp.spnego-keytab-file and yarn.web-proxy.webapp.spnego-principal settings to make differentiation.  Even if those settings are defined, they are not being used.  Further analysis on Hadoop 2.7.5, /proxy URL is not secured by any HTTP principal when running in RM embedded mode., I would say that since this one line patch requires so much discussion that it supports my feeling that we need to revisit this whole filter chain mechanism. It always feels like there is some magic combination that we *think* will solve some issue that shouldn't have existed in the first place.

I suggest that we get a picture or set of pictures that represent what the filter chain/s are and how global vs servlet specific layer on to each other.

Especially since we are talking about ignoring configuration that was added with explicit intent in mind - we need to be able to articulate how things work in a very clear way. This information will be valuable if/when we decide to replace this mechanism., Kerberos SPN support by browser definition are:

HTTP/<host name>, where <host name> is either white list server name or canonical DNS name of the server.  Chrome, IE, and Firefox all shares the similar logic.  Firefox and IE don't allow canonical DNS to prevent MITM attack.  Safari and Chrome supports canonical DNS with options to disable canonical DNS.

From Server point of view, a single server can host multiple virtual hosts with different web applications.  This is technically possible to configure web server to run with multiple SPN.  It is incorrect to assume that same virtual host can serve two different SPN for two different subset of URLs.  All browsers do not support subset of URLs to be served by one SPN, while other subset of URLs to be served by another SPN.
  
In Hadoop 0.2x, Hadoop components are designed to serve a collection of servlets (log, static, cluster) per port.  Therefore, AuthenticationFilter can cover the entire port by targeting the fixed set of servlet for filtering, that matches browser expectation without problem.  AuthenticationFilter was later reused in Hadoop 1.x and 2.x as Kerberos SPNEGO filter.

The current problem is only surfaced when multiple web contexts are configured to share on the same port with same server hostname, and each web contexts tried to initialize its own SPN.  This is not by design and it just happened due to code reuse and lack of testing.  For Hadoop 2.x+ to offer embedded services securely, the individual AuthenticationFilter can be turned into one [security handler|http://www.eclipse.org/jetty/documentation/9.3.x/architecture.html#_handlers] to match Jetty design specification.  This fall through the crack in open source when no one is looking because the first security mechanism for Hadoop was to implement a XSS filter (was committed as part of Chukwa) instead of security handler.  Unfortunately, Hadoop security mechanisms followed the bottom up approach to implement as filter instead of following web application design to write security handler as Handlers.  Due to lack of understanding that session persistence require authentication and authorization security mechanism to be built differently from web filters.

The one line change is to loop through all Context and ensure all contexts are registered with the same AuthenticationFilter to apply one filter globally to all URLs.  This is the reason that this one line patch can plug this security hole in the short term bug fix.  The long term solution is writing security handler to match handler design to ensure no API breakage during jetty version upgrade and improve session persistence in Hadoop web applications, which is beyond the scope of this JIRA., cc [~rkanter] and [~tucu00] 
, Hi [~eyang] and [~kbadani],

Would you please confirm that this jira is a blocker for 3.0.3 release? If so, can we expedite the work here since we are in the process of preparing 3.0.3 release?

Thanks a lot.

, [~yzhangal] This is a regression in Hadoop 3.x, hence it is marked as a blocker.  Friendly reminder to PMCs to review this patch to bring this to closure., Hi [~eyang],

It seems the issue also exists in 3.0.2 release. The above discussion indicates that it might take some time for the solution to converge, should we move 3.0.3 out of the target release and list this jira as a known issue for 3.0.3? or we should fix this issue in 3.0.3?

Thanks.
, [~yzhangal] My preference is to fix this in 3.0.3 release.  If consensus is not reached, release manager can push this out of 3.0.3 release, and release note this as an known issue.  I am fine with the plan., HADOOP-15518 seems to overlap this JIRA but it is unclear to me whether it will solve this particular issue.

If it does, I would prefer this patch over moving another filter to be global - I think.

 , [~lmccay] If the entire webserver only serves secure contents, then it is best to have security handler applied globally to avoid loophole of information leak.  HADOOP-15518 will solve the overlapping AuthenticationFilter, but it will not solve /proxy being unprotected because yarn.web-proxy.webapp.spnego-principal and yarn.web-proxy.webapp.spnego-keytab-file  are not documented properties.  RM's spnego-principal is the real one that grant TGS to client and first authentication filter to verify client token because patch in HADOOP-15518 ensures that additional instances of AuthenticationFilter to skip duplicated call to authenticate method.  This means /proxy is still unprotected unless this patch is added to apply AuthenticationFilter to /proxy path., Post YARN-7605, we need a way to safely protect servlet certain endpoints like Api server. However it need not have to load all existing filters for its pathSpecs (or mappings). Currently it is loading all pathSpecs across all filters and causing request replay problem.

{{pathSpecs=*.html,*.jsp,/stacks,/logLevel,/jmx,/conf,/cluster/*,/ws/*,/proxy/*,/app/*,/proxy/*,/*}}

/proxy is added two times. Now to fix this, its upto the servlet to decide whether to load all its pathSpecs under all loaded filters. This patch gives that options now. Debugged with [~eyang] and [~vinodkv]. Thank you for the support.

Attaching patch which address this issue.

Also tested in a single node kerberized cluster and did below tests
 # Accessed */proxy* endpoint directly from a kerberized browser (Before this patch, we used to get a GSSEXception saying request is a reply)
 # Accessed proxy link from a RUNNING and COMPLETED application from RM Web UI. This works fine.
 # From a kinited shell, accessed /proxy endpoint and it loaded fine.
 # From a kinited shell, accessed /api endpoint to submit an app and also to get service details. Both are working as per expectation.
 # From an unsecure shell, both /proxy and /api endpoint are giving 401 exception which is expected., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 11s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 25m 53s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  8m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 19s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 48s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 18s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 11s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  7m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  7m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 45s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 16s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m  7s{color} | {color:green} hadoop-yarn-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 66m 57s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}150m  2s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:abb62dd |
| JIRA Issue | YARN-8108 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12929122/YARN-8108.002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 1d06f1a17cff 3.13.0-137-generic #186-Ubuntu SMP Mon Dec 4 19:09:19 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 35ec940 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_171 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/21106/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/21106/testReport/ |
| Max. process+thread count | 847 (vs. ulimit of 10000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/21106/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, cc [~eyang] [~vinodkv] pls help to review the change, [~sunilg], thank you for the patch.  Patch 2 will remove the conflict between SpnegoFilter and RMAuthentication filter conflicts that exists in Hadoop 3.1+ for web-proxy.  For 3.0 and earlier release, user must configure:
{code:java}
  <property>
    <name>yarn.web-proxy.keytab</name>
    <value>/etc/security/keytabs/rm.service.keytab</value>
  </property>

  <property>
    <name>yarn.web-proxy.principal</name>
    <value>rm/_HOST@EXAMPLE.COM</value>
  </property>{code}
The keytab and principal must be identical to RM's principal and keytab, otherwise /proxy is unprotected when those settings are not configured.  The failed unit test is caused by patch 2 changes, can you take a look?, Thanks [~eyang] This tests failed for fair scheduler. While handling a Fair Scheduler patch, I see below comment. This seems not caused by this patch. Thoughts?, [~sunilg], you are correct, the test case is intermittent and does not appear to be caused by the patch.  +1 on patch 2.  My preference would be applying security filter globally to prevent other loopholes in newly introduced servlets.  However, we did not get +1 on patch 1.  Hence, patch 002 is the second best option that we prevent security filter to be applied multiple times.  I will commit this to branch 3.1 and trunk in a few hours if there is no objections.  Hadoop 3.0.x is removed from the target version because the code does not apply to 3.0, and there is a way to workaround this issue via configuration., +1 for patch 2., +1 for patch 2.  Patch 002 commited to trunk and branch-3.1.

Thank you [~sunilg] for the patch., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14483 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14483/])
YARN-8108.  Added option to disable loading existing filters to prevent  (eyang: rev b69ba0f3307a90500aeb0c5db9e582fcda60b501)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/WebApps.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java
]