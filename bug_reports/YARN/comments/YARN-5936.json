[Thank you for reporting this issue [~zhengchenyu]!
When you mentioned "But the cpu bandwidth of cgroup would lead to bad performance in our experience.", do you mean that it is due to the design that it limits the CPU usage to the vCore share affecting overall utilization, or do you mean that the container got less resources than what was assigned to it? In other words, is this a remark of the strict CPU cgroup design or the implementation? Thank you!, Hello [~zhengchenyu]

How many cores your system has, can you attach the output of top command that contains each core usages (by type 1 after top)?

It looks like it was not fair because your system still has plenty of resources, so each container gets whatever it needs to run happily.  It only maintains fairness relatively. 

According to the [cgroups document|https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html]

bq. cpu.shares contains an integer value that specifies a relative share of CPU time available to the tasks in a cgroup ... Note that shares of CPU time are distributed per all CPU cores on multi-core systems. Even if a cgroup is limited to less than 100% of CPU on a multi-core system, it may use 100% of each individual CPU core.

What you are asking for (container level strict limit) looks like YARN-810, unfortunately it has been left out there for quite a long time already., I think two reason both affect the performance, but I can't evaluate which is the major reason. 
First，The linux kernel source code of cpu bandwidth will add too many timer, and add more function to be called. 
Secondly, limit utilization ratio will lead to bad performance.
Closing the cpu bandwith limit is inevitabe. Here I only wanna to a idea that keep justice when only use cpu share., You didn't catch my meaning! In fact, I knew the reason of the unfairness. 
The processes and threads has the same level of scheduling. In Linux-3.10, task are scheduled by a red–black tree, and update the red–black tree periodically or manually(by other calling). the left-most is next task to be scheduled. The red-black tree is update by this formula:
    curr->vruntime+=delta_exec*nice_0_load/curr->load.weight
here curr->load.weight is just cpu.share. So Mulit Thread obtain more cpu than Single-Thread because of more child thread are participating the scheduler. 


, Thank you, for the reply [~zhengchenyu]!
{quote}
First，The linux kernel source code of cpu bandwidth will add too many timer, and add more function to be called. 
Secondly, limit utilization ratio will lead to bad performance.
{quote}
I did an experiment with he following cpu heavy app:
{code}
//a.c
int main() {
  int i;
  int j = 0;
  for (i = 0; i < 1000000000; ++i) {
    j++;
  }
  return j & 1;
}
{code}
I ran it in parallel in a single cgroup, multiple cgroups and multipre cgroups with CPU throttling enabled on a single CPU.
{code}
for j in `seq 1 10`; do export i=$j;sh -c 'time ./a.out&'; done
for j in `seq 1 10`; do export i=$j;sh -c 'echo $$ >/cgroup/cpu/$i/tasks;echo -1 >/cgroup/cpu/$i/cpu.cfs_quota_us;time ./a.out&'; done
for j in `seq 1 10`; do export i=$j;sh -c 'echo $$ >/cgroup/cpu/$i/tasks;echo 10000 >/cgroup/cpu/$i/cpu.cfs_quota_us;time ./a.out&'; done
{code}
The runtime in the first case (no cgroups) was 24.7154, the second (no group throttle) was 24.6907 seconds on average, the runtime in the latter case was 24.7469 respectively.
The difference less than 0.25% in these cases. I ran it a few more times and I received very similar numbers.
This means to me that what you are seeing is the utilization drop, if the container group limits the CPU usage and not an inefficiency in the Linux kernel., Because your program is a single-thread. , I run 10 of them in parallel.
{quote}
First，The linux kernel source code of cpu bandwidth will add too many timer, and add more function to be called. 
{quote}
Are you saying you see this only if an app is running multiple threads and not multiple processes?, Indeed, I see some 6% performance loss in the second case above compared to the first. This is when I move away from the root cgroup and use the hierarchy but do not use the limit, yet. This happens, when I am running 10 processes in different cgroups with 100 threads each., But your "time" command is only related to it's own program, every program is single thread. 
My question is below:
     two process that has different numbers of threads has different ability of schedule, though they have the same cpu.share. 
I know the reason, but I don't have a proper suggestion to avoid this problem., I don't care the performance loss of cpu bandwith, because I know it must happen. I only wanna know the method of keep fair without cpu bandwith., In the latest test I used 100 threads per program, I just did not share the code. They run in parallel, so the sum of time command results measures, whether the whole set spent time in additional CPU cycles other than the activity loop. The reason I checked, is to ask whether you like a solution that uses {{cpu.cfs_quota_us}}.
I could imagine a dynamic cfs algorithm like the following.
A timer callback with a certain period could do:
{code}
if CPU is saturated
  for each container
    if previous usage > fair share
      limit to fair share
else
  release all limits
{code}
It has drawbacks. It only works with saturated CPU, when not much time is spent waiting on I/O. It has a delay, since it works on historic data. This means also that it adds some utilization loss, which can be larger with multiple cores. On the other hand, it provides the requested fairness, when the CPU is saturated.
Does your node have multiple cores? The algorithm may not help much in that case. For example there are 8 cores. One container runs 8 threads, one container runs 2 threads. The fair share requested is 50%-50%. Without throttling the two containers will share 80%-20%. Even, if we set the fair share by throttling, when the cores are saturated, the usage will be 50%/25% when the quota is applied, so there is a utilization loss for a period. Now then, the algorithm may get more complicated..., We open cpu strict mode, because  the container can't use the free resource. But we close cpu strict mode, it's not fair for every container. If i set the cpu bandwidth of cgroup dynamically, I think this problem is solved., Another option for the future is the use of the cgroup pids subsystem on newer kernels. The main reason fairness is not enforced in non-strict mode, is that it allows the container to run as many threads with the same cgroup and weight as needed. You can limit the amount of threads with the pids namespace, so that the effective overall container weight becomes weight*pids_limit. The drawback of this approach is that it limits multitasking and the number of launcher processes. The possible ideal value of pids_limit is the <number of cores>/<desired thread count>, so that we do not starve single threaded containers.]