[I think this is related to /etc/hosts mapping. Does /etc/hosts mapping exits in all the machines of NodeManager for *qadoop-nn001.apsalar.com*? 
In your changed code, you are setting for an ip which will work. Can you set hostname and try? It wont work I guess. , Yes, we have the same /etc/hosts on all machines.
We also have internal DNS with those.
The problem is not name-resolution.

Our yarn-site.conf is being read for all the other values.
only the yarn.resourcemanager.scheduler.address is not properly loaded into the conf object.

I won't be able to do a hostname test (in place of the hard-code IP) until sometime late next week., Here is a copy of our common /etc/hosts:

::1        localhost
127.0.0.1  localhost loghost
10.1.0.220      35d47c6c-97c4-429c-a189-aeb5349eea43    loghost
127.0.0.1       pgbackup
10.1.26.11      qadoop-batch.apsalar.com
10.1.26.1       qadoop-nn001.apsalar.com
10.1.26.2       qadoop-nn002.apsalar.com
10.1.26.3       qadoop-jt001.apsalar.com
10.1.26.4       qadoop-d001.apsalar.com
10.1.26.5       qadoop-d002.apsalar.com
10.1.26.6       qadoop-d003.apsalar.com
10.1.26.7       qadoop-master1a.apsalar.com
10.1.26.8       qadoop-shard1a.apsalar.com
10.1.26.9       qadoop-shard2a.apsalar.com
, Here is a copy of our yarn-site.xml:

<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

   <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
   </property>

   <property>
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>

   <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>qadoop-nn001.apsalar.com</value>
   </property>

   <property>
      <name>yarn.resourcemanager.scheduler.address</name>
      <value>qadoop-nn001.apsalar.com:8030</value>
   </property>

   <property>
      <name>yarn.resourcemanager.address</name>
      <value>qadoop-nn001.apsalar.com:8032</value>
   </property>

   <property>
      <name>yarn.resourcemanager.webap.address</name>
      <value>qadoop-nn001.apsalar.com:8088</value>
   </property>

   <property>
      <name>yarn.resourcemanager.resource-tracker.address</name>
      <value>qadoop-nn001.apsalar.com:8031</value>
   </property>

   <property>
      <name>yarn.resourcemanager.admin.address</name>
      <value>qadoop-nn001.apsalar.com:8033</value>
   </property>

   <property>
      <name>yarn.log-aggregation-enable</name>
      <value>true</value>
   </property>

   <property>
      <description>Where to aggregate logs to.</description>
      <name>yarn.nodemanager.remote-app-log-dir</name>
      <value>/var/log/hadoop/apps</value>
   </property>

   <property>
      <name>yarn.web-proxy.address</name>
      <value>qadoop-nn001.apsalar.com:8088</value>
   </property>

</configuration>
, Yes, we have a proper /etc/hosts
That is not the problem.

I am able to make it work by forcing the conf to have our yarn scheduler
address IP.

, How many nodemanagers are running? If it more than 1 then I am thinking what would have happen in your case is yarn-site.xml never read by clent i.e oozi job but still you are able to submit the job because you might be submitting job from the local machine i.e where RM is running. So with default port job is able to submit , but when AppplicationManster is launched , it is launched in different machine where NodeManager is running. Since scheduler address is not loaded by any configuration, AM tries to connect default address i.e 0.0.0.0:8030 which never connect. 

I suggest that you can make sure your yarn-site.xml is loaded into classpath before submitting the job. So the AM gets the yarn.resourcemanager.scheduler.address and connect to RM. Otherway is explicitely set yarn.resourcemanager.scheduler.address  using job client., Rohith, Thank you very much for your help!  You are correct.  We are running 3 nodemanagers.  After adding our yarn-site.xml to the CLASSPATH, (and putting back original code) we are able to successfully submit and run an oozie job.  Our problem is solved., Closing as Invalid.

If there is any queries or basic environment problems , I suggest to use user mailing lists to ask queries.]