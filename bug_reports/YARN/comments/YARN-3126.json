[In fact I found this issue in cdh5.1.0-hadoop2.3.0, then I read the version2.3.0 codes, and this issue still exist.  This patch is submitted for version 2.3.0. , [~Xia Hu], does this problem still exist in trunk version?, I have no idea. But I found it seems no one have ask about or deal with this problem, and this morning I download the source code of 2.3.0, this problem still exist. , [~Xia Hu], I checked the latest trunk version. The problem is still there. Could u rebase a patch for the trunk? Normally we fix the problem in trunk, instead of previous released version. And we may need to get YARN-2083 committed firstly.
Hey, [~kasha], do u have time look YARN-2083?, patch based trunk., Hi Wei, I submit a new patch "resourcelimit-02",this one is based on the trunk version, which I download from github this morning. , [~ywskycn], [~ywskycn] how about this problem ?, in fairscheduler, it doesn't check if a new container's allocation may make this queue's used resources be more than its max resource limit. This patch is for this situation. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12697432/resourcelimit-02.patch
  against trunk revision e37ca22.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 2 warning messages.
        See https://builds.apache.org/job/PreCommit-YARN-Build/7035//artifact/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/7035//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/7035//console

This message is automatically generated., Hi [~Xia Hu], thanks for putting together a patch for this.  Could you add some unit tests to verify the fix?  , Add a unit test for this patch. , I have submitted a unit test just now, review it again, thx~, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |   5m 19s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 31s | There were no new javac warning messages. |
| {color:green}+1{color} | release audit |   0m 20s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 44s | There were no new checkstyle issues. |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 3  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 31s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 31s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   1m 16s | The patch appears to introduce 1 new Findbugs (version 3.0.0) warnings. |
| {color:red}-1{color} | yarn tests |  60m 19s | Tests failed in hadoop-yarn-server-resourcemanager. |
| | |  77m 34s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-yarn-server-resourcemanager |
|  |  Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.isHDFS; locked 66% of time  Unsynchronized access at FileSystemRMStateStore.java:66% of time  Unsynchronized access at FileSystemRMStateStore.java:[line 156] |
| Timed out tests | org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestNodeLabelContainerAllocation |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12733746/resourcelimit-test.patch |
| Optional Tests | javac unit findbugs checkstyle |
| git revision | trunk / 93972a3 |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/7993/artifact/patchprocess/whitespace.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-YARN-Build/7993/artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/7993/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/7993/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/7993/console |


This message was automatically generated., I think this issue is quite common, and we have met the same problem.
The root cause is that when we should make the max-limitation check in assignment, we should compare *current usage* + *resource to assign* with *max resource limit*. However when have resource to assign to a queue, we know only *current resource usage* and *max resource limit*, we don't know *resource to assign* until we assign resource to an appAttempt.
This patch seems add a additional check(checkQueueResourceLimit) on *leaf queue* then assign to AppAttempt, but *parent queue* resource usage may still over max resource limit.
Also we already have *FSQueue.assignContainerPreCheck* for max resource limit. If we add a new check, the former one seems to be unnecessary here.
[~kasha], would like to hear your thoughts., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red} 0m 4s {color} | {color:red} YARN-3126 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12733746/resourcelimit-test.patch |
| JIRA Issue | YARN-3126 |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/11091/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

, Commenting without looking into the related code. Thoughts:
# We should check it in one place. In other words, we should not duplicate the checks neither should we disperse it along multiple code paths.
# Would it be possible to pass this maxResources to assignContainer, so we could check either directly in assignContainer or assignContainerPreCheck? , Since it has been unassigned for a while, I am gonna take this. Hi [~Xia Hu], please let me know if you are still working on this., It is fixed by YARN-3655. So I just close it.]