[Per [~sseth], it is likely that you are confusing the ports because it is MiniYarnCluster setup where you are running multiple NMs on the same machine? The bug seems valid, but may be the analysis isn't. Not sure completely either ways. It'll be useful if you can capture RM logs specifically for this container., I've attached the output from one of the tests; the RM logs are intermixed in it; but its easy to just grep from the container in question. 
 
I've also attached the syslog from one of the containers ({{container_1394161202967_0004_01_000004}}) that had the problem.  I modified the NMTokenCache to print out the tokens whenever getToken is called, so that's in there too.  , Looking at the printouts I added to the NMTokenCache, I think I figured out some more:
During the tests, we run 2 NodeManagers.  With YARN-713, the NMTokenCache only ever has 1 token in it; when in the cases where a container is trying to use one NM and the token is for the other, we get the InvalidToken error.  I tried running without YARN-713, and the NMTokenCache usually has 2 tokens in it, so the containers are able to find the token in the NMTokenCache.  

I haven't had a chance to look into it more yet, but I did notice that YARN-713 changes NMTokenSecretManagerInRM's createAndGetNMTokens method, which returns a list of tokens, to be createAndGetNMToken, which returns a single token.  Perhaps that has something to do with this?  , We've now seen this problem in an actual cluster, not just Oozie's unit tests; so this is definitely a problem and not something funny we're doing in the tests.  

I've also determined that this only happens with the FairScheduler; the CapacityScheduler seems to work fine.

I've updated the name of the JIRA accordingly.  , [~rkanter], 
{code}
2014-03-06 19:01:24,731 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1394161202967_0004_01_000004 taskAttempt attempt_1394161202967_0004_m_000001_0
2014-03-06 19:01:24,733 INFO [ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1394161202967_0004_m_000000_0
2014-03-06 19:01:24,733 INFO [ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1394161202967_0004_m_000001_0
2014-03-06 19:01:24,734 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: AAA numTokens = 1
  NMToken :: 172.16.1.64:52707 :: 172.16.1.64:52707                             
2014-03-06 19:01:24,734 INFO [ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : 172.16.1.64:52707
2014-03-06 19:01:24,748 INFO [ContainerLauncher #1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: AAA numTokens = 1
  NMToken :: 172.16.1.64:52707 :: 172.16.1.64:52707  
{code}
How are you printing the logging? why two duplicate NMTokens printed? but numTokens == 1, Sorry, I didn't explain more specifically what I had printed out.  
Each line is a for a token and in this format: {{NMToken :: <key> :: <service>}}
where the {{<key>}} is the key from the hash map in NMTokenCache and the {{<service>}} is the service in the token.  So those end up being the same.

So, its only printing one token in that snippet, Taking this up to investigate. , Hi Karthik, thanks for taking it up. YARN-1839 filed, but I'm not sure whether this jira is related to that., Thanks for point out YARN-1839, [~jianhe].  That looks like possibly the same problem; or at least related.  The code snippet you mentioned in the other JIRA was something added by YARN-713 so that could be the problem., I'm seeing this on a real cluster, too, without running Oozie. Out of a job with 1000 tasks I typically see a few tasks early in the job's lifetime (first wave of task assignment) fail, all on the same host. EG:

{code}
14/03/14 19:15:38 INFO mapreduce.Job:  map 0% reduce 0%
14/03/14 19:15:42 INFO mapreduce.Job: Task Id : attempt_1394818402366_5229_m_000066_0, Status : FAILED
Container launch failed for container_1394818402366_5229_01_000074 : org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for d2208.halxg.cloudera.com:8041
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

14/03/14 19:15:42 INFO mapreduce.Job: Task Id : attempt_1394818402366_5229_m_000107_0, Status : FAILED
Container launch failed for container_1394818402366_5229_01_000118 : org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for d2208.halxg.cloudera.com:8041
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

14/03/14 19:15:51 INFO mapreduce.Job: Task Id : attempt_1394818402366_5229_m_000066_1, Status : FAILED
Container launch failed for container_1394818402366_5229_01_000135 : org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for d2208.halxg.cloudera.com:8041
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)
        at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}

, I tried the patch posted at YARN-1839 and it fixes the problem.  Marking this as a duplicate of that.]