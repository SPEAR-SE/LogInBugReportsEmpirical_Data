[Note that the killing of the nodemanager itself with SIGKILL should not cause the containers to be killed in itself.  Instead the problem seems to be that when the nodemanager restarts it is either failing to reacquire the containers that were running or it reacquires them and the RM fails to tell the NM to kill them when it re-registers.  Updating the summary accordingly.  Also by "the AM and its container" I assume you mean the application master and some other container that the AM launched.  Please correct me if I'm wrong.  

Is work-preserving nodemanager restart enabled on this cluster?  Without it nodemanagers cannot track containers that were previously running, so it will not be able to reacquire them and kill them.  If they don't exit on their own then they will "leak" and continue running outside of YARN's knowledge.  If that feature is not enabled on the nodemanager then this behavior is expected, since killing it with SIGKILL gave the nodemanager no chance to perform any container cleanup on its own.

If restart is enabled on the nodemanager then this behavior could be correct if the application running told the RM that containers should not be killed when AM attempts fail.  In that case the container should be left running and its up to the AM to reacquire it via some means.  (I believe the RM does provide a bit of help there in the AM-RM protocol.)

If the containers were supposed to be killed when the AM attempt failed then we need to figure out which of the two possibilities above is the problem.  Could you look in the NM logs and see if it said it was able to reacquire the previously running containers before it was killed?  If it didn't then we need to figure out why, and log snippets around the restart/recovery would be a big help.  If it did reacquire the containers and register to the RM with those containers then apparently the RM didn't tell the NM to kill the undesired containers.  In that case the log from the RM side around the time the NM re-registered would be helpful., [~jlowe] Thanks for your comments, very helpful.
yarn.resourcemanager.work-preserving-recovery.enabled is indeed set to false. The reason we have set it to false is because we run samza jobs on the yarn cluster and they don't work well with this feature turned on (https://issues.apache.org/jira/browse/SAMZA-750).

Apologies for my ignorance in this area, but if the application master (AM) is dead, shouldn't it be responsibility of the container to kill itself? I'd imagine every container should be required to heartbeat to its application master and killing itself if it misses a few?
, SAMZA-750 is discussing RM restart, but this is NM restart.  They are related but mostly independent features, and one can be enabled without the other.  Check if yarn.nodemanager.recovery.enabled=true on that node.  If you want to support rolling upgrades of the entire YARN cluster they both need to be enabled, but if you simply want to restart/upgrade a NodeManager independent of the ResourceManager then you can turn on nodemanager restart without resourcemanager restart.  NodeManager restart should be mostly invisible to applications except for interruptions in the auxiliary services on that node (e.g.: shuffle handler).

bq. if the application master (AM) is dead, shouldn't it be responsibility of the container to kill itself?

That is completely application framework dependent and not the responsibility of YARN.  A container is completely under the control of the application (i.e.: user code) and doesn't have to have any YARN code in it at all.  Theoretically one could write an application entirely in C or Go or whatever that generates compatible protocol buffers and adheres to the YARN RPC protocol semantics.  No YARN code would be running at all for that application or in any of its containers at that point.  (I know of no such applications, but it is theoretically possible.)

Also it is not a requirement that containers have an umbilical connection to the ApplicationMaster.  That choice is up to the application, and some applications don't do this (like the distributed shell sample YARN application).  MapReduce is an application framework that does have an umbilical connection, but if there's a bug in that app where tasks don't properly recognize the umbilical was severed then that's a bug in the app and not a bug in YARN.  Once the nodemanager died on the node, YARN lost all ability to control containers on that node.  If the container decides not to exit then that's an issue with the app more than an issue with YARN.  There's not much YARN can do about it since YARN's actor on that node is no longer present.

If NM restart is not enabled then the nodemanager should _not_ be killed with SIGKILL.  Simply kill it with SIGTERM and the nodemanager should attempt to kill all containers before shutting down.  Killing the NM with SIGKILL is normally only done when performing a work-preserving restart on the NM, and that requres that yarn.nodemanager.recovery.enabled=true on that node to function properly., [~jlowe] Setting yarn.nodemanager.recovery.enabled=true does solve the issue with orphaned containers.
Note that the SIGKILL was only done locally to emulate few production issues we had that caused nodemanagers to fall over.
Thanks very much for your clear explanation!]