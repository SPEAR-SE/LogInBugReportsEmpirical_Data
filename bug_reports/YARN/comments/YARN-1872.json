[Output from console., I faced this issue too. But the failure is a bit different from Ted's report:

{code}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell
-------------------------------------------------------------------------------
Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 74.081 sec <<< FAILURE! - in org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell
testDSShell(org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell)  Time elapsed: 0.032 sec  <<< FAILURE!
java.lang.AssertionError: null
        at org.junit.Assert.fail(Assert.java:92)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertTrue(Assert.java:54)
        at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShell(TestDistributedShell.java:186)
{code}
, The failure I reported is fixed on YARN-1873. Please ignore it and this JIRA should focus on the bug Ted's reported., I met the timeout too. But I can't reproduce it.
Could you reproduce the timeout? Can you attach the TestDistributedShell-output.txt under surefire-reports?, I reproduced it occasionally and fixed it., After the DistributedShell AM requested numTotalContainers containers, RM main allocate more than that. To keep this example AM simple, the "extra" containers are accepted and launched.

So we may get more than numTotalContainers completed containers. But the check of "numCompletedContainers.get() == numTotalContainers" is done outside of the loop "for (ContainerStatus containerStatus : completedContainers)". So there's chance that numCompletedContainers become greater than numTotalContainers but the flag "done" is not set. In such case, we'll get endless loop in finish().

The check is moved inside the loop in this patch. Another simple solution is to change
{code}
numCompletedContainers.get() == numTotalContainers
{code}
to:
{code}
numCompletedContainers.get() >= numTotalContainers
{code}, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12638402/YARN-1872.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/3508//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3508//console

This message is automatically generated., bq. After the DistributedShell AM requested numTotalContainers containers, RM main allocate more than that.

[~zhiguohong], thanks for working on the test failure. Do you know why RM is likely to allocate more containers than AM requested? Is it related to what YARN-1902 described?, Yes. It is. And MapRedue V2 AM contains some code to work around for this strange behavior.
I'll review YARN-1902 patch later.
But anyway, it's better to move the check to inside the loop (What's done in this patch).
, bq. But anyway, it's better to move the check to inside the loop (What's done in this patch).
It seems to be trick to set done to true before the loop is completed, given more than required containers are allocated.

However, the root problem seems to be AMRMClient is likely to allocate more containers than configured, and
{code}
numCompletedContainers.get() > numTotalContainers
{code}
is not expected.

So shall we close it as duplicate of YARN-1902?, I suggest to take this patch since it makes DistributedShell more robust regardless of AMRMClient accuracy.
And it realy fixes this failure.

In MapReduce V2 AM, there's similar logic to be tolerance of extra allocation., all current AMs have to deal with this event; surplus containers need to be released -does the AM do this once patched? This is important not so much for the test but because Distributed Shell is the foundation of most YARN apps -it needs the robustness for others to pick up, bq. all current AMs have to deal with this event;

This is why I'm thinking we should fix the issue in the scope of AMRMClient(Async), to isolate it from AMs. Given the AM use the client library, the developers don't need to worry about it. On the other hand, if the AM interacts with ApplicationMasterProtocol directly (like MR), they have to work out themselves.
, Loughran, please see YARN-1902.
But I suggest to take this patch besides YARN-1902., As a workaround for 2.4.1 release, +1 for [~zhiguohong]'s patch(non-binding).

[~zjshen], [~stevel@apache.org], how about solving the issue essentially on YARN-1902 against 2.5.0 release as Hong mentioned? What do you think?, I was assuming that it is a test only fix. It isn't. No point in putting wrong work arounds in DistShell main code. Let's fix it correctly. Cancelling the patch.

I think we should close this as dup of YARN-1902. Will do so in a day or two unless I hear strong objections., Also, IAC, this is an occassionally failing test. There are others like this, either due to code-bugs or otherwise, and this shouldn't be a blocker for 2.4.1. Reducing priority. Comment if you disagree., Hi, testDSShell fails with asser failed, don't know whether it is relevant:

https://builds.apache.org/job/Hadoop-Yarn-trunk/561/consoleText

testDSShell(org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell)  Time elapsed: 27.557 sec  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.yarn.applications.distributedshell.TestDistributedShell.testDSShell(TestDistributedShell.java:198)


Results :

Failed tests: 
  TestDistributedShell.testDSShell:198 expected:<1> but was:<0>

Tests run: 8, Failures: 1, Errors: 0, Skipped: 0
 , Binglin, I got same failure. The phenomenon and reason of your failure is different with this one reported by Ted Yu.
I fixed it by YARN-2081., Hi,  Vinod, this is not just a test failure. It occurs frequently in out real cluster. When this happens, the application remains running forever until it's killed manually.

And I don't think the fix is just a work around before YARN-1902 get in. It makes DistributedShell taking less assumption about outside, and more tolerant to unexpected behavior of outside., Moving bugs out of previously closed releases into the next minor release 2.8.0., Looks like this is no longer an issue.]