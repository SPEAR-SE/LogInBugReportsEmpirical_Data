[Is this dupe of YARN-4273?, I don't think this is the duplication of YARN-4273 as that one is about the leak of application (with step to kill application from users). This scenario should be well addressed as NM should recover and report the running containers to RM no matter if it has failover or not.
[~sandflee], do we see this is a real problem in the cluster? If so, would you put on more details, like: some exceptions in nm/rm log for us to understand what's going wrong there? Thanks!
CC to [~jlowe]., If I'm reading the problem description properly, the issue is the RM failover causing the RM to forget what containers are there and relying on the NM to tell it.  When the RM times out the node it cannot contact the node to tell it to kill all the containers, and it forgets that those containers are there after the RM failover.  Then later when the NM re-registers it will tell the RM that it has these containers that should be dead.  The NM was never told they should be dead, and the RM forgot about them before the NM re-registered.

If the application has exited in the interim then the containers will be killed as part of app shutdown handling, but as long as the app is still active then it looks like the containers will be allowed to exist despite the RM previously telling the AM that they were gone., yes, this's a problem in our cluster, our NM hangs for a long time because some thread we added not exit and we have not merge YARN-3585.  this happended  weeks ago,  logs are lost.

, thanks [~jlowe] , you have explained very clearly., Is there any plan to store NM info?  [~jlowe] [~djp] [~jianhe],  we could just store NM info not containers running on NM.
Without NM info, 
1,  containers could be leaked as in  this issue. 
2,  AM knows nothing if nm crashed forever and RM failover, There was some previous discussion of storing at least some state for NMs.  See YARN-2047 and YARN-2567.]