[Attaching v0 patch for review. cc/[~leftnoteasy], Sorry, earlier patch was having an error. Re-attaching correct patch., Thanks [~sunilg],

Haven't look at all the details, a little bit background for why this is required: This is majorly for intra-queue user-limit preemption, today's user-limit is designed for allocation instead of preemption. Because it only calculate a same limit for all the users in the queue, which is not correct. So we need to calculate per-queue limit (because some users may not need more resource, and some users may need more), this value will be used to do allocation and preemption. The goal is to make the behavior same to today's user limit as much as possible. But it is still possible that some minor behavior change could happen, this is why we want an option to enable it. 

+ [~eepayne]., Thanks [~sunilg] and [~leftnoteasy] for the heads up. I will look at this in the next few days., Updating v1 patch addressing few init issues. , Thanks [~sunilg] for working on this refactoring. Here are my comments for {{YARN-5889.v1.patch}}


- CapacitySchedulerConfiguration.java: 
-- {{getUserLimitAynschronously}} should be {{getUserLimitAsynchronously}}

- {{CapacitySchedule#serviceStart}}: Shouldn't this check before dereferencing {{computeUserLimitAsyncThread}}:
{code}
   public void serviceStart() throws Exception {
     startSchedulerThreads();
    computeUserLimitAsyncThread.start();
     super.serviceStart();
   }
{code}

- {{CapacitySchedule#ComputeUserLimitAsyncThread#run}}:
{code}
Thread.sleep(1);
{code}
It seems like this should be longer than 1 ms. Isn't the default scheduling interval 5 seconds? That may be too long, but I think it should be at least a second.

- {{CapacitySchedule#ComputeUserLimitAsyncThread#run}}:
This is just a very small nit, but it seems to me like {{getAllLeafQueues()}} should return a list of {{LeafQueue}}'s instead of a list of {{CSQueue}}'s.
{code}
List<CSQueue> leafQueues = cs.getAllLeafQueues();
{code}

- LeafQueue.java:
Another tiny nit, but since {{computeUserLimit}} and {{getComputedUserLimit}} have the same arguments, can the arguments to {{getComputedUserLimit}} be in the same order as those for {{computeUserLimit}}?

- {{LeafQueue#getComputedUserLimit}}
I don't think we want to always return {{Resources.unbounded()}} when {{userLimitPerSchedulingMode}} is null. If computing user limit the legacy way, the return value of the {{computeUserLimit}} method should be returned.

- ActiveUsersManager.java: Is the import of FiCaSchedulerApp needed?
{code}
import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp;
{code}
, Thanks [~eepayne]. Addressed most of the comments.

bq.It seems like this should be longer than 1 ms. Isn't the default scheduling interval 5 seconds? That may be too long, but I think it should be at least a second.
I could slightly explain here. I think 1msec is also smaller. It could be possible that containers are released and created very fast in a big cluster. Each such change in resource will have an impact on user-limit calculation. Earlier it was not affecting this problem since we were calculating user-limit in every allocation cycle. So we could try with this small timeout, and can do some SLS tests to see the trend. I will post some results very soon., {quote}
bq. It seems like this should be longer than 1 ms.
It could be possible that containers are released and created very fast in a big cluster.
{quote}

[~sunilg], I now realize that with this design, the {{preComputedUserLimit}} cache will become out of date very quickly if the {{ComputeUserLimitAsyncThread}} thread is not run in a very tight loop. Even with that, {{preComputedUserLimit}} could still be out of date at the moment the scheduler needs to fill a large request.

On the other hand, with this design the user limit resource is being calculated a lot more often than it is currently. Currently, it is only being calculated during the scheduler loop, and only then for apps that are asking for resources. However, this design calculates it twice every millisecond (once with partition exclusivity and once without). If a cluster is not full and has mostly apps with long-running containers, then this is being calculated thousands of times when it doesn't need to be.

Instead could we add a boolean flag to {{UserToPartitionRecord}}? This flag would be set when a container is allocated or releaseed for an app from that user. Then, whenever {{getComputedUserLimit}} is called, if the flag is set, it calls {{computeUserLimit}} and clears the flag. What do you think?
, Thanks [~eepayne]

Yes. for scheduler, 1ms is also smaller. It was a tradeoff to see the performance gain and its impact. With SLS test, i could be see good improvement   in allocation speed.

Now to bridge the gap, there are 2 cases
- How to make sure that every allocation gets correct and accurate user-limit value given computation happens at 1ms?
- In a lousy cluster, how can we save CPU cycles to prevent too much of unnecessary computations?

Yes, an ideal way is as suggested by you.
- Any change in resource (allocation and release of container  etc) for a given user could set a state variable. This will set off by the computation thread if next cycle falls immediate.
- Its not ideal to ask allocation thread to hold till computation. So by seeing this state variable, we might need to compute user-limit in same allocation thread. 

I was looking in second step to see how much impact it can cause if user-limit is slightly older. We may over allocate or we may under allocate. I think under-allocate scenario is fine as we will allocate more from next milli second. However overallocate scenario may be a worry. Still we have preemptions/opportunistic ways to handle this.

Ideally we were looking to avoid user-limit computation from same allocation thread. So after step 1), we can force the user-allocate thread to push for an immediate computation. Still there could some exceptionally rare case where user-limit thread is doing computation as per release/allocate demand. But another allocation thread (heartbeat) may also go in same time frame. If this is fine, I could update my patch to handle this case.

Thoughts?, bq. Any change in resource (allocation and release of container etc) for a given user could set a state variable. This will set off by the computation thread if next cycle falls immediate.
Just so we're on the same page and what I'm suggesting is more clear.

I'm suggesting that we remove the computation thread ({{ComputeUserLimitAsyncThread}}), and when {{LeafQueue#assignContainers}} or {{LeafQueue#releaseResource}} is called, we can set the flag in {{UserToPartitionRecord}}. This flag should also be set when queues are refreshed. {{getComputedUserLimit}} would then be called as it is in your patch, and compute user limit resource if the flag is set.

bq. Its not ideal to ask allocation thread to hold till computation. So by seeing this state variable, we might need to compute user-limit in same allocation thread.
I recognize that my suggested approach does the calculation during the allocation thread. My assertion would be that
# this calculation is being done currently in the allocation thread
# my suggested approach is an optimization that would decrease (sometimes greatly) the number of times the calculation happens
# my suggested approach would be more accurate than with a separate computation thread., Yes [~eepayne], I understood your view here.

However in ideal cases, we might need to compute user limit in allocation thread if there are more allocations happened in prior heartbeat or some release container happened between two heartbeats. This means that we will be doing same as what we do earlier too with some minor improvements in a busy cluster (I agree that normal clusters, we can see some improvement). Ideally when we tested with SLS, user-compute-limit was done under writelock and was consuming good amount of time.

If we are taking user-limit computation out of allocation thread, we have some good advantages:
- Unblocking allocation from computing user-limit
- Giving a read-only user-limit for other modules such as preemption (user-limit/priority etc)
- Such a user thread running from a user manager will be easier to maintain.
- Still this is configuration driven, hence user can know the minor limitations and choose to get more performance.

As I see now, there is only one case by which scheduler may get an older limit. 
- Container release/allocation happened
- CS placed a push-to-recompute-user-limit flag to ComputeUserLimitAsyncThread or Manager.
- ComputeUserLimitAsyncThread is computing the limit and is in that process. Yet to publish
- At same time, another allocation thread used old data to do one allocation.

I will now do some SLS tests with and without allocation thread and the suggested improvements. So we can also see the performance improvements over both., bq. This means that we will be doing same as what we do earlier too with some minor improvements in a busy cluster

It shouldn't take a busy cluster to see the improvement.  If a user is running many applications that are all asking for resources but the user has hit the user limit, today it will redundantly recompute the user limit for each application on each heartbeat.  The lazy-compute-when-dirty approach will not compute it at all unless a container has been allocated or released for that user in that queue.  I would argue that's much more than a minor improvement, and users hitting their limits is a common case on our clusters even when they're not completely full.

The asynchronous approach is very concerning to me.  We are essentially trading correctness for performance, and that seems to me like a reckless pursuit when there are still ways to improve performance without adding new race conditions and constraint violations.  Obviously moving the calculation outside of the allocate thread will show significant improvements in benchmarks, but those results don't show the cost of the scheduler violating its constraints.  IMHO that's a misleading result.

I also question the logic of relying on preemption and opportunistic containers to "solve" the constraint violation problems.  Both of those features aren't free.  Preemption loses work, and opportunistic containers aren't guaranteed to be allocated in a timely manner (or could in turn be preempted).  In theory this should eventually converge to a more correct constraint value, but I would argue at a cost of allocation latency and lost work.

This feature is blocking user-limit-based in-queue preemptions which we are very eager to see.  I propose we go with a simple approach that is easy to implement and simple to prove correctness.  Adding something that can violate the schedulers constraints doesn't seem necessary to unblock the in-queue preemption work.  Let's get that work unblocked and we can continue to discuss asynchronous constraint violation approaches in parallel., Thanks [~jlowe] for pitching in.

Yes, I understood the concern here. So I will improve this in line with the discussion with [~eepayne] done earlier and will explore more on asynchronous line once the preemption is done related to this.

As asynchronous way has more gaps as of now, i think i will spin off that discussion in another ticket and i will update a patch here so that  preemption work will go faster., Thanks [~sunilg] for working on the patch and suggestions from [~jlowe], [~eepayne]. 

Personally I think the title and desc are a little confusing.

First of all, I think the most important target of this JIRA is not improving performance. It is to make user-limit preemption correct. Currently we compute an unique user-limit value for each leaf queue, this is enough for allocation but not enough for preemption. Here is an example.

A queue has cap=max-cap=100, min-user-limit-percent=50, user-limit-factor=1, at time T, there're 2 users using resources:
{code}
u1.used = 75, u2.used = 25
{code}
Only u2 is active user,

According to existing user limit computation:
{code}
user_limit =
  round_up(
    min(
        max(current_capacity / #active_user,
             current_capacity * user_limit_percent),
        queue_capacity * user_limit_factor)),
    minimum_allocation)
{code} 
Computed user-limit=100, more than any user's usage, so there's nothing will be preempted.

We can give many other examples like:
{code}
minimum-user-limit-percent = 33
3 users:
u1.used = 50, u2.used = 20, u3.used = 30
u2/u3 are active users 
{code}
The computed user-limit = 50, which makes preemption cannot kick in.

This problem could happen when #active-user < #total-user. The problem is, at the allocation stage, we only need check active users. But in preemption, we need to preempt resource from non-active users.

To solve the problem, we need to compute user limit considering non-active users. If a non-active user uses less than minimum-user-limit, we can continue distribute its available quotas to other active users; in the other hand, if a non-active user uses more than minimum-user-limit, we could also get resource from the user. This computation is more expensive, it should be O(N), N is number of applications in the queue.

That is why we need an async thread to do all these stuffs: we cannot put a computation which is O(N) to allocation thread. To me, the common things between computation of (actual) user-limit and fair share (FS) are: 
- They're all too expensive to do when checking every application.
- They're all instant limit, no user should understand the computed instant limit. The instant limit and usage could keep changing, but it will converge to a balance over a period of time.

I haven't checked patch implantation yet. Please let us know your thoughts about the overall points. I don't want to make this change to block user-limit preemption effort too, so it will be more helpful if you could share ideas about how we can achieve user-limit preemption without the async thread approach.

Thanks,, Thanks [~leftnoteasy]. I am still absorbing your previous comment, but I have couple of thoughts.

bq. That is why we need an async thread to do all these stuffs: we cannot put a computation which is O(N) to allocation thread.
If all apps are asking for resources, then the current algorithm already does this calculation in the allocation thread.

bq. This computation is more expensive, it should be O(N), N is number of applications in the queue.
I believe that N is the number of users, not applications. So, theoretically, the calculations could be based on each user's {{used}} value. Is that accurate?
, bq. If all apps are asking for resources, then the current algorithm already does this calculation in the allocation thread.
It may not be true if users uses / asks different resources, and order of application matters as well.

bq. I believe that N is the number of users, not applications. So, theoretically, the calculations could be based on each user's used value. Is that accurate?
Maybe not, as I mentioned above, order of application matters. Per my understanding, we have to iterate all apps according to FIFO order and calculate UL., bq. To solve the problem, we need to compute user limit considering non-active users. If a non-active user uses less than minimum-user-limit, we can continue distribute its available quotas to other active users; in the other hand, if a non-active user uses more than minimum-user-limit, we could also get resource from the user.

As I understand it, for allocation purposes we need to compute the user limit where the number of users is the number of users who have at least one application that is requesting resources.  For preemption purposes, we need to compute the user limit where the number of users is the total number of users who have at least one app in the queue (whether they have apps that are requesting or not).

bq. This computation is more expensive, it should be O(N), N is number of applications in the queue.

I don't see how this is related to the number of applications.  If you look at how the user limit is calculated, there are no terms in that calculation that have anything to do with how many apps the user has.  Am I missing something?  If so, maybe an example where number of applications or order of applications would help clarify.

I still think a simple flag to indicate a user limit needs to be recomputed would go a long way here.  We are already tracking total resources associated with each user in each queue in the User structure and caching the user limit there as well.  We could add a flag to indicate the cached user limit needs to be recalculated, and when we go to get the cached value it can on-the-fly recalculate it if it is dirty.  The calculation would only become dirty if one of the following events occur:
- An allocation is assigned to one of the user's apps
- A container associated with one of the user's apps completes
- A user becomes active in the queue (i.e.: now has apps requesting resources)
- A user becomes inactive in the queue (i.e.: no longer has apps requesting resources)
- The queue capacity changes (e.g.: nodes added/removed)
- The queue settings are refreshed

We can speed up the last four by having a queue-level sequence number that is incremented when active users or queue changes.  The User structure can cache the sequence number used when the user limit is recalculated and compare with the queue sequence number to know a user's limit needs recalculating.  This can also become the user-level flag when containers are allocated/completed by setting the cached sequence number in the User structure to the queue's sequence number minus one, which will force the sequence numbers to mismatch and cause a recalculation when the user limit is requested.

The idea here is that we will only calculate the user limit lazily and only when absolutely necessary.  This will be much faster than what we do today and not require asynchronous computation that breaks the constraints of the scheduler.  For purposes of doing preemptions, we can have the same concept used for a separate cached user limit, one that considers all users instead of only active ones.

When it comes time to calculate preemptions (which are only necessary when the queue cannot get more resources), we can sort the users by how far they are beyond their all-users user limit (either in absolute terms or by percentage).  This may or may not require a computation depending upon whether the cached value is out of date.  Then we can walk down the list starting with the user most past their limit.  We can stop traversing when we either preempt enough resources or we get to a user that is below their limit.



, Thanks [~jlowe] for such detailed suggestions, after look at existing logic and your solution, I finally understand why we have different proposals:

The first thing we need to decide is, what the semantic of minimum-user-limit-percent (MULP) should be.
From the doc: https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html,
The MULP is not a "minimum guarantee" at all, it is actually upper limit of users (which of course will be capped by user-limit-factor and queue-maximum-capacity):
bq. "... If a third user submits an application, no single user can use more than 33% of the queue resources ..."

This semantic is clear enough when #users <= (100 / MULP), but it is unclear when when #users > (100 / MULP). For example:

{code}
Let's say we have 5 apps belong to 5 users:

App: a1, a2, a3, a4, a5 
Usr: u1, u2, u3, u4, u5

MULP set to 25.
At the time=T, resource usage: a1=25,a2=20,a3=30,a4=20,a5=5 

Assume now a4/a5 are active application, which one should get the next available resource?
{code}

I believe it should be u4, but how about next example:
{code}
Let's say we have 6 apps belong to 5 users:

App: a1, a2, a3, a4, a5, a6
Usr: u1, u2, u3, u4, u5, u4

MULP set to 25.
At the time=T, resource usage: a1=25,a2=20,a3=30,a4=15,a5=5,a6=5

Assume now a5/a6 are active application, which one should get the next available resource?
{code}

Existing behavior in scheduler is a5 get resource first. But with this, we cannot have guaranteed capacity of user any more. Since we have MULP set to 25, but the user u5 get new resource before u4 reaches configured MULP.

If we all agree this behavior (FIFO order comes before user limit), we can use approach from Jason. In the other hand, if we want the first (100/MULP) users have guaranteed (100/MULP) capacity, the order of application mattters.

_TL;DR_

Now I change my mind, it's better to make the behavior consistent :). So we don't have to make user limit calculated in a separated thread. As proposed by Jason, we can calculate two user limits:

1) Active user limit for allocation: We will use resource-used-by-active-users and #active-users to calculate active user limit.

{code}
active-user-limit = min(
        max(resource-used-by-active-users, 
            queue-configured-resource - resource-used-by-non-active-users)
             / min(#active-users, 100 / MULP),
        queue-configured-resource * ULP)
{code}

This looks very similar to how we compute user limit today, the only difference is it uses resource-used-by-active-users instead of total-resource because we should fairly divide available resource among active users in the queue.

2) User limit for preemption:

When we doing preemption, what we will do is:

{code}
all-user-limit = min(
        max(resource-used-by-all-users, 
            queue-configured-resource) / min(#users, 100 / MULP),
        queue-configured-resource * ULP)
{code}

Thoughts?, +1 for keeping the user limit behavior consistent with what it does today.  Resources will still be offered to apps in priority order (typically FIFO unless overridden by the app's priority), and any user under the minimum limit should still be able to receive that resource.  As such I'm not sure I agree with the proposal for active user limits.  It appears that limit can go below the configured minimum if a lot of users are all asking for the available resources.  That would change the behavior and affect app priorities beyond what the user limit does today.  For purposes of unblocking the user limit preemption work, I'd like to preserve the current computation so adding the preemption feature doesn't affect the behavior of the scheduler even when the feature isn't used.

Speaking of preemption, we will need to be careful on how the two limits are applied to prevent oscillations.  If resources are available then the active user limit should be applied as it is today.  However if resources are not available then the preemption limit should be applied not only to the user being targeted for preemption but also to the user who is requesting the resource to trigger the preemption.  If we mix the two limits then preemption can oscillate between two users because the active limit can be larger than the preemption limit.

For calculating the preemption all-user-limit, I think the equation can be simplified to absolute queue capacity * MULP.  I personally do not want preemptions to give users resources beyond the minimum.  For example if the MULP is configured to 25% and there are two users in the queue, A at 70% usage and B at 30%, I'd rather not lose work by shooting A's containers to give B resources beyond the configured minimum limit.  Preemptions can be very costly to an application, so I don't think this should be completely fair (that's the job of FairScheduler).  We should only preempt work to get users up to the minimum limit and only if others are above the minimum limit. Thoughts on this?
, bq. If we all agree this behavior (FIFO order comes before user limit), we can use approach from Jason.
I agree with this statement. I think this is the current scheduler's behavior.

{quote}
I personally do not want preemptions to give users resources beyond the minimum. For example if the MULP is configured to 25% and there are two users in the queue, A at 70% usage and B at 30%, I'd rather not lose work by shooting A's containers to give B resources beyond the configured minimum limit. Preemptions can be very costly to an application, so I don't think this should be completely fair (that's the job of FairScheduler). We should only preempt work to get users up to the minimum limit and only if others are above the minimum limit. Thoughts on this?
{quote}
I agree. I recognize that if A gives up resources, the current behavior of the scheduler is to give those to B. But preempting A's resources under these circumstances is not what we want because the cost of lost work is high.
, Thanks [~jlowe], [~eepayne].

bq. As such I'm not sure I agree with the proposal for active user limits. It appears that limit can go below the configured minimum if a lot of users are all asking for the available resources. That would change the behavior and affect app priorities beyond what the user limit does today.

I still think we may need to change the behavior a little bit, today's behavior allow first user go beyond user limit a lot.
For example:
{code} 
5 users, 5 apps, MULP=20, queue-configured-resource=100
App: a1, a2, a3, a4, a5 
Usr: u1, u2, u3, u4, u5

At the time=T, resource usage: a1=25,a2=20,a3=30,a4=20,a5=5; a4/a5 are active user
{code}
Today's logic will get user-limit = 100 / 2 = 50, so if there's more available resource, a4 can get up to 50 resource, and a5 will be starved.

This causes oscillations if we preempt when user's usage beyond {{queue-capacity * MULP}}, a4 will get available resource, and soon it will be preempted because it above {{queue-capacity * MULP}}. 

So my proposal is to have a new user limit calculation for active users, it should be:

{code}
active-user-limit = max(
   resource-used-by-active-users / #active-users,
   queue-capacity * MULP
)
{code}

This should addressed your point:
bq. limit can go below the configured minimum if a lot of users are all asking for the available resources 
Since it make sure each user can get at least queue-capacity * MULP, and it avoid one user dominate all queue's available resource like today. 



bq. For calculating the preemption all-user-limit, I think the equation can be simplified to absolute queue capacity * MULP.
Generally agree, we can discuss more when implementing preemption logic.

bq. We should only preempt work to get users up to the minimum limit and only if others are above the minimum limit. Thoughts on this?
Totally agree.

Thoughts? [~sunilg]., I think I can get behind a proposal that preserves the FIFO/priority behavior at least up through usage < MULP and then becomes fair once a user is beyond MULP.  We need to be careful that active users are still allowed to grow to fill the queue (e.g.: a single user in the queue doesn't get capped at MULP).  Also note that a cached active user limit calculation will become out-of-date more often than before, since resource-used-by-active-users changes a lot more often than queue-capacity does.  Essentially any resource change for an active user invalidates the user limit cache for all active users.  I'm not too worried about that, just pointing out that it will get computed more often with the new proposal.
, We should be able to make resource-used-by-active-users always up-to-date, if there's any changes on active user set or used resource from active users, we can update the field, it should not have significant impact on performance., Generally I am also agreeing with the direction at which we are going towards.

Few points from end:
- For preemption calculation, one of the main problem could have been about the *free resources* in the queue even when some users are over-utilizing its resource quota (these users could become active/non-active). Because preemption module will be handling {free_resources + to_be_preempted_resources} and need to think more like scheduler.
- Above point will play a big factor to decide when preemption need to kick in. It could be when free/used become very smaller OR it could also be when there is a lot of violation from few users which holds resource more than MULP but became non-active users.

As far as I understood, we will still have pre-computed user-limit model. But this cache will be computed based on any event change on resource changes for non-active users. I think in a busier and short-living app's cluster, we may recalculate more. But I think preemption module will have a better accuracy.

On this note, could I update a patch with approach mentioned above. I think free resource also need to be part to trigger preemption. But for user-limit calculation, I will be making changes in {{ActiveUserManager}} to track of non-active-users as well with a state to reflect changes in resource., bq.  I think in a busier and short-living app's cluster, we may recalculate more.
This is probably true for the active-user limit. For the preemption limit, my thinking is that it only needs to be calculated during a preemption cycle. Thoughts?, bq. For preemption calculation, one of the main problem could have been about the free resources
I think so, but I think the *minimum* preemption threshold is we should not preempt res from user if used of the user < queue-capacity * MULP. The higher threshold is, the safer of the preemption. 

bq. I think in a busier and short-living app's cluster, we may recalculate more.
I agree with [~eepayne], we should be able to make preemption / allocation UL calculation independent (or at least it's better to not allocation UL has dependency on preemption UL). [~sunilg] could you add some details here?

bq. On this note, could I update a patch with approach mentioned above.
Please go ahead when you get chance :). We should generally agree the approach, there are some debatable details, but starting prototype can help us understand the scope.

Thoughts?

, Thanks [~leftnoteasy] for the clarification.
I will update a patch soon., Sorry for the slight delay here. I will put up a patch later today as per discussion., Attaching an initial version of patch as per discussion., Thanks [~sunilg] for providing this work.

I am concerned about the way in which {{LeafQueue#getComputedUserLimit}} and {{LeafQueue#getComputedActiveUserLimit}} are deciding when to recompute user limits.
  - In the case of {{getComputedActiveUserLimit}}, if number of active users has increased or decreased, all active users in {{preComputedActiveUserLimit}} are invalided, and not just the one that was activated/deactivated. This requires recalculation for other users when it is not necessary.
  - In the case of {{getComputedUserLimit}}, comparing a cached {{lastUserCount}} with {{users.size()}} could miss occasions when recomputation would be necessary.

My understanding of [~jlowe]'s [comment (above)|https://issues.apache.org/jira/browse/YARN-5889?focusedCommentId=15745552&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15745552] is that the {{LeafQueue}} would increment a change counter which would be cached in each {{User}}'s object and then would be used to determine when recalculation of user limit should occur.

Rather than document each occurrence, I have uploaded a patch with my suggestions. {{YARN-5889.0001.suggested.patchnotes}} is the diff between {{YARN-5889.0001.patch}} and my suggestions.

I haven't tested all of the scenarios yet (I'll complete that soon), but I think this covers my concerns about the current patch, and I think it is more efficient. Please have a look and let me know your thoughts.
, Thanks [~eepayne] for the detailed explanation.

I was also having similar thought. Jus one point to clarify here, 
bq. if number of active users has increased or decreased, all active users in preComputedActiveUserLimit are invalided, and not just the one that was activated/deactivated. This requires recalculation for other users when it is not necessary.

Since number of active users are changed, we need to recalculate all active users limits, correct?. Because we divide total-resource-used-byactive-user with active-user count. In the proposed changed patch also, cached limit will be different with actual user count when we query user-limit for that user.
In my patch, i cleared all map because of that. could you please help to elaborate a little more.

I also feel cachedLimit make code more simpler, hence no issue in making change. However I need to have 2 cacheLimit in user data structure (one for active user and another for all users). Is my thinking in line with yours. pls help to clarify. Thank You.
, bq. Since number of active users are changed, we need to recalculate all active users limits, correct?
Correct. Comparing each user's "cached recompute count" value against the the queue's value should trigger that to happen when {{getComputedActiveUserLimit}} or {{getComputedUserLimit}} is called for each specific user.

bq. I need to have 2 cacheLimit in user data structure (one for active user and another for all users).
Are you asking if we need two versions of {{cachedRecalcULCount}} in the {{User}} class? If that's the question, then, no, I don't think so. The queue value will change for all of the conditions outlined in [~jlowe]'s [comment (above)|https://issues.apache.org/jira/browse/YARN-5889?focusedCommentId=15745552&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15745552], and that will trigger the recalculation.

However, I do think {{reComputeUserLimits}} needs to be modified to update {{preComputedUserLimit.get}} when recomputing user limits. That is not done anywhare in the current patch., Thanks [~sunilg] for providing the patch and reviews from [~eepayne].

Quickly scanned the patch,

If everybody agrees with approach in my comment: https://issues.apache.org/jira/browse/YARN-5889?focusedCommentId=15749129&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15749129

{code} 
active-user-limit = max(
   resource-used-by-active-users / #active-users,
   queue-capacity * MULP
)
{code} 

We can:
1) Update {{resource-used-by-active-users}} when any user enters/exits set of active users.
2) Update #active-users (using approaches like ActiveUsersManager approach)

Both of #1/#2 can be done within the same thread, and since we will have an identical UL for all the active users, I think this can be done without adding a new thread. Please correct me if I'm wrong.

It looks like Sunil and Eric have different implementation suggestions, I have looked at details of the two approaches. However I think the goals are:
a. Can always get up-to-date UL
b. Avoid re-computation of UL as much as possible.
Any approach can achieve above goals is good to me.

And in addition, we can add a UsersManager to LQ to manager all user-related information, such as user-limit / active-user-limit / #active-users. 
Ideally, LQ/PCPP should get activeUserLimit / userLimit from UsersManager

And for last, we can look at logics for #active-users per partition, now we have an identical #active-users for all partition, which causes some problems. Since we will make a major changes to logics around this, it could be a good chance to fix the problem as well., Thanks [~leftnoteasy] for your review and comments.

bq. If everybody agrees with approach in my comment
I agree. I think [~jlowe] summed it up well when he said your "... proposal ... preserves the FIFO/priority behavior at least up through usage < MULP and then becomes fair once a user is beyond MULP."

{quote}
And in addition, we can add a UsersManager to LQ
...
And for last, we can look at logics for #active-users per partition, now we have an identical #active-users for all partition
{quote}
Let's please do these as separate JIRAs. We are extremely anxious to move this JIRA forward since it is blocking YARN-2113 (user limit-based intra-queue preemption)., bq. Let's please do these as separate JIRAs. We are extremely anxious to move this JIRA forward since it is blocking YARN-2113 (user limit-based intra-queue preemption).
I understand, however I prefer to do the refactoring together with the patch, If we don't actively do refactoring to make a clean code structure with major behavior changes, it will cause a lot of trouble to maintain the code and add new functionalities. I'm OK with moving other changes like partition-related changes to a separate JIRA if it needs considerable effort. , [~sunilg],
bq. However, I do think {{reComputeUserLimits}} needs to be modified to update {{preComputedUserLimit}} when recomputing user limits. That is not done anywhare in the current patch.
Actually, we may want to have separate methods for {{reComputeUserLimits}} and {{reComputeActiveUserLimits}}, Thank You Eric. Yes. I missed this. Will update in next patch., Thanks [~leftnoteasy] and [~eepayne]

Attached patch has below changes
- Used cahcedLimit model suggested by [~jlowe] and [~eepayne] to invalidate computed user-limits
- In the attached notes patch from Eric, {{recalcULCount}} is changed in {{LQ#reinitialize}}. But I am already changing it in {{updateClusterResource}}. This will be invoked when node is added/removed, or queue s reinitialized. So we can skip it from {{reinitialize}}, correct?
- I created a class named {{UsersManager}} and moved all user-limit computation etc to that class. This class also will have activeUsersManager instance as well. Its a refactoring majorly.

Please share your thoughts, and I will do some more tests with same and update feedback., Thanks [~sunilg],

Following are my comments for overall code structure and call flow:

1) LeafQueue:
- Several unused members, could you check? 
- Can we move the users map to UsersManager? Ideally all operations on users should be redirected to UsersManager (UM)
- recalculateULCount is implementation details of user limit calculation, better to be moved to UM.
- Move all user-limit related configurations parameter (like ULF) to UM? Ideally UM should be more self-contained to make less dependencies and risk of deadlock.

2) UsersManager
- Better to move to cpaacity package, since it handles CS-only functionalities like user limit.
- Add a method like {{userLimitNeedsRecompute}} to handle the original logics of LQ#recalculateULCount
- User#setCachedCount, should we invalidateUL for the user who allocates/releases containers, or we should invalidate all user limit? I think the latter one is more safe to me. If you agree, I suggest LQ to call UM#userLimitNeedsRecompute to notify UM. 

3) UM, logics to compute UL
First, the UL is classified by user-name, active state, scheduling-mode, partition. However I think we don't need user-name. Existing UL will be identical for users in active set and users in all-set.
Second, existing logic automatically computes all schedulingMode, which may not necessary. The ignore-exclusivity is not common used, we can compute it only when necessary.

If you agree above, we can simplify API a little bit, we only need userName (to get if it's an activeUser), clusterResource, partition. ResourceCalculator can be stored inside UM, we don't need to pass it as parameter everytime.

And the call flow may look like:
{code}
UM#getActiveUserLimit(userName, clusterResource, partition, schedulingMode) {
	if (needRecompute) {
		return recompute(userName, clusterResource, partition, schedulingMode)
	}
	return getCachedActiveUserLimit(userName, clusterResource, partition, schedulingMode);
}
{code}

4) ActiveUserManager
- I think we don't need to use the class in CS. Adding {{Set<ApplicationId>}} of UM#User, and add other fields to UM. It could have some duplicated code, but the code structure will be more clean., [~sunilg],
- Shoule {{resetUserAddedOrRemoved}} just be {{setUserAddedOrRemoved}}
- {{LeafQueue}}: I think {{totalUserConsumedRatio}} should be removed, since it'
s not used.
- {{LeafQueue#recalculateULCount}} / {{UsersManager#User#cachedULCount}}: I know I came up with the name originally, but I think a better name would be {{recalculateUL}}
- {{getComputedActiveUserLimit}} / {{getComputedUserLimit}}: User's {{cachedULCount}} needs to be updated when the UL is recomputed or else it will always be out of sync and will always be recomputed:
{code}
    if (userLimitPerSchedulingMode == null
        || user.getCachedULCount() != lQueue.getRecalculateULCount()) {
      userLimitPerSchedulingMode = reComputeUserLimits(rc, userName,
          nodePartition, clusterResource, false);
      user.setCachedULCount(lQueue.getRecalculateULCount());
    }
{code}

[~leftnoteasy],
bq. User#setCachedCount, should we invalidateUL for the user who allocates/releases containers, or we should invalidate all user limit? I think the latter one is more safe to me.
Yes, unfortunately, I think that once the queue goes above its guarantee, the ratio will change when containers are allocated or released. We may be able to do an optimization to only reset the specific user's count when the queue is under its guarantee and all users when it is over, but that may not be worth the added complexity.
, Thanks for comments. Generally makes sense. I ll update a patch in a shortwhile., Thanks [~leftnoteasy] and [~eepayne] for the valuable comments.

bq.We may be able to do an optimization to only reset the specific user's count when the queue is under its guarantee and all users when it is over, but that may not be worth the added complexity.
I agree. We can validate all userlimit when a container is allocated or released. Earlier we were invalidating user limit for respective user. Now we will invalidate all. Correct?

bq.I think we don't need to use the class in CS. Adding Set<ApplicationId> of UM#User, and add other fields to UM. It could have some duplicated code, but the code structure will be more clean.
I would like to do this in separate jira. {{LQ#getActiveUserManager}} is reference from various classes as well as around 7 test class. Since we still have the functionality in tact, I will removed the use of activeUserManager from UM in another ticket.


, Thanks for update the patch, my comments for ver.3 patch.

1) LQ:
- getUserAndAddIfAbsent could be move to UM
- addUser/removeUser could be private method of UM
- getUsers -> getUsersInfo, and move implemenation to UM
- getComputedActiveUserLimit -> getComputedResourceLimitForActiveUsers (or shorter name)
- getComputedUserLimit -> getComputedResourceLimitForAllUsers (mostly for more consistency naming to above, and more acurate and less ambiguous)
- Add more description to above two methods.
- Is it better to move recalculateQueueUsageRatio/calculateUserUsageRatio into UM, and recalculateQueueUsageRatio can be renamed to "clusterResourceUpdated"
- inc/decUsedResource, keep User reference to avoid hit hashMap twice.
- Not caused by your patch: could you add a desc to UsageRatios?

2) UM:
- UM#userCountToRecalculateUserlimit could be AtomicLong to avoid writelock and less frequently wrapped to MIN_VALUE. Rename it to latestVersion.
- rename UM#getUserCountToRecalculateUserlimit -> getLatestVersion and readlock is not required. 
- A potential race condition:
{code} 
 ....
    // In allocation thread, cross check whether user became active or not.
    // Such a change needs recomputation for all user-limits.
    if (activeUsersManager.isUserAddedOrRemoved()) {  // 1
      // Clear computed user-limits and ask for recomputation. 
      userLimitNeedsRecompute(); // 2

      // reset the flag so that we can further track for active users.
      activeUsersManager.setUserAddedOrRemoved(false); // 3
    }
 ....
{code} 
If thread#1 at  // 2, and thread #2 calls setUserAddedOrRemoved, the {{isUserAddedOrRemoved}} could be overwritten and cannot properly trigger another update.
To simplify interface and avoid the race condition, can we change {{isUserAddedOrRemoved}} to use an AtomicBoolean, invokes getAndSet(false) to do the "flip" operation.
- Renaming {{userCountToRecalculateUserlimit}} to {{localVersion}}?
- Similiarily,
{code}
    if (isRecomputeNeeded(user, userLimitPerSchedulingMode)) {
      // recompute
      userLimitPerSchedulingMode = reComputeUserLimits(rc, userName,
          nodePartition, clusterResource, schedulingMode, false);

      // update user count to cache so that we can avoid recompute if no major
      // changes.
      user.setUserCountToRecalculateUserlimit(
          getUserCountToRecalculateUserlimit());
    }
{code}
To avoid race condition, is it better to save value of "latestVersion" and set it to "local" version, pesudo code like:
{code}
   int latestVersion = UM#getLatestVersion
   if (getLocalVersion != latestVersion) {
       // recompute ...
       setLocalVersion(latestVersion);
   }
{code}
- {{getComputedUserLimit}} / {{getComputedActiveUserLimit}}: I'm wondering if the UserToPartitionRecord is required, since existing computation doesn't need the "user" field.
- Does existing logic handle following scenario? Assume userLimitPerSchedulingMode is null and no new changes to user-limit, will NPE happen if we get a RESPECT_EXCLUSIVITY first and then get a IGNORE_EXCLUSIVITY?
- Inside {{computeUserLimit}}, it calls {{user.setUserResourceLimit(userLimitResource);}}, I think we need to file a separate JIRA to make sure REST API / web UI can get expected UL.
- TODO, after you update the patch, need to double check locks of UM., Thanks [~leftnoteasy] for the detailed comments.

bq.I'm wondering if the UserToPartitionRecord is required, since existing computation doesn't need the "user" field.
We still update {{user.setUserResourceLimit(userLimitResource);}} while computing user limit.

Yes. I will also take one more look regarding lock. Also will raise another ticket for REST api., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 25s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 31 new + 499 unchanged - 8 fixed = 530 total (was 507) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 4 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m  5s{color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 7 new + 0 unchanged - 0 fixed = 7 total (was 0) {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 18s{color} | {color:red} hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager generated 2 new + 913 unchanged - 0 fixed = 915 total (was 913) {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 38m 54s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 60m 24s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
|  |  Comparison of String objects using == or != in org.apache.hadoop.yarn.server.resourcemanager.UserToPartitionRecord.equals(Object)   At UserToPartitionRecord.java:== or != in org.apache.hadoop.yarn.server.resourcemanager.UserToPartitionRecord.equals(Object)   At UserToPartitionRecord.java:[line 72] |
|  |  Comparison of String objects using == or != in org.apache.hadoop.yarn.server.resourcemanager.UserToPartitionRecord.equals(Object)   At UserToPartitionRecord.java:== or != in org.apache.hadoop.yarn.server.resourcemanager.UserToPartitionRecord.equals(Object)   At UserToPartitionRecord.java:[line 71] |
|  |  Increment of volatile field org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.activeApplications in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.activateApplication()  At UsersManager.java:in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.activateApplication()  At UsersManager.java:[line 204] |
|  |  Increment of volatile field org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.pendingApplications in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.activateApplication()  At UsersManager.java:in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.activateApplication()  At UsersManager.java:[line 203] |
|  |  Increment of volatile field org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.activeApplications in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.finishApplication(boolean)  At UsersManager.java:in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.finishApplication(boolean)  At UsersManager.java:[line 214] |
|  |  Increment of volatile field org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.pendingApplications in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.finishApplication(boolean)  At UsersManager.java:in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.finishApplication(boolean)  At UsersManager.java:[line 216] |
|  |  Increment of volatile field org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.pendingApplications in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.submitApplication()  At UsersManager.java:in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User.submitApplication()  At UsersManager.java:[line 194] |
| Failed junit tests | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestNodeLabelContainerAllocation |
|   | hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicyMockFramework |
|   | hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicyIntraQueue |
|   | hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicyForReservedContainers |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerNodeLabelUpdate |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimitsByPartition |
|   | hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicyForNodePartitions |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5889 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12847642/YARN-5889.0004.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 28a675b13d26 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2f8e9b7 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14661/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/14661/artifact/patchprocess/whitespace-eol.txt |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/14661/artifact/patchprocess/new-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/14661/artifact/patchprocess/diff-javadoc-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/14661/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14661/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14661/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, There are few findbugs errors and test failures. I think I ll wait for comments on latest patch and will fix these along with that., Thanks [~sunilg] for updating the patch.

Some minor comments:

1) I made a mistake of my previous comment: recalculateQueueUsageRatio should belong to LQ, and it invokes UM#calculateUserUsageRatio.

2) UM#latestVersionOfUserCount is a little bit confusing, since it doesn't only consider user count, how about rename it to latestVersionOfUsersState? (And update comments, getters/setters accordingly). 

3) ActiveUM#isUserAddedOrRemoved, parameter is not required since it is always false.

4) Locks of UM:
- Add writeLock for following code block:
{code}
    long latestVersionOfUserCount = getLatestVersionOfUserCount();
    if (isRecomputeNeeded(user, userLimitPerSchedulingMode,
        latestVersionOfUserCount, schedulingMode)) {
      // recompute
      userLimitPerSchedulingMode = reComputeUserLimits(rc, userName,
          nodePartition, clusterResource, schedulingMode, true);

      // update user count to cache so that we can avoid recompute if no major
      // changes.
      user.setLocalVersionOfUserCount(latestVersionOfUserCount);
    }
{code}
For getComputedResourceLimitFor*Users.
This can make sure update of UL can be sequentially done and version increases while updating UL can always trigger another update.

5) Regarding to UserToPartitionRecord and computeUserLimit: 
My gut feeling is that we can optimize/simplify this part a little bit, let's revisit it after this patch: This patch could lead to wrong REST UL computation (returns all user limit instead of active user limit), and it recomputes UL for every user instead of every partition., Thanks [~leftnoteasy]
Attaching new patch addressing the comments., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 38s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 26s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 21 new + 496 unchanged - 11 fixed = 517 total (was 507) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 3 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 41m 20s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 67m 15s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestNodeLabelContainerAllocation |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerNodeLabelUpdate |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimitsByPartition |
|   | hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicyIntraQueue |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5889 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12847786/YARN-5889.0005.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux bfa3dfd5f0fc 3.13.0-105-generic #152-Ubuntu SMP Fri Dec 2 15:37:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 7ee8be1 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14673/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/14673/artifact/patchprocess/whitespace-eol.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/14673/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14673/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14673/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, [~leftnoteasy] and [~sunilg], I'm sorry for coming back to this point, but I just now realized the full consequences.
{code: title=UsersManager#computeUserLimit}
active-user-limit = max(
   resource-used-by-active-users / #active-users,
   queue-capacity * MULP
)
{code}
With the above algorithm, {{active-user-limit}} never goes above {{resource-used-by-active-users / #active-users}} if MULP is less than 100%. I think this is because {{consumed}} is never greater than {{queue-capacity}} in that case.

That is to say:
- {{queue-capacity}} = {{partitionResource * queueAbsCapPerPartition}}
- {{queue-capacity}} = {{(consumed < queue-capacity) ? queue-capacity : (consumed + minAllocation)}}

Since {{consumed}} never gets over {{queue-capacity}} when MULP is less than 100%, {{queue-capacity}} will never equal {{consumed + minAllocation}}.

I have tested this to verify., [~eepayne], just want to clarify, since UL is a {{>}} check instead of {{>=}}, it should be able to increase until queue's max capacity, correct? I may not fully understand the problem you mentioned., [~leftnoteasy], the crux of the problem is that if MULP is less than 100% (for example 20%), the user limit resource is never calculated to be more than {{resource-used-by-active-users / #active-users}}, bq. the user limit resource is never calculated to be more than resource-used-by-active-users / #active-users
Sorry, I meant to say that it never gets above {{queue-capacity * MULP}}, bq. it never gets above {{queue-capacity * MULP}}
[~sunilg] and [~leftnoteasy], although this statement is true and I correctly stated the symptoms, I misdiagnosed the root cause in my [comments above|https://issues.apache.org/jira/secure/EditComment!default.jspa?id=13021186&commentId=15829005]. Sorry for the confusion.

It appears that the root cause is that {{UM#User#assignContainer}} is not incrementing {{TotalResUsedByActiveUsers}} for the AM. The first time through {{assignContainer}} for a new app, the user isn't active yet, so the used resources count is not incremented. Consequently, {{resource-used-by-active-users}} is always smaller than the actual value, and never gets bigger than {{queue-capacity * MULP}}:
{code: title=UsersManager#computeUserLimit}
active-user-limit = max(
   resource-used-by-active-users / #active-users,
   queue-capacity * MULP
)
{code}

[~sunilg], do we need the {{isAnActiveUser}} checks in {{assignContainer}} and {{releaseContainer}}? I removed these checks in my local build and the application is able to use all of the queue and cluster., Hi [~eepayne]
Thank you for the detailed comments.

bq.do we need the isAnActiveUser checks in assignContainer and releaseContainer?
bq.I removed these checks in my local build and the application is able to use all of the queue and cluster.
If we remove the active user check, then {{activeUsersManager.getTotalResUsedByActiveUsers}} will be for all users. And hence it works like old. But I agree that the computation is not very correct. For example, *user1* was initially active and whenever a container was allocated for *user1*, we incremented resource to  {{AUM#TotalResUsedByActiveUsers}}. Now this user has become in-active since it doesnot have any more outstanding resource requests. So *user1* resources has to be removed from  {{AUM#TotalResUsedByActiveUsers}} at that time. This is not happening now. Eventhough I fix this, there are some changes in behavior. I can explain.

{noformat}
    // User limit resource is determined by:
    // max{resourceUsedForActiveUsers / #activeUsers, queueCapacity *
    // user-limit-percentage%)
{noformat}


Now here, lets assume 2 cases: ( 1. usedResource < queuCap and 2. usedResource > queueCap)

1. {{resourceUsedForActiveUsers / #activeUsers}} will be much lesser value now as we consider only active-users used cap. In old case, {{total_used/#activeUsers}} will be definitely more. So as per above equation, UL will be {{queueCapacity * userLimit%}} for higher MULP (something like 80~99%). Hence UL will be less than queueCapacity. (If MULP is lesser value, then UL will also be lower)
2. If {{usedResource > queueCap}}, then the UL can go more than queue cap based on two factors. If #active_users is lesser and active_users resource usage is more than queue cap OR usedResource which is more than queuCap is multiplied with a higher MULP value.

Altogether, first part of the existing UL compute equation will matter only if #active-users is lesser or MULP is very low in cluster. I think its somewhat fine. Please share your thoughts., Thanks [~sunilg] for the explanation.
bq. first part of the existing UL compute equation will matter only if #active-users is lesser or MULP is very low
I'm not sure what you may consider to be very low, but we often have large queues with a large number of users, so the MULP can be as low as 1%. So, this calculation needs to be accurate even when MULP is very low.

My original concern is that in {{YARN-5889.0005.patch}}, {{UM#User#assignContainer}} is not incrementing used resources when a new AM container is assigned, but the size of the AM _is_ being decremented by {{UM#User#releaseContainer}}. This causes the resource usage to be inaccurate, it causes the headroom algorithm to never allow a user to use more than the MULP, and it causes the usage count to go negative when the application is finished.

This is borne out by the [unit test failures from above|https://builds.apache.org/job/PreCommit-YARN-Build/14673/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt] such as
{code}
  TestApplicationLimits.testHeadroom:657 expected:<<memory:163840, vCores:1>> but was:<<memory:81920, vCores:1>>
{code}
and 
{code}
  TestCapacitySchedulerNodeLabelUpdate.testResourceUsageWhenNodeUpdatesPartition:320->checkUserUsedResource:188 expected:<0> but was:<-1024>
{code}, Thanks [~eepayne] for the clarification

bq.I'm not sure what you may consider to be very low,
I think my comment was slightly vague there. Actually i didnt not mean it ll be wrong.
{noformat}
active-user-limit = max(
   resource-used-by-active-users / #active-users,
   queue-capacity * MULP
)
{noformat}

I was trying to lay down chances of hitting first part above equation since we do a max. 

bq.UM#User#assignContainer is not incrementing used resources when a new AM container is assigned, but the size of the AM is being decremented by UM#User#releaseContainer
{{LQ#allocateResource}} will invoke {{UM#User#assignContainer}}. So all containers including AM will hit here.

Now I am fixing the test failure as mentioned by you. I fixed almost all, but now checking TestApplicationLimitsByPartition and TestLeafQueue. I ll feedback in few time once i fix that.
, bq. LQ#allocateResource will invoke UM#User#assignContainer. So all containers including AM will hit here.
[~sunilg], I'm sorry I wasn't clear.

As you say, {{UM#User#assignContainer}} is called for the AM. However, since {{userName}} is not active at that point, {{incUsed}} is not called and the AM's resources are not incremented:
{code}
      if (activeUsersManager.isAnActiveUser(userName)) {
        activeUsersManager.getTotalResUsedByActiveUsers().incUsed(nodePartition,
            resource);
      }
{code}, Yes. You are correct. I am revisiting the logic to get total active-users usage. Will update a patch in short-while., Uploading a new patch.

Changes:
1. CS will no longer user ActiveUsersManager (AUM). AbstractUsersManager interface will help in this case. UsersManager has  internally handling what AUM was earlier doing. This helps us to track per user resource as well.
2. Fixed few test cases. However there  are some more failures. I ll wait for jenkins to make final touch.
3. Fixed AM container resource issue w.r.t active user along with point 1.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 13s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 7 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  1s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 31s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 18 new + 1027 unchanged - 14 fixed = 1045 total (was 1041) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 12 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 46m 34s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 16s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 67m 45s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.scheduler.fair.TestContinuousScheduling |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimitsByPartition |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestNodeLabelContainerAllocation |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerAsyncScheduling |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5889 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12849666/YARN-5889.0006.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 79b87438325e 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2034315 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14768/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/14768/artifact/patchprocess/whitespace-eol.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/14768/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14768/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14768/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Test failures are related. I ll be fixing them now., Thanks [~sunilg] for updating the patch.

Some more comments for the latest refactoring:

1) UsersManager#getComputedResourceLimitForAllUsers and FifoIntraQueuePreemptionPlugin#calculateIdealAssignedResourcePerApp: {{User}} can be removed from parameter list. And calculateIdealAssignedResourcePerApp: partitionBasedResource is not used, is it a mistake or just a unnecessary param?

2) Are changes of {{testQueueMaxCapacitiesWillNotBeHonoredWhenNotRespectingExclusivity}} related to this patch?

3) More comments for the latest UsersManager:

3.1 Better to move following two methods from User to UsersManager:

{code}
    public void assignContainer(Resource resource, String nodePartition) {
      userResourceUsage.incUsed(nodePartition, resource);

      usersManager.incResourceUsagePerUser(resource, nodePartition, userName);
    }

    public void releaseContainer(Resource resource, String nodePartition) {
      userResourceUsage.decUsed(nodePartition, resource);

      usersManager.decResourceUsagePerUser(resource, nodePartition, userName);
    }
{code}

Instead of invoking usersManager from User, it's better to do that in the reverse way. When resource allocated/released, it calls UsersManager#inc/decResourceUsagePerUser, and calls following methods:

{code}
1. User#incUsed
2. UM#userLimitNeedsRecompute
3. UM#updateQueueUsageRatio
{code}

3.2 For UM#inc/decResourceUsagePerUser, now the logic is a little confusing:

We already stored user's usage in User#userResourceUsage, I think we don't need a Map<UserName, ResourceUsage> for activeUsersResourceUsage/nonActiveUsersResourceUsage. What we only need is Set<UserName> for active user and non-active user, correct?

So existing logic of {{UM#inc/decResourceUsagePerUser}} should be changed to:

{code}
UM#incResourceUsagePerUser() {
  writelock {
      /* As mentioend above (3.1), adding following logic
       * 1. User#incUsed
	   * 2. UM#userLimitNeedsRecompute
	   * 3. UM#updateQueueUsageRatio
       */

	  // ... keep existing logics to updawte total usage

	  if (active-set.contains(userName)) {
	     // increase total-active-usage
	  } else if (non-active-set.contains(userName)) {
	     // increase total-non-active-usage
	  } else {
	     // User's neither in active set nor non-active set, is it possible?
	     // I think it is not possible if other logics are correct. (@Sunil)
	  }
  }
}
{code}

And we should keep your logic when user moved between active and non-active state, we will update both of active/non-active usages.

3.3 For activateApplication/deactivateApplication, 
- Use writeLock? And annotation for lock seems outdated, please check all {{@Lock}} inside the class
- Since {{updatePerUserResourceUsage}} has two duplicated logics -- toActive=false/true, it might be better to have two method: updateUsageForNew(Non)ActiveUser.
- setUserAddedOrRemoved looks like a duplicated logic of {{userLimitNeedsRecompute}}, I think we can remove the logic and can simplify a little bit for other method like {{getComputedResourceLimitForActiveUsers}}

3.4 Some miscs:
- Could you make sure all Log.DEBUG is wrapped by isDebugEnabled?
- Add Log.DEBUG for resource usage update of total/active/non-active? Which can help troubleshooting potential issues a lot.
- In {{computeUserLimit}}: resourceUsed/usersCount initial value is redundant. And {{//      resourceUsed = currentCapacity;}} is commented intentially or by mistake? 
- getActiveUsersResourceUsage/getActiveUsersResourceUsage are never used by anyone.
- {{UserToPartitionRecord}} is only consumed by UsersManager, if User is a internal class of UsersManager, I would suggest to convert UserToPartitionRecord to an internal class as well., And one suggestion is, for such major changes to CS, it might be better to run all tests under {{org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity}} before submitting the patch. Which can save you a lot of time to wait for Jenkins come back. That is typically what I do :)., Thanks [~leftnoteasy] for the detailed comments. Updating new patch.

For point 1) related to PCPP, I raised another ticket to do the inspection on the unused parameter., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 7 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 36s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 21 new + 1027 unchanged - 15 fixed = 1048 total (was 1042) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 6 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 14s{color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 25s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 44m 34s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 69m 31s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
|  |  Unread field:UsersManager.java:[line 112] |
| Failed junit tests | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerAsyncScheduling |
|   | hadoop.yarn.server.resourcemanager.TestRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5889 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12849957/YARN-5889.0007.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 6cbae24b0a7b 3.13.0-105-generic #152-Ubuntu SMP Fri Dec 2 15:37:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 312b36d |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14790/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/14790/artifact/patchprocess/whitespace-eol.txt |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/14790/artifact/patchprocess/new-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.html |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/14790/artifact/patchprocess/patch-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/14790/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14790/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14790/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, TestCapacitySchedulerAsyncScheduling is passing locally. I ll still check few more times. I ll wait for comments and ll address findbugs etc errors in next  patch. Please review. cc/ [~leftnoteasy] [~eepayne], Major comments of UsersManager:
1) Remove UM#incResourceUsagePerUser, instead LQ should use updateUserResourceUsageDuringAllocate to make queue ratio get updated as well.

2) Instead of storing user limit for every user/partition/scheduling-mode, we should only store it for every partition and scheduling-mode since now we have an identical UL for all users in active set and total set. 

3) Instead of storing local version inside user, we should store it for every partition/scheduling-mode to make sure we can always get up-to-date UL. And also, I think we may need two maps to store active-user/all-users local version.

4) Headroom of user can be fixed in a separate patch with lower priority: now headroom is set to queue-capacity * MULP, and it will slowly increase with more container allocation.

Minor comments for UsersManager
5) public to private: updateQueueUsageRatio/getUsers/getLatestVersionOfUsersState

6) Merge implementation of updateUserResourceUsageDuringAllocate/updateUserResourceUsageDuringRelease.

7) resourceCalculator can be stored in UsersManager, which we can avoid passing RC in all parameters.

8) getTotalResourceUsagePerUser, instead of using nested {{? .. : ..}}, suggest to use if..else

9) UM#UsageRatios: not caused by your patch: there're some necessary boxing operation. Better to update inc/setUsageRatio, all new Float is not required.

10) Following method is too simple to be a separated method: removeFromUsersSet/removeFromUsersSet

11) Need writeLock: UM#removeUser, 

12) isRecomputeNeeded: if you agree with #3, I think the parameter should be: partition/schedulingMode only. Map<SchedulingMode, Resource> is not required since if partition=x and scheduling-mode=y exists, the value of resource should not null, correct?

13) getComputedResourceLimitForAll/ActiveUsers, I think the logic can be simplified to:
{code}
writelock {
  if (isRecomputationNeede(...)) {
      // ...
  }
}
return map.get(scheduling_mode, partition); 
{code}

14) {{userLimitPerSchedulingMode.clear()}} can be removed from {{reComputeUserLimits}} if you agree with #12.

15) {{Resource resourceByLabel}} can be removed from parameter list of {{updateUserResourceUsageDuringAllocate}}

16) calculateUserUsageRatio could be removed and all logics can be done inside setUsageRatio which wrapped by writelock. And rename {{setUsageRatio}} to {{updateUsageRatio}}

17) rename updateQueueUsageRatio to incQueueUsageRatio

18) (TODO) Please review all usage of getUserAndAddIfAbsent, I think some of them might not necessary., Thanks [~leftnoteasy] for the comments. Updating new patch.

For points 12/13, {{isRecomputeNeeded}} needs to check {{Map<SchedulingMode, Resource>}} for very first time. And we update this map when one of SchedulingMode is updated. So I guess we need a null check as well. I ll ping offline., Thanks [~sunilg] for updating the patch.

Last wave (hopefully :p) of comments for the latest patch

1) updateUserResourceUsage:
- javadocs parameter need to change
- remove LOG.info for debug

2) incResourceUsagePerUser/decResourceUsagePerUser are mostly identical, suggest to add the "allocated" parameter and rename it to updateResourceUsagePerUser. And writeLock is not necessary

3) getComputedResourceLimitForActiveUsers:
- Why {{userLimitNeedsRecompute}} is called here? Will it make the following {{isRecomputeNeeded}} to always return true?
My guess is, now we have only one localVersionOfUsersState for both of active user and total user. If we have two such map, one for active user and one for total user, it could solve the problem, correct?

4) isRecomputeNeeded:
- When userLimitPerSchedulingMode gonna be null?
- I'm still not quite sure about why {{userLimitPerSchedulingMode}} is required for {{isRecomputeNeeded}}:
{{getLocalVersionOfUsersState}} returns -1 when userLimitPerSchedulingMode doesn't contain schedulingMode, correct?
- And also, we don't need {{latestVersionOfUserCount}}, instead we should call {{latestVersionOfUsersState.get()}}.

5) So a summary of 3/4: I think we need two maps for local version, and isRecomputeNeed should take 3 parameters: schedulingMode, partition, and {{boolean activeUsers}}. Existing logic looks not correct to me: if version updated to 2 for partition=x, and scheduling_mode=y; then we get user-limit for active-user/total-user; and then version update to 3 for partition=x and scheduling_mode=y; then we get user-limit for active-user/total-user again, the 2nd time UL of total-user will not be updated.

6) getLatestVersionOfUsersState is too simple to be a method, better to remove.

7) userLimitNeedsRecompute:
Need to consider value becomes negative, and we use "-1" as default value for "not found" local version, we should make sure value is always >= 0.
I think you can do things like:
{code}
int x = version.incrementAndGet();
if (x < 0) {
    x = version.get();
	while (x < 0 && !version.compareAndSet(x, 0)) {
		x = version.get();
	}
}
{code}

8) Possible redundant null checks:
I'm not sure if they're required, we can keep them to make RM not crash, but highly suggest to print warning:
- incResourceUsagePerUser/decResourceUsagePerUser: {{totalResourceUsageForUsers}}, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 7 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 30s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 19 new + 1025 unchanged - 18 fixed = 1044 total (was 1043) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 1 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javadoc {color} | {color:red}  0m 19s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 40m 24s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 18s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 63m 56s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationPriority |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer |
|   | hadoop.yarn.server.resourcemanager.scheduler.TestAbstractYarnScheduler |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerNodeLabelUpdate |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerAsyncScheduling |
|   | hadoop.yarn.server.resourcemanager.TestRMRestart |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5889 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850466/YARN-5889.0008.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux b3f3083ac984 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b6f290d |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14805/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/14805/artifact/patchprocess/whitespace-eol.txt |
| javadoc | https://builds.apache.org/job/PreCommit-YARN-Build/14805/artifact/patchprocess/patch-javadoc-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/14805/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14805/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14805/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks [~leftnoteasy] for helping to review the patch thoroughly. Updating a new patch

bq.4) isRecomputeNeeded:
I am slightly confused here. I think we might need null check. I ll help to share detailed view for that.

Assume that there are no precomputed user-limit at the start when RM is started or queue is refreshed. So all cache ill be empty, and we ll do our first computation when a container request comes. 
So in this case, userLimitPerSchedulingMode will be null. And we ll do a recompute and then userLimitPerSchedulingMode will have some entires. So a null check is needed at the very beginning scenario. I can see whether this check can be done outside or note. Am i missing something here? pls help to share your view.

bq.And also, we don't need latestVersionOfUserCount, instead we should call latestVersionOfUsersState.get().
userLimitNeedsRecompute or getLatestVersionOfUsersState are not writeLock protected. Hence in getComputedResourceLimitForAll/ActiveUsers  , it may be possible that latestVersionOfUsersState may change within writeLock block while operating.  Since we save the saved version of latestVersionOfUserCount to update local cache (per partition nd sch mode), even though some other thread changed the real  latestVersionOfUsersState, cache will be invalidate it correctly. Pls pool in your thoughts., Hi Sunil,

bq. I am slightly confused here. I think we might need null check. I ll help to share detailed view for that. ...
I think the latest patch do the correct thing, except one thing:

Instead of doing:

{code}
    Map<String, Map<SchedulingMode, Resource>> userMap = (isActive)
        ? preComputedActiveUserLimit
        : preComputedAllUserLimit;

    return !userMap.containsKey(nodePartition)
        || (getLocalVersionOfUsersState(nodePartition, schedulingMode,
            isActive) != latestVersionOfUserCount);
{code}

I think it should be enough to do:

{code}
return getLocalVersionOfUsersState(nodePartition, schedulingMode,
            isActive) != latestVersionOfUserCount;
{code}

The reason is, getLocalVersionOfUsersState will always return -1 when userMap doesn't contains nodePartition.
And {{reComputeUserLimits}} will insert the map and return.

Is my understand correct?

bq. userLimitNeedsRecompute or getLatestVersionOfUsersState are not writeLock protected ...

{{userLimitNeedsRecompute}} is now writeLock-protected (I think you don't need atomicLong any more but it's fine to keep it as-is). And usage of latestVersionOfUsersState is always under read/writeLock. So latestVersionOfUsersState.get() is always up-to-date value and will not be changed during calculation (it is also fine if it is changed during usage, our goal is to make local cache invalidated).

And one additional minor comment:
1) computeUserLimit:
- Remove {{User user}} from parameter list, and instead of calling lQueue.getUser, it's better to call this.getUser()

+1 to the latest patch beyond above comments, I think the patch is ready-to-go once Jenkins give a +1. , | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 13s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 7 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 30s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 23 new + 1025 unchanged - 18 fixed = 1048 total (was 1043) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  1s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 39m 44s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 62m 40s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5889 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850622/YARN-5889.0009.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 1ea4dce72a65 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 0914fcc |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14810/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14810/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14810/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Yes [~leftnoteasy]. Makes sense to me. Uploading a patch fixing the comments given. Thank You very much., | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 26s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 7 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 32s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 20 new + 1024 unchanged - 19 fixed = 1044 total (was 1043) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  6s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 40m  4s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 18s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 64m 10s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5889 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850780/YARN-5889.0010.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 6f5d3bd044c8 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 0914fcc |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14821/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14821/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14821/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, +1 to latest patch, will commit it next Monday if no opposite opinions.

Thanks [~sunilg]!, Thanks [~sunilg] and [~leftnoteasy] for your work on this feature. I do have one concern. I think there is a race condition where if a container fails, the freed resources are not recorded for that user about half the time.

Use Case:
- Queue is 50% of cluster
- MULP = 50%
- One app fills the cluster
- Some containers fail
-- I simulate this by using {{yarn container -signal container_1486159534159_0004_01_000029  FORCEFUL_SHUTDOWN}}
- The app is only given new containers about half the time.
-- That is to say, the app is asking for resources, and the cluster has free space, but the app is not being given those resources.

I'm sorry I can't go into more detail at this time. I just discovered this issue, and I have not had time to investigate further. However, since you are about to complete the work on this JIRA, I felt I should provide the information I have so far., bq. resources are not recorded for that user about half the time.resources are not recorded for that user about half the time.
Another symptom is that the app never completes. It stays hung in the RUNNING state. The app attempts that failed are re-tried, but they stay in the 'STARTING	NEW' state. , Thank you very much [~eepayne] for sharing the scenario. I will dig into details and will share analysis soon., HI [~eepayne]. 
Sorry for troubling you again. I ran many similar test cases in my local cluster. I could not get the case mentioned by you, even though i tested for many time today and yesterday. So i thought I might be missing something. 

I will try share my test case results here:
- I used {{YARN-5889.0010.patch}} patch for my tests.
- One node cluster (8GB).
- I used sleep job to make sure i have a control on test time and on number of containers.
- AM and other containers were using 2G containers always.


*Test Scenario* (For 3 test cases, same scenario were used, only change is in configuration)
1) Ran 4 containers including AM. Pending RR is 2Gb for one more container
2) Killed all non-AM containers 3 times (totally 9 containers were killed using FORCEFUL_SHUTDOWN signal)
3) Killed AM containers once and job re-ran.

+Scenario 1:+ (One Queue 100%)
MULP = 100, ULFactor = 1

Output:
Application retook whole cluster always when one/more container were killed.

+Scenario 2:+ (One Queue 100%)
MULP = 50, ULFactor = 1


Output:
Application retook whole cluster always when one/more container were killed. UL was 50%, still it got whole cluster.


+Scenario 3:+ (Two Queues 50% each)
MULP = 50, ULFactor = 2

Output:
Application retook whole cluster always when one/more container were killed. UL was 50%, still it got whole cluster as UL factor was 2. (If its 1, then only 6Gb could be used).

Could you please help to take a look and share me what I might have missed from your test scenario. I ll continue test some more scenarios and will keep posted., [~sunilg], Thanks for running those manual tests. Those were the same tests that I was running.

I discovered that this was not something introduced by this patch, but it happens in trunk as well. Also, I discovered that it doesn't happen for a cluster with 1, 2, or 3 nodemanagers, but when the 4th nodemanager is added, it starts happening.

At any rate, the good news is that this is not a problem with your patch. I have a couple of more things I want to check out on the patch before my review is complete. I'll get back to you soon., OK. Latest patch looks good.

[~sunilg], [~leftnoteasy], are you planning on backporting to branch-2 / branch-2.8?, Thanks [~eepayne] for the review.
Yes, I would like to backport this change to branch-2 as well. However I am not very sure whether we can put to branch-2.8 since RC cut is happening. Could you or [~leftnoteasy] confirm whether its safe to go to branch-2.8 since RC cut is on the way., Cool, thanks [~eepayne]. I just triggered another Jenkins build to run, if everything goes fine, I will commit the patch tomorrow.

Since we're close to 2.8 release now, let's try to see if this patch can go to 2.8.1 or not after 2.8.0 release., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 7 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 15s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 31s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 19 new + 1024 unchanged - 19 fixed = 1043 total (was 1043) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 19s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 40m 42s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 18s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 63m 19s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.TestRMAdminService |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | YARN-5889 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12850780/YARN-5889.0010.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 0f4409d0750a 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d88497d |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/14841/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/14841/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/14841/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/14841/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Test case failure is not related to this patch. It looks like an intermittent issue and is tracked via YARN-5652., [~leftnoteasy]
bq. Since we're close to 2.8 release now, let's try to see if this patch can go to 2.8.1 or not after 2.8.0 release.
Since the branch-2.8.0 branch has already been created, wouldn't it be safe to go into branch-2.8(.1)? Or are you concerned that if they need to pull more things into the 2.8.0 branch before the RC, this patch may conflict?, We checked with Junping and there are not major changes related to this area which is pending. So we should be good to go for 2.8.1. I ll first give a branch-2 patch which has some conflicts now. After that, 2.8 patch could be backported., [~eepayne], [~sunilg],

I would rather prefer to backport this patch once we have user-limit preemption support. This is just a refactoring patch with minor behavior changes, it should be backport once we get whole feature ready (intra-queue preemption for user limit). We can discuss more once we get that stage. , Committed to trunk, thanks [~sunilg] and thanks reviews from [~eepayne]/[~jlowe]., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11227 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11227/])
YARN-5889. Improve and refactor user-limit calculation in Capacity (wangda: rev 5fb723bb77722d41df6959eee23e1b0cfeb5584e)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/CapacitySchedulerLeafQueueInfo.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSParentQueue.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/FifoIntraQueuePreemptionPlugin.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/ActiveUsersManager.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerNodeLabelUpdate.java
* (add) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractUsersManager.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestNodeLabelContainerAllocation.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityHeadroomProvider.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicyMockFramework.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSLeafQueue.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimitsByPartition.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CSQueue.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/Queue.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestApplicationLimits.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestSchedulerApplicationAttempt.java
* (add) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/UsersManager.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java
, Thanks for thorough reviews and commit [~leftnoteasy] , and thanks for the detailed review from [~eepayne] and [~jlowe]. Really appreciate the same.
I am working on user-limit preemption poc based on this patch and will shortly upload in preemption jira., Re-resolving as "fixed" rather than "resolved" so it shows properly in the release notes., I cherry-picked this commit to branch-2 as a prereq for YARN-2113]