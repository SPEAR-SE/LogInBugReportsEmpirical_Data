[We could have the RM wait until it receives hard confirmation from the NM before it releases the resources associated with a container, but that would needlessly slow down scheduling in some cases.  For example, if a user is at the scheduler user limit but releases a container on node A, I don't see why we have to wait until that container is confirmed dead over two subsequent NM heartbeats (one to tell the NM to shoot it and another to confirm its dead) before allowing the user to allocate another container of the same size on node B.  However I do think it's bad for us to allocate the new container on the _same_ node as the released one since we can accidentally overwhelm the node if the old container isn't cleaned up fast enough.

Therefore I propose that we go ahead and let the scheduler queues and user limit computations update immediately so other nodes can be scheduled, but we don't release the resources in the SchedulerNode itself until the node confirms a previously running container is dead.  IMHO if the RM ever sees a container in the RUNNING state on a node, it should never think that node has freed the resources for that container until the node itself says that container has completed.

There is an interesting corner case where the RM has handed out a container to an AM (i.e.: container is in the ACQUIRED state) but it hasn't seen it running on a node yet.  If the container is killed by the RM or AM, there's still a chance where the container could appear on the node after the RM has considered those resources freed.  We'll have to decide how to handle that race.  One way to solve it is to assume the container resources could still be "used" until it has had a chance to tell the NM that the container token for that container is no longer valid and confirmed in a subsequent NM heartbeat that the container has not appeared since.  Maybe there's a simpler/faster way to safely free the containers resources for that race condition?, It seems to be another one in a series of bugs rooted in mismatch of state between NMs and the RM.
Aside from playing whack-a-mole is it possible to make a more structural / architectural fix?

, Thanks [~jlowe] for reporting the issue!

We came across the issue some time ago. I tried the thought in YARN-4148: RM does not release app's resource until containers actually finish and NM releases the resource.

Another thought(copied from YARN-4148): NM records its total resource and available resource. When launching a container, NM checks available resource and waits until there is enough resource for container. But there might be a time gap from AM's perspective, AM thinks it has launched container, however container might be waiting for its resource., Thanks for the pointer, Jun!  This is a duplicate of YARN-4148.  I'll move this discussion over there so we have it in one place.]