[Attaching patch for review, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  23m  9s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |  10m 27s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  13m 31s | There were no new javadoc warning messages. |
| {color:red}-1{color} | release audit |   0m 25s | The applied patch generated 1 release audit warnings. |
| {color:green}+1{color} | checkstyle |   1m 33s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   2m  1s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 44s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 58s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:red}-1{color} | yarn tests |  60m 14s | Tests failed in hadoop-yarn-server-resourcemanager. |
| | | 114m  7s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12766071/0001-YARN-4254.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 0ff1216 |
| Release Audit | https://builds.apache.org/job/PreCommit-YARN-Build/9405/artifact/patchprocess/patchReleaseAuditProblems.txt |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/9405/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/9405/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf907.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/9405/console |


This message was automatically generated., Thanks for the report and patch, Bibin!

The patch seems to be trying to fix a very specific failure mode, but in practice it will lead to a lot of AM attempt failures which isn't ideal.  Would it make more sense if the RM simply refused to accept nodemanagers into the cluster that are unresolvable?  Also the fact that we try forever seems broken to me.  We should be giving up at some point and failing the attempt, whether that be due to unknown host exceptions or other persistent errors.  Checking specifically for unknown host exception makes me think we'll just hit this type of problem again but for some other persistent error.

, [~jlowe]
Thanks for looking into issue
Had cancelled the patch .Sorry forgot to mention the same in JIRA .
Looking further got to know that the retry is for DNS related case. But attempt should give up after a fixed period of time. 

For this jira  
{quote}
Would it make more sense if the RM simply refused to accept nodemanagers into the cluster that are unresolvable?
{quote}

This solutions sounds good.

Also 

{quote}
Also the fact that we try forever seems broken to me. We should be giving up at some point and failing the attempt, whether that be due to unknown host exceptions or other persistent errors.
{quote}
Will try to find out further why timeout is not happening or the same is not available.



, bq.RM simply refused to accept nodemanagers into the cluster that are unresolvable
If we do this check in {{registerNodeManager}}, will it take more time if DNS is present (with timeout)., True, registering could take significantly longer if DNS is slow.  However IIRC the NameNode also resolves datanodes when they register and rejects datanodes that cannot be resolved, so I believe there is precedent for it.  Curious, was this new node added as a datanode as well, and if so what did the NameNode do?

Anyway we don't have to do registration rejection as part of this JIRA, and even with that fix it wouldn't solve the problem if the node was resolvable when it joined but not when the AM launched.  The real issue for this JIRA is why did it try forever on a bad nodename resolution.  Did it really try forever, or was it a case of something like YARN-3208 where it would eventually complete but just not for a really long time due to retries at multiple levels?
, {quote}
 Did it really try forever?
{quote}
It did get stuck for for more than 48 hours. And 4-5 logs files with same exception.
, RMAppAttempt will be registered to AMLivenessMonitor when AM container is launched. I think here this has not happened. Could you please share the last status of application and appattempt., Hi [~sunilg]
Attached test patch to reproduce the same. AM attempt is in scheduled state, Hi [~bibinchundatt]
Thank you for sharing the details. As you mentioned, AM attempt is in scheduled state but container is not yet launched here.
But container allocation is done in the NM heartbeat for this app attempt (AM container), and yet to be pulled from RMAppAttempt AMContainerAllocatedTransition. Based on our offline discussion, this must be failing due to the DNS lookup/etc-hosts lookup. Thus causing the looping of attempt retries as you mentioned. In my opinion I am also agreeing with your point of view, and this is to be handled.

Currently in some cases, there are chances that DNS may be off for a while, hence we must retry to pull such containers again. This is done currently in FicaSchedulerApp. However in cases like this JIRA, it will cause permanent hang for application, since container is allocated by RM but cannot be pulled due to continuous host lookup errors.

So if we do a validation for valid host in register/heartbeat, we also must ensure that we remove such containers from newly allocated list. OR, we could handle the exception while trying to create container token and then remove from {{newlyAllocatedContainers}} list. Thoughts?

, {quote}
 we could handle the exception while trying to create container token and then remove from newlyAllocatedContainers list. 
{quote}
But after some time period only we should be removing the same rt? Immediate removal will skip DNS retry logic i think., But this will solve only particular case, not sure anything else could cause AM not getting container other than resource related cases.]