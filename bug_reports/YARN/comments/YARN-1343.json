[The problem is that the RMNodeImpl.AddNodeTransition() is not dispatching a NodesListManagerEventType.NODE_USABLE event to the NodeListManager. Plus, the NodeListManager handle() for NODE_UNUSABLE should dispatch RMAppNodeUpdateType.NODE_USABLE events to the apps even if the node was not in the unusable list.
, I've also verified in a deployment the AM does received the node update., +1, pending jenkins, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12609939/YARN-1343.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2266//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2266//console

This message is automatically generated., If the node was marked as unhealthy then it should have been in unusableRMNodesConcurrentSet. So when it becomes healthy it must be found in that list. Why are we moving the code outside the check?
{code}
       if (unusableRMNodesConcurrentSet.contains(eventNode)) {
         LOG.debug(eventNode + " reported usable");
         unusableRMNodesConcurrentSet.remove(eventNode);
-        for (RMApp app : rmContext.getRMApps().values()) {
-          this.rmContext
-              .getDispatcher()
-              .getEventHandler()
-              .handle(
-                  new RMAppNodeUpdateEvent(app.getApplicationId(), eventNode,
-                      RMAppNodeUpdateType.NODE_USABLE));
-        }
{code}, because if not, added/restarted nodes never make it to AMs via node updates as they are not in the unusable list., Should we create a NODE_ADDED instead of overloading NODE_USABLE in case downstream consumers need to differentiate the cases. The update type enum (eg. NODE_USABLE) is passed in the event sent to the consumers (in this case RMApp).

Reconnect of a NM after restart has 2 cases, it comes back with identical state/resources. Does the app need to know about this? It comes back with different state. Does the app need to know about this? If the answer is yes to both then do the 2 situations need to be differentiated?, We will need to add the update type into the message that goes to the AM. The current code in YARN informs the AM about unhealthy/healthy status of the nodes that the AM knows about. For the reconnect case, the AM already knows about the node and so an additional data field is needed to tell it about reconnection etc.
Btw, reading the reconnect code looks like it has a bug because if the node resource changes then it does not change the RMNodeImpl actually stored in the rmContext map. So the node information does not get updated in the RM., Also, to be clear, the original code was added to update the AM about unhealthy/healthy node changes in order to reach compatibility with MR1. Here we are making an addition to that mechanism to also pass the added/reconnected nodes. That is a useful improvement. We can also pass removed node information to complete all the cases. My suggestion is to pass the additional information in the NodeStatus object that is sent to the AM. The additional status would be what the change was. addition, unhealthy, healthy, restarted/reconnected, removed., bq. My suggestion is to pass the additional information in the NodeStatus object that is sent to the AM. The additional status would be what the change was. addition, unhealthy, healthy, restarted/reconnected, removed.

IMO this suggestion is a separate JIRA and it will required changes in the API.

What this JIRA is fixing a bug where a AM never gets notified via updates with a node is RUNNING (for the first time or again), only when a node goes !RUNNING., This is not a bug since sending the AM information about added/removed nodes is a feature that was never added to the RM. The feature was added to reporting healthy/unhealthy updates for an existing node. So adding support to notify about additions/removals is an improvement and I like this improvement. I am trying to understand how we can make this improvement useful to the AM that receive this information.

If we simply send the nodestatus object to the AM how is the AM expected to make sense of it? Are there any existing fields in nodestatus that tell the AM about addition/reconnection? If not, then is this patch complete wrt the intent of the jira to inform AMs about addition/reconnection and (potentially removal)?, [~bikassaha], I disagree that this is not a bug.

From {{AllocateResponse}} javadocs

{code}
  /**
   * Get the list of <em>updated <code>NodeReport</code>s</em>. Updates could
   * be changes in health, availability etc of the nodes.
   * @return The delta of updated nodes since the last response
   */
  @Public
  @Stable
  public abstract  List<NodeReport> getUpdatedNodes();
{code}

And from {{AMRMClientAsync}}:

{code}
    /**
     * Called when nodes tracked by the ResourceManager have changed in health,
     * availability etc.
     */
    public void onNodesUpdated(List<NodeReport> updatedNodes);
{code}

A node re/joining is *availability*.

Following your reasoning, a LOST node should not be reported as it is gone, it does have a status any more. But it is currently reported.

The current behavior is asymmetric and not expected and that should be fixed along the lines of this JIRA.

And we can follow up with another JIRA to improve things as you suggested., Bikas, Vinod, Sandy and I had an offline chat on this. I had as an action item to debug the RM current behavior:

If a NM is shutdown, 'sbin/yarn-daemon.sh stop nodemanager', the RM does not receive any notification of such. The RM will only detect the NM is gone after the {{nm.liveness-monitor.expiry-interval-ms}} elapses (default 10mins) which triggers a {{DeactivateNodeTransition}}.

If the expired interval kicked in, then the NM is removed from the RM context and the NM rejoining will be treated as a NM add.

If the expired interval did not kick in (the NM was restarted before the expire interval elapsed), the NM rejoining will be treated as a NM reconnect *only* if the NM address ({{yarn.nodemanager.address}}) is using a fixed port. By default is using {{zero}}, an ephemeral port, causing the the NM rejoin to be treated as NM add.

While using a fixed port the NM will make the NM to be treated as a reconnect, the NodeListManager does not received the NODE_USABLE event because the {{ReconnectNodeTransition}} transition does not dispatch a {{NodeListManagerEvent}} *(bug-1)*.

Also, it seems the {{NodeListManager}} {{unusableRMNodesConcurrentSet}} is never cleaned up. Using NM ephemeral ports this is a memory leak. Using NM fixed ports the leak is contained to the max number of NM *(bug-2)*.

Having AMs receiving node updates on NM added/rejoined where the NM are reported as RUNNING seems to be, different from what the javadocs state, not the original intention of this but seems a reasonable feature *(improvement-1)*.
 
Given than *bug-1* and *improvement-1* are tightly related I propose taking care of bothas part of this JIRA and I'll open a new JIRA for *bug-2*.
, updated patch as per last comment., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12610663/YARN-1343.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2301//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2301//console

This message is automatically generated., It looks like in the reconnect with different capacity case we will end up sending 2 NODE_USABLE events for the same node.
{code}
        }
        rmNode.context.getRMNodes().put(newNode.getNodeID(), newNode);
        rmNode.context.getDispatcher().getEventHandler().handle(
            new RMNodeEvent(newNode.getNodeID(), RMNodeEventType.STARTED)); // <=== First instance when this triggers the ADD_NODE_Transition
      }
      rmNode.context.getDispatcher().getEventHandler().handle(
          new NodesListManagerEvent(
              NodesListManagerEventType.NODE_USABLE, rmNode)); // <=== Second instance
{code}

So we could probably move the second instance to the first if-stmt where it also sends the NodeAddedSchedulerEvent. That would handle the case of the same node coming back while the STARTED event in the else stmt will cover the case of a different node with the same node name coming back (same as a new node being added).
{code}
if (rmNode.getTotalCapability().equals(newNode.getTotalCapability())
          && rmNode.getHttpPort() == newNode.getHttpPort()) {
        // Reset heartbeat ID since node just restarted.
        rmNode.getLastNodeHeartBeatResponse().setResponseId(0);
        if (rmNode.getState() != NodeState.UNHEALTHY) {
          // Only add new node if old state is not UNHEALTHY
          rmNode.context.getDispatcher().getEventHandler().handle(
              new NodeAddedSchedulerEvent(rmNode));
        }
      }
{code}

I modified the patch testcase to try out reconnect with different capability and the above issue showed up., [~bikassaha], thanks for the review and catching the double dispatching. Uploading a patch with the changes you suggested and also adding a test to verify the NODE_USABLE event is dispatched when a reconnect happens and the node has different capabilities., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12611185/YARN-1343.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2321//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12611185/YARN-1343.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2322//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2322//console

This message is automatically generated., Can you please double check that testReconnectWithDifferentCapacity is actually resulting in reconnection? The test alters the existing node's capacity and thus I would expect the equality check in ReconnectTransition to consider the nodes same as before. We probably need to create a new node with same name and different capacity. Maybe stepping through in the debugger may show whats really happening.
If reconnect with different capability code is getting executed then I would expect mock rm context to have to mock getRMNodes() method and a listener to be added for RMNodeEvents. Or else the test will have exceptions in the output., TheTestRMNodeTransition tests only verify the expected follow up events for the {{NodeListManager}} are dispatched. 

To test that reconnect is happening with different capabilities we need to add a test for the {{ResourceTrackerService.registerNodeManager()}}.

Uploading a patch that tests a RECONNECTED event dispatching with same and different capabilities., lgtm.
In the new testReconnect() we should check that the number of RMNode in rmContext.getRMNodes() is still 1. eg. the second node actually replaced the previous node (desired behavior) as opposed to both getting into the list., +1 for committing. Thanks!, [~bikassaha], on your ask about checking for the node count, I don't think is necessary, if a reconnect is triggered, it means it was found, in the {{ResourceTrackerService.registerNodeManager()}}:

{code}
      RMNode oldNode = this.rmContext.getRMNodes().putIfAbsent(nodeId, rmNode);
    if (oldNode == null) {
      this.rmContext.getDispatcher().getEventHandler().handle(
          new RMNodeEvent(nodeId, RMNodeEventType.STARTED));
    } else {
      LOG.info("Reconnect from the node at: " + host);
      this.nmLivelinessMonitor.unregister(nodeId);
      this.rmContext.getDispatcher().getEventHandler().handle(
          new RMNodeReconnectEvent(nodeId, rmNode));
    }
{code}

thx, SUCCESS: Integrated in Hadoop-trunk-Commit #4680 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4680/])
YARN-1343. NodeManagers additions/restarts are not reported as node updates in AllocateResponse responses to AMs. (tucu) (tucu: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1537368)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMReconnect.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #379 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/379/])
YARN-1343. NodeManagers additions/restarts are not reported as node updates in AllocateResponse responses to AMs. (tucu) (tucu: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1537368)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMReconnect.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1569 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1569/])
YARN-1343. NodeManagers additions/restarts are not reported as node updates in AllocateResponse responses to AMs. (tucu) (tucu: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1537368)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMReconnect.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1595 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1595/])
YARN-1343. NodeManagers additions/restarts are not reported as node updates in AllocateResponse responses to AMs. (tucu) (tucu: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1537368)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/resourcetracker/TestNMReconnect.java
, This change introduced a test failure in TestRMContainerAllocator#testUpdatedNodes MAPREDUCE-5632 since it is counting the jobUpdatedNodeEvents. Can someone [~tucu00] or [~bikassaha] verify the patch and make sure that the test reflects the new proper behavior and that I'm not masking a real error in the code.]