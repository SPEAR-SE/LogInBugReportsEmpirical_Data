[Any other details?, [~templedf]
For example, a queue resource usage time slices as follows:
Max Resources: *<memory:10, vCores:10>*
Current used resources:	*<memory:9, vCores:9>*
Pending resource request: *<memory:2, vCores:2>*

This time a node manager report the heartbeat and it has *<memory:10, vCores:10>* available resources.
Before assigning containers it will do follows check:
{code:java}
@Override
  public Resource assignContainer(FSSchedulerNode node) {
    Resource assigned = Resources.none();
    if (LOG.isDebugEnabled()) {
      LOG.debug("Node " + node.getNodeName() + " offered to queue: " +
          getName() + " fairShare: " + getFairShare());
    }

    if (!assignContainerPreCheck(node)) {
      return assigned;
    }
{code}

Because it used resources is less than maxResources. So it will assign *<memory:2, vCores:2>* to
this queue, and in this time the queue's used resources exceed *maxResources*.

, That's valid. The solution would be adding the resource request to current usage, and comparing the new resource usage with the maxResource., I would like to work on this one if you don't mind

I think two things are getting mixed up: the queue used resources are not linked to the node. It is the sum of all the resources of containers from applications that run in a queue. A node heartbeat with a changed usage does not mean that the usage changed because an application in the queue has changed it. It could have changed due to a different queue/application adding a container.

We're also not allocating anything just yet and have thus not gone over. When the application is updated, at a later point in time, that is when we do that check. We just have a preliminary check here to see if we can offer this node to the queue. Another point to take into account: we are not checking what the application asked for here. That is the next step that follows just below when we run over all the applications that have a demand:

{code}
    for (FSAppAttempt sched : fetchAppsWithDemand(true)) {
      if (SchedulerAppUtils.isPlaceBlacklisted(sched, node, LOG)) {
        continue;
      }
      assigned = sched.assignContainer(node);
{code}

This is the earliest we can find what the ask is. If there are more applications with a demand for the queue we walk over the list. We call [assignContainer |https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java#L830]
and that is where the checks happen.
One of the checks we perform is in hasContainerForNode for the FSAppAttempt:
{code}
    } else if (!getQueue().fitsInMaxShare(resource)) {
      // The requested container must fit in queue maximum share
      updateAMDiagnosticMsg(resource,
          " exceeds current queue or its parents maximum resource allowed).");

      ret = false;
{code}

Which makes the allocation fail and thus we drop out and check the next request for the application and if that all fails we check the next application in the list from apps with demand.

Do you have any logs that show that this is not working as it should?
, [~wilfreds] You can take it for free!, Thanks [~wilfreds] for pointing out. {{FSQueue#fitsInMaxShare}} does check the max resource by recursively bottom-up traversing schedulable tree. I goofed., Based on the current analysis I do not think we have a problem.
[~daemon] if you have logs that show this is not working please attach otherwise I will close this as not a problem, No issue found the code shows that we check the queue size in the FS and we have no logs that show this is not working]