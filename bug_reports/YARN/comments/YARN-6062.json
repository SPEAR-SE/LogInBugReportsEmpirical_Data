[Is that due to NM metric leak? If so, HADOOP-13362 should fix this., We need more details. Could you provide as much information as you can, including
- version of hadoop you're running
- the conditions under which this happens
- any heap analysis from the heap dump you may have, Recently,  similar issue(YARN-6017) has been reported in same community Hadoop version. But not sure root cause is same. It would be help full if we get more info such as 
# *jmap -heap 6653*
# out put of */proc/6653/smaps*
# jstack 6653 (do not help directly, but to see thread details)

IIUC, HADOOP-13362 leads to heap memory OOM right. But here RES memory is leaking. Does HADOOP-13362 fixes RES leak also? cc:-/[~djp], Also, the OS and java versions. It does sound like a native memory growth, but without relevant information, the problem won't be diagnosed., bq. HADOOP-13362 leads to heap memory OOM right. But here RES memory is leaking.
In my understanding, RES include a process' heap memory. No?

We have met so many NM OOM cases in different 2.7.x clusters, all of them are end up with NM metrics leak that HADOOP-13362 resolved. As mentioned by [~sjlee0], I would suggest to provide dump analysis with some tools (like MAT, etc.) rather than guessing from limited information. It should be clear soon afterwards., bq. In my understanding, RES include a process' heap memory. No?
Yes, JVM heap is included. But if heap size is growing, then JVM throws an OOM and kill the process.  But if leak in native memory, then process JVM wont get OOM and it continues to run as long as system memory is utilized fully and OS will kill the process. 

As per analysis in YARN-6017, this issue also looks very similar from given information(NM maximum heap memory configured is *2GB* but RES is grown till *10GB*). In YARN-6017, it was very clear(from /proc/<pid>/smaps) that native memory was leaking. But did not get what native calls were leaking :-(

However, lets wait for reporter to give all informations. , jmap、jstack、 /proc/pid/smaps  file  information , Already attached, mat  information:

使用的堆内存	41M
对象数量	413,471
类数量	6,033
Class Loader 数量	92
GC ROOT 数量	1,904
文件格式	hprof
日期	2016-12-27 10:38:18
位数	64-bit

 类名称   (类ID)	对象个数	本身所占内存	本身加引用所占内存
java.util.concurrent.ConcurrentHashMap   (37247)	530	41K	25M
java.util.concurrent.ConcurrentHashMap$Segment[]   (41421)	530	78K	25M
java.util.concurrent.ConcurrentHashMap$Segment   (34044)	5,938	278K	25M
java.util.concurrent.ConcurrentHashMap$HashEntry[]   (37906)	5,938	1,239K	25M
java.util.concurrent.ConcurrentHashMap$HashEntry   (34043)	41,944	1,966K	25M
byte[]   (35091)	2,566	16,633K	16M
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService   (41148)	1	0K	11M
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl   (37582)	138	13K	11M
org.apache.hadoop.mapred.ShuffleHandler$HttpPipelineFactory   (45434)	1	0K	10M
org.apache.hadoop.mapred.ShuffleHandler$Shuffle   (45435)	1	0K	10M
org.apache.hadoop.mapred.IndexCache   (89561)	1	0K	10M, It appears to be attached details are different from described NM process. Would you confirm that is it same process or different?, [~gehaijiang]
Could you provide java version used, java version "1.7.0_65"
Java(TM) SE Runtime Environment (build 1.7.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode), pid  6653 process restart.     Cluster nodemanager have memory leaks, [~gehaijiang]
Could you please have a look at [YARN-6017|https://issues.apache.org/jira/browse/YARN-6017?focusedCommentId=15812167&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15812167]

Is it possible to change the JRE version and check by any chance., Is NM recovery enabled ?, thanks！   JDK 7U80  and  JDK8  Is using beta,  JDK 7U45 as per the github report the issue is not available.]