[h2. my test cluster configuration files:


h3. core-ste.xml:
{code:xml}
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://htc2n1:8020</value>
  </property>
</configuration>
{code}


h3. hdfs-site.xml:
{code:xml}
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///grid/0/hadoop/hdfs/namenode,file:///grid/1/hadoop/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///grid/1/hadoop/hdfs/data,file:///grid/2/hadoop/hdfs/data</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>file:///grid/0/hadoop/hdfs/namesecondary,file:///grid/1/hadoop/hdfs/namesecondary</value>
  </property>
  <property>
    <name>dfs.permissions.superusergroup</name>
    <value>hdfsadmin</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>htc2n2:50090</value>
  </property>
  <property>
    <name>dfs.namenode.http-address</name>
    <value>htc2n1:50070</value>
  </property>
</configuration>
{code}


h3. yarn-site.xml:
{code:xml}
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
   <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>htc2n2</value>
  </property>
</configuration>
{code}

h3. mapred-site.xml
{code:xml}
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
{code}


h3. capacity-scheduler.xml:
{code:xml}
<configuration>

  <property>
    <name>yarn.scheduler.capacity.maximum-applications</name>
    <value>10000</value>
    <description>
      Maximum number of applications that can be pending and running.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>0.1</value>
    <description>
      Maximum percent of resources in the cluster which can be used to run 
      application masters i.e. controls number of concurrent running
      applications.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.resource-calculator</name>
    <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
    <description>
      The ResourceCalculator implementation to be used to compare 
      Resources in the scheduler.
      The default i.e. DefaultResourceCalculator only uses Memory while
      DominantResourceCalculator uses dominant-resource to compare 
      multi-dimensional resources such as Memory, CPU etc.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default</value>
    <description>
      The queues at the this level (root is the root queue).
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>100</value>
    <description>Default queue target capacity.</description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
    <value>1</value>
    <description>
      Default queue user limit a percentage from 0.0 to 1.0.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
    <value>100</value>
    <description>
      The maximum capacity of the default queue. 
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.default.state</name>
    <value>RUNNING</value>
    <description>
      The state of the default queue. State can be one of RUNNING or STOPPED.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.acl_submit_applications</name>
    <value> group1</value>
    <description>
      The ACL of who can submit jobs to the default queue.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.acl_administer_queue</name>
    <value> group1</value>
    <description>
      The ACL of who can administer jobs on the default queue.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
    <value> group1</value>
    <description>
      The ACL of who can submit jobs to the default queue.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
    <value> group1</value>
    <description>
      The ACL of who can administer jobs on the default queue.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.node-locality-delay</name>
    <value>40</value>
    <description>
      Number of missed scheduling opportunities after which the CapacityScheduler 
      attempts to schedule rack-local containers. 
      Typically this should be set to number of nodes in the cluster, By default is setting 
      approximately number of nodes in one rack which is 40.
    </description>
  </property>

</configuration>
{code}, from the resourcemanager log file:
{quote}
2014-08-21 14:37:21,550 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 4
2014-08-21 14:37:22,573 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 4 submitted by user user1
2014-08-21 14:37:22,573 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=user1    IP=9.148.29.243 OPERATION=Submit Application Request    TARGET=ClientRMService  RESULT=SUCCESS  APPID=application_1408540602935_0004                                                                                                                                                                                
2014-08-21 14:37:22,573 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1408540602935_0004                                                        
2014-08-21 14:37:22,573 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1408540602935_0004 State change from NEW to NEW_SAVING                                                
2014-08-21 14:37:22,573 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1408540602935_0004                                                        
2014-08-21 14:37:22,573 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1408540602935_0004 State change from NEW_SAVING to SUBMITTED                                          
2014-08-21 14:37:22,578 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1408540602935_0004 user: user1 leaf-queue of parent: root #applications: 1                                                                                                                                                                                                
2014-08-21 14:37:22,578 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1408540602935_0004 from {color:red}user: user1, in queue: default{color}      
2014-08-21 14:37:22,579 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1408540602935_0004 State change from SUBMITTED to ACCEPTED                                            
2014-08-21 14:37:22,579 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1408540602935_0004_000001                                           
2014-08-21 14:37:22,579 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1408540602935_0004_000001 State change from NEW to SUBMITTED                            
2014-08-21 14:37:22,580 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1408540602935_0004 from user: user1 activated in queue: default              
2014-08-21 14:37:22,581 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1408540602935_0004 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@2690c123, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1              
2014-08-21 14:37:22,581 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1408540602935_0004_000001 to scheduler from user user1 in queue default                                                                                                                                                                                               
2014-08-21 14:37:22,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1408540602935_0004_000001 State change from SUBMITTED to SCHEDULED                      
2014-08-21 14:37:23,209 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1408540602935_0004_01_000001 Container Transitioned from NEW to ALLOCATED                   
2014-08-21 14:37:23,210 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=user1    OPERATION=AM Allocated Container        TARGET=SchedulerApp     RESULT=SUCCESS  APPID=application_1408540602935_0004  CONTAINERID=container_1408540602935_0004_01_000001
...
2014-08-21 14:37:54,564 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER={color:red}user2{color}    IP=9.148.29.244 OPERATION=Kill Application Request      TARGET=ClientRMService  RESULT=SUCCESS  APPID=application_1408540602935_0004
{quote}, [~MeMir] I think you're missing a setting in your yarn-site.xml. You need to set "yarn.acl.enable" to true and "yarn.admin.acl" to the users and/or groups who are administrators. You can find more details [here|http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml].

When an request to kill an app is submitted, YARN checks for administrator privileges and queue administrator privileges. If yarn.acl.enable is set to false(by default), any user can kill any app. In addition, please don't forget to set yarn.admin.acl to the admin users because the default for that is "*" which also means that any user is an admin., Thank you Varun, 
You're right I missed the yarn.admin.acl setting.

After setting the  yarn.acl.enable (to true) and  yarn.admin.acl (to yarn), it now behaves as expected.


Maybe It's worth mentioning that in the [CapacityScheduler|http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html] doc page, Your suggestion makes sense. Can you file a separate JIRA for that and resolve this one as invalid? Thanks!, I was missing the following setting in my yarn-site.xml:
"yarn.acl.enable" => true
"yarn.admin.acl"  => <the default is '*' which allow everyone to be admin>
]