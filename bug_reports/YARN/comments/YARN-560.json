[Wasn't clear, so commenting: This is closed as duplicate of MAPREDUCE-3688., Just caught with all the discussion at MAPREDUCE-3688, seems like it isn't fixed completely. Reopening., [~raviprak] says [on MAPREDUCE-3688|https://issues.apache.org/jira/browse/MAPREDUCE-3688?focusedCommentId=13606901&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13606901]:
bq. From my testing on trunk, I notice that even for the case where the AM goes over container limits (which I trigger with -Dyarn.app.mapreduce.am.resource.mb=512 -Dyarn.app.mapreduce.am.command-opts="-Xmx3500m" on a sleep job), sometimes the error is propagated back and sometimes its not. Can you please corroborate this? When State == FinalState == FAILED, the error is propagated back. However about half the times, State == FINISHED and FinalState == KILLED, in which case there is no message anywhere to help me. Not in the diagnostics, and there are no logs., I'll work on this between other stuff I'm working on. I've been trying to find where the race is. The events from the NM looked good. Seems like the race is in the depths of RMAppImpl or RMAppAttemptImpl. Will keep digging.

I just checked, we can see this with the fair scheduler as well as the capacity scheduler., The race seems to be in between receiving a RMAppAttemptContainerFinishedEvent in RMContainerImpl.java's FinishedTransition and a FinishApplicationMasterRequest in ApplicationMasterService. Any preferences on how to fix it? A couple of options come to my mind:
1. Make the AM not send the FinishApplicationMasterRequest when it detects (if it can) that the NM is killing it.
2. Have the NM contact the RM before killing an AM container so that when the AM does send the FinishApplicationMasterRequest, the RM knows to ignore it.
3. Make the RMAppAttemptEventType.CONTAINER_FINISHED change the state of the AppAttempt even after FinishApplicationMasterRequest has changed the state to FINISHING / KILLED.

What do you think?, I discussed this with Tom and Daryn.

* If the NM was able to tell the AM why its being shot, the AM could technically include that diagnostic message in the FinishApplicationMasterRequest however such a mechnanism (to tell the AM the reason its being shot) doesn't exist yet and would be very brittle.
* If the NM told the RM before shooting the AM, there would still be a race. What if the AM had completed everything it needed to do, and then got shot by the NM? Then the job would have been successful but be marked as FAILED by the RM.
* If we changed the State of a FINISHED / KILLED application to FAILED / FAILED on receiving RMAppAttemptEventType.CONTAINER_FINISHED, the client might still only get the FINISHED / KILLED message and the user would have to go to the RM page to see what really happened. Our current opinion is that this is probably the best way to go for now.

Opinions anybody?
, What do you all think about this patch? Applies to trunk., The way to surely trigger the race is
1. Debug the NM and RM. 
2. Set a breakpoint on these lines: 
  - ContainerLaunch.java:347 : new DelayedProcessKiller(user, processId, sleepDelayBeforeSigKill, Signal.KILL, exec).start();
  - RMContainerImpl.java:289 : RMContainerFinishedEvent finishedEvent = (RMContainerFinishedEvent) event;
Run a job sure to exceed its container limits. Continue on both breakpoints (its just to give enough time to the AM to unregister)
, Attaching a patch for trunk and branch-2. Doesn't apply cleanly to branch-0.23, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578979/YARN-560.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/749//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/749//console

This message is automatically generated., Ravi,

The patch looks good to me.  Nit: can you use block comments ( /** ... */ ) instead of single line comments over the tests?, Thanks for your review Sandy. This patch uses the block comment, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578990/YARN-560.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/750//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/750//console

This message is automatically generated., Tried to understand this, am confident that I am reading it correctly at the least because of the test-cases, but need your help to make sure I am not wrong.

So
 - In the usual case of NM killing AM while it is running and before AM unregisters with RM, the error is visible on UI. Right?
 - If the AM successfully unregistered with RM, but got killed before it could cleanly exit, that's when the error is not visible on the UI? And is the only case addressed in the patch?

Assuming the above, comments on the patch:
 - We shouldn't be changing an AppAttempt from FINISHED state to FAILED state - FINISHED is a final state. At any rate, I don't see when this can happen - a RMAppAttempt will move to FINISHED state only after the remote container is clearly done/exited.
 - Secondly Ii indeed the AM is killed after it unregistered, should we mark the App as FAILED like the patch does? I don't think so but I may be wrong - I unfortunately didn't get to review MAPREDUCE-4157 which originally added the FINISHING state. Bobby/Jason?, State diagram for reference, {quote}
* In the usual case of NM killing AM while it is running and before AM unregisters with RM, the error is visible on UI. Right?
{quote}
Right. After the NM has sent SIGTERM to the AM and then a SIGKILL, before the AM is able to send an unregister to the RM, the diagnostic message is visible.

{quote}
* If the AM successfully unregistered with RM, but got killed before it could cleanly exit, that's when the error is not visible on the UI? And is the only case addressed in the patch?
{quote}
When the AM gets a SIGTERM, it begins shutdown. If it is able to send an unregister message to the RM, it sets off the RMAppAttemptImpl onto the path to FINISHING->FINISHED. Irrespective of whether the AM got enough time to exit cleanly or not, the CONTAINER_FINISHED event from the NM containing the diagnostic message will not effect the RMAppAttemptImpl now. Here's the state diagram for the RMAppAttemptImpl for reference. 
!RMAppAttemptImplsmall.png!

We didn't like this approach much either. Could you please think over how to fix the race?]