[Hi Thomas,

Would you please share some more information about reproducing the exception? More logs, configuration files, system information, etc.

Thanks,
Zhijie, Sorry the only other thing I can think of that would matter is having security on.  I had security on and the code that throws the exception is looking at the Token, so if you don't have security on you probably won't see it.

Other then that it was running any simple job - sleep, wordcount.  , That's correct, the validation today is only done in secure mode. Extending it to non-secure mode is pending - MAPREDUCE-2744.

Also the scheduler? The scheduler in use doesn't seem to be normalizing requests correctly., If you are asking what scheduler I was using it is the CapacityScheduler as stated in description, otherwise can you clarify the question., CapacityScheduler of branch-2 seems to normalize the request as we expect. Below is the additional log I've enforced to output.

2013-02-05 02:04:59,997 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: ask before: <memory:1536, vCores:1>
2013-02-05 02:04:59,997 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: ask end: <memory:2048, vCores:1>

It is very likely that authorizeRequest failed because StartContainerRequest.getContainerLaunchContext().getResource() was not normalized (still 1536 in log).
, I agree, independently verified CS seems to work fine.

Looks like AMLauncher might be broken... it uses Container from ApplicationSubmissionContext rather than allocated Container from RMAppAttempt.getMasterContainer., Maybe something like this?

{code}
diff --git hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java hadoop-ya
index c8bd877..3e63462 100644
--- hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java
+++ hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java
@@ -759,8 +759,10 @@ public void transition(RMAppAttemptImpl appAttempt,
           EMPTY_CONTAINER_RELEASE_LIST);
 
       // Set the masterContainer
-      appAttempt.setMasterContainer(amContainerAllocation.getContainers().get(
-                                                                           0));
+      Container amContainer = amContainerAllocation.getContainers().get(0);
+      appAttempt.setMasterContainer(amContainer);
+      appAttempt.submissionContext.getAMContainerSpec().setResource(
+          amContainer.getResource());
{code}, I've tested the change, with which ContainerManagerImpl saw the updated the resource, i.e., 2048 mem. It should fix exception.

The user define 1.5G for AM container, such that we need to update the resource of ApplicationSubmissionContext according to the real allocated size. One remaining issue is whether other containers automatically created by the system will be assigned the memory size which is not the multiple of the min alloc size or not. If it will, the problem will happen on the non-AMcontainer as well., I agree with you that at that point the submission context and the allocated container have different resources, the weird thing is that this bug doesn't appear in 0.23 and when I print out the info at the same spot in 0.23 they match. Both say 2048, where as printing them in 2.0 at that point one is 2048 and the other 1536.  

I'm just wondering if the bug isn't really somewhere else.  I'll dig a bit more., Ok, I figured out what the difference is. The submission context am container spec is being updated on the call in RMAppAttempImpl to appAttempt.scheduler.allocate.  In 0.23 in normalizeRequests it modifies the resource directly:
    ask.getCapability().setMemory(


Whereas in 2.0 it sets the capability so it doesn't modify the original resource that was based in up in the RMAppAttemptImpl:
ask.setCapability(normalized);

Hacking a quick fix in and changing 2.X to set ask.getCapability().setMemory(normalized.getMemory()) instead of calling setCapability in normalRequest fixes the issue. 

, This seems to be root reason of the bug. ApplicationSubmissionContext has the reference to the original resource object, while appAttempt receive an updated one.

I've checked this fix as well, and ContainerManagerImpl saw 2048 mem as well.

One problem is that the fix failed TestSchedulerUtilties. I'll fix it and create a new patch. Thanks!, Thanks Thomas!

Zhijie, to be safe I think we should go with Thomas's fix rather than mine since it's fairly late in the game for 2.0.3. Ok? Thanks., Sure, I've attached the newest patch, which adopts Thomas' fix. And I've also updated the related test cases. Thanks!, I think we should add a comment there about why we are doing it that way. If Arun agrees, file a followup jira to investigate better long term fix., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12568128/YARN-370-branch-2_1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/383//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/383//console

This message is automatically generated., I just committed this. Thanks Zhijie & Thomas!, Integrated in Hadoop-trunk-Commit #3327 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3327/])
    YARN-370. Fix SchedulerUtils to correctly round up the resource for containers. Contributed by Zhijie Shen. (Revision 1442840)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1442840
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestSchedulerUtils.java
, Integrated in Hadoop-Yarn-trunk #119 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/119/])
    YARN-370. Fix SchedulerUtils to correctly round up the resource for containers. Contributed by Zhijie Shen. (Revision 1442840)

     Result = FAILURE
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1442840
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestSchedulerUtils.java
, Integrated in Hadoop-Hdfs-trunk #1308 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1308/])
    YARN-370. Fix SchedulerUtils to correctly round up the resource for containers. Contributed by Zhijie Shen. (Revision 1442840)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1442840
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestSchedulerUtils.java
, Integrated in Hadoop-Mapreduce-trunk #1336 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1336/])
    YARN-370. Fix SchedulerUtils to correctly round up the resource for containers. Contributed by Zhijie Shen. (Revision 1442840)

     Result = SUCCESS
acmurthy : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1442840
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/TestSchedulerUtils.java
]