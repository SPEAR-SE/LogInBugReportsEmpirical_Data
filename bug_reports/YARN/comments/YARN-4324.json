[When AM is launched and registered, RM expects AM to send heartbeat in timely manner. If RM does not receive an heartbeat from AM for certain time(default is 10min) then RM kills the AM. This is expected behavior. 

In your scenario, try to find what expiry happened either AM heartbeat expiry OR container expiry. This info you will get in ResourceManager log. Does NodeManager is restarted? If Yes, and NM recovery not enabled then it is expected behavior., Because the job failure is random, i dump the am jstack and pstack when am from RUNING to KILLING event, I upload my log, Thanks for the jstack report!! would you provide AM and RM logs?
, I upload the new jstack and am logs, Thank you for your attention，I have already upload AM Logs, From AM logs I see that Job is still running. But AM received an kill signal. Not sure who killed it. Does preemption enabled? 
Can you update RM logs too?, I found the RMContainerAllocator last contact RM in AM logs ，and it Does not apply to reduce

2015-12-15 02:57:39,893 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:732 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:5773 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:8995 ContRel:3222 HostLocal:5310 RackLocal:338
 
AM received an kill signal 

2015-12-15 03:01:29,383 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1449835724839_219910_m_001345_1 TaskAttempt Transitioned from NEW to UNASSIGNED
2015-12-15 03:08:54,073 INFO [Thread-1] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler.

I guess AM in 10min not send a heartbeat to RM，RM logs Rolling too fast，I will try to get RM logs and update




, i found this message in the jstack,is a  JDK epollWait bug?:

"IPC Client (2118999553) connection to RMHost:8030 from UserName" daemon prio=10 tid=0x00007f298c664000 nid=0x6e2d runnable [0x00007f297d9a8000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked <0x0000000785182940> (a sun.nio.ch.Util$2)
	- locked <0x0000000785182930> (a java.util.Collections$UnmodifiableSet)
	- locked <0x0000000785182718> (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:457)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	- locked <0x000000078023aa40> (a java.io.BufferedInputStream)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:995)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:891)

, I update other AM Log]