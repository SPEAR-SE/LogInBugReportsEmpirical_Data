[Hi Krishna,

Please set yarn.app.mapreduce.am.command-opts to include a more reasonable -Xmx. Or perhaps increase yarn.nodemanager.vmem-pmem-ratio to a higher ratio if you don't need that much physical memory.
, Hi Ravi,

  The problem I am reporting is, why should it give this kind of error very
randomly when it could run fine all other times with the same amount of
memory allocations. I see this error once in 500 times. Also, as I said it
is only the date command that I am running with the Distributed Shell
example jar that comes with the hadoop-2.0.3-alpha. So, I suppose this
should work the same way all the times.

Thanks,
Kishore




, What I have observed today is that this error is coming at some regular
intervals of 50 minutes. And at that particular interval of time, I am
seeing the following kind of messages in the node manager's log:  So, I
think being the node manager busy with some other task like this monitoring
is causing the error of virtual memory for AM's container.

2013-04-12 15:51:02,048 INFO  [Container Monitor]
monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(346)) -
Starting resource-monitoring for

container_1365688251527_6643_01_000003
2013-04-12 15:51:02,048 INFO  [Container Monitor]
monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(346)) -
Starting resource-monitoring for

container_1365688251527_6642_01_000004
2013-04-12 15:51:02,049 INFO  [Container Monitor]
monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(346)) -
Starting resource-monitoring for

container_1365688251527_6641_01_000005
2013-04-12 15:51:02,049 INFO  [Container Monitor]
monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(346)) -
Starting resource-monitoring for

container_1365688251527_6640_01_000006
2013-04-12 15:51:02,049 INFO  [Container Monitor]
monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(356)) -
Stopping resource-monitoring for

container_1365688251527_6524_01_000001
2013-04-12 15:51:02,049 INFO  [Container Monitor]
monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(356)) -
Stopping resource-monitoring for

container_1365688251527_6525_01_000002
2013-04-12 15:51:02,049 INFO  [Container Monitor]
monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(356)) -
Stopping resource-monitoring for

container_1365688251527_6525_01_000003
2013-04-12 15:51:02,049 INFO  [Container Monitor]
monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(356)) -
Stopping resource-monitoring for

container_1365688251527_6525_01_000004



On Sun, Mar 24, 2013 at 3:54 PM, Krishna Kishore Bonagiri <

, Added information dated 24th March, and 15th April. Please see above. As mentioned, it is not really issue with using more than allocated for the container. This issue happens randomly when ran the same application either mine, or the Distributed Shell example also, with just a date command.

Thanks,
Kishore, @Kishore, thanks for the logs. Looking at them. Would it be possible for you to generate another set but with the RM and NM running at DEBUG log level? , No problem Hitesh, I can do that. Can you please tell me how do I set that
log level?

Thanks,
Kishore



, [~write2kishore] can you please provide debug logs for RM and NM? This might be useful in setting DEBUG logs. [Setting DEBUG logs |https://issues.apache.org/jira/browse/YARN-541?focusedCommentId=13706650&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13706650], Not sure if any more logs are any useful. The AM was started with Xmx to be 128MB and it is using 532MB virtual memory and the shell itself of about 100MB. For some reason, this 1 in a 500 time error of JVM using vmem more than usual is what is killing it. May be something to do with specific version of linux and/or JVM? You could try submitting with more memory to avoid it, but it'll be worth tracking the randomness down.

Clearly YARN can't do much in this case, other than may be prolonging the monitoring cycle - we watch over memory processes for about 3 seconds (1 cycle/3seconds) and kill them. You could try increasing yarn.nodemanager.container-monitor.interval-ms., Haven't gotten a response on my last comment in a while. IAC, it is unlikely YARN can do much in this situation. Closing this again as not-a-problem.]