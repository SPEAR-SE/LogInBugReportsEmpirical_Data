[When RMNodeImpl do DeactivateNodeTransition, check whether there is already a new NodeManager with different port, if yes, don't add it to "Losts Nodes"., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12637354/YARN-1888.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/3482//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3482//console

This message is automatically generated., Don't think this is a bug. People should be able to run multiple NMs on a node. The port 0 is primarily for convenience. , Closing this as "Not A Problem". Please re-open if you think otherwise. , The problem here is our cluster use port 0, but when restart NodeManager, the "Lost Nodes" became inaccurate:
Host A have a NodeManager with ID: $HOSTA:$PORTA,
after restart, the NodeManager now with ID: $HOSTA:$PORTB,
since the ID changed, so ResourceManager didn't think it is a reconnected NodeManager.
Then few minutes later, NodeManager $HOSTA:$PORTA expired, and marked as LOST.
This make people confused, at first I don't think it is a bug too, but after few peoples asked me why there are so many nodes LOST, then I come up with this simple patch: if there is already another NodeManager in the same node (in real production cluster, I don't think people will start more than one NodeManager on one machine), then don't mark expired NodeManager as LOST.



, I agree with [~kasha] on this.  A nodemanager coming up on a different port isn't necessarily the same nodemanager from a previous instance.  For exampe, the minicluster runs multiple nodes on the same host with different ports, so if one of these nodes disappears then it will no longer be reported as lost with this patch since there are others still running with the same host?

I think the real fix is to run the nodemanager with a non-ephemeral nodemanager port specified in yarn-site.xml.  This helps solve a number of issues:

# lost nodes count will be accurate
# a NM that reboots and rejoins the cluster before the RM expires the old instance will be correctly recognized as the same NM, and we avoid the RM thinking there are really two NMs on the host for up to the NM expiry interval
# attempts to start a subsequent NM on the same host where an NM is already running will fail rather than accidentally overcommit the node, Thank you Jason Lowe and  Karthik Kambatla for your time.
Now I agree with you, close it as "Not a Problem".]