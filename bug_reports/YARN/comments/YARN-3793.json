[[~kasha] thanks for reporting this jira.. It's dupe of HADOOP-11878..., [~kasha] one possible scenario is : When disk became bad and NM stopped.. I had seen this NPE( where good dir's will be null)..

{noformat}
2015-06-19 03:09:10,528 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl: Uploading logs for container container_1434452428753_0522_01_000162. Current good log dirs are 
2015-06-19 03:09:10,528 ERROR org.apache.hadoop.yarn.server.nodemanager.DeletionService: Exception during execution of task in DeletionService
java.lang.NullPointerException
	at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)
	at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:458)
	at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}, [~kasha], I think I know whats happening.
When disks become bad(say due to disk full), there is a problem when uploading container logs.

In {{AppLogAggregatorImpl#doContainerLogAggregation}} only good log directories are considered for log aggregation. This leads to {{AggregatedLogFormat#getPendingLogFilesToUploadForThisContainer}} returning no log files to be uploaded.

The caller of {{doContainerLogAggregation}} is {{AppLogAggregatorImpl#uploadLogsForContainers}} which as can be seen under will call {{DeletionService#delete}}. If {{uploadedFilePathsInThisCycle}} is empty *(which will be if disks are full)*, this will lead to both sub directory and base directories being null. This explains the NPEs' being thrown.
When these deletion tasks are stored in state store, they will be stored with nulls as well and this can explain why it happens on recovery as well.
{code}
      boolean uploadedLogsInThisCycle = false;
      for (ContainerId container : pendingContainerInThisCycle) {
        ContainerLogAggregator aggregator = null;
        if (containerLogAggregators.containsKey(container)) {
          aggregator = containerLogAggregators.get(container);
        } else {
          aggregator = new ContainerLogAggregator(container);
          containerLogAggregators.put(container, aggregator);
        }
        Set<Path> uploadedFilePathsInThisCycle =
            aggregator.doContainerLogAggregation(writer, appFinished);
        if (uploadedFilePathsInThisCycle.size() > 0) {
          uploadedLogsInThisCycle = true;
        }
        this.delService.delete(this.userUgi.getShortUserName(), null,
          uploadedFilePathsInThisCycle
            .toArray(new Path[uploadedFilePathsInThisCycle.size()]));
       ......
   }
{code}

Log aggregation should consider full disks as well otherwise there will be nothing to be aggregated if disks are full. Anyways log aggregation would lead to deletion of local logs.

I verified the occurrence of this issue via TestLogAggregationService#testLocalFileDeletionAfterUpload by making good log directories return nothing.
, [~kasha], can I work on this JIRA ?, Thanks for the pointing this out. Looked for scenarios when disk becomes bad and found one issue., [~varun_saxena] - all yours. , Thanks, While NPEs' are a problem, on close look at the code shows that there is a bigger problem here and that is *container logs can be lost* if disk has become bad(become 90% full).

When application finishes,  we upload logs after aggregation by calling {{AppLogAggregatorImpl#uploadLogsForContainers}}. But this call in turns checks the eligible directories on call to {{LocalDirsHandlerService#getLogDirs}} which in case of disk full would return nothing. So none of the container logs are aggregated and uploaded.
But on application finish, we also call {{AppLogAggregatorImpl#doAppLogAggregationPostCleanUp()}}. This deletes the application directory which contains container logs. This is because it calls {{LocalDirsHandlerService#getLogDirsForCleanup}} which returns the full disks as well.

So we are left with neither aggregated logs for the app nor the individual container logs for the app.

This sounds like a critical if not a blocker. [~kasha], [~jlowe], can you have a look ? I will upload a patch shortly.


, It sounds like the NPEs are scary in the logs but benign in practice, since they occur in situations where we don't actually want to delete anything anyway.

Regarding loss of logs, I agree with your analysis.  Makes me think there should be a getLogDirsForRead that can be used for places to search for files that are already there.  The NPE and the log loss are unrelated, so arguably the blocker of log loss should be tracked in a separate JIRA., Thanks for looking at this [~jlowe].
I will raise a separate JIRA for this.

getLogDirsForRead will be same as getLogDirsForCleanup but I guess would be semantically correct to use it., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m 44s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 25s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 28s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 21s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 37s | The applied patch generated  1 new checkstyle issues (total was 17, now 17). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 33s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 31s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m  9s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |   6m 20s | Tests passed in hadoop-yarn-server-nodemanager. |
| | |  43m 11s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12741937/YARN-3793.01.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / aa5b15b |
| checkstyle |  https://builds.apache.org/job/PreCommit-YARN-Build/8349/artifact/patchprocess/diffcheckstylehadoop-yarn-server-nodemanager.txt |
| hadoop-yarn-server-nodemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8349/artifact/patchprocess/testrun_hadoop-yarn-server-nodemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8349/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf907.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8349/console |


This message was automatically generated., Moving out all non-critical / non-blocker issues that didn't make it out of 2.7.1 into 2.7.2., Changing the priority back to original. Was bumped up to Critical because of issues fixed in YARN-3850, \\
\\
| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  16m 12s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 37s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 52s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 37s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 31s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 14s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |   6m 17s | Tests passed in hadoop-yarn-server-nodemanager. |
| | |  44m 21s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12742618/YARN-3793.02.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 8e33372 |
| hadoop-yarn-server-nodemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8379/artifact/patchprocess/testrun_hadoop-yarn-server-nodemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8379/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8379/console |


This message was automatically generated., [~kasha@cloudera.com] / [~jlowe], kindly review, +1 lgtm.  Will commit later today if no objections., Thanks, Varun!  I committed this to trunk, branch-2, and branch-2.7., FAILURE: Integrated in Hadoop-trunk-Commit #8108 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8108/])
YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena (jlowe: rev b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java
, Thanks for the review and commit [~jlowe], FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #246 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/246/])
YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena (jlowe: rev b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #976 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/976/])
YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena (jlowe: rev b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #244 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/244/])
YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena (jlowe: rev b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #234 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/234/])
YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena (jlowe: rev b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #2173 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2173/])
YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena (jlowe: rev b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2192 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2192/])
YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena (jlowe: rev b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java
]