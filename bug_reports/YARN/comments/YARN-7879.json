[We hit this before, and it was fixed in YARN-1386 by adding group execute permissions to the directories in the user's filecache.  I think it could be YARN-2185 which added more restrictive permissions on some directories during localization.  I'll run some quick tests locally to verify., This was caused by YARN-2185.  That change locked down the top-level directory for a non-public localized file to 0700 which prevents the nodemanager user from checking file presence on secure clusters.
, Attached a patch that undoes the permission changes from YARN-2185.  Technically we really only need group execute permissions and the group to be the NM group, but that's a much more involved change as it would require changes to the native container executor.  The user's top-level filecache directory is already locked down to 0710 permissions, so tweaking these permissions to be as minimal as possible is not semantically important.  They simply need to be sufficiently permissive to let the NM user stat the localized resources, as the user's filecache directory is preventing arbitrary access.

[~miklos.szegedi@cloudera.com] could you take a look at the patch?, I also manually tested the patch on a secure cluster and verified non-private resources are not re-localized with each application., | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 23s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  4s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m  9s{color} | {color:green} hadoop-yarn-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 46m 28s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | YARN-7879 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12908999/YARN-7879.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux b15ac493c66c 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 4aef8bd |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/19582/testReport/ |
| Max. process+thread count | 407 (vs. ulimit of 5000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/19582/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Are we conformable in assuming all files in filecache are world readable?  In health care, and financial industry, user's default umask is set to 027.  Can there be private files that are exposed as result of the umask change?  Should we check every file, or assume that pipe archive always expands properly with single checksum file test?  Would it be possible to make this detection using privileged access and report back to nodemanager to trigger reinitialization?, bq. Are we conformable in assuming all files in filecache are world readable?

Non-public localized files are not world readable.  The top-level directory of the user's filecache directory is mode 0710 with the NM group, so only the user and those in the NM's group can see files are there regardless of what the permissions of the underlying paths are.  The localized files themselves are mode 0500.  In short, the NM user can see that a file is there but cannot read it., [~jlowe] Thank you for the reply.  We are allowing file cache to be mounted in docker container as read only in YARN-7815.  The risk of exposing filename is marginally small, but I like to confirm that is not a problem even the filename contains sensitive information exposed in docker containers.  Is it possible to use 750 and group is owned by NM's group?  

Can cache directory contain subdirectories to prevent this arrangement from working?
, bq. We are allowing file cache to be mounted in docker container as read only in YARN-7815.

If we are mounting a file cache directory into a container then I assume the user running in the Docker container should have the right to read every file under that file cache directory.  I do not see the security concern there if that's the case, but maybe I'm missing a key scenario that would be problematic?

bq. The risk of exposing filename is marginally small, but I like to confirm that is not a problem even the filename contains sensitive information exposed in docker containers.

The only way I can see it being an issue specific to Docker is if somehow something in the Docker container is not trusted that runs as a different user within the Docker  container (but still in the hadoop group or equivalent for the Docker container) pokes around for the filename.  That thing would have to probe for filenames since there's no read access on the filecache top-level directory, only group-execute permissions.

However I would argue that if the user is running untrusted things within the Docker container it's simply much easier to access the sensitive files _as the user_.  Then there would be access to the file's contents in addition to the filename.

bq. Can cache directory contain subdirectories to prevent this arrangement from working?

Yes, if the cache directory manager is being used there can be subdirectories to limit the total number of entries in a single directory.  In those cases the intermediate directories are setup with similar 0755 permissions so the NM user can access them easily, see ContainerLocalizer#createParentDirs.

This patch is restoring the usercache permissions behavior from before YARN-2185 went in.  YARN-2185 wasn't about addressing directory permissions, but it had a sidecar permission change that broke the ability for the NM to reuse non-public localized resources.  Therefore I'd like to see this go in so we aren't regressing functionality, and if there are concerns/improvements for how usercache permissions are handled we should address those in a separate JIRA.  Either that or we revert YARN-2185, remove the unrelated permissions change, recommit it, and still end up addressing any usercache permissions concerns in a separate JIRA. ;-)

, Thank you, [~shanekumpf@gmail.com] for the report, [~jlowe] for the patch. I checked and the change seems good to me. Since this is a regression, [~eyang], would you mind if I commit it and we continue the discussion here or on another patch?, Thanks for the patch [~jlowe] - I've tested the patch and it fixes the problem I reported. +1 (non-binding) from me., {quote}

The only way I can see it being an issue specific to Docker is if somehow something in the Docker container is not trusted that runs as a different user within the Docker container

{quote}

[~jlowe] Thanks for the reassurance.  I think YARN-7516 and YARN-7221 combination will eliminate the risk to make sure only authorized sudoers can impersonate in docker containers to remove this loophole.

 

[~miklos.szegedi@cloudera.com] Yes, I think this change is fine, and there are possible solutions to eliminate the concerns.  Thanks, +1 I will commit this shortly., Committed to trunk. Thank you for the contribution [~jlowe], for the report and verification [~shanekumpf@gmail.com] and for the review [~eyang]., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13610 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13610/])
YARN-7879. NM user is unable to access the application filecache due to (szegedim: rev c7101fe21ba7b9aa589f0a9266ed34356f30b35f)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestFSDownload.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/FSDownload.java
]