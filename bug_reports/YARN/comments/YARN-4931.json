[Log snippet showing the behavior in detail., If it is helpful, it looks like these applications have a huge number of outstanding container requests (thousands!) and this number has to get drained down to zero before other applications requesting only 1 container can be served., Thanks for reporting this. The issue here seems to be that when the scheduler preempts resources for an application A, it doesn't reserve these resources for A. Before it considers A again for allocation, other applications can take these resources. We are planning to address this as part of YARN-4752. , Hi [~milesc], 
I have met the same issue. After investigation I noticed that if I use minResources property in fair-scheduler.xml and it's greather that queue's FairShare, it might affect the FairShare of other queues and cause this strange behavior after preemption. 

May you please provide your fair-scheduler.xml, yarn-site.xml and mapred-site.xml if possible? It would be helpful for better understanding of the issue. Thanks. , {code}
<?xml version="1.0"?>
<allocations>
    <defaultQueueSchedulingPolicy>drf</defaultQueueSchedulingPolicy>
    <defaultMinSharePreemptionTimeout>5</defaultMinSharePreemptionTimeout>
    <defaultFairSharePreemptionTimeout>20</defaultFairSharePreemptionTimeout>
    <defaultFairSharePreemptionThreshold>0.8</defaultFairSharePreemptionThreshold>
    <defaultQueueSchedulingPolicy>fair</defaultQueueSchedulingPolicy>
</allocations>
{code}
We use Amazon's EMR, so the yarn-site and mapred-site.xml are a part of their managed setup., Hi [~kasha], any update in addressing this issue in FairScheduler? Thanks. , This should have been fixed by YARN-6432, which is included since 2.9]