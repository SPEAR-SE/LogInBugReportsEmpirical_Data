[I suspect YARN-7053 would be the cause of this issue! , Hmmm.. that is interesting, It looks like YARN-7053 was part of 3.0.0-beta1 as well. Don't think anything has changed after that., I tried to debug this issue, but couldn't get any root cause. I used even tests TestZKRMStateStore#testZKRootPathAcls with minor modifications pointing to running ZooKeeper, couldn't reproduce in test. The same error is getting trunk. I believe this issue must exist in 3.0-beta as well.
The recent JIRAs committed wrt state store ACLs are HADOOP-14741 YARN-7053 and YARN-6840. , Hmm, YARN-6840 shouldn't affect it, especially if you are not setting {{yarn.scheduler.configuration.store.class}} to {{zk}}, but there was some minor refactoring there, I'll see if I can repro this...

EDIT: I tried starting to active multiple times (manual failover) and didn't get any errors here. [~rohithsharma] was this consistently reproducible?, [~jhung], thanks for testing..
Were you testing this on trunk or the 2.9.0 RC ? Also, can you maybe share the conf ?

, Hi [~asuresh], actually, I built this from source (bb6a1aa5fac18941ecd15502e0dec23997f81e44).

Here are the relevant confs: {noformat}
  <property>
    <name>yarn.resourcemanager.hostname.rm1</name>
    <value>host1.com</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname.rm2</name>
    <value>host2.com</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>rm1,rm2</value>
  </property>
  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>
  <property>
    <name>yarn.resourcemanager.zk.address</name>
    <value>host1.com:2191</value>
  </property>
  <property>
    <name>hadoop.zk.address</name>
    <value>host1.com:2191</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.id</name>
    <value>rm1</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>test</value>
  </property>
{noformat}
({{yarn.resourcemanager.ha.id}}=rm2 for the other RM)., Looks like that is the last commit on *branch-2.9*. Everything else on *branch-2.9.0* is documentation.

{code}
commit bb6a1aa5fac18941ecd15502e0dec23997f81e44
Author: Arun Suresh <asuresh@apache.org>
Date:   Thu Nov 2 22:49:38 2017 -0700

    Preparing for 2.9.1 development
{code}

If you don't mind and if you have your test setup ready, can you maybe try using bits from the
[RC|http://home.apache.org/~asuresh/hadoop-2.9.0-RC0/], Sure, thanks, switched to the RC, but still wasn't able to reproduce it.

Relevant logs: {noformat}17/11/07 19:50:26 INFO resourcemanager.ResourceManager: Recovery started
17/11/07 19:50:26 INFO recovery.RMStateStore: Loaded RM state version info 1.5
17/11/07 19:50:26 INFO security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.
17/11/07 19:50:26 INFO resourcemanager.RMAppManager: Recovering 0 applications
17/11/07 19:50:26 INFO resourcemanager.RMAppManager: Successfully recovered 0 out of 0 applications
17/11/07 19:50:26 INFO resourcemanager.ResourceManager: Recovery ended
...
17/11/07 19:50:26 INFO resourcemanager.RMAuditLogger: USER=jhung	IP=host1.com	OPERATION=transitionToActive	TARGET=RM	RESULT=SUCCESS
...
17/11/07 19:50:43 INFO resourcemanager.RMAuditLogger: USER=jhung	IP=host1.com	OPERATION=transitionToStandby	TARGET=RM	RESULT=SUCCESS
...
17/11/07 19:56:43 INFO recovery.RMStateStore: Loaded RM state version info 1.5
17/11/07 19:56:43 INFO security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.
17/11/07 19:56:43 INFO resourcemanager.RMAppManager: Recovering 0 applications
17/11/07 19:56:43 INFO resourcemanager.RMAppManager: Successfully recovered 0 out of 0 applications
17/11/07 19:56:43 INFO resourcemanager.ResourceManager: Recovery ended
...
17/11/07 19:56:43 INFO resourcemanager.ResourceManager: Transitioned to active state
17/11/07 19:56:43 INFO resourcemanager.RMAuditLogger: USER=jhung	IP=host1.com	OPERATION=transitionToActive	TARGET=RM	RESULT=SUCCESS
{noformat}

This was after issuing:{noformat}yarn rmadmin -transitionToActive rm1
yarn rmadmin -transitionToStandby rm1
yarn rmadmin -transitionToActive rm1
{noformat}

I also turned on automatic failover, and didn't get any exceptions there either., Thanks [~jhung] !!, From the vote for 3.0.0-beta1 RC0, I can see [~rohithsharma] tested HA.
[Link to the archive|http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201710.mbox/ajax/%3CCAGi%3DjZDPP%2BUwamnPQz89t4xy2bnPbvDUtYRRWG6sN3Dnojf05w%40mail.gmail.com%3E]:
{code}
Built from source and deployed YARN HA cluster with ATSv2 enabled in non-secured cluster.
- tested for RM HA/work-preservring-restart/ NM-work-preserving restart for ATSv2 entities.
- verified all ATSv2 REST end points to retrieve the entities
- ran sample MR jobs and distributed jobs
{code}

If I'm not wrong, YARN-7053 and HADOOP-14741 were already in 3.0.0-beta1., Verified it myself on a fresh cluster running 2.9.0 RC.

Manual mode:
{code}
2017-11-07 15:39:14,420 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2017-11-07 15:39:14,420 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hadoop   IP=10.200.91.22 OPERATION=transitionToActive    TARGET=RM       RESULT=SUCCESS
....
2017-11-07 15:40:02,791 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hadoop   IP=10.200.91.22 OPERATION=transitionToStandby   TARGET=RM       RESULT=SUCCESS
2017-11-07 15:41:08,378 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/home/hadoop/cisl-linux-021/conf/yarn-site.xml
....
2017-11-07 15:41:08,624 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2017-11-07 15:41:08,624 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hadoop   IP=10.200.91.22 OPERATION=transitionToActive    TARGET=RM       RESULT=SUCCESS
{code}

Commands:
{code}
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -transitionToStandby rm1
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                standby
cisl-linux-024:8033                                standby
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -transitionToActive rm2
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                standby
cisl-linux-024:8033                                active
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -transitionToStandby rm2
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                standby
cisl-linux-024:8033                                standby
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -transitionToActive rm1
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                active
cisl-linux-024:8033                                standby
{code}

Automatic mode:
{code}
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                active
cisl-linux-024:8033                                standby
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                active
cisl-linux-024:8033                                standby
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn-daemon.sh stop resourcemanager
stopping resourcemanager
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
17/11/07 15:49:33 INFO ipc.Client: Retrying connect to server: cisl-linux-021/10.200.91.22:8033. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
cisl-linux-021:8033                                Failed to connect: Call From cisl-linux-021/10.200.91.22 to cisl-linux-021:8033 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
cisl-linux-024:8033                                active
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /home/hadoop/cisl-linux-021/yarn-log/yarn-hadoop-resourcemanager-cisl-linux-021.out
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                standby
cisl-linux-024:8033                                active
...
[Went to other RM and killed it]
...
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                active
17/11/07 15:50:28 INFO ipc.Client: Retrying connect to server: cisl-linux-024/10.200.91.25:8033. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
cisl-linux-024:8033                                Failed to connect: Call From cisl-linux-021/10.200.91.22 to cisl-linux-024:8033 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
...
[Restarted the other RM]
...
[hadoop@cisl-linux-021 cisl-linux-021]$ yarn rmadmin -getAllServiceState
cisl-linux-021:8033                                active
cisl-linux-024:8033                                standby
{code}, Reverted YARN-6840's ResourceManager and ZKRMStateStore changes. This solves the issue for now. Detailed analysis will be shared a bit later, Previous patch contains a little bit additional modifications, attached required only changes patch!, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  9s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 38s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 49s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 24s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 25s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 21 new + 37 unchanged - 0 fixed = 58 total (was 37) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 10s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 56m 28s{color} | {color:green} hadoop-yarn-server-resourcemanager in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 23s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}102m 52s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | YARN-7453 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12896632/YARN-7453.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux f015a5df495e 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / e4c220e |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/18399/artifact/out/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/18399/testReport/ |
| Max. process+thread count | 866 (vs. ulimit of 5000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/18399/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 10s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 59s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 31s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 30s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 1 new + 37 unchanged - 0 fixed = 38 total (was 37) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 58s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 60m 56s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 28s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}114m 40s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestIncreaseAllocationExpirer |
|   | hadoop.yarn.server.resourcemanager.TestOpportunisticContainerAllocatorAMService |
|   | hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerResizing |
| Timed out junit tests | org.apache.hadoop.yarn.server.resourcemanager.TestRMStoreCommands |
|   | org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | YARN-7453 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12896637/YARN-7453.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux aa45fb2f4809 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / e4c220e |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/18401/artifact/out/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/18401/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/18401/testReport/ |
| Max. process+thread count | 794 (vs. ulimit of 5000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/18401/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, +1 for the patch.
I ran the failed and timeout tests locally - it works for me. They just seem to be flaky

Committing this shortly (will take care of checkstyle when I commit), Committed this to trunk, branch-3.0. branch-2, branch-2.9 and branch-2.9.0
Thanks for the quick turnaround [~rohithsharma] and for the testing/investigation [~jhung], [~goiri], [~subru] and [~leftnoteasy], SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13203 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13203/])
YARN-7453. Fix issue where RM fails to switch to active after first (arun suresh: rev a9c70b0e84dab0c41e480a0dc0cb1a22efdc64ee)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/ZKConfigurationStore.java
, Thanks [~asuresh] for committing the patch. I thank all of you for helping investigating the issue and verifying the patch. Special thanks to [~jianhe] [~sunilg] for helping isolating the problem, quick build verifications and for initial patch Sunil provided to me offline!, Thanks [~rohithsharma], [~jhung], [~asuresh],  [~leftnoteasy], [~elgoiri], [~sunilg], [~jianhe] for pulling together and resolving this fast, great teamwork!]