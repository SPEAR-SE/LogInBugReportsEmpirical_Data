[stack 
{code}
2014-03-17 10:41:31,833 [AMRM Callback Handler Thread] INFO  HoyaAppMaster.yarn - Shutdown Request received
2014-03-17 10:41:31,841 [AMRM Callback Handler Thread] INFO  impl.AMRMClientAsyncImpl - Shutdown requested. Stopping callback.
2014-03-17 10:41:32,841 [main] INFO  appmaster.HoyaAppMaster - Triggering shutdown of the AM: Shutdown requested from RM
2014-03-17 10:41:32,842 [main] INFO  appmaster.HoyaAppMaster - Process has exited with exit code 0 mapped to 0 -ignoring
2014-03-17 10:41:32,843 [main] INFO  state.AppState - Releasing 1 containers
2014-03-17 10:41:32,843 [main] INFO  appmaster.HoyaAppMaster - Application completed. Signalling finish to RM
2014-03-17 10:41:32,843 [main] INFO  appmaster.HoyaAppMaster - Unregistering AM status=FAILED message=Shutdown requested from RM
2014-03-17 10:41:32,855 [main] INFO  appmaster.HoyaAppMaster - Failed to unregister application: org.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException: Application doesn't exist in cache appattempt_1395049102171_0001_000001
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:329)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster(ApplicationMasterService.java:288)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.finishApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:75)
	at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:97)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)
 
org.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException: Application doesn't exist in cache appattempt_1395049102171_0001_000001
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:329)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster(ApplicationMasterService.java:288)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.finishApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:75)
	at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:97)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)
 
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.finishApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:94)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy21.finishApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.unregisterApplicationMaster(AMRMClientImpl.java:310)
	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl.unregisterApplicationMaster(AMRMClientAsyncImpl.java:157)
	at org.apache.hoya.yarn.appmaster.HoyaAppMaster.finish(HoyaAppMaster.java:763)
	at org.apache.hoya.yarn.appmaster.HoyaAppMaster.createAndRunCluster(HoyaAppMaster.java:627)
	at org.apache.hoya.yarn.appmaster.HoyaAppMaster.runService(HoyaAppMaster.java:362)
	at org.apache.hadoop.yarn.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:178)
	at org.apache.hadoop.yarn.service.launcher.ServiceLauncher.launchServiceRobustly(ServiceLauncher.java:397)
	at org.apache.hadoop.yarn.service.launcher.ServiceLauncher.launchServiceAndExit(ServiceLauncher.java:328)
	at org.apache.hadoop.yarn.service.launcher.ServiceLauncher.serviceMain(ServiceLauncher.java:532)
	at org.apache.hoya.yarn.appmaster.HoyaAppMaster.main(HoyaAppMaster.java:1470)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException): Application doesn't exist in cache appattempt_1395049102171_0001_000001
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:329)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster(ApplicationMasterService.java:288)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.finishApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:75)
	at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:97)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)
 
	at org.apache.hadoop.ipc.Client.call(Client.java:1406)
	at org.apache.hadoop.ipc.Client.call(Client.java:1359)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy20.finishApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.finishApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:91)
	... 17 more
{code}, Wondering if this is a case where the NM or AM somehow failed to heartbeat and expired from the RM's point of view.  At that point the RM will ask the NM to kill all containers when it resyncs and will have cleaned up the bookkeeping on the AM (hence an unknown app attempt).  The RM log should shed some light on what happened there.

Normally when an AM is told to "go away" by the RM there will be a subsequent AM attempt following it up (assuming there are app attempt retries left).  In those cases the AM attempt should leave without causing any damage to subsequent attempts (e.g.: don't cleanup staging areas and prevent subsequent attempts from launching).  However if the attempt is the last one then it should go ahead and perform any normal shutdown cleanup as there will not be any subsequent attempts to clean up the mess., The original exception occurred on osx with hadoop 2.3.0, but after trying on debian with hadoop 2.2.0 the result is the same. I attached some logs. 
The Flume agent starts with a provider creating the shell command to exec.

Other than that, the application should run in the requested container, right? Because it seems that it is running outside of it (setting the yarn.memory to 8mb and the jvm heap size of the application a much larger and works fine)., -even if you start up a process with lots of RAM, it still runs in a container unless you tell YARN to enforce memory limits:

{code}
  <property>
    <description>Whether physical memory limits will be enforced for
      containers.
    </description>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>true</value>
  </property>
  {code}, I'm curious whether this bug is causing the application to hang up after container shutdown., Hi,

This seems to be an issue on OS/X and Debian only. We have just tried on CentOS (for automatic Hoya install on CentOS feel free to use this script - https://github.com/sequenceiq/hadoop-docker/blob/master/hoya-centos-install.sh) and it works fine launching HBase containers. 

Also we have tried our custom Apache Flume provider (https://github.com/sequenceiq/hoya) and it works well - launching and stoping containers as supposed. 

A quick note: on Debian and OS/X there are different exceptions if you launch the containers using IP address or localhost (hoya create hbase --role master 1 --role worker 1 --manager localhost:8032 --filesystem     hdfs://localhost:9000 --image hdfs://localhost:9000/hbase.tar.gz --appconf file:///tmp/hoya-master/hoya-core/src/main/resources/org/apache/hoya/providers/hbase/conf --zkhosts localhost)

, Hey there,

We are seeing this bug occur while shutting down Samza containers as well. We are running Hadoop 2.3.0 on Ubuntu 12.10. The container hangs indefinitely in the KILLING state.

Here is the stack trace:

{code}
2014-04-22 20:25:08 SamzaAppMaster$ [ERROR] Error occured in amClient's callback
org.apache.samza.SamzaException: Received a reboot signal from the RM, so throwing an exception to reboot the AM.
	at org.apache.samza.job.yarn.SamzaAppMasterLifecycle.onReboot(SamzaAppMasterLifecycle.scala:59)
	at org.apache.samza.job.yarn.SamzaAppMaster$$anonfun$onShutdownRequest$1.apply(SamzaAppMaster.scala:136)
	at org.apache.samza.job.yarn.SamzaAppMaster$$anonfun$onShutdownRequest$1.apply(SamzaAppMaster.scala:136)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.samza.job.yarn.SamzaAppMaster$.onShutdownRequest(SamzaAppMaster.scala:136)
	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:285)
2014-04-22 20:25:09 ELContextCleaner [INFO] javax.el.BeanELResolver purged
2014-04-22 20:25:09 ContextHandler [INFO] stopped o.e.j.w.WebAppContext{/,jar:file:/mnt/data/hadoop/yarn/usercache/brian/appcache/application_1397507485520_0040/filecache/10/samza-job-package-0.7.0-dist.tar.gz/lib/samza-yarn_2.10-0.7.0.jar!/scalate}
2014-04-22 20:25:10 ELContextCleaner [INFO] javax.el.BeanELResolver purged
2014-04-22 20:25:10 ContextHandler [INFO] stopped o.e.j.w.WebAppContext{/,jar:file:/mnt/data/hadoop/yarn/usercache/brian/appcache/application_1397507485520_0040/filecache/10/samza-job-package-0.7.0-dist.tar.gz/lib/samza-yarn_2.10-0.7.0.jar!/scalate}
2014-04-22 20:25:10 SamzaAppMasterLifecycle [INFO] Shutting down.
2014-04-22 20:25:10 SamzaAppMaster$ [WARN] Listener org.apache.samza.job.yarn.SamzaAppMasterLifecycle@3c9ead34 failed to shutdown.
org.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException: Application doesn't exist in cache appattempt_1397507485520_0040_000001
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:329)
	at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster(ApplicationMasterService.java:288)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.finishApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:75)
	at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:97)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)
{code}, Hey folks, can someone who can reproduce it with ease upload both RM as well as the AM logs?, Took a look at this, I'm wondering if it's caused by this case
1) Client asked kill application, 
2) After RM transferred application's state to killed, and before AM container actually killed by NM, the AM asked to finish application
Since the RMAppAttempt already called AMS.unregisterAttempt, the attempt will be cleaned from cache, thus the InvalidApplicationMasterRequestException will be raised.

I guess this after reading log uploaded by [~keyki], 
Still pretty good in following log,
{code}
2014-03-18 19:36:50,802 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1395167286771_0002 State change from ACCEPTED to RUNNING
2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1395167286771_0002_01_000002 Container Transitioned from NEW to ALLOCATED
2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=keyki	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1395167286771_0002	CONTAINERID=container_1395167286771_0002_01_000002
2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1395167286771_0002_01_000002 of capacity <memory:1024, vCores:1> on host localhost:56214, which currently has 2 containers, <memory:2048, vCores:2> used and <memory:6144, vCores:6> available
2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application=application_1395167286771_0002 container=Container: [ContainerId: container_1395167286771_0002_01_000002, NodeId: localhost:56214, NodeHttpAddress: localhost:8042, Resource: <memory:1024, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: 127.0.0.1:56214 }, ] containerId=container_1395167286771_0002_01_000002 queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:1024, vCores:1>usedCapacity=0.125, absoluteUsedCapacity=0.125, numApps=1, numContainers=1 usedCapacity=0.125 absoluteUsedCapacity=0.125 used=<memory:1024, vCores:1> cluster=<memory:8192, vCores:8>
2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:2>usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=2
2014-03-18 19:36:52,535 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:2> cluster=<memory:8192, vCores:8>
2014-03-18 19:36:52,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1395167286771_0002_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2014-03-18 19:36:53,536 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1395167286771_0002_01_000002 Container Transitioned from ACQUIRED to RUNNING
{code}

Client asked kill application, and AMS.unregisterAttempt called, attempt will be removed from AMS cache
{code}
2014-03-18 19:38:50,427 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=keyki	IP=37.139.29.192	OPERATION=Kill Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1395167286771_0002
2014-03-18 19:38:50,427 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing info for app: application_1395167286771_0002
2014-03-18 19:38:50,427 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1395167286771_0002 State change from RUNNING to KILLED
2014-03-18 19:38:50,428 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1395167286771_0002_000001
{code}

After that, AM asked finishApplication, but unfortunately, attempt is already removed from cache
{code}
2014-03-18 19:38:51,397 ERROR org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AppAttemptId doesnt exist in cache appattempt_1395167286771_0002_000001
2014-03-18 19:38:52,415 ERROR org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Application doesn't exist in cache appattempt_1395167286771_0002_000001
{code}

I'm not sure if it's possible in current Hoya design, please correct me if I was wrong. , The code to work in the AM is still there in the slider project, so it still exists, So I think we should improve the message reported to AM when FinishApplicationRequest received, and application is already finished/killed/failed in RM side. Just throwing "Application doesn't exist in cache" exception doesn't make sense to user., 
We had the same issues on OSX (during dev) and Debian server(s), but since we switched to CentOS it works fine (Hadoop 2.3, Hoya 0.13, HBase 0.98, Zookeeper 3.3.6).

I was hoping to find that this is some env/os related issue, and I have built a new docker image(s) starting from the same Dockerfile we use with CentOS but I wasn't able to reproduce it on Ubuntu. You can get the Ubuntu based Hoya image from https://github.com/matyix/hoya-docker-ubuntu or the CentOS one from https://github.com/sequenceiq/hoya-docker if you'd like to try it.

For us the problem was coming when we were freezing HBase or Flume clusters (custom provider) with Hoya ( hoya freeze hbase --manager localhost:8032 --filesystem hdfs://localhost:9000).

I will try it on Debian tomorrow, but I think I will need to skim through the code and see what we have changed to get rid of this issue.

We are migrating this to Slider but as Steve mentioned the code in AM is still there ...


, Same problem here with Hadoop 2.6 and Zookeeper 3.4 on Debian. , It seems to be a Linux Kernel problem, see SAMZA-498]