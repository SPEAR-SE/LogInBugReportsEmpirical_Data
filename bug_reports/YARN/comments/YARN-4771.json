[The problem occurs because removeVeryOldStoppedContainersFromCache will remove containers from the state store that have completed at least yarn.nodemanager.duration-to-track-stopped-containers milliseconds ago.  Once the container state is removed from the state store there's nothing to recover for that container when the NM restarts.  With no information about that container to recover, the log aggregation service doesn't know it needs to aggregate the logs for that container, so the container is skipped during log aggregation., Simple patch which preserves the container as long as the application is still active, and that should solve the reported problem.  However it may have issues with very long-running apps that churn a lot of containers, since the container state won't be released until the application completes., Updated patch to fix the unit test in case it's useful., Sounds bad given also the high possibility of leak on the file-system with non-aggregated / non-deleted container-logs, bumping priority., 002 patch LGTM. 
An additional fix is we'd better to use MonotonicTime to replace System.currentTimeMillis() for tracking timeout - just an optional comment, we can address here or in a separated jira., bq. However it may have issues with very long-running apps that churn a lot of containers, since the container state won't be released until the application completes.
This is going to be problematic, impacting NM memory usage.

I think the right solution is to decouple log-aggregation state completely from the rest of the container-state, and persist that separately in state-store etc irrespective of container / application state.

IAC, I am dropping this off 2.7.3 given that this is a long standing issue and as I'd to proceed with that release now., Changing to Major to remove from the scope of 2.7.4 release.
Feel free to add back if you plan to fix it., Moving target version to 2.7.5 due to 2.7.4 release.]