[Is this because the keep-containers flag is on? Why was the AM stopped and not the the app killed if that is what they want., Slider stop command was called which initiates the Slider Storm application to stop (and hence the Slider AM to stop). 

Which property sets the keep-containers flag on?, Vinod probably means the AM restart flag.

When the Slider AM is stopped it can be done in two ways
{code}
slider stop $clustername
{code}

Sends an RPC call to the AM, which then unregisters and shuts down. I'll check to make sure we explicitly release containers.
{code}
slider stop $clustername --force
{code}

This asks YARN to kill the app; the AM doesn't get told about it.

there's a third way

{code}
slider am-suicide $clustername
{code}

This is only for testing, causes the AM to call {{System.exit(-1)}}; YARN will restart it unless it has failed too many times already., I checked the code and Slider does set {{keepContainersAcrossApplicationAttempts}} to true. 

In this case a graceful Slider stop was issued, which calls {{AMRMClientAsync.releaseAssignedContainer}} for all containers. The NodeManager also initiated the stop of all containers. As seen below in the log snippet, the container state transitioned from {{RUNNING}} to {{KILLING}} and the application state transitioned from {{RUNNING}} to {{FINISHING_CONTAINERS_WAIT}}, but got lost somewhere and the resource-monitor continued to run.

Also note, that we never encountered this issue in our test and dev environments. Could this be OS specific (debian 7)? The Slider AM log snippet from the time it received the stop command is provided below.

*Snippet from NM log in host-03*
{noformat}
2015-04-29 00:56:00,486 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:stopContainerInternal(953)) - Stopping container with container Id: container_1428575950531_0021_01_000002
2015-04-29 00:56:00,487 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=yarn	IP=10.84.104.129	OPERATION=Stop Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1428575950531_0021	CONTAINERID=container_1428575950531_0021_01_000002
2015-04-29 00:56:00,487 INFO  container.Container (ContainerImpl.java:handle(999)) - Container container_1428575950531_0021_01_000002 transitioned from RUNNING to KILLING
2015-04-29 00:56:00,489 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(370)) - Cleaning up container container_1428575950531_0021_01_000002
2015-04-29 00:56:00,802 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(408)) - Memory usage of ProcessTree 21259 for container-id container_1428575950531_0021_01_000002: 15.4 MB of 3.5 GB physical memory used; 280.3 MB of 7.3 GB virtual memory used
2015-04-29 00:56:02,494 INFO  application.Application (ApplicationImpl.java:handle(464)) - Application application_1428575950531_0021 transitioned from RUNNING to FINISHING_CONTAINERS_WAIT
2015-04-29 00:56:03,849 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(408)) - Memory usage of ProcessTree 21259 for container-id container_1428575950531_0021_01_000002: 15.5 MB of 3.5 GB physical memory used; 280.3 MB of 7.3 GB virtual memory used
2015-04-29 00:56:06,892 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(408)) - Memory usage of ProcessTree 21259 for container-id container_1428575950531_0021_01_000002: 15.5 MB of 3.5 GB physical memory used; 280.3 MB of 7.3 GB virtual memory used
.
.
.
{noformat}

*Slider AM log:*
{noformat}
2015-04-29 00:55:59,189 [IPC Server handler 0 on 1024] INFO  appmaster.SliderAppMaster - SliderAppMasterApi.stopCluster: stop command issued:  exit code = 0, SUCCEEDED: stop command issued;
2015-04-29 00:56:00,189 [AmExecutor-006] INFO  appmaster.SliderAppMaster - SliderAppMasterApi.stopCluster: stop command issued
2015-04-29 00:56:00,190 [main] INFO  appmaster.SliderAppMaster - Triggering shutdown of the AM: stop command issued:  exit code = 0, SUCCEEDED: stop command issued;
2015-04-29 00:56:00,190 [main] INFO  appmaster.SliderAppMaster - Process has exited with exit code 0 mapped to 0 -ignoring
2015-04-29 00:56:00,190 [main] INFO  workflow.WorkflowCompositeService - Child service completed Service RoleLaunchService in state RoleLaunchService: STOPPED
2015-04-29 00:56:00,191 [main] INFO  state.AppState - Releasing 5 containers
2015-04-29 00:56:00,191 [main] INFO  state.AppState - Releasing container. Log: http://host-02:19888/jobhistory/logs/host-05:45454/container_1428575950531_0021_01_000003/ctx/yarn
2015-04-29 00:56:00,192 [main] INFO  state.AppState - Releasing container. Log: http://host-02:19888/jobhistory/logs/host-07:45454/container_1428575950531_0021_01_000005/ctx/yarn
2015-04-29 00:56:00,192 [main] INFO  state.AppState - Releasing container. Log: http://host-02:19888/jobhistory/logs/host-03:45454/container_1428575950531_0021_01_000002/ctx/yarn
2015-04-29 00:56:00,193 [main] INFO  state.AppState - Releasing container. Log: http://host-02:19888/jobhistory/logs/host-04:45454/container_1428575950531_0021_01_000004/ctx/yarn
2015-04-29 00:56:00,193 [main] INFO  appmaster.SliderAppMaster - Application completed. Signalling finish to RM
2015-04-29 00:56:00,193 [main] INFO  appmaster.SliderAppMaster - Unregistering AM status=SUCCEEDED message=stop command issued
2015-04-29 00:56:00,202 [main] INFO  impl.AMRMClientImpl - Waiting for application to be successfully unregistered.
2015-04-29 00:56:00,304 [main] INFO  appmaster.SliderAppMaster - Exiting AM; final exit code = 0
2015-04-29 00:56:00,306 [main] INFO  util.ExitUtil - Exiting with status 0
2015-04-29 00:56:00,307 [Shutdown] INFO  mortbay.log - Shutdown hook executing
2015-04-29 00:56:00,307 [Shutdown] INFO  mortbay.log - Stopped SslSelectChannelConnector@0.0.0.0:54797
2015-04-29 00:56:00,311 [Shutdown] INFO  mortbay.log - Stopped SslSelectChannelConnector@0.0.0.0:41739
2015-04-29 00:56:00,315 [Thread-1] INFO  mortbay.log - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:1025
2015-04-29 00:56:00,414 [Shutdown] INFO  mortbay.log - Shutdown hook complete
2015-04-29 00:56:00,425 [Thread-1] INFO  ipc.Server - Stopping server on 1024
2015-04-29 00:56:00,426 [IPC Server listener on 1024] INFO  ipc.Server - Stopping IPC Server listener on 1024
2015-04-29 00:56:00,427 [IPC Server Responder] INFO  ipc.Server - Stopping IPC Server Responder
2015-04-29 00:56:00,429 [Thread-1] INFO  impl.ContainerManagementProtocolProxy - Opening proxy : host-05:45454
2015-04-29 00:56:00,455 [Thread-1] INFO  impl.ContainerManagementProtocolProxy - Opening proxy : host-07:45454
2015-04-29 00:56:00,470 [Thread-1] INFO  impl.ContainerManagementProtocolProxy - Opening proxy : host-03:45454
2015-04-29 00:56:00,491 [Thread-1] INFO  impl.ContainerManagementProtocolProxy - Opening proxy : host-04:45454
2015-04-29 00:56:00,507 [AMRM Callback Handler Thread] INFO  impl.AMRMClientAsyncImpl - Interrupted while waiting for queue
java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2052)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:274)
2015-04-29 00:56:00,507 [AmExecutor-005] INFO  actions.QueueService - QueueService processor terminated
2015-04-29 00:56:00,507 [AmExecutor-006] WARN  actions.ActionStopQueue - STOP
{noformat}, [~leftnoteasy] thanks for linking YARN-2904. Seems like there is something common between the two.

Slider non-AM containers register a SIGINT and SIGTERM at the start. When Slider AM initiates a graceful shutdown, all non-AM containers receive a SIGTERM.

In this scenario, I saw that the non-AM containers never received a SIGTERM. 

I checked {{/proc/<pid>/status}} in one of those nodes and the signals SIGINT and SIGTERM were registered correctly 

{code}
SigCgt: 0000000180004202
{code}
, [~rakesh_sahoo] (accidentally?) made it patch-available. Canceling it.., bq. Could this be OS specific (debian 7)?
Possible. Can you post the full NM logs?, Logs for the run used to file the bug are not available anymore.

Attached logs (app0001.zip) for one of the recent attempts. It contains RM and NM logs from hosts running Slider AM and non-AM application containers.

container_1430849660771_0001_01_000001 - host04 - SliderAM
container_1430849660771_0001_01_000002 - host04 - NIMBUS
container_1430849660771_0001_01_000003 - host05 - STORM_UI_SERVER
container_1430849660771_0001_01_000004 - host03 - DRPC_SERVER
container_1430849660771_0001_01_000005 - host07 - SUPERVISOR

*Timing of issuing the commands:*
Slider start command : 2015-05-06 00:01:04,710
Slider stop command : 2015-05-06 00:03:13,608
, [~gsaha], I tried to look at this, but couldn't find the "container_1428575950531_0021_01_000002" log in any of the uploaded NM logs.  could you point me which container is misbehaving in any of the NM logs you uploaded here ?, [~jianhe] The logs which were used to file the bug was not available anymore. Hence logs for a newer run were uploaded as a zip. Please look into container with id {{container_1430849660771_0001_01_000002}}., [~gsaha], from the description, this is running against 2.6 ?  this could be related to YARN-2825, but that's fixed in 2.6 
From the logs, I can only see the container is still sort of waiting for the process to finish.  Is this easy to reproduce? It'll be great if we have NM logs with debug level on., [~jianhe] it is consistently reproducible on debian 7. Can you provide a quick instruction on how to enable debug level in NM logs?

[~chackra] If possible, can you set debug level on for NM logs, re-run the test and provide the logs again?, [~gsaha] / [~jianhe]

Attached logs (application_1431771946377_0001.zip) with debug level enabled. It contains RM and NM logs from hosts running Slider AM and non-AM application containers.

container_1431771946377_0001_01_000001 - host3 - SliderAM
container_1431771946377_0001_01_000002 - host7 - NIMBUS
container_1431771946377_0001_01_000003 - host5 - STORM_UI_SERVER
container_1431771946377_0001_01_000004 - host3 - DRPC_SERVER
container_1431771946377_0001_01_000005 - host6 - SUPERVISOR

*Timing of issuing the commands:*

Slider start command : 2015-05-16 15:57:11,954
Slider stop command : 2015-05-16 15:59:06,480, Seems like the issue is related with 'setsid' enabled in the hosts and unable to execute the 'kill' command constructed (with hyphen).

Analysis as follows,
 
From debug logs, its found that SIGTERM signal sent to SliderAgent by NM did not succeed.
{quote}
2015-05-16 15:59:08,588 DEBUG launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(411)) - Sending signal to pid 4083 as user yarn for container container_1431771946377_0001_01_000002
2015-05-16 15:59:08,589 DEBUG nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:signalContainer(399)) - Sending signal 15 to pid 4083 as user yarn
2015-05-16 15:59:08,596 DEBUG launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(421)) - Sent signal SIGTERM to pid 4083 as user yarn for container container_1431771946377_0001_01_000002, *result=failed*
2015-05-16 15:59:08,848 DEBUG nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:signalContainer(399)) - Sending signal 9 to pid 4083 as user yarn
{quote}

NM gets the PID for the running slider agent and tries to send SIGTERM(15) first, but it fails. Then it tries to send SIGKILL(9) and that too fails.
The reason being containerIsAlive(pid) method always returning false and hence killContainer() method never executed which need to send KILL signal.

{code:title=DefaultContainerExecutor.java|borderStyle=solid}
// 
public boolean signalContainer(String user, String pid, Signal signal)
{
    if (!containerIsAlive(pid)) {
      return false;
    }
	killContainer(pid, signal) // sending kill signal here only.... AND IT NEVER REACHES HERE
    return true;
}
{code} 

containerIsAlive returning false because of improper construction of kill command which results in ExitCodeException : 
{quote}
kill -0 -<proc_id>
{quote} 
{quote}
kill -0 -4083
{quote} 

{code:title=Shell.java|borderStyle=solid}
/** Return a command for determining if process with specified pid is alive. */
  public static String[] getCheckProcessIsAliveCommand(String pid) {
    return Shell.WINDOWS ?
      new String[] { Shell.WINUTILS, "task", "isAlive", pid } :
      new String[] { "kill", "-0", isSetsidAvailable ? "-" + pid : pid };
  }
{code}


In this environment, setsid been enabled and hence "-" been added as prefix while constructing this command. But not sure why this command execution gets ExitCodeException, if this is the correct way of executing command in setsid enabled environment. When I try to execute the same command (with hyphen) in the host shell, same improper usage error came.
, Improper (specific to this env) kill command construction was the issue. I tested by making changes in Shell.java class to construct the kill command as follows (including two hyphens) :
{noformat}
kill -signalNo -- -<process_id>
{noformat} 

It works fine with this change in debian7., I see you filed HADOOP-11989. Assuming _that_ is the root-cause, we can close this as a duplicate., Duplicate of HADOOP-12317.]