[Thanks for raising this issue [~vinodkv], yes it would be better to correct the order, as in future if we start collecting ATSv2 system events then we will be having issues later. Issue i faced when i was testing with correcting the order was : the timeline service was having some issue starting the web service though the port was correctly set (was not an expert with jersey and guice, hence had stopped further analysis there) 

Also one more point(not related to this jira) to note in MiniYARNCluster, we no more support old  AHS interfce so basically {{yarn.timeline-service.generic-application-history.store-class}} should not be configured in {{ApplicationHistoryServerWrapper}} so that levelDBtimelinestore is created. which i feel is correct atleast for existing 2.7.x versions. And if you agree with it, can we fix it along with this jira as its a small thing ?, , Attaching a patch that should fix this.

The patch does the following:
 - Reordered the service creation in MiniYARNCluster to be AHS -> RM -> NMs.
 - Moved URI creation in TimelineClient to service-start so that RM can safely start sending events. This should be fine for existing users of TimelineClient also as they cannot do anything for real before client.start().
 - Added a simple test in TestMiniYARNCluster to validate the service order.

[~Naganarasimha]

Responding to your comments on this JIRA as well as YARN-2859.

bq. the timeline service was having some issue starting the web service though the port was correctly set (was not an expert with jersey and guice, hence had stopped further analysis there)
Can you try my uploaded patch here together with YARN-4350 and see if you still find issues?

bq. In MiniYARNCluster RM servicewrapper is first added and then AHSwrapper, and also actual AHS service is started in a thread, so RM's will be using the wrong timelineclient address(port is zero) as AHS service is not yet initialized.
This should be fixed in the patch - I changed the order to be AHS -> RM -> NM. The separate thread is not an issue as RM will only start after AHS successfully starts.

bq. In Timeline client Impl's serviceInit URI for timeline REST service is set. So even though we create the correct service order (as per previous step), RM's SMP will fail to publish, as timelineweb address is got only after the AHS service is started.
Fixed TimelineClient to delay the URI creation.

bq. Also one more point(not related to this jira) to note in MiniYARNCluster, we no more support old AHS interfce so basically yarn.timeline-service.generic-application-history.store-class should not be configured in ApplicationHistoryServerWrapper so that levelDBtimelinestore is created. which i feel is correct atleast for existing 2.7.x versions. And if you agree with it, can we fix it along with this jira as its a small thing ?,
Even if that store is specified, TimelineDataManager will continue to be instantiated in the server, so your assumption is wrong that levelDB is not created. Agree?, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 6s {color} | {color:blue} docker + precommit patch detected. {color} |
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue} 0m 7s {color} | {color:blue} The patch file was not named according to hadoop's naming conventions. Please see https://wiki.apache.org/hadoop/HowToContribute for instructions. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 55s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 55s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 7s {color} | {color:green} trunk passed with JDK v1.7.0_85 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 27s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 32s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 44s {color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 1m 16s {color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common in trunk has 3 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 3s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 17s {color} | {color:green} trunk passed with JDK v1.7.0_85 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 22s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 41s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 41s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 3s {color} | {color:green} the patch passed with JDK v1.7.0_85 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 3s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 43s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 16s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 2s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 18s {color} | {color:green} the patch passed with JDK v1.7.0_85 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 52s {color} | {color:green} hadoop-yarn-common in the patch passed with JDK v1.8.0_66. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 58m 27s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.8.0_66. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 5m 35s {color} | {color:red} hadoop-yarn-server-tests in the patch failed with JDK v1.8.0_66. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 7s {color} | {color:green} hadoop-yarn-common in the patch passed with JDK v1.7.0_85. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 59m 35s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.7.0_85. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 5m 42s {color} | {color:red} hadoop-yarn-server-tests in the patch failed with JDK v1.7.0_85. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 24s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 168m 26s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_66 Failed junit tests | hadoop.yarn.server.resourcemanager.TestClientRMTokens |
|   | hadoop.yarn.server.resourcemanager.TestAMAuthorization |
|   | hadoop.yarn.server.TestContainerManagerSecurity |
| JDK v1.7.0_85 Failed junit tests | hadoop.yarn.server.resourcemanager.TestClientRMTokens |
|   | hadoop.yarn.server.resourcemanager.TestAMAuthorization |
|   | hadoop.yarn.server.TestContainerManagerSecurity |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:date2015-11-19 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12773341/YARN-4372-20151119.1.txt |
| JIRA Issue | YARN-4372 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 7d9790774b7f 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/patchprocess/apache-yetus-3f4279a/precommit/personality/hadoop.sh |
| git revision | trunk / 747455a |
| findbugs | v3.0.0 |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/branch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_66.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests-jdk1.8.0_66.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.7.0_85.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests-jdk1.7.0_85.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_66.txt https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests-jdk1.8.0_66.txt https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.7.0_85.txt https://builds.apache.org/job/PreCommit-YARN-Build/9742/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-tests-jdk1.7.0_85.txt |
| JDK v1.7.0_85  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/9742/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests U: hadoop-yarn-project/hadoop-yarn |
| Max memory used | 76MB |
| Powered by | Apache Yetus   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/9742/console |


This message was automatically generated.

, hi [~vinodkv],
bq. Can you try my uploaded patch here together with YARN-4350 and see if you still find issues?
took the latest code from trunk as YARN-2859 is commited, later just added the below modification in 
{{TestDistributedShell}}
{code}
@@ -80,6 +80,7 @@ protected void setupInternal(int numNodeManager) throws Exception {
     conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 128);
     conf.set("yarn.log.dir", "target");
     conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);
+    conf.setBoolean(YarnConfiguration.RM_SYSTEM_METRICS_PUBLISHER_ENABLED, true);
     conf.set(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class.getName());
     conf.setBoolean(YarnConfiguration.NODE_LABELS_ENABLED, true);
     conf.set("mapreduce.jobhistory.address",
{code}
Even after the patch {{TestDistributedShell.testDSShellWithoutDomain}} is failing (test case passes but the in the console logs there were logs for unreachable timlineserver for each smp events). Crossverified the {{resURI}} is getting set properly in {{TimelineClientImpl}}. This is the same which i had faced when i was last analyzing and i too had done the similar modifications in MiniYarnCluster. 
Few console logs 
{code}
2015-11-20 23:17:15,867 ERROR [AsyncDispatcher event handler] impl.TimelineClientImpl (TimelineClientImpl.java:doPosting(336)) - Failed to get the response from the timeline server.
2015-11-20 23:17:15,868 ERROR [AsyncDispatcher event handler] metrics.SystemMetricsPublisher (SystemMetricsPublisher.java:putEntity(485)) - Error when publishing entity [YARN_CONTAINER,container_1448041615020_0001_01_000002]
{code}
bq. The separate thread is not an issue as RM will only start after AHS successfully starts.
Yes you are right, my mistake had just observed that it was started as part of thread but we are further checking whether the service is started and throwing exception if necessary.

bq. Even if that store is specified, TimelineDataManager will continue to be instantiated in the server, so your assumption is wrong that levelDB is not created. Agree?
Oops had wrongly mentioned *levelDBtimelinestore* but wanted to mention as {{ApplicationHistoryManagerOnTimelineStore}}. 
As any way we are starting Timelinestore so better to have ApplicationHistoryManagerOnTimelineStore than the almost deprecated {{yarn.timeline-service.generic-application-history.store-class}} interface . (not a must fix but better to have), bq. Even after the patch TestDistributedShell.testDSShellWithoutDomain is failing (test case passes but the in the console logs there were logs for unreachable timlineserver for each smp events).
You are right, *sigh*, this is the same bug we ran into at YARN-3087: Guice not letting us run two UI services at the same time. This used to work because Timeline Service started last before this patch. Need to think more, not sure how we can fix this.]