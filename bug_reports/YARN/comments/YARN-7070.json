[The clues for this will be in the nodemanager logs.  The first step is to verify the NM believes the application has completed, as it will not clean up anything in an application's appcache until it thinks the application has completed.  Even if the NM is no longer running any containers for an application,  it will continue to host an application's appcache files until the RM tells it the application has completed.

Assuming the NM has been told the application completed, it will then proceed to delete the application's appcache directories.  If that failed for some reason then there should be some evidence in the NM logs for this.  I have a hunch you are seeing the same thing as described in YARN-6846.
, Hi Lowe

Thanks for your comments. I checked the log of the application again. It seems that the application finished successfully without any WARNs and ERRORs.
I found that those blockmgr*** directories refer to shuffle.ExternalShuffleBlockResolver, maybe something wrong happened during the shuffle process.

I attached the log file and It would be very appreciated if you could take a look at it.
, [~yechangyao] - The lack of clean up may be by design.

From the log you provided:
{code}
2017-08-22 05:20:01,260 INFO org.apache.spark.network.shuffle.ExternalShuffleBlockResolver: Application application_1501810184023_55949 removed, cleanupLocalDirs = false
{code}

Note: cleanupLocalDirs = false.

The comment in the code here seems misleading as the false indicates that the local dirs shouldn't be cleaned up:
https://github.com/apache/spark/blob/master/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java#L275
https://github.com/apache/spark/blob/master/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolver.java#L178

At this point, I don't believe this is a YARN bug. You'll want to reach out on the spark mailing lists to see if there might be away to configure the desired behavior., bq. At this point, I don't believe this is a YARN bug.

I disagree.  Even if the spark shuffle handler doesn't clean anything up, these files are underneath the application's appcache area in YARN.  The nodemanager is supposed to clean this up when the application completes regardless of what the auxiliary services are doing.

From the log we see it at least tried to do this:
{noformat}
2017-08-22 05:20:01,260 INFO org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Deleting absolute path : /tmp/hadoop-yarn/nm-local-dir/usercache/hdfs/appcache/application_1501810184023_55949
{noformat}

This looks a lot like the scenario that was fixed in YARN-6846, since the container is getting killed just as the application completes.  Note how close the two deletes are occurring near each other:
{noformat}
2017-08-22 05:20:01,260 INFO org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Deleting absolute path : /tmp/hadoop-yarn/nm-local-dir/usercache/hdfs/appcache/application_1501810184023_55949/container_e24_1501810184023_55949_01_000079
2017-08-22 05:20:01,260 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1501810184023_55949
2017-08-22 05:20:01,260 INFO org.apache.spark.network.yarn.YarnShuffleService: Stopping application application_1501810184023_55949
2017-08-22 05:20:01,260 INFO org.apache.spark.network.shuffle.ExternalShuffleBlockResolver: Application application_1501810184023_55949 removed, cleanupLocalDirs = false
2017-08-22 05:20:01,260 INFO org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Deleting absolute path : /tmp/hadoop-yarn/nm-local-dir/usercache/hdfs/appcache/application_1501810184023_55949
{noformat}

These two deletes are probably racing in parallel.  I highly recommend applying the patch from YARN-6846 and see if things improve.
, [~jlowe] [~shanekumpf@gmail.com]

Thank you guys so much.
I'll try applying the patch from YARN-6846 first and let you know the problem solved or not., Hi

I applied the patch YARN-6846, unfortunately the problem was not fixed.
Those directories were not removed properly., Hmm, at this point we cannot know for sure what's going on here without more information in the logs.  I'd recommend instrumenting the container-executor binary so it logs somewhere what it's doing during deletes to see if there's a clue why these directories aren't getting cleaned up.  For example, have it log each and every path it is trying to delete to see if there's a clue as to where it stops deleting or if it even tries to delete these paths that remain.  Also emitting a log at the end stating it completed successfully would help if there's some issue where it is actually crashing during deletes but the NM is not recognizing that properly and reporting it.

Also to clarify -- are these directories always getting left behind or only under certain circumstances (e.g.: only when an application or container is getting killed, etc.)?
, [~jlowe]
Sorry for the late reply.
I've checked and compared the logs between successful jobs(which completed successfully and cleared cache dirs) and failed jobs(which completed successfully, but cannot delete cache dirs)  and found that there are WARNs occurred when abnormal cases happened. Just like the attachment file warn_logs.txt. 
I'm not sure this is reference to the problem or not. Please have a check.

Thanks., Those warnings are "normal" and are a result of the container being killed by YARN.  Exit code 143 is a result of the bash process being killed by SIGTERM.  SIGTERM is signal 15, and bash returns with an exit code of 128 + signalnum when killed by a signal.  128 + 15 = 143.

So it sounds like this only happens when a container is getting killed?  Does it always happen when a container is killed or only when a container is killed just as the application completes as well (e.g.: container is getting killed because application was killed)?  I suspect this is only happening when apps are getting killed and the app dir delete is racing with the container delete, but that should have been fixed by YARN-6846.  Are you sure there are no similar container executor warnings in the NM logs regarding the deletion command processing?  If the container executor is failing to cleanup a directory I would expect it to exit with a non-zero exit code which should result in similar WARN logs in the NM as you posted above, although instead of being command 1 (the third argument to the executor) it would be command 3.
]