[Uploading a unit test that demonstrates this., When a queue is in the state as described above, {{FifoIntraQueuePreemptionPlugin#calculateToBePreemptedResourcePerApp}} decides (erroneously, I believe) that {{app2}} has preemptable resources. Since {{app2}} is the youngest with apparent resources, {{FifoIntraQueuePreemptionPlugin#preemptFromLeastStarvedApp}} selects a container to preempt from {{app2}}. However, when it calls {{FifoIntraQueuePreemptionPlugin#skipContainerBasedOnIntraQueuePolicy}}, it decides that preempting the selected container would bring the user limit down too far, so it skips the container. However, it doesn't go on to the next youngest app with resources.

The logic breaks down to basically this:
{code}
calculateToBePreemptedResourcePerApp {
  // preemtableFromApp will be used to select containers to preempt.
  preemtableFromApp = used - (userlimit - AmSize)
}

skipContainerBasedOnIntraQueuePolicy {
  if (used - selectedContainerSize) <= (userlimit + AmSize) {
    Skip this container
  } 
}
{code}
We get into this starvation mode when {{selectedContainerSize}} ends up being the same size as {{preemtableFromApp}}, Attaching a proposal for a patch to fix this problem.

Proposed fix: In {{calculateToBePreemptedResourcePerApp}}, if {{USERLIMIT_FIRST}}  policy is set, subtract off minimum container size. Basically, the code in {{skipContainerBasedOnintraQueuePolicy}} skips the container if it will bring it down to the user limit because the capacity scheduler assigns one container more than the user limit.

Also, in 2.8, this fix has a problem of oscillation due to the difference in how user limit is calculated between 2.8 and later releases. Basically (ignoring ULF, MULP, and maybe others), the calculation in 2.8 is {{total used resources / number of active users}} while the calculation in later releases is {{total active resources / number of active users}}. With this fix in 2.8, it would cause the value of {{getResourceLimitForAllUsers}} (used by preemption monitor) to be greater than {{getHeadroom}} used by leafqueue, which would cause more preemption to occur than necessary.

Bottom line is that I'm still working on a 2.8 solution., Hi [~eepayne]
This is a nice catch and nasty one to debug too.

I think the proposed patch helps to solve the issue. In broader perspective, i think we are lacking dead zone here. In a way, now min container is the dead zone here. But if user gets more control on this, may be more oscillations could be avoided. May be we can take up that also in another ticket., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 16m 19s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 30s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 31s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 25s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 24s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager: The patch generated 4 new + 56 unchanged - 0 fixed = 60 total (was 56) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m  7s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 58m 42s{color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}123m  0s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Timed out junit tests | org.apache.hadoop.yarn.server.resourcemanager.TestRMStoreCommands |
|   | org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA |
|   | org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | YARN-7469 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12897348/YARN-7469.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 26ea803d0aff 3.13.0-117-generic #164-Ubuntu SMP Fri Apr 7 11:05:26 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 0d6bab9 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/18463/artifact/out/diff-checkstyle-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/18463/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/18463/testReport/ |
| Max. process+thread count | 794 (vs. ulimit of 5000) |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/18463/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, bq. In broader perspective, i think we are lacking dead zone here. In a way, now min container is the dead zone here. But if user gets more control on this, may be more oscillations could be avoided. May be we can take up that also in another ticket.
[~sunilg], Thanks for looking at the patch. Yes, I agree that a dead zone above the user limit would be a very helpful feature to add., bq.  now min container is the dead zone here
I filed YARN-7501 to include a "dead zone" around the user limit.

bq. in 2.8, this fix has a problem of oscillation due to the difference in how user limit is calculated between 2.8 and later releases. 
[~sunilg], I think this patch should be used to fix the user starvation problem and the 2.8-specific oscillation problem can be handled by YARN-7496. {{YARN-7469.001.patch}} will apply cleanly to all branches back to branch-2.8., Thanks [~eepayne]. This makes sense.

I am good with v1 patch. I could commit this later if its fine., Thanks [~sunilg]! That would be great!, +1 Committing shortly., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13246 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13246/])
YARN-7469. Capacity Scheduler Intra-queue preemption: User can starve if (sunilg: rev 61ace174cdcbca9d22abce7aa0aa71148f37ad55)
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/FifoIntraQueuePreemptionPlugin.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicyIntraQueueUserLimit.java
* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicyMockFramework.java
, Thanks [~eepayne] . Committed to trunk/branch-3.0/branch-2/branch-2.8. Hopefully I didnt miss any branch. please correct me if i missed some, Thank you very much [~sunilg]. Just one concern is that this fix should also go into branch-2.9 since it is also in branch-2.8., Yes. :) I missed 2.9. Will backport now., Pushed to 2.9 as well., Merge to branch-2.8.3 given we previously set fix version to 2.8.3., I pulled this into branch-3.0.0 as well, thanks folks.]