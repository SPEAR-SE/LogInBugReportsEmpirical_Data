[[~leftnoteasy],WangDa how do you think?, + [~kasha]. , bq. When your allocated minShare of all queue`s added up exceed cluster capacity you can get some queue for 0 fairshare

Why is this a problem? , Perhaps I was not clear,if R value is some very wee like 0.00390625,when a queue
is 0 minshare(not configed in fair-scheduler.xml) and weight 1 by default.
Its fairshare formula is (int)Max(R*weight,minShare) will equal to 0.
And this queue is nerver get preempt forever,This is serious.
As i think,minshare configation is uncontrollable by now,There is no check
in now code if exceed Total.
But if base on above situation,if this happened,some 0 minshare queue will suffer.
So i suggestion wipe off minshare when compute fairshare.
Thanks., I am still unable to understand. 

Are you able to share an allocations file you had encountered this problem with? And, call out the specific allocation/preemption that was contrary to the expectation? , Hi,[~kasha]:
          root
          / | \
         a  b  c
Assumpt,root fairshare(fs) is 10.
And minshare of childs:
a:6
b:5
c:0
Procedure is:
1.When compute fs in ComputeFairShares#computeSharesInternal:
2.In function resourceUsedWithWeightToResourceRatio,It will 
find a,b,c minshare add up greater than 10.
3.Then jump out "while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)".
4.Go into for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {...}
5.When jump out 4th ,R value is a very wee num.
6.Then when go into setResourceValue(computeShare(sched, right, type), sched.getFairShare(), type); 
7.Will find queue c fairshare is ZERO.--(int)Max(R*weight,minShare)

So in the beginning,My viewpoint is:
If a queue(c) is active,its fairshare could not be 0.

And i have some questions：
1.Need refer to minshare when compute fairshare?
2.Childs minshare added up exceeded parents fairshare——(as for root fairshare is cluster capacity.)，
Is this situation rightful? 
Should we check this rule in load configuration file? 
How could we control users behavior?

Hope any suggestions. Thanks!
, I think I understand it better now. 

Let us assume you are only talking about vcores (cpu). In this case, it appears cluster capacity is 10 vcores (fairshare of root). When the admin sets the minshare of 'a' to 6, the message to the scheduler is to give queue 'a' 6 vcores if they are available. So, if the minshares of 'a' and 'b' are 6 and 5 respectively, all the resources should go to those two queues. Queue 'c' should not be given any resources unless 'a' and 'b' do not end up using all the resources. That is exactly what a zero fairshare means. Note that a queue with zero fairshare might still be allocated resources if other queues don't use theirs. 

To me, the behavior is expected and makes sense. If anything, if sum of all minshares exceeds cluster capacity, we should likely warn the user; on a dynamic cluster, it is possible for the cluster to loose a few nodes temporarily and the cluster capacity go below sum of all minshares. , In fact, we recommend using a weight of 0 (that translates to a fairshare of 0) for ad hoc queues. 

Note that we fixed a few bugs over the time with weight/fairshare of 0 and maxAMShare. If you are running into apps not starting or not getting enough resources, please try a later version. , I am closing this as "Not a Problem". Please reopen it if this is indeed an issue. , Greatlly,I think i get your means.
Like that,for some temporary queue,such as created by the policy(user,group).
Which minshare is default(0).Is there someway to set it`s minshare beyound 0.So we can let
it get fairshare a positive num.
Or if i can create a new issue(new feature) to implement this? [~kasha]
Hope your suggestion!
, In my opinion, on this cluster, it makes more sense to ensure sum of all minshares is less than total capacity than artificially increasing the fairshare. , [~kasha],assumpt this scenario:
capacity is 10,queue a,b,c minshare is 3,3,4.Now it is allright.But when you add queue d,you need reconfig all of four queues minshare to make sure four queue work as expected(2,2,2,3)--on this occasion you need 
coordinate four queues minshare(you need config 4 times),and this thing is infinite,when you add queue e,f,g...
So as for me,it is more suitable to config 1 times.
add queue e,config once such as 4(because you dont need to care the upper limit)...
And one thing is more important——we cant ensure all queues will be active,so if according to what you said,there is
only a part of queues active,and all these queue`s minshare added up is less than capacity i think this maybe is waste.(I am not very confirm.), Excuse the long-winded response.

I believe minshare was originally introduced to handle a queue's *urgent* requirement on a *saturated* cluster:
# When preemption is enabled, mishare worth of resources are preempted from other queues. This was necessary because fairshare preemption was very rigid. Since then, we have augmented fairshare preemption with a threshold and timeout giving more control to the admins. I would encourage trying these new controls out instead of using minshare preemption. 
# When preemption is not enabled, setting minshare for a queue forcibly sets the fairshare of the queue to at least that value. Using minshare makes sense only when used for special cases. In a cluster where most queues have a minshare set, there is no more *fairness*. 

Also, minshare is an absolute value and needs to be updated as the cluster grows/shrinks. For these reasons, I would discourage the use of minshare. At Cloudera, we discourage our customers too. There are exceptions: a high-priority, latency-sensitive workload that needs at least {{x}} resources to start. 

In your example, I think either minshares are being abused or the cluster is too small. If all the queues require at least those many resources to be functional, clearly the cluster cannot accommodate all of them coming together. 

PS: If backward compatibility were not important, I would have advocated for removing minshare altogether. , [~kasha] Thanks your detailedness reply.I think your explain solves my questions.]