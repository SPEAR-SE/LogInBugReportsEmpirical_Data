[bq. This can be improved with adding date as a subdirectory

When a log link points to the original node and the node redirects back to the YARN log server, how will it compose the redirect URL (i.e.: how will it know what the date is)?  Will the same URL used to day work, requiring a scan on the part of the log server?  SImilarly for the yarn logs CLI command, I assume it needs to do a scan of the date directories in order to find the proper application?, Date can be retrieved from the timestamp present in the application id while creating date subdirectory. So while scanning we will know which date subdirectory to check directly. The URL can remain the same.  , The timestamp in the application ID is the cluster start timestamp (i.e.: when the RM started), not a timestamp when the application ID was created.  Unless the RM is getting restarted at least once every retention period (7 days per above) then all of the applications will hash to the same date bucket and nothing will be different than it is today.
, Thanks, missed it. Hash can be generated from ApplicationID#getId() with yarn.log-aggregation.retain-seconds * 24 buckets (Hoping 1 hour will have less than millions of apps). This way random read and write of appDir is possible. Deletion Service will traverse on these hashDirs for every userDir., bq. Hoping 1 hour will have less than millions of apps

I think the scheme is too hung up on wall clock time or date.  Wouldn't it be simpler to just take the app ID modulo the desired max bucket size?  Then even if there are a million apps in an hour there will not be more than one million apps in a single directory.  We would need to account for the cluster timestamp changing to prevent a bucket size from being exceeded due to a replaying of app IDs after the RM restart.

I'm thinking a hierarchy like this:
  aggregation_log_root / user / cluster_timestamp / (app_id/ bucket_size) / (app_id%bucket_size)
, Yes got it.  I think Max Bucket Size can be derived from yarn.log-aggregation.retain-seconds (in days) say, yarn.log-aggregation.retain-seconds (in days) * 24 and so it will scale with any number of configured retention period. Else an optimal max bucket size for 7 days retention won;t be for 30 days.

And why we need two sub directories (app_id/ bucket_size) and (app_id%bucket_size). I think below itself should solve.

{code}
aggregation_log_root / user / cluster_timestamp / (app_id%bucket_size) 
         where bucket_size determined from yarn.log-aggregation.retain-seconds (in days) * 24
{code}, bq.  I think Max Bucket Size can be derived from yarn.log-aggregation.retain-seconds (in days)

The whole point of bucketing here is to avoid a maximum limit per directory.  Trying to derive a bucket size from a time value like retention seconds means we need to know the apps-per-second rate of the cluster which we do not know.  In addition it risks blowing the directory limit if that rate ends up higher than what was calculated for a sustained period.  It makes more sense to derive the bucket size from the directory limit since that's what's driving this change.

bq. And why we need two sub directories (app_id/ bucket_size) and (app_id%bucket_size).

Sorry, we just need it to be (app_id / bucket_size), we don't need the modulo.  So the bucket path for an app would be:
  aggregation_log_root / user / cluster_timestamp / (app_id/ bucket_size)
with at most bucket_size number of app directories in each.

The log deletion service can clean up empty bucket directories when it removes the last app from a directory.  It's a little tricky for the bucket delete case since we need to consider the scenario where we want to delete just as a long-running app tries to aggregate to the same bucket, but as long as the aggregation process creates the bucket if necessary and the deletion service takes care to only delete empty bucket directories (i.e.: no recursive delete) we should be fine.

This should handle any app submission rate up to a point where we are aggregating the square of bucket_size apps in the retention period.  Beyond that point we'd have more than bucket_size bucket directories., Yes clear now. 
{code}
aggregation_log_root / user / cluster_timestamp / (app_id/ bucket_size)
    where bucket_size = DFS_NAMENODE_MAX_DIRECTORY_ITEMS_KEY
{code}, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 11s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  9s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 13s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  8s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  5m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  5m 51s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 50s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn: The patch generated 23 new + 166 unchanged - 8 fixed = 189 total (was 174) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  9s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch 7 line(s) with tabs. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:red}-1{color} | {color:red} shadedclient {color} | {color:red} 10m 30s{color} | {color:red} patch has errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 15s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  3m  8s{color} | {color:red} hadoop-yarn-common in the patch failed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 15m  8s{color} | {color:red} hadoop-yarn-server-nodemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 32s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 80m 55s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.logaggregation.TestAggregatedLogDeletionService |
|   | hadoop.yarn.logaggregation.TestAggregatedLogsBlock |
|   | hadoop.yarn.client.api.impl.TestTimelineClientV2Impl |
|   | hadoop.yarn.server.nodemanager.webapp.TestNMWebServer |
|   | hadoop.yarn.server.nodemanager.TestNodeManagerShutdown |
|   | hadoop.yarn.server.nodemanager.security.TestNMTokenSecretManagerInNM |
|   | hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService |
|   | hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerBehaviorCompatibility |
|   | hadoop.yarn.server.nodemanager.webapp.TestNMWebServices |
|   | hadoop.yarn.server.nodemanager.containermanager.scheduler.TestContainerSchedulerQueuing |
|   | hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers |
|   | hadoop.yarn.server.nodemanager.TestNodeStatusUpdater |
|   | hadoop.yarn.server.nodemanager.TestNodeManagerReboot |
|   | hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps |
|   | hadoop.yarn.server.nodemanager.containermanager.TestContainerManager |
|   | hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery |
|   | hadoop.yarn.server.nodemanager.TestNodeManagerResync |
|   | hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage |
|   | hadoop.yarn.server.nodemanager.TestEventFlow |
|   | hadoop.yarn.server.nodemanager.security.TestNMContainerTokenSecretManager |
|   | hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor |
|   | hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService |
|   | hadoop.yarn.server.nodemanager.scheduler.TestDistributedScheduler |
|   | hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:ca8ddc6 |
| JIRA Issue | YARN-6929 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12893767/YARN-6929.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  xml  findbugs  checkstyle  |
| uname | Linux eaa34313507b 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 1c5c2b5 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/18110/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn.txt |
| whitespace | https://builds.apache.org/job/PreCommit-YARN-Build/18110/artifact/patchprocess/whitespace-tabs.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/18110/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/18110/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/18110/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/18110/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} docker {color} | {color:red} 16m 17s{color} | {color:red} Docker failed to build yetus/hadoop:ca8ddc6. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | YARN-6929 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12893940/YARN-6929.1.patch |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/18137/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  3m 50s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 20s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 22m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 10m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 24s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 14m 49s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 51s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 19s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  0m 51s{color} | {color:red} hadoop-yarn-server-nodemanager in the patch failed. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  2m 57s{color} | {color:red} hadoop-yarn in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  2m 57s{color} | {color:red} hadoop-yarn in the patch failed. {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m 18s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn: The patch generated 1 new + 180 unchanged - 32 fixed = 181 total (was 212) {color} |
| {color:red}-1{color} | {color:red} mvnsite {color} | {color:red}  0m 54s{color} | {color:red} hadoop-yarn-server-nodemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:red}-1{color} | {color:red} shadedclient {color} | {color:red}  4m  8s{color} | {color:red} patch has errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 34s{color} | {color:red} hadoop-yarn-server-nodemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 35s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m 54s{color} | {color:green} hadoop-yarn-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  1m 19s{color} | {color:red} hadoop-yarn-server-nodemanager in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 88m 33s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:5b98639 |
| JIRA Issue | YARN-6929 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12894253/YARN-6929.2.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  xml  findbugs  checkstyle  |
| uname | Linux 929085d754a5 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b1de786 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| mvninstall | https://builds.apache.org/job/PreCommit-YARN-Build/18174/artifact/patchprocess/patch-mvninstall-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt |
| compile | https://builds.apache.org/job/PreCommit-YARN-Build/18174/artifact/patchprocess/patch-compile-hadoop-yarn-project_hadoop-yarn.txt |
| javac | https://builds.apache.org/job/PreCommit-YARN-Build/18174/artifact/patchprocess/patch-compile-hadoop-yarn-project_hadoop-yarn.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/18174/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn.txt |
| mvnsite | https://builds.apache.org/job/PreCommit-YARN-Build/18174/artifact/patchprocess/patch-mvnsite-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt |
| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/18174/artifact/patchprocess/patch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/18174/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/18174/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/18174/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 10s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 49s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  3s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  1s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  9s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  5m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  5m 58s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 51s{color} | {color:orange} hadoop-yarn-project/hadoop-yarn: The patch generated 1 new + 180 unchanged - 32 fixed = 181 total (was 212) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m  7s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 20s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  2m 47s{color} | {color:green} hadoop-yarn-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 16m 26s{color} | {color:red} hadoop-yarn-server-nodemanager in the patch failed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 25s{color} | {color:red} The patch generated 3 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 83m  6s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.yarn.server.nodemanager.scheduler.TestDistributedScheduler |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:5b98639 |
| JIRA Issue | YARN-6929 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12894301/YARN-6929.3.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  xml  findbugs  checkstyle  |
| uname | Linux e11ef5eb9a70 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 36e158a |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/18184/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/18184/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-nodemanager.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/18184/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-YARN-Build/18184/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/18184/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, [~jlowe] [~rohithsharma] Need your help in reviewing this patch. Failing test case is an existing one YARN-7299. Have did functional testing with below test cases

{code}
1. New application logs gets written into correct folder structure inside yarn.nodemanager.remote-app-log-dir
2. yarn logs cli works fine
3. Accessing Logs from RM UI / HistoryServer UI works fine while job is running / complete.
{code}

, [~jlowe] Can you review this when you get some time. Thanks., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  7s{color} | {color:red} YARN-6929 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | YARN-6929 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12894301/YARN-6929.3.patch |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/19139/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Sorry for the delay.  Looks like the patch needs to be rebased.

How is this going to work with existing log aggregation setups?  In other words, as soon as this is deployed, will users be able to access logs that were aggregated under the old scheme?  Or during a rolling upgrade where some nodemanagers are aggregating to the old location and some are aggregating to the new location, will a log reader be able to find all of the logs?

I'm not thrilled with seeing the hdfs and hdfs-client dependency added for what is essentially a config key.  This is especially true since YARN should not require HDFS be the underlying filesystem, and therefore this key may not be set.  Even if HDFS is being used as the underlying filesystem, dfs.namenode.fs-limits.max-directory-items is a property intended to be set and read on the namenode server and not intended for client consumption.  There's no guarantee it has been set properly on the client side.  And if it isn't set properly on the client side, the log reader will not be able to find the logs properly.  As such I think it makes more sense to have a separate config property for this.  If users want to tie the two values together they can set the YARN property value to "$\{dfs.namenode.fs-limits.max-directory-items\}"

These logs looks like they were added for debug purposes and either should be removed or marked as a debug log:
{noformat}
          LOG.info("UserDir="+userDir.getPath());
[...]
            LOG.info("USERDIR="+userDirPath);
[...]
              LOG.info("CLUSTERTIMETSMA="+clusterTimestampDir.getPath());
[...]
                  LOG.info("BUCKET="+bucketDir.getPath());

 {noformat}

CLUSTERTIMETSMA is a typo if the log is preserved.

To make this more efficient and reduce the load on the remote filesystem, we should avoid a double liststat call.  We know how many entries are in the original liststat, and we know that deleting any child in the directory is going to update the modtime of the parent directory.  If we delete a directory then we know the modtime is updated, so it won't be worth immediately checking again to see if it should be deleted.  It will only be deleted if there were zero entries in the directory at the start of the loop, so we don't need to stat it again after the loop.

Speaking of deleting bucket/cluster directories, won't it be a problem if one thread is trying to delete a bucket just as another thread tries to aggregate an app there?  In other words, just because the directory looks empty now, how to we know it will remain empty?  The writing code path goes out of its way to check if directories are there and creates them if missing, but the deletion code could be removing a directory just after the write code checks to make sure it is there.

Nit: To get a string value the code should use String.valueOf or Long.toString, Integer.toString, etc. rather than adding an empty string to something.

Nit: This change seems unnecessary:
{noformat}
-      LOG.info("aggregated log deletion started.");
+      LOG.info("aggregated log deletion started here.");
{noformat}

Nit: It looks like there are a number of reformatting/whitespace-only changes which are unrelated to this patch that makes it harder to read and backport, e.g.:
{noformat}
         logIOException("Error reading root log dir this deletion " +
-                       "attempt is being aborted", e);
+             "attempt is being aborted", e);
       }
{noformat}
]