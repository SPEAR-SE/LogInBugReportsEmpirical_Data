[NM should be able to access container.pid files to kill existing containers before the files are cleaned up following an unclean shutdown., Pids might get reused. so we need to guard against that.
Is there any generic Linux OS mechanism that would make the running containers die when the NM dies? e.g. On Windows, NM can use JobObject's to make this happen., Code already exists to kill existing containers when the resource manager requests it.  Three events are dispatched to make this happen: a COMPLETED_CONTAINERS event is handled by the ContainerManager, which dispatches a KILL_CONTAINER event for each container to be killed, which the ContainerImpls handle by dispatching CLEANUP_CONTAINER events, which are finally handled by the ContainersLauncher, which tries to kills the containers.

Does it make more sense to use this chain events or to try to call the kill code directly?  For the former, the issue would be how do we know when the cleanup has been completed?  It looks like ContainerImpls have their state changed when their containers are killed, so the shutdown code could monitor them until they all reach the correct state, but a fair bit of plumbing would be required for the shutdown code to be able to get to them.  For the latter, similar plumbing would be required for the shutdown code to reach the ContainerImpls, and the other issue would be circumventing the event system, which might have consequences that I'm not able to foresee?

This is my first foray into nodemanager code, so maybe someone who understands it better can provide some perspective?, My patch only handles killing containers on shutdown, not previous ones when starting back up., Do we intend to stall shutdown indefinitely for this activity?
{code}
+    while (!containers.isEmpty()) {
+      try {
+        Thread.sleep(1000);
+      } catch (InterruptedException ex) {
+        LOG.warn("Interrupted while sleeping on container kill", ex);
+      }
+    }
{code}

This patch deserves some good tests to verify the new functionality.

Overall the approach seems reasonable but I will defer to someone with a better understanding of NM.

I was wondering if the NM could make itself part of a process group (like setsid) such that everything it spawns is also part of that process group. And the process group could be configured to terminate if the NM root process (NM) dies. Then the OS will take care of cleaning up the orphan processes. This might solve YARN-72 and YARN-73. Is something like this possible?, You're right, we should have some sort of timeout, and just move on and exit after that.

My worries about the process group approach would be:
* A process to be killed via process group is sent a SIGHUP signal, which it can choose to catch and ignore.  The current NodeManager mechanism that my patch makes use of ultimately sends a SIGKILL, which cannot be ignored.
* Processes are allowed to change their own process group.
* The proposed solution to YARN-3 also relies on a possibly conflicting use process groups (I believe a single one for each container?).
* From cursory Googling, there doesn't seem to be any nice way in Java to deal with them.

That said, I'd also defer to someone with a better understanding of NM., Will the timeout fix be made in a patch refresh?

I did not mean cgroups used in YARN-3 but the setsid unix process groups. I have asked a question on YARN-3 on how the changes in that might affect the semantics of the operation in this jira and YARN-73. What do you think? Does anything need to be done specially for cgroups?, Yeah, wanted to get feedback before a refresh, but I'll put in the timeout fix and tests if others think this is a good approach?

I had thought that cgroups relied on unix process groups, but searching around now, I couldn't find a connection, so it seems like interfering with YARN-3 wouldn't actually be a problem.  The other comments still apply to unix process groups., Sandy, this looks like a good start, hooking in the code for container cleanup. I would focus on the part to cleanup on shutdown in this patch, and tackle cleanup on startup in YARN-73.

As Bikas mentioned there needs to be a timeout on waiting for the containers to shutdown. The shutdown process waits for up to yarn.nodemanager.process-kill-wait.ms for the PID to appear, then yarn.nodemanager.sleep-delay-before-sigkill.ms before sending a SIGKILL signal (after a SIGTERM) if the process hasn't died - see ContainerLaunch#cleanupContainer. Waiting for a little longer than the sum of these durations would be sufficient.

Regarding testing, you could have a test like the one in TestContainerLaunch#testDelayedKill to test that containers are correctly cleaned up after stopping a NM., Newest patch contains a test and timeout.  The timeout is yarn.nodemanager.sleep-delay-before-sigkill.ms + yarn.nodemanager.process-kill-wait.ms + 1000.  Should I make this configurable?, I don't think it needs to be configurable, since it's a best effort cleanup anyway. What you have seems reasonable to me, although you might want to make the 1000 a constant so it's clear what it is for (slop value).

Does the test fail without the change?

Nit: I would change waitForContainersOnShutdownMs to waitForContainersOnShutdownMillis., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12554934/YARN-72-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/186//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/186//console

This message is automatically generated., Latest pass makes slop value a constant, changes ms to millis, and has test failing without the change., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12555273/YARN-72-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/191//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/191//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12555274/YARN-72-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/192//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/192//console

This message is automatically generated., Looks good. Minor nit

If these conf values have already been read to actual member values then we might want to use them instead of reading the conf directly. This way we can account for any slop that those values may have added of their own.
{code}
+    waitForContainersOnShutdownMillis =
+        conf.getLong(YarnConfiguration.NM_SLEEP_DELAY_BEFORE_SIGKILL_MS,
+            YarnConfiguration.DEFAULT_NM_SLEEP_DELAY_BEFORE_SIGKILL_MS) + 
+        conf.getLong(YarnConfiguration.NM_PROCESS_KILL_WAIT_MS,
+            YarnConfiguration.DEFAULT_NM_PROCESS_KILL_WAIT_MS) +
+        SHUTDOWN_CLEANUP_SLOP_MS;
{code}, Thanks for looking it over Bikas.  Currently the other place these conf values are read is on instantiation of ContainerLaunch, one of which is created for each container.  Is there somewhere else that it would make sense to store them that should be more authoritative than the Configuration object?, If there is no other master value for these values then never mind :), +1, Updated summary to reflect current scope., minor suggestion (by no means blocking since I'm so late on this) it might be useful to print the containers that didn't finish in cleanupContainers:

+    // All containers killed
+    if (containers.isEmpty()) {
+      LOG.info("All containers in DONE state");
+    }, Added that in., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12555583/YARN-72-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/204//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/204//console

This message is automatically generated., I just committed this. Thanks, Sandy!, Integrated in Hadoop-trunk-Commit #3084 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3084/])
    YARN-72. NM should handle cleaning up containers when it shuts down. Contributed by Sandy Ryza. (Revision 1416484)

     Result = SUCCESS
tomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1416484
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/CMgrCompletedContainersEvent.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/MockNodeStatusUpdater.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
, Integrated in Hadoop-Mapreduce-trunk #1276 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1276/])
    YARN-72. NM should handle cleaning up containers when it shuts down. Contributed by Sandy Ryza. (Revision 1416484)

     Result = SUCCESS
tomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1416484
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/CMgrCompletedContainersEvent.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/MockNodeStatusUpdater.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
, I pulled this into branch-0.23 too., Integrated in Hadoop-Yarn-trunk #56 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/56/])
    YARN-72. NM should handle cleaning up containers when it shuts down. Contributed by Sandy Ryza. (Revision 1416484)

     Result = SUCCESS
tomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1416484
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/CMgrCompletedContainersEvent.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/MockNodeStatusUpdater.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
, Integrated in Hadoop-Hdfs-0.23-Build #455 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/455/])
    YARN-72. NM should handle cleaning up containers when it shuts down. (Sandy Ryza via tomwhite) (Revision 1416562)

     Result = SUCCESS
tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1416562
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/CMgrCompletedContainersEvent.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
, Integrated in Hadoop-Hdfs-trunk #1246 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1246/])
    YARN-72. NM should handle cleaning up containers when it shuts down. Contributed by Sandy Ryza. (Revision 1416484)

     Result = FAILURE
tomwhite : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1416484
Files : 
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/CMgrCompletedContainersEvent.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/MockNodeStatusUpdater.java
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
, Integrated in Hadoop-Hdfs-0.23-Build #585 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/585/])
    YARN-72. Forgot to add 2 files to branch-0.23 (Revision 1469056)

     Result = UNSTABLE
tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1469056
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/MockNodeStatusUpdater.java
* /hadoop/common/branches/branch-0.23/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeManagerShutdown.java
]