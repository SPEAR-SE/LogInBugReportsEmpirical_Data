[Zhijie, a meta comment: It's better to post the problem only in description (and summary) and provide solutions and logs as a followup. Logs can be in follow up comments or attached as separate files.

, Logs:

{code}
2013-02-07 19:05:05,947 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1
2013-02-07 19:05:06,477 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Storing Application with id application_1360292699925_0001
2013-02-07 19:05:06,479 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1360292699925_0001
2013-02-07 19:05:06,479 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1 submitted by user zshen
2013-02-07 19:05:06,481 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=zshen	IP=127.0.0.1	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1360292699925_0001
2013-02-07 19:05:06,493 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1360292699925_0001 State change from NEW to SUBMITTED
2013-02-07 19:05:06,494 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering appattempt_1360292699925_0001_000001
2013-02-07 19:05:06,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1360292699925_0001_000001 State change from NEW to SUBMITTED
2013-02-07 19:05:06,506 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1360292699925_0001 from user: zshen activated in queue: default
2013-02-07 19:05:06,506 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1360292699925_0001 user: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue$User@4965d0e0, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2013-02-07 19:05:06,506 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1360292699925_0001 user: zshen leaf-queue of parent: root #applications: 1
2013-02-07 19:05:06,506 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Submission: appattempt_1360292699925_0001_000001, user: zshen queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=1, numContainers=0, currently active: 1
2013-02-07 19:05:06,508 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1360292699925_0001_000001 State change from SUBMITTED to SCHEDULED
2013-02-07 19:05:06,509 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1360292699925_0001 State change from SUBMITTED to ACCEPTED
2013-02-07 19:05:07,163 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 19:05:08,164 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 19:05:09,167 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 19:05:10,168 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 19:05:11,170 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 19:05:12,173 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 19:05:13,175 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 19:05:14,177 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 19:05:15,179 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
...
2013-02-07 23:51:02,976 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 23:51:03,977 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 23:51:04,978 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 23:51:05,979 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 23:51:06,981 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 23:51:07,982 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
2013-02-07 23:51:08,983 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default usedResources: <memory:0, vCores:0> clusterResources: <memory:1024, vCores:16> currentCapacity 0.0 required <memory:2048, vCores:1> potentialNewCapacity: 2.0 (  max-capacity: 1.0)
...
{code}

[~zjshen]'s solution repasted:
{quote}
In my opinion, the attempt of assigning containers should be terminated in the following two cases.
1. Required > Cluster's absolute capacity: the assignment is impossible to be accomplished. The assignment should be failed immediately.
2. Required + Already used > Cluster's absolute capacity: the assignment should be failed after a certain number of rounds of assignment attempt or a certain duration. The number of rounds or the duration length should be configurable.
{quote}, Zhijie, what is your setting for yarn.scheduler.maximum-allocation-mb? That is the configs that should reject allocations that aren't satisfiable.

In either case, there should be infinite assignments, can you investigate? Thanks!, The way to fix this to cap yarn.scheduler.maximum-allocation-mb to be max(cluster-capacity, yarn.scheduler.maximum-allocation-mb) in a dynamic manner (as nodes come up or go down)., Vinod, I didn't specify yarn.scheduler.maximum-allocation-mb, such that by default it should be 8G. I'll investigate this property as well., {quote}
Zhijie, what is your setting for yarn.scheduler.maximum-allocation-mb? That is the configs that should reject allocations that aren't satisfiable.
{quote}

Hi Vinod, I set yarn.scheduler.maximum-allocation-mb <= yarn.nodemanager.resource.memory-mb (single node cluster). However, the requested resource wasn't bounded by yarn.scheduler.maximum-allocation-mb. On the other side, I saw the code to set amMemory to maxMemory if amMemory > maxMemory. I need to dig deeper.
, I've checked reason why the requested AM size is larger than yarn.scheduler.maximum-allocation-mb. In fact, the AM size is not checked before requesting a container. It is because MR jobs are submitted through ResourceMgrDelegate and managed by MRAppMaster, while checking AM size against the maximum resource capability is only implemented in Client and ApplicationMaster of the distributed shell.

In addition, since the requested AM size will be rounded up, it is at the risk of the roundup value goes beyond yarn.scheduler.maximum-allocation-mb.

Anyway, the aforementioned situation is a separate issue, which I've filed in YARN-407. Here, the focused issue is that if the requested resource is impossible to be supplied, there should be some mechanism to stop the infinite loop of assignment. This issue seems to be similar to YARN-56 and YARN-394.



, bq. I've checked reason why the requested AM size is larger than yarn.scheduler.maximum-allocation-mb. In fact, the AM size is not checked before requesting a container. It is because MR jobs are submitted through ResourceMgrDelegate and managed by MRAppMaster, while checking AM size against the maximum resource capability is only implemented in Client and ApplicationMaster of the distributed shell.

The AM launcher checking limits is great but its the RM's responsibility to not accept requests that it cannot fulfill. From reading the comments, it not clear to me what exactly the root issue is in the RM itself. Is it that the RM is accepting container requests that are greater than the maximum resource available on any 1 node? I dont think we should be comparing against the entire cluster resource since a container request needs to be satisfied within a single node., @Bikas, the problem was produced by two issues:

1. The problematic configuration, which made the AM container require more resource than a node had (In my previous comments, I wrongly mentioned the entire cluster resource, because I was testing it on one-node cluster, such that the maximum cluster resource is equal to that of a single node). This issue can be eliminated when YARN-193 gets fixed.

2. Anyway, when the request is impossible to be fulfilled (e.g., the requested resource is more than any node has), it should be somehow prohibited instead of being executed again and again. YARN-56 and YARN-394 are the two tickets that want to solve the similar issue., [~zjshen] [~bikassaha] I think we should reject the problematic requests at allocate call but not when it is accepted. As that will be a problem.
* For allocate call today we are only rejecting requests if their request is more than what cluster has but we don't do any validation w.r.t. how much a single container will need to run. I think we should add that check. SchedulerUtils#validateResourceRequest().. thoughts??
* We can not reject requests once they are accepted. How the AM will come to know which requests were rejected later? is there anyway we can inform AM about the accepted (earlier) but now rejected requests? One more thing to be considered here is that Node manager having large amount of resources may go down and come back in short span.. (node reconnect or..node removed and added back after very small time)..in whichever case we should not reject that request if it was accepted....large jobs will definitely suffer if few nodes restart in very short span.. thoughts?, Wouldn't make sense to add a new status DISCARDED where an accepted resource request could transition to DISCARDED if there are no nodes that can satisfy the resource request?, We should check for container level requests during allocate RPC request processing. The check that container resource > resource in the cluster is probably less useful compared to checking the container resource > resource on the largest machine.

I think its fine to be able to reject requests later on if they cannot be fulfilled because the cluster has changed state. eg. the only machine with enough resources for a container has failed or container wants a specific machine and that has failed. Thats why I had linked this jira to YARN-394., bq. The check that container resource > resource in the cluster is probably less useful compared to checking the container resource > resource on the largest machine.

Agree, and should the resource here be the remaining unused resource?

bq. I think we should reject the problematic requests at allocate call but not when it is accepted

IMHO, should we give a second chance to the request? The resource of a cluster is dynamic due to NM churn and container acquisition/release. A requests that cannot be fulfilled immediately may be affordable later. Shall we allow some time or retry quota to the request? As [~tucu00] suggested, maybe we can accepted the request, and move it to DISCARDED if no NM can fulfill the request before it expires., Today this limit is static. This by default (under well maintained cluster) will be less than or equal to maximum single node manager resource capability. However for node update or when node is considered dead we don't update it. Probably we should update it or some other flag and start logging this information when it drops below this value... thoughts?
{code}
      this.maximumAllocation = 
        Resources.createResource(conf.getInt(
            YarnConfiguration.RM_SCHEDULER_MAXIMUM_ALLOCATION_MB,
            YarnConfiguration.DEFAULT_RM_SCHEDULER_MAXIMUM_ALLOCATION_MB));
{code}, I dont think we can be good at predicting the future and so it may not be useful to add more logic that allows unsatisfiable requests for some time before rejecting them. We should reject requests when we see that they cannot be allocated. The current example that works in the code is when the request is for resources > than configured maximum. Later on, when requests become unsatisfiable then we need a way reject requests and inform the AM during the allocate heartbeat. This is tracked by YARN-394.

For this jira itself, is it correct to say that the cluster was misconfigured? MR AM container request was set to 1.5G which is greater than memory on NM 1G. What was the value of YarnConfiguration.RM_SCHEDULER_MAXIMUM_ALLOCATION_MB? That should have been <= 1G. Then the request should have been rejected by the RM ApplicationMasterService., The static limit is there for a reason. No application should ask for a container above a certain limit as defined by the admins. For example, if most nodes in a cluster have 4 GB resources, it can be used to set the cap to 4 GB to ensure that even if all large nodes ( ones with more than 4 GB ) disappear, the cluster is still healthy. 

The issue at hand is a scheduling/allocation problem:
  - can this allocation request be fulfilled? 
     - can it be fulfilled now?
     - can it be fulfilled within a short window?
     - can it ever be fulfilled? 
  - When an allocation request deemed to be non-fulfill-able?
     - is this based on static configuration?
     - is this based on a single snapshot of the dynamic view of the cluster?
     - is this based on snapshots over a period of time?
  - If time, on what basis is time defined?
     - clock time?
     - no. of rounds of heartbeats by all healthy nodes in the cluster.

How is the application informed of the necessary information that it needs to make a decision? Information could be:
  - your request could not be fulfilled
  - your partial request could not be fulfilled 
    - the reason why it could not be fulfilled
  - the current view of the cluster such as max available container size
  

, Let's see if we can do something for 2.1.1.]