[I uploaded a patch YARN-3655.000.patch for review., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 39s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 35s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 31s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 53s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   1m 19s | The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings. |
| {color:red}-1{color} | yarn tests |  60m 16s | Tests failed in hadoop-yarn-server-resourcemanager. |
| | |  96m 47s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-yarn-server-resourcemanager |
|  |  Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.isHDFS; locked 66% of time  Unsynchronized access at FileSystemRMStateStore.java:66% of time  Unsynchronized access at FileSystemRMStateStore.java:[line 156] |
| Timed out tests | org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestNodeLabelContainerAllocation |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12733282/YARN-3655.000.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 8f37873 |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-YARN-Build/7956/artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/7956/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/7956/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf901.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/7956/console |


This message was automatically generated., findbugs warning is not related to the attached patch, I created YARN-3667 to fix the findbugs warning.
Also the test failure TestNodeLabelContainerAllocation is not related to the attached patch. It looks like a fake test failure, because the test report( 
https://builds.apache.org/job/PreCommit-YARN-Build/7956/testReport/ ) doesn't have this failure., I uploaded a new patch YARN-3655.001.patch, which added a test case to verify this fix. Without the fix, the test will fail., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 32s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 29s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 36s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 48s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  1s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   1m 18s | The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings. |
| {color:red}-1{color} | yarn tests |  60m 18s | Tests failed in hadoop-yarn-server-resourcemanager. |
| | |  96m 33s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-yarn-server-resourcemanager |
|  |  Inconsistent synchronization of org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.isHDFS; locked 66% of time  Unsynchronized access at FileSystemRMStateStore.java:66% of time  Unsynchronized access at FileSystemRMStateStore.java:[line 156] |
| Timed out tests | org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestNodeLabelContainerAllocation |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12733478/YARN-3655.001.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / a46506d |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-YARN-Build/7964/artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/7964/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/7964/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/7964/console |


This message was automatically generated., Thanks for the patch [~zxu],

I was just wondering though.. with your approach, assume the following situation (please correct me if I am wrong)

* We have 3 nodes with say 4GB capacity.
* Currently, applications are using up 3GB on each node (assume they are all fairly long running tasks..).
* At time T1, A new app (appX) is added, and requires 2 GB.
* At some time T2, the next allocation event (after all nodes have sent heartbeat.. or after a continuousScheduling attempt) happens, a reservation of 2GB is made on each node for appX.
* At some time T3, during the next allocation event, As per your patch, the reservation for appX will be removed from ALL nodes.. 
* Thus reservations for appX will flip-flop on all nodes. It is possible that during the period when there is no reservation for appX. other apps with < 1GB requirement might come in and be scheduled on the cluster... thereby starving appX
, If allocating a container is going to take the amShare over the maxAMShare, not allocating and hence unreserving resources seems reasonable. That said, we should also add the same check before making such a reservation in FSAppAttempt#assignContainer.

There is already a check to ensure we won't go over maxShare. In terms of code organization, I would like for us to create a helper method (okayToReserveResources) that would check the maxShare for all containers and maxAMShare for AM containers. 

Also, looking at the code, I see fitsInMaxShare method is a static in FairScheduler. We should just make it a non-static method in FSQueue, it can call parent.fitsInMaxShare. Can we file a follow-up JIRA for it? , thanks [~asuresh] for the review. I think the flip-flop won't happen.
bq. At some time T2, the next allocation event (after all nodes have sent heartbeat.. or after a continuousScheduling attempt) happens, a reservation of 2GB is made on each node for appX.
The above reservation won't succeed because maxAMShare limitation.
If it succeeded, then the reservation for appX won't be removed.

thanks [~kasha] for your review. these are great suggestions.
I made the change based on your suggestions. Also I fixed fitsInMaxShare issue in this JIRA instead of creating a follow-up JIRA.
I also did some optimizations to remove some duplicate logic.
I find hasContainerForNode already covered getTotalRequiredResources.
If we check hasContainerForNode, then we don't check getTotalRequiredResources.
So I remove getTotalRequiredResources check in assignReservedContainer and assignContainer.
Also because okToUnreserve checked hasContainerForNode, we don't need to check it again for reserved container in assignContainer.
I uploaded a new patch YARN-3655.002.patch with above change., \\
\\
| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 58s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 52s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 47s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 48s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  1s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 16s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |  50m 22s | Tests passed in hadoop-yarn-server-resourcemanager. |
| | |  87m 39s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12734328/YARN-3655.002.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / fb6b38d |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8037/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8037/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8037/console |


This message was automatically generated., Thanks for the update [~zxu]. I have only one nit :
In the {{okToUnreserve}} method, do we also need to do the {{!hasContainerForNode}} check ?? I assume the patch was meant to unreserve those containers which can fit in a node, but we put a limit on this if it exceeds maxAMShare.., [~asuresh], thanks for the review. As [~kasha] suggested, I did some refactor for the code to combine all the unreserve conditions check in {{okToUnreserve}}. maxAMShare check is only used for AM container reservation and there are none AM container reservations which need {{!hasContainerForNode}} check.
, makes sense...
+1 from me.. will commit, unless [~kasha] has any comments, I would like to take a look at the patch as well.  , Comments on the patch:
# okToUnreserve 
## It was a little hard to wrap my head around. Can we negate it and call it {{isValidReservation(FSSchedulerNode)}}? 
## Can we get rid of the if-else and have a simple {{return hasContainerForNode && fitsInMaxShare && !isOverAMShareLimit}}?
# Add an {{if (isValidReservation)}} check in {{FSAppAttempt#reserve}} so all the reservation logic stays in one place? 
# In {{FSAppAttempt#assignContainer(node, request, nodeType, reserved)}}, 
## We can get rid of the fitsInMaxShare check immediately preceding the call to {{reserve}}.
## Given {{if (fitsIn(capability, available))}}-block ends in return, we don't need to put the continuation in else. 
# While adding this check in {{FSAppAttempt#assignContainer(node)}} might work in practice, it somehow feels out of place. Also, assignReservedContainer could also lead to a reservation? 
# Instead of calling {{okToUnreserve}}/{{!isValidReservation}} in {{FairScheduler#attemptScheduling}}, we should likely add it as the first check in {{FSAppAttempt#assignReservedContainer}}.
# Looks like assign-multiple is broken with reserved-containers. The while-loop for assign-multiple should look at both reserved and un-reserved containers assigned. Can we file a follow-up JIRA to fix this?  , Oh, and I found it hard to understand the test. Can we add some documentation to clarify what the test is doing? We should essentially test the following:
# Container gets reserved when not over maxAMShare
# Container doesn't get reserved when over maxAMShare
# If the maxAMShare were to go down due to fairshare going down, container gets unreserved. , Hi [~kasha], thanks for the review.
bq. 1. okToUnreserve
fixed in the new patch YARN-3655.003.patch

bq. 2. Add an if (isValidReservation) check in FSAppAttempt#reserve so all the reservation logic stays in one place?
IMHO, It is not good to add if (isValidReservation) check in FSAppAttempt#reserve because all the conditions checked in isValidReservation are already checked before we call FSAppAttempt#reserve, it will be duplicate code which will affect the performance.

bq. 3.In {{FSAppAttempt#assignContainer(node, request, nodeType, reserved)}}...
fixed in the new patch YARN-3655.003.patch, In order to remove fitsInMaxShare check, I merged {{fitsInMaxShare}} check into {{hasContainerForNode}}, which also make the code cleaner.

bq. 4. While adding this check in FSAppAttempt#assignContainer(node) might work in practice, it somehow feels out of place. Also, assignReservedContainer could also lead to a reservation?
It looks like assignReservedContainer won't lead to a reservation({{FSAppAttempt#reserve}}), assignReservedContainer won't call {{FSAppAttempt#reserve}} because {{FSAppAttempt#reserve}} will only be called when the node Available Resource is smaller than the requested/reserved resource. assignReservedContainer will only call assignContainer when the node Available Resource is no less than the reserved resource. So only {{FSAppAttempt#assignContainer(node)}} can lead to a reservation when the node Available Resource is smaller than the requested resource.

bq. 5. Instead of calling okToUnreserve/!isValidReservation in FairScheduler#attemptScheduling...
fixed in the new patch YARN-3655.003.patch

bq. 6. Looks like assign-multiple is broken with reserved-containers. The while-loop for assign-multiple should look at both reserved and un-reserved containers assigned. Can we file a follow-up JIRA to fix this?
I suppose you mean assign-multiple is broken after assignReservedContainer turns the reservation into an allocation.
Yes, I created YARN-3710 to fix this issue.

bq. Oh, and I found it hard to understand the test....
fixed in the new patch YARN-3655.003.patch, please review it., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 37s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 34s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 31s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 46s | The applied patch generated  1 new checkstyle issues (total was 123, now 120). |
| {color:green}+1{color} | whitespace |   0m  3s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 33s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 36s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 15s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |  50m 10s | Tests passed in hadoop-yarn-server-resourcemanager. |
| | |  86m 31s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12735189/YARN-3655.003.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / ada233b |
| checkstyle |  https://builds.apache.org/job/PreCommit-YARN-Build/8072/artifact/patchprocess/diffcheckstylehadoop-yarn-server-resourcemanager.txt |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8072/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8072/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8072/console |


This message was automatically generated., \\
\\
| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 35s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 31s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 30s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 54s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  5s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 16s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |  50m 14s | Tests passed in hadoop-yarn-server-resourcemanager. |
| | |  86m 38s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12735229/YARN-3655.003.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / ada233b |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8077/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8077/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8077/console |


This message was automatically generated., bq. IMHO, It is not good to add if (isValidReservation) check in FSAppAttempt#reserve because all the conditions checked in isValidReservation are already checked before we call FSAppAttempt#reserve, it will be duplicate code which will affect the performance.
Is it possible to avoid the checks before the call, and do all the checks in the call. The reasoning behind this is to have all reservation-related code in as few places as possible. If this is not possible, we can leave it as the patch has it now.

bq. While adding this check in FSAppAttempt#assignContainer(node) might work in practice, it somehow feels out of place. 
Instead of adding the check to assignContainer(node) can we add it to assignContainer(node, request, nodeType, reserved)?, Hi [~kasha], thanks for the review.
bq. Is it possible to avoid the checks before the call, and do all the checks in the call. The reasoning behind this is to have all reservation-related code in as few places as possible. If this is not possible, we can leave it as the patch has it now.
IMHO, it is not possible because {{FSAppAttempt#reserve}} will only be called from {{assignContainer(node)}}, if we move all the condition checks into {{FSAppAttempt#reserve}}, it may return early, which will cause failing to reserve or allocate container for other priorities, also since {{assignReservedContainer}} won't call {{FSAppAttempt#reserve}}, we still need keep {{isValidReservation}} check in {{assignReservedContainer}}.

bq. Instead of adding the check to assignContainer(node) can we add it to assignContainer(node, request, nodeType, reserved)?
IMHO, It will have problem, if we add it to {{assignContainer(node, request, nodeType, reserved)}}, then {{getAllowedLocalityLevelByTime}}/{{getAllowedLocalityLevel}} will be called before the check instead of after the check, which will change the Scheduling behavior, also it will affect the performance(late check will increase CPU usage)., Thanks for the clarifications, Zhihai. The latest patch looks mostly good, nice test. Few nit picks before we get this in:
# In hasContainerForNode, the patch has some spurious changes. Also, would be nice to add a comment for the newly added check.
# File a follow-up JIRA to separate out the code paths for assigning a reserved container and a non-reserved container. 
# File a follow-up JIRA to move all reservation-related tests from TestFairScheduler to TestFairSchedulerReservations, [~kasha], thanks for the thorough review, I uploaded a new patch YARN-3655.004.patch which addressed your first comment.
And I created two follow up JIRAs YARN-3776 and YARN-3777 which addressed your second and third comments. Please review it. Many thanks., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m 51s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 30s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 35s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 46s | The applied patch generated  1 new checkstyle issues (total was 122, now 119). |
| {color:green}+1{color} | whitespace |   0m  5s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 32s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 24s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |  50m 12s | Tests passed in hadoop-yarn-server-resourcemanager. |
| | |  87m 55s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12738145/YARN-3655.004.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / b3ffa87 |
| checkstyle |  https://builds.apache.org/job/PreCommit-YARN-Build/8202/artifact/patchprocess/diffcheckstylehadoop-yarn-server-resourcemanager.txt |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8202/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8202/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8202/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  15m  2s | Findbugs (version ) appears to be broken on trunk. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 35s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 36s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 23s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  4s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 26s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:red}-1{color} | yarn tests |  50m 17s | Tests failed in hadoop-yarn-server-resourcemanager. |
| | |  86m 55s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12738190/YARN-3655.004.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 71de367 |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8208/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8208/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8208/console |


This message was automatically generated., \\
\\
| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m 57s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 36s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 36s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 45s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  3s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m 26s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |  50m 13s | Tests passed in hadoop-yarn-server-resourcemanager. |
| | |  88m 11s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12738203/YARN-3655.004.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 71de367 |
| hadoop-yarn-server-resourcemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8209/artifact/patchprocess/testrun_hadoop-yarn-server-resourcemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8209/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8209/console |


This message was automatically generated., Updated the patch to fix the checkstyle issue. The latest patch YARN-3655.004.patch passed the Jenkins test., +1, checking this in. , Zhihai - thanks for fixing this critical issue and patience through the reviews.

Just committed this to trunk and branch-2. , FAILURE: Integrated in Hadoop-trunk-Commit #7984 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7984/])
YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha) (kasha: rev bd69ea408f8fdd8293836ce1089fe9b01616f2f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
, thanks [~asuresh] for the review! thanks [~kasha] for reviewing and committing the patch., FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #222 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/222/])
YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha) (kasha: rev bd69ea408f8fdd8293836ce1089fe9b01616f2f7)
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #952 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/952/])
YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha) (kasha: rev bd69ea408f8fdd8293836ce1089fe9b01616f2f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #2150 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2150/])
YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha) (kasha: rev bd69ea408f8fdd8293836ce1089fe9b01616f2f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk-Java8 #211 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/211/])
YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha) (kasha: rev bd69ea408f8fdd8293836ce1089fe9b01616f2f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2168 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2168/])
YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha) (kasha: rev bd69ea408f8fdd8293836ce1089fe9b01616f2f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #220 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/220/])
YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha) (kasha: rev bd69ea408f8fdd8293836ce1089fe9b01616f2f7)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSQueue.java
* hadoop-yarn-project/CHANGES.txt
]