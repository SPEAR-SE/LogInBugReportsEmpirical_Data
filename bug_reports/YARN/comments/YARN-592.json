[Setting target version to be 2.0.05-beta.

Devaraj, I see you assigned it to yourselves, I suppose you have cycles to get this done in time for 2.0.5-beta. Let me know otherwise, thanks., Please find the patch for this issue., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12581866/YARN-592.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/869//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/869//console

This message is automatically generated., Devaraj, can you please summarize what your patch does so that we are on the same page. This is a general tip to help progress. Please state
 - What is the objective?
 - What is the solution?
 - And what specifically the patch does - changes in specific classes, tests etc.

I tried to review the patch but couldn't do it without the above.

Tx.
, Thanks Vinod for having a look into the patch.

Here I see these two problem cases for restart.

1. Node Manager graceful Shutdown
     If we shutdown NM while running job, it is aggregating for all the containers in it and renaming the .tmp aggregate file. After the NM starts again, it is creating the .tmp aggregate file and doing the aggregation for the completed containers.  And finally it is not renaming this .tmp file because aggregate file already exist. When we try access logs fior tasks, it gives for the tasks which all executed before NM goes down.
 
2. Node Manager abrupt kill : 
   If we kill NM while running job after running some containers, it is having the .tmp aggregate file for the application on that host. After restart it is overwriting the existing .tmp file and continuing the aggregation with this. After completing the aggregation for that job, it contains the logs for the tasks which executed after restart. Here it doesn't contain the tasks which got executed before the NM goes down. And also after aggregation it deletes the logs directory from the  local log dir. 

In the attached patch, you can find the fix for these two above cases.

   1. If the aggregate file already exist, then creating .tmp file and copying the previously aggregated logs into new .tmp file and deleting the old aggregated file.
   
   2. If the .tmp aggregate file already exist, after completion of the application reading the container logs from the local log dirs and aggregating for the containers which has not yet done., I just looked at your patch.... I need more information to understand it better....
* are you assuming that after nm restarts application for which containers were running on that node manager will again get new container on the same node manager? at present NM doesn't remember the applications which were running on it across restart. Also RM doesn't inform NM about all the running applications in the cluster.
* Now across NM restart applications might be still running or it might have just finished before restart. Do you want to upload the logs for both scenarios? at present we upload logs only when application finishes...
, Thanks Omkar for looking into the patch and trying to understanding.

This JIRA is trying to address these two problems while running containers for an application NM goes down and comes up and then launch containers for the same application. 

1. Graceful shutdown of NM and start again 
2. NM Crash(or abrupt kill) and start again 


bq.•are you assuming that after nm restarts application for which containers were running on that node manager will again get new container on the same node manager? at present NM doesn't remember the applications which were running on it across restart. Also RM doesn't inform NM about all the running applications in the cluster.
Yes, This Jira is mainly to address the case where containers running for the same application before and after NM restart. It is the important case because NM gets the application completed event and deletes the all container logs(including the container logs which ran before crash) for that application, and those logs(not aggregated) will not be available in the HDFS as explained in the previous comment. If NM doesn't get application completed event from RM then the logs atleast will be availble in the local logs dir.
 
bq.•Now across NM restart applications might be still running or it might have just finished before restart. Do you want to upload the logs for both scenarios? at present we upload logs only when application finishes...
This patch is trying to upload logs for the applications which run before and after NM restart. If the application gets completed after NM crash and before starting NM, atleast logs for the containers ran on that node can get from NM local logs dirs. 

If the NM gets stopped properly, presently NM uploads logs for all the running containers before going down. This case we may not need to handle anything.
, Just to be sure I might be wrong.... I am bit skeptical about .tmp file... are you sure it contains all the logs? My understanding is that it was still in the process and didn't finish with all. However even for completed logs.. it will enqueue them into the deletion service for future deletion....which may or may not happen even for graceful shutdown as we kill NM after some time...right? thoughts?

bq. This patch is trying to upload logs for the applications which run before and after NM restart. If the application gets completed after NM crash and before starting NM, atleast logs for the containers ran on that node can get from NM local logs dirs.

This seems to be problematic. The time difference between AM finishing and NM starting can be as low as sec..or as high as hours.. we need to have definite policy for handling logs.. because if we don't handle this logs will be lying on nm waiting for already finished app to finish ... right?.. thoughts?]