[As per Leafqueue assignContainers(), application will be fetched from activeApplications.
In this scenario, Application1 is fetched first when Node Update of Node_2 came.

So below check in assignContainers failed 
          // Check queue max-capacity limit
          if (!assignToQueue(clusterResource, required)) {
            return NULL_ASSIGNMENT;
          }
Here the queue limit was crossing the limit. But as per the return statement, the loop never tried for the second application.
Application2 has only 2GB demand to launch. And this could have launched in Node_2.

So instead of return NULL_ASSIGNMENT, it is better to break from the inner loop. 
some user limit check also breaking from inner loop.

Kindly check this patch and please share your thoughts., Updated with test case to reproduce this scenario.
Please review the same., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12625843/Yarn-1631.2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/2957//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/2957//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12625843/Yarn-1631.2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/4455//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/4455//console

This message is automatically generated., [~sunilg], this patch and solution seems good to me, if an app1 cannot get a container with sizeX doesn't mean we should stop allocation for apps submitted after app1.

+[~vinodkv], [~jianhe]. Thoughts?, Just offline discussed with [~vinodkv] and [~jianhe], now our consensus is, if we allow skip first submitted application because of queueMax resource limit, it violates FIFO. This is different from user-limit, if we skip it, first application with large resource request may be starved. 

Another problem in your example is, app1 shouldn't reserve container in node_1 because app_1's AM is running on that node too. Queue continuous reservation looking can solve this problem.

Closing it as won't fix. [~sunilg], please reopen it if you still think it's a problem., we need to be careful with this.  You could end up starving out the first application.  It definitely changes current semantics.

What version of hadoop are you seeing this issue? With my patch for reservations continue looking it should actually look at node 2 and take that one and unreserve node 1.  There is the logic for the needsContainer that might be affecting this that I would have to look at more., [~tgraves], we have a concurrent comment, just closed this JIRA, see my comment: https://issues.apache.org/jira/browse/YARN-1631?focusedCommentId=14524039&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14524039, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12625843/Yarn-1631.2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / d3d019c |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/7582/console |


This message was automatically generated.]