[In my latest test, I updated the following configuration values at 12:41PM and restarted the App Timeline Server, NodeManager and ResourceManager, and noticed an updated YARN cache size in around an hour.  This occurred despite the currently configured "yarn.nodemanager.localizer.cache.cleanup.interval-ms" value.

h5.  Current Values
{code}
yarn.nodemanager.localizer.cache.target-size-mb=3072
yarn.nodemanager.localizer.cache.cleanup.interval-ms=300
{code}


h5. Log snippet (45 minutes of no logs)
{code}
2016-01-04 12:41:44,917 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:registerWithRM(358)) - Notifying ContainerManager to unblock new container-requests
2016-01-04 13:25:09,196 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:deleteAsUser(457)) - Deleting absolute path : /hadoop/yarn/local/usercache/<username>/filecache/2438
2016-01-04 13:25:09,201 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:deleteAsUser(457)) - Deleting absolute path : /hadoop/yarn/local/usercache/<username>/filecache/2439
2016-01-04 13:32:45,371 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:deleteAsUser(457)) - Deleting absolute path : /hadoop/yarn/local/usercache/<username>/filecache/2440
2016-01-04 13:32:45,372 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:deleteAsUser(457)) - Deleting absolute path : /hadoop/yarn/local/usercache/<username>/filecache/2443
2016-01-04 13:32:45,372 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:deleteAsUser(457)) - Deleting absolute path : /hadoop/yarn/local/usercache/<username>/filecache/2441
2016-01-04 13:32:45,373 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:deleteAsUser(457)) - Deleting absolute path : /hadoop/yarn/local/usercache/<username>/filecache/2442
{code}

h5.  YARN Cache Size
{code}
[root@linux02-<username> hadoop]# date && du -m /hadoop/yarn/ | sort -nr | head -n 20
Mon Jan  4 13:11:53 EST 2016
3753    /hadoop/yarn/
3752    /hadoop/yarn/local/usercache/<username>/filecache
3752    /hadoop/yarn/local/usercache/<username>
3752    /hadoop/yarn/local/usercache
3752    /hadoop/yarn/local
163     /hadoop/yarn/local/usercache/<username>/filecache/2467
163     /hadoop/yarn/local/usercache/<username>/filecache/2465
163     /hadoop/yarn/local/usercache/<username>/filecache/2463
163     /hadoop/yarn/local/usercache/<username>/filecache/2461
163     /hadoop/yarn/local/usercache/<username>/filecache/2459
163     /hadoop/yarn/local/usercache/<username>/filecache/2457
163     /hadoop/yarn/local/usercache/<username>/filecache/2455
163     /hadoop/yarn/local/usercache/<username>/filecache/2453
163     /hadoop/yarn/local/usercache/<username>/filecache/2451
163     /hadoop/yarn/local/usercache/<username>/filecache/2449
163     /hadoop/yarn/local/usercache/<username>/filecache/2447
163     /hadoop/yarn/local/usercache/<username>/filecache/2445
163     /hadoop/yarn/local/usercache/<username>/filecache/2443
163     /hadoop/yarn/local/usercache/<username>/filecache/2441
163     /hadoop/yarn/local/usercache/<username>/filecache/2439
[root@linux02-<username> hadoop]#
[root@linux02-<username> hadoop]#
[root@linux02-<username> hadoop]# date && du -m /hadoop/yarn/ | sort -nr | head -n 20
Mon Jan  4 13:54:11 EST 2016
3002    /hadoop/yarn/local/usercache/<username>/filecache
3002    /hadoop/yarn/local/usercache/<username>
3002    /hadoop/yarn/local/usercache
3002    /hadoop/yarn/local
3002    /hadoop/yarn/
163     /hadoop/yarn/local/usercache/<username>/filecache/2467
163     /hadoop/yarn/local/usercache/<username>/filecache/2465
163     /hadoop/yarn/local/usercache/<username>/filecache/2463
163     /hadoop/yarn/local/usercache/<username>/filecache/2461
163     /hadoop/yarn/local/usercache/<username>/filecache/2459
163     /hadoop/yarn/local/usercache/<username>/filecache/2457
163     /hadoop/yarn/local/usercache/<username>/filecache/2455
163     /hadoop/yarn/local/usercache/<username>/filecache/2453
163     /hadoop/yarn/local/usercache/<username>/filecache/2451
163     /hadoop/yarn/local/usercache/<username>/filecache/2449
163     /hadoop/yarn/local/usercache/<username>/filecache/2447
163     /hadoop/yarn/local/usercache/<username>/filecache/2445
88      /hadoop/yarn/local/usercache/<username>/filecache/2466
88      /hadoop/yarn/local/usercache/<username>/filecache/2464
88      /hadoop/yarn/local/usercache/<username>/filecache/2462
{code}, As far as I have seen, {{yarn.nodemanager.localizer.cache.target-size-mb}} set the limit size of each localized resources per user. In the case, the total of resources can be the sum of {{/hadoop/yarn/local/usercache/<username>/filecache/*}} sizes. In other words 163 * 15 = 2475. It does not exceed 3072 at 13:11. It seems to exceed 3072MB at 13:25 according the log. 
Deletion task is running periodically with interval 300ms with the given setting., Just to follow up.  The "/hadoop/yarn/local/usercache/<username>/filecache" is a total of 3752 MB in size.  Also the command shown above "date && du -m /hadoop/yarn/ | sort -nr | head -n 20" is specifically showing only the top 20 values ("head -n 20"), so there is a whole bunch of filecache sizes not shown. As such, the calculation mentioning "163 * 15" is only the sum of a partial set.

Just a quick update to try and clear up any misunderstanding.  I was NOT running any YARN jobs (My Hadoop cluster is for testing and was idle), and as such the directory "/hadoop/yarn/local" was at a stable constant size. Initially my "/hadoop/yarn/local" size was around 10 GB, and then I updated "yarn.nodemanager.localizer.cache.target-size-mb" to a much smaller value (Think around half).  Then I ran the test again when the "/hadoop/yarn/local" size was around 5GB and set the "yarn.nodemanager.localizer.cache.target-size-mb" property to about 4GB.  Then I ran another test.  My "/hadoop/yarn/local" size was around 4GB and I set the "yarn.nodemanager.localizer.cache.target-size-mb" property to about 3 GB., Sorry for misunderstanding.
How can you put a log files without running jobs on YARN? Do you mean put or generate log files by yourself under "/hadoop/yarn/local"?

My assumption is that you decreased "yarn.nodemanager.localizer.cache.target-size-mb" one by one. And caches were deleted under the target-size-mb correctly. But it takes more time than "yarn.nodemanger.localizer.cache.cleanup.interval-ms". Is it correct?, In response to your first two questions.  Basically I ran a lot of YARN jobs and noticed that the "/hadoop/yarn/local/usercache/<username>" directory grew to about 10 GB.  I then stopped all my YARN jobs.  Then I attempted to reduce the "/hadoop/yarn/local/usercache/<username>" directory size by setting the "yarn.nodemanager.localizer.cache.target-size-mb" property and waiting.

In response to your last question.  Yes, it seemed that the caches where deleted to the target size, but in a much greater time interval then specified in the "yarn.nodemanger.localizer.cache.cleanup.interval-ms" property.

]