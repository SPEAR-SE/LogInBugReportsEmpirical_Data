[We ran into this in our rolling upgrade tests. , The issue is with counting container-launch-failures against the 4 task failures. We could potentially go about this in different ways:
# Support retries when launching containers. Start/stop containers are @AtMostOnce operations. This works okay for NM restart cases. When an NM goes down, this will lead to the job waiting longer before trying another node.
# On failure to launch container, return an error code that explicitly annotates it as a system error and not a user error. The AMs could choose to not count system errors against number of task attempt failures. 
# Without any changes in Yarn, MR should identify exceptions on startContainers() different from failures captured in StartContainersResponse#getFailedRequests. That is, NMNotYetReadyException and IOException will not be counted against the number of allowed failures. 

Option 2 seems like a cleaner approach to me. , This is a long standing issue - we added the exception in YARN-562.

I think that instead of blanket retries (solution #1) above, the right solution is for clients to retry NMNotYetReadyException. We can do that in NMClient library for java clients? /cc [~jianhe], This wasn't as big an issue without work-preserving RM restart, as the AM itself would be restarted and the window of opportunity for it to try launching containers was fairly small.

bq. the right solution is for clients to retry NMNotYetReadyException
I kind of agree, but this is a remote exception for the client (MR-AM in this case). What is the best way to handle remote exceptions? , By the way, here is the stack trace:

{noformat}
2015-06-16 17:31:36,663 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1434500031312_0008_m_000035_0: Container launch failed for container_e04_1434500031312_0008_01_000037 : org.apache.hadoop.yarn.exceptions.NMNotYetReadyException: Rejecting new containers as NodeManager has not yet connected with ResourceManager
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainers(ContainerManagerImpl.java:693)
	at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.startContainers(ContainerManagementProtocolPBServiceImpl.java:60)
	at org.apache.hadoop.yarn.proto.ContainerManagementProtocol$ContainerManagementProtocolService$2.callBlockingMethod(ContainerManagementProtocol.java:95)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:99)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy40.startContainers(Unknown Source)
	at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:151)
	at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.NMNotYetReadyException): Rejecting new containers as NodeManager has not yet connected with ResourceManager
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainers(ContainerManagerImpl.java:693)
	at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.startContainers(ContainerManagementProtocolPBServiceImpl.java:60)
	at org.apache.hadoop.yarn.proto.ContainerManagementProtocol$ContainerManagementProtocolService$2.callBlockingMethod(ContainerManagementProtocol.java:95)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1468)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy39.startContainers(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl.startContainers(ContainerManagementProtocolPBClientImpl.java:96)
{noformat}, bq. I kind of agree, but this is a remote exception for the client (MR-AM in this case). What is the best way to handle remote exceptions? 
The client should already be unwrapping and throwing the right exception locally. The diagnostic message you posted also seems to be pointing the same.., I'm actually thinking do we still need the NMNotYetReadyException.. the NMNotYetReadyException is currently thrown when NM starts the service but not yet register/re-register with RM.  it may be ok to just launch the container. 

1. For work-preserving NM restart(scenario in this jira), I think it's ok to just launch the container instead of throwing exception.
2. For NM restart with no recovery support,  startContainer will fail anyways because the NMToken is not valid.
3. For work-preserving RM restart, containers launched before NM re-register can be recovered on RM when NM sends the container status across. startContainer call after re-register will fail because the NMToken is not valid. , We should also consider graceful NM decommission. For graceful decommission, the RM should refrain from assigning more tasks to the node in question. Should we also prevent AMs that have already been assigned this node from starting new containers? In that case, I guess we would not be throwing NMNotYetReadyException, but another YarnException - NMShuttingDownException?

On the client side (MR-AM in this case), we should probably consider any {{YarnException}} as a system error and count it against KILLED?, I agree with Jian that we probably don't need the not ready exception.  I was never a fan of it in the first place, as IMHO we should just avoid opening or processing the client port until we've registered with the RM if it's really a problem in practice.  As Jian points out, I think the NMToken will cover the cases where someone is trying to launch something they shouldn't be launching, so I don't think we need to wait for the RM registration., bq. We should also consider graceful NM decommission. For graceful decommission, the RM should refrain from assigning more tasks to the node in question. Should we also prevent AMs that have already been assigned this node from starting new containers? In that case, I guess we would not be throwing NMNotYetReadyException, but another YarnException - NMShuttingDownException?
[~kasha], we could. Let's file a separate JIRA?

bq. we should just avoid opening or processing the client port until we've registered with the RM if it's really a problem in practice
[~jlowe], this is not possible to do as the NM needs to report the RPC server port during registration - so, server start should happen before registration.

bq. 2. For NM restart with no recovery support, startContainer will fail anyways because the NMToken is not valid.
bq. 3. For work-preserving RM restart, containers launched before NM re-register can be recovered on RM when NM sends the container status across. startContainer call after re-register will fail because the NMToken is not valid.
[~jianhe], these two errors will be much harder for apps to process and react to than the current named exception.

Further, things like Auxiliary services are also not setup already by time the RPC server starts and depending on how the service order changes over time, users may get different types of errors. Overall, I am in favor of keeping the named exception with clients explicitly retrying. , bq. this is not possible to do as the NM needs to report the RPC server port during registration - so, server start should happen before registration.
Yes, but that's a limitation in the RPC layer.  If we could bind the server before we start it then we could know the port, register with the RM, then start the server.  IMHO the RPC layer should support this, but I understand we'll have to work around the lack of that in the interim.  I think we all can agree the retry exception is just a hack being used because we can't keep the client service from serving too soon., bq. this is not possible to do as the NM needs to report the RPC server port during registration - so, server start should happen before registration.
For RM work-preserving restart, this is not a problem as the NM remain as-is.
For NM restart with no recovery, all outstanding containers allocated on this node are anyways killed.
For NM work-preserving restart, I found the code already make sure everything starts first before starting the containerManager server.
{code}
    if (delayedRpcServerStart) {
      waitForRecoveredContainers();
      server.start();
{code}

Overall, I think it's fine to add a client retry fix in 2.7.1;    But long term I'd like to re-visit this, may be I still miss something. , bq.  For NM work-preserving restart, I found the code already make sure everything starts first before starting the containerManager server.
I didn't realize this, tx for pointing out.

Had an offline discussion with [~jianhe] and couldn't come up with a case where not blocking the calls will be a problem. In all the cases, whether the calls are blocked or not, eventually they will be rejected for invalid-token-error or container-given-by-old-RM. Even if the calls are not blocked, the same errors happen right-away.

I am +1 now for not throwing this exception from the NM side. But given that it is part of the contract, I don't think we should remove the class in case., [~kasha] said I can take this over., I moved this to MAPREDUCE because I'm doing the 3rd suggestion that Karthik mentioned where MR handles this type of failure differently, and doesn't count it against the retries., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  15m 51s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 45s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 53s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 37s | The applied patch generated  4 new checkstyle issues (total was 261, now 264). |
| {color:green}+1{color} | whitespace |   0m  1s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 35s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m  7s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m 12s | Tests passed in hadoop-mapreduce-client-app. |
| | |  47m  0s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12740777/MAPREDUCE-6409.001.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 20c03c9 |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5816/artifact/patchprocess/trunkFindbugsWarningshadoop-mapreduce-client-app.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5816/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5816/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5816/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5816/console |


This message was automatically generated., How about we remove NMNotYetReadyException in trunk and branch-2 to minimize any risk, and do MR-only changes for 2.7.1? Filed YARN-3839 to handle that.

[~rkanter] - thanks for picking this up. The patch looks mostly good to me. One nit: what do you think of suffixing the event-type {{FAILED_BY_YARN}} instead of {{FAILED_DUE_TO_YARN}}? 

Also, what do you think of catching {{YarnException}} instead of {{NMNotYetReadyException}}? If there is an exception that shouldn't be caught, it shouldn't be a YarnException? We can fix this as part of YARN-3839. /cc [~vinodkv], [~jlowe], [~jianhe] , The new patch renames the enum to {{FAILED_BY_YARN}}.  The checkstyle warnings are all for lines in the state machine transitions, which currently match the rest of the lines so I don't want to fix those.

, I think it's a little harsh to treat NMNotYetReadyException as a failure to launch without any retries.  We don't do this for connection refused or socket connection timeout, yet this is effectively an application-level connection refusal.  I agree with what Vinod mentioned earlier -- we should simply retry the exception.  Can we have NMProxy setup the proxy to retry NMNotYetReadyException?  In most cases the retries will eventually succeed before it times out, and that's preferable to throwing away the container and needing to allocate a new one.
, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  16m  1s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 43s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 42s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   0m 36s | The applied patch generated  4 new checkstyle issues (total was 261, now 264). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 34s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   1m  7s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | mapreduce tests |   9m  7s | Tests passed in hadoop-mapreduce-client-app. |
| | |  46m 53s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12741082/MAPREDUCE-6409.002.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 445b132 |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5826/artifact/patchprocess/trunkFindbugsWarningshadoop-mapreduce-client-app.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5826/artifact/patchprocess/diffcheckstylehadoop-mapreduce-client-app.txt |
| hadoop-mapreduce-client-app test log | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5826/artifact/patchprocess/testrun_hadoop-mapreduce-client-app.txt |
| Test Results | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5826/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf908.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/5826/console |


This message was automatically generated., My bad. I misinterpreted Vinod's suggestion to catch only NMNotYetReadyException, don't ask me how. I somehow thought he was against retries. 

I fully agree with retrying on NMNotYetReadyException alone. , That makes sense.  The patch is also a lot simpler; it just adds a retry policy for {{NMNotYetReadyException}}, and a test., \\
\\
| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  17m 20s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 46s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 42s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   1m 30s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 33s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 47s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |   1m 58s | Tests passed in hadoop-yarn-common. |
| {color:green}+1{color} | yarn tests |   6m  5s | Tests passed in hadoop-yarn-server-nodemanager. |
| | |  49m 40s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12741131/YARN-3842.001.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 445b132 |
| hadoop-yarn-common test log | https://builds.apache.org/job/PreCommit-YARN-Build/8314/artifact/patchprocess/testrun_hadoop-yarn-common.txt |
| hadoop-yarn-server-nodemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8314/artifact/patchprocess/testrun_hadoop-yarn-server-nodemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8314/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf908.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8314/console |


This message was automatically generated., Thanks for the quick turnaround on this, Robert. 

One nit-pick on the test: would the following be more concise? 

{code}
        if (retryCount < 5) {
          retryCount++;
          if (isExpectingNMNotYetReadyException) {
            containerManager.setBlockNewContainerRequests(true);
          } else {
            throw new java.net.ConnectException("start container exception");
          }
        } else {
          containerManager.setBlockNewContainerRequests(false);
        }
        return super.startContainers(requests);
{code}, I had sort of just split {{startContainers}} into two sections (one for each part of the test), but this is a lot more concise.  I'll do that., The new patch make the changes Karthik suggested.  I also added a few comments and renamed {{isExpectingNMNotYetReadyException}} to {{shouldThrowNMNotYetReadyException}} for clarity., I think the latest patch is safe for 2.7.1,  +1, +1, pending Jenkins. 

Thanks for your review, [~jianhe]. I ll go ahead commit this if Jenkins is fine with it. , \\
\\
| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  17m 16s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 38s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 37s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   1m 28s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 47s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | yarn tests |   1m 56s | Tests passed in hadoop-yarn-common. |
| {color:green}+1{color} | yarn tests |   6m 27s | Tests passed in hadoop-yarn-server-nodemanager. |
| | |  49m 43s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12741154/YARN-3842.002.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 077250d |
| hadoop-yarn-common test log | https://builds.apache.org/job/PreCommit-YARN-Build/8317/artifact/patchprocess/testrun_hadoop-yarn-common.txt |
| hadoop-yarn-server-nodemanager test log | https://builds.apache.org/job/PreCommit-YARN-Build/8317/artifact/patchprocess/testrun_hadoop-yarn-server-nodemanager.txt |
| Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/8317/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/8317/console |


This message was automatically generated., Thanks everyone for your inputs on this, and Robert for your patch.

Just committed this to trunk, branch-2, and branch-2.7. , FAILURE: Integrated in Hadoop-trunk-Commit #8050 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8050/])
YARN-3842. NMProxy should retry on NMNotYetReadyException. (Robert Kanter via kasha) (kasha: rev 5ebf2817e58e1be8214dc1916a694a912075aa0a)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/ServerProxy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestNMProxy.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #967 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/967/])
YARN-3842. NMProxy should retry on NMNotYetReadyException. (Robert Kanter via kasha) (kasha: rev 5ebf2817e58e1be8214dc1916a694a912075aa0a)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/ServerProxy.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestNMProxy.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk-Java8 #237 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/237/])
YARN-3842. NMProxy should retry on NMNotYetReadyException. (Robert Kanter via kasha) (kasha: rev 5ebf2817e58e1be8214dc1916a694a912075aa0a)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/ServerProxy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestNMProxy.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2165 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2165/])
YARN-3842. NMProxy should retry on NMNotYetReadyException. (Robert Kanter via kasha) (kasha: rev 5ebf2817e58e1be8214dc1916a694a912075aa0a)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/ServerProxy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestNMProxy.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #226 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/226/])
YARN-3842. NMProxy should retry on NMNotYetReadyException. (Robert Kanter via kasha) (kasha: rev 5ebf2817e58e1be8214dc1916a694a912075aa0a)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestNMProxy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/ServerProxy.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #235 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/235/])
YARN-3842. NMProxy should retry on NMNotYetReadyException. (Robert Kanter via kasha) (kasha: rev 5ebf2817e58e1be8214dc1916a694a912075aa0a)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/ServerProxy.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestNMProxy.java
* hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2183 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2183/])
YARN-3842. NMProxy should retry on NMNotYetReadyException. (Robert Kanter via kasha) (kasha: rev 5ebf2817e58e1be8214dc1916a694a912075aa0a)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestNMProxy.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/ServerProxy.java
, Running into this in a couple of places, we should get this into 2.6.3., I committed this to branch-2.6.]