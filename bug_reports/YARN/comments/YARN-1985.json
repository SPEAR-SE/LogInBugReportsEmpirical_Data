[Just adding more info. Foe my 4 container app I get a single
{code}
- Received completed contaners callback: [ContainerStatus: [ContainerId: container_1398429077682_0005_01_000003, State: COMPLETE, Diagnostics: Container [pid=11152,containerID=container_1398429077682_0005_01_000003] is running beyond virtual memory limits. Current usage: 39.6 MB of 256 MB physical memory used; 1.8 GB of 537.6 MB virtual memory used. Killing container.
. . .
{code}
and then 4
{code}
State: COMPLETE,. . .
State: COMPLETE,. . .
State: COMPLETE,. . .
State: COMPLETE,. . .
{code}, Actually, the 4 COMPLETE reports are log duplication, so it is actually two. The other two containers didn't even start. Which is fine, but the real issue is that while its clearly an ERROR condition it reports is as simple COMPLETE., Do you have the relevant portions of the RM log for these 4 containers showing it has marked them completed?  If these all occurred on the same node, the relevant NM log would be great as well., Actually as I stated in my last comment its 2 containers. So out of four 2 were started and killed immediately and 2 were not started at all. So while I have to fix my problem of properly counting how many containers were started vs finished/running, the real issue is that such a major error condition is reported essentially as success. Basically if I didn't have a bug on my end which made my AM hang, i would probably end up seeing SUCCEEDED in the RM console, I am fixing it now and will follow up if I do see SUCCEEDED. But in any event its confusing when it simply reports COMPLETED while describing a major error., There are only three states for a container: NEW, RUNNING, or COMPLETED.  Note that COMPLETED does not imply success rather that the container is no longer running.  In order to discern success or failure from a completed container one must examine the exit code of the container (i.e.: the ContainerStatus#getExitStatus method).

Are both containers running over their memory limits or is only one running over and somehow both are being killed?  That's where the RM/NM logs would help., I'd agree with you by the reported status is 0.
{code}
ExitStatus: 0, ]
{code}, Also you are saying only 3 states. What about all those LOCALIZING, LOCALZED, KILLING, ACQUIRED  etc...
Anyway, here is the NM log fragment:
{code}
2014-04-25 12:47:45,230 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Removed ProcessTree with root 12510
2014-04-25 12:47:45,230 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1398429077682_0006_02_000003 transitioned from RUNNING to KILLING
2014-04-25 12:47:45,230 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1398429077682_0006_02_000003
2014-04-25 12:47:45,630 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1398429077682_0006_02_000005 by user oleg
2014-04-25 12:47:45,631 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=oleg	IP=192.168.19.10	OPERATION=Start Container Request	TARGET=ContainerManageImpl	RESULT=SUCCESS	APPID=application_1398429077682_0006	CONTAINERID=container_1398429077682_0006_02_000005
2014-04-25 12:47:45,631 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Adding container_1398429077682_0006_02_000005 to application application_1398429077682_0006
2014-04-25 12:47:45,632 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1398429077682_0006_02_000005 transitioned from NEW to LOCALIZING
2014-04-25 12:47:45,632 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1398429077682_0006
2014-04-25 12:47:45,634 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1398429077682_0006_02_000005 transitioned from LOCALIZING to LOCALIZED
2014-04-25 12:47:45,660 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1398429077682_0006_02_000005 transitioned from LOCALIZED to RUNNING
2014-04-25 12:47:45,677 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [nice, -n, 0, bash, /tmp/hadoop-oleg/nm-local-dir/usercache/oleg/appcache/application_1398429077682_0006/container_1398429077682_0006_02_000005/default_container_executor.sh]
2014-04-25 12:47:48,230 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1398429077682_0006_02_000005
2014-04-25 12:47:48,248 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 12598 for container-id container_1398429077682_0006_02_000005: 39.7 MB of 256 MB physical memory used; 1.8 GB of 537.6 MB virtual memory used
2014-04-25 12:47:48,248 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Process tree for container: container_1398429077682_0006_02_000005 running over twice the configured limit. Limit=563714432, current usage = 1985445888
2014-04-25 12:47:48,249 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Container [pid=12598,containerID=container_1398429077682_0006_02_000005] is running beyond virtual memory limits. Current usage: 39.7 MB of 256 MB physical memory used; 1.8 GB of 537.6 MB virtual memory used. Killing container.
Dump of the process-tree for container_1398429077682_0006_02_000005 :
	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
. . .
{code}, My theory is confirmed. After fixing my bug application finished with SUCCEEDED status which is obviously wrong.

What makes it even a bigger problem IMHO is that it seems like YARN decided not to even attempt to start the other two containers which creates an interesting dilemma. 
How do you monitor overall application completion when:
4 Containers are allocated
2 Started and killed
2 Didn't start.
Sure I can use AtomicInteger and increment/decrement it. But if I don't have any guarantee around container start attempts I may be exiting too soon. For example in my case such counter would go from 0 to 2 and then back to 0 signifying completion and as I am exiting YARN may decide to start another container, Actually a bit of a good news. The other two containers didn't start because one of my nodes had its date/time messed up resulting 
{code}
org.apache.hadoop.yarn.exceptions.YarnException: Unauthorized request to start container. 
This token is expired. current time is 1398449721411 found 1398448925681
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
. . . 
{code} 
So handling 'onStartContainerError' event would do.
So this makes it much less of an issue and I can work around it (actually already did), but the fact that _ExitStatus_ for the containers that did start was 0 is still a problem.
Downgrading it to minor, The exit status should be whatever exit status came from the process when it exited.  When a container is killed the NM first sends a SIGTERM and then a short time later (250 msec IIRC) it sends SIGKILL.  A process that exits with a status code of 0 despite receiving SIGTERM could explain the behavior.  It could also happen if the container exited on its own after the NM logged that it was going to kill it but before it actually tried to kill it.

Looking at the DefaultContainerExecutor code it certainly appears that the process being killed must have returned an exit code of zero unless you are seeing logs such as "Exit code from container container_1398429077682_0006_02_000005 is : " in the logs.  I'm not sure exactly what's being run in the container, but checking if that will return an exit code of 0 despite being killed by SIGTERM seems like the next best place to look.]