[I believe this is an artifact of the NM appearing to be two separate instances of nodemanagers.  Note that the NM port changed between the two instances.  It originally was rb0101.local:43892 but became rb0101.local:42627 after it was restarted.  That explains why the node shows up twice when listing all nodes.  The RM did not understand that the newly joining NM at port 42627 was supposed to be the same one that was at port 43892.  The RM does not preclude multiple NMs running at the same node, and indeed that's how the mini clusters used for unit tests can run multiple NMs with only one host.

It is surprising that the shutdown NM instance does not appear when explicitly asking for nodes in the shutdown state.  I suspect somewhere in the RM's bookkeeping it is dropping the port distinction and the RUNNING instance ends up superceding the SHUTDOWN one for that query.

Simplest workaround for this is to use a fixed port for the NM.  Then the RM will understand that the new node joining is the same node that left previously.  That also has the benefit of precluding an accidental double-startup of an NM on a node which is not going to go well if not configured intentionally for that scenario.  Both NMs will think they have control of the node's resources and end up using far more resources on the node than intended.
, This behavior seems to be caused by the lack of property *yarn.nodemanager.address* inside yarn-site.xml files of NodeManagers.


When explicitly defining that, this behavior does-not occur:

 <property>
   <name>yarn.nodemanager.address</name>
   <value>rb0101.local:9999</value>
</property>, This could be caused by buildNodeId( ), as the Port # it generates appears to be random when yarn.nodemanager.address is not defined in a NodeManager's yarn-site.xml., Closing as invalid]