[For completed applications before starting in secured mode, clientTokenMaterKey is null. After starting in secured mode, recovery of apps fails since clientTokenMasterKey is null. During recovering application, rm should have intellegence to decide whether recovering applicaiton has run in secured mode or non secured mode. This is possible by checking cilentTokenMasterKey for null. , Uploading patch without test written.

Thinking of how to write test, should complete flow need to consider or only RMAppAttempt.recoveryApplication() can be called.?!!, Please, can this be considered a "Blocker" as there seems no way to recover from this and still transition to secured mode?, I do think this is a critical issue, but don't believe it is essentially a blocker for 2.4.1.

In terms of the fix, I believe the fix shouldn't be security-specific. We should probably add a recovery-specific config to say it is okay to continue with starting the RM even if we fail to recover some applications. , Thank you [~kasha] for reviewing patch. I update the patch for continue on recovery failure for Finished applications.

Correct me if am wrong, continuing on recovery failure for running application,I think it may cause application to hang. So need to consider final state of applicaion., [~rohithsharma] - I don't see an updated patch. Can you please check?

Even for running applications, I think it is good to not kill the RM if we can't recover the application. We should just kill the application and set an appropriate diagnostic message. , Apologies for not attaching patch :-(

This patch handles only applications recovery failure.
I have updated patch for above changes, Please review.

, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12643680/YARN-2010.1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/3709//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3709//console

This message is automatically generated., Hi [~rohithsharma], 
I'm wondering is it better to make RM resilient to such app-level errors as ONLY behavior? I noticed there's an option for user in your patch., [~rohithsharma] - do you mind me collaborating with you on this patch? I was just going through some relevant code and thought it could use some cleanup., bq. I'm wondering is it better to make RM resilient to such app-level errors as ONLY behavior? I noticed there's an option for user in your patch.
There are 2 level recovery i.e RMdelegationTokenSecretManager and Applications. After going through recovery flow, I concluded(correct me if am wrong) that RMdelegationTokenSecretManager  recovery do not fail. So I handled only application recovery failure scenario.
Do you think this behaviour should be default? and handle for all the recovery?, bq. do you mind me collaborating with you on this patch?
[~kasha]Yes please, thank you :-), Thank you for taking this JIRA, [~rohithsharma], [~kasha]. About the patch, should we clean states in ZKRMStateStore and  treat the recover-failed app as failed app by handling RMAppFailedAttemptEvent or something in exception handling as Karthik mentioned?

{code}
     for (ApplicationState appState : appStates.values()) {
-      recoverApplication(appState, state);
+      try {
+        recoverApplication(appState, state);
+      } catch (Exception e) {
+        if (!isContinueOnRecoveryFailure) {
+          throw e;
+        }
+      }
     }
{code}, New patch with following changes - 
# Noticed that RMAppManager#recoverApplication wasn't failing running applications in all the code-paths corresponding to failed recovery. Fixed that and cleaned it up futher.
# Changed the config name to be shorter. 
# Added comments to make sure we document why we are doing what we are doing., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12643991/yarn-2010-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/3722//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3722//console

This message is automatically generated., Hi folks, thanks for working on this. I agree that we should not fail RM if app failed to recover.YARN-2019 seems taking care of this. But in this particular case, IIUC, the problem is that RM was running in non-secure mode and so clientTokenMaterKey is null. After RM restarts, RM starts running in  secure mode and expects clientTokenMaterKey non-null and then fails.

In non-workpreserving restart,  since the old attempt will be essentially killed on RM restart, new attempt will be automatically started and it will have the  new clientTokenMaterKey key generated. So we may not need to fail this app.
In work-preserving restart, because the old AM running before RM restart(non-secure) was not given the clientToAMMasterKey, even though RM is now running in secure mode, client without the clientToken should also be able to talk with the AM? [~vinodkv] is this the case?, bq. In non-workpreserving restart, since the old attempt will be essentially killed on RM restart, new attempt will be automatically started and it will have the new clientTokenMaterKey key generated. So we may not need to fail this app.
The stack trace corresponds to non-work-preserving restart. I am not sure I understand the concern.

This JIRA addresses all cases where an app recovery fails. Examples include token issues, queue ACL changes that disallow the user from submitting to the queue etc. In any of these cases, users should have the option of continuing with starting the RM., bq. The stack trace corresponds to non-work-preserving restart. I am not sure I understand the concern.
What I meant is, in this scenario, it shouldn't matter whether the old attempt has the master key or not, since the old attempt will be anyways killed by NM on RM restart. The newly started attempt will have the proper master key generated. If we just check whether the key is null and move on, the next attempt should be able to succeed. So we don't need to explicitly fail the app ?, I see. Thanks for the input. Let me check if that is indeed the case, and attempt recovering the app even if the key is null

Regardless, do we agree that we still need to address the case where the app recovery fails for potentially other reasons?, I agree that failing to recover an app shouldn’t fail the RM.  I think for cases where the failure will be simply resolved by launching a new attempt like this, we should not  fail the app. We can fail the app for cases where starting a new attempt can’t resolve the issue like failing to renew DT on recovery. , New patch that gets rid of the config and addresses the issue where the masterKey is null. , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12647268/yarn-2010-3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/3854//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/3854//console

This message is automatically generated., bq. For completed applications before starting in secured mode, clientTokenMaterKey is null. After starting in secured mode, recovery of apps fails since clientTokenMasterKey is null. During recovering application, rm should have intellegence to decide whether recovering applicaiton has run in secured mode or non secured mode. This is possible by checking cilentTokenMasterKey for null.
bq. Please, can this be considered a "Blocker" as there seems no way to recover from this and still transition to secured mode?
Apologies for coming in real late. When is this exception/crash manifesting?

It seemed like this is when you try to upgrade from a non-secure cluster to a secure cluster. Is that so? That is a completely unsupportable use-case. There are so many other things that will be broken when you do such an upgrade with existing applications - think tokens needed, localized files etc.

Just trying to make sure we are not fixing 'issues' to support unsupportable use-cases., Agree with Vinod that non-secure cluster to secure cluster is not currently supported and bound to have tons of issues.  I've come across other "bugs" that have turned out to stem from this.  If this is the only situation where we could conceivably face this issue, I'm somewhat dubious about whether it needs to be fixed.  On the other hand, in general, being defensive about allowing a transition to active even when an app recovery fails makes sense to me., Let me clarify a couple of things. It is true that the first time we encountered this was during an upgrade from non-secure to secure cluster. However, as I mentioned earlier in the JIRA, it is possible to run into this in other situations. 

Even in the case of upgrading from non-secure to secure cluster, I totally understand we can't support recovering running/completed applications. However, one shouldn't have to explicitly nuke the ZK store (which by the way is involved due to the ACLs-magic and lacks an rmadmin command) to be able to start the RM. , SUCCESS: Integrated in Hadoop-trunk-Commit #5632 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5632/])
YARN-2010. Document yarn.resourcemanager.zk-auth and its scope. (Robert Kanter via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1598636)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/resources/yarn-default.xml
, FAILURE: Integrated in Hadoop-Yarn-trunk #569 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/569/])
YARN-2010. Document yarn.resourcemanager.zk-auth and its scope. (Robert Kanter via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1598636)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/resources/yarn-default.xml
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1760 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1760/])
YARN-2010. Document yarn.resourcemanager.zk-auth and its scope. (Robert Kanter via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1598636)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/resources/yarn-default.xml
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1787 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1787/])
YARN-2010. Document yarn.resourcemanager.zk-auth and its scope. (Robert Kanter via kasha) (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1598636)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
* /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/resources/yarn-default.xml
, Sorry, the commit messages are for the wrong JIRA. Will fix them up., bq. It is true that the first time we encountered this was during an upgrade from non-secure to secure cluster.
My point is that this is a non-supported use-case. Let's make that explicit by throwing appropriate exception with the right message * (1)

bq. However, as I mentioned earlier in the JIRA, it is possible to run into this in other situations.
Let's figure out what these situations are and make sure they are handled correctly * (2). Skipping apps in all cases is likely not the right solution.

bq. Even in the case of upgrading from non-secure to secure cluster, I totally understand we can't support recovering running/completed applications. However, one shouldn't have to explicitly nuke the ZK store (which by the way is involved due to the ACLs-magic and lacks an rmadmin command) to be able to start the RM.
On the other hand, couple with [(1) above, that is exactly what I'd expect. If we skip applications automatically in all cases, that may be a worse thing to happen. - suddenly users will see that they are losing apps for a reason that is not so obvious to them. The risk of crashing the RM is that there is a need manual intervention with a longer downtime. But with (2) above, that risk will be mitigated a lot. Even if we decide to skip them, the outcome is the same - losing the apps.. But it rather be a conscious decision by the admins.

Crux of my argument is, let's not do a blanket 
{code}
try {
  .. 
} catch (Exception) {
 continue;
}
{code}
Instead do
{code}
try {
  .. 
} catch (Exception type1) {
 // handle correctly
}  catch (Exception type2) {
 // handle correctly
} ...
.....
} catch (Exception catchAll) {
 // Decide to skip the app or crash the RM.
}
{code}
, SUCCESS: Integrated in Hadoop-trunk-Commit #5643 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5643/])
YARN-1877. Updated CHANGES.txt to fix the JIRA number. It was previously committed as YARN-2010. (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1599348)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #572 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/572/])
YARN-1877. Updated CHANGES.txt to fix the JIRA number. It was previously committed as YARN-2010. (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1599348)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1763 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1763/])
YARN-1877. Updated CHANGES.txt to fix the JIRA number. It was previously committed as YARN-2010. (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1599348)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1790 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1790/])
YARN-1877. Updated CHANGES.txt to fix the JIRA number. It was previously committed as YARN-2010. (kasha: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1599348)
* /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt
, We ran into this again:
{noformat}
2014-10-08 15:51:04,968 WARN org.apache.hadoop.ha.ActiveStandbyElector: Exception handling the winning of election
org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active
        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:118)
        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:804)
        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:415)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
Caused by: org.apache.hadoop.ha.ServiceFailedException: Error when transitioning to Active mode
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:291)
        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:116)
        ... 4 more
Caused by: org.apache.hadoop.service.ServiceStateException: java.net.ConnectException: Call From bcsec-1.ent.cloudera.com/10.20.193.164 to bcsec-1.ent.cloudera.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wik
i.apache.org/hadoop/ConnectionRefused
        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:204)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:928)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:968)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:965)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:965)
        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:282)
        ... 5 more
Caused by: java.net.ConnectException: Call From bcsec-1.ent.cloudera.com/10.20.193.164 to bcsec-1.ent.cloudera.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
        at org.apache.hadoop.ipc.Client.call(Client.java:1415)
        at org.apache.hadoop.ipc.Client.call(Client.java:1364)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy101.renewDelegationToken(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewDelegationToken(ClientNamenodeProtocolTranslatorPB.java:916)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy102.renewDelegationToken(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient$Renewer.renew(DFSClient.java:1089)
        at org.apache.hadoop.security.token.Token.renew(Token.java:377)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:473)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:470)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:469)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:391)
        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.addApplicationSync(DelegationTokenRenewer.java:353)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:326)
        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:425)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1124)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:498)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        ... 13 more
Caused by: java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
{noformat}
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12647268/yarn-2010-3.patch
  against trunk revision 2a51494.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5339//console

This message is automatically generated., Changing assignee to [~kasha] since he has done all the work. Thanks  [~kasha]., Re-uploading the last patch, that has a single {{catch(Exception)}}.

[~vinodkv] - would you still prefer having multiple catch-blocks, one for each exception. IMO, catching {{ConnectException}} doesn't seem very readable; we could add a comment on why we are adding that catch, but we might not be able to enumerate all possible cases. That said, I am okay with catching ConnectException and Exception separately. Please advise. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12675843/yarn-2010-3.patch
  against trunk revision d5084b9.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5459//console

This message is automatically generated., Sorry missed this. Lost context, so please help clarify. This time, we got a ConnectException to Zookeeper due to which we are skipping apps? That doesn't sound right either., In this particular case, we are unable to renew HDFS delegation token due to ConnectException to HDFS. We are not yet clear why this happens. Even if this a transient HDFS issue, both RMs fail to transition to active and the individual RMActiveServices instances transition to STOPPED state. Any subsequent attempts to transition the RM to active fail because RMActiveServices is not INITED, as in the Standby case.

I spent some more time thinking about this, and think there might be merit to catch exceptions separately. ConnectException hopefully is due to a transient issue, I don't think we can do much in case of a permanent issue. When we run into this, we should probably cleanly transition to standby, so subsequent attempts to transition to active may succeed. , We recently ran into a case where an application tried to recover with an expired token and the InvalidToken exception thrown by the delegation token secret manager for this application prevented the RM from coming up., I should have an updated patch with tests later today. Would be nice to fix this for 2.6. , This JIRA has been an open for a while and went through several discussions. I ll try to consolidate everything here so we can iterate on this quickly.

Sometimes, the RM fails to recover an application. It could be because of turning security on, token expiry, or issues connecting to HDFS etc. The causes could be classified into (1) transient, (2) specific to one application, and (3) permanent and apply to multiple (all) applications. Today, the RM fails to transition to Active and ends up in STOPPED state and can never be transitioned to Active again. 

Vinod suggested we handle these cases (exceptions) separately, so we can do the right thing for each exception. The latest patch (v4) is along these lines - it catches a potentially transient issue (ConnectException) and transitions the RM to Standby. If the issue were to persist (case - 3), the RM would eventually run out of number of failovers and crash. For application-specific issues (as of now, all other exceptions), we just skip recovering that app.

In addition to this, the patch cleans up RMAppManager#recoverApplication and also adds a null-check in RMAppAttempt#recoverAppAttemptCredentials per Jian's suggestion. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676476/issue-stacktrace.rtf
  against trunk revision 3b12fd6.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5505//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676471/yarn-2010-4.patch
  against trunk revision 3b12fd6.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1267 javac compiler warnings (more than the trunk's current 1266 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5504//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/5504//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Javac warnings: https://builds.apache.org/job/PreCommit-YARN-Build/5504//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5504//console

This message is automatically generated., Updated patch to fix test failure, findbugs warning, and suppress javac warnings (we call getEventHandler().handle() at several other places, I don't quite get why it leads to a javac warning only here)., [~jianhe] - can you please verify the changes to TestWorkPreservingRMRestart are reasonable. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676770/yarn-2010-5.patch
  against trunk revision 828429d.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5531//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/5531//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5531//console

This message is automatically generated., Updated patch to fix the findbugs issue, it was due to an empty if-block that got left around by mistake. , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676796/yarn-2010-6.patch
  against trunk revision db45f04.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5533//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5533//console

This message is automatically generated., Marking it a blocker as the RM crashes when trying to recover a regular job with an expired token. , [~vinodkv], [~jlowe], [~jianhe] - will any of you be able to review this? Thanks. , sorry, was caught up with something. I'll review today. , bq. Any subsequent attempts to transition the RM to active fail because RMActiveServices is not INITED, as in the Standby case
 I think YARN-2588 fixed this.  are you running into this error with the patch ?
-  How about moving “addApplicationSync” into RMAppRecoveredTransition. We can catch the exception inside the transition and return failed state directly ?
{code}
      // If security is enabled and the application is NOT in a final state,
      // parse the credentials and renew delegation token
      if (UserGroupInformation.isSecurityEnabled() &&
          !isApplicationInFinalState(appState.getState())) {
        Credentials credentials = parseCredentials(appContext);
        // synchronously renew delegation token on recovery.
        rmContext.getDelegationTokenRenewer().addApplicationSync(appId,
            credentials, appContext.getCancelTokensWhenComplete());
      }

      // Actual recovery of the application
      application.handle(new RMAppEvent(appId, RMAppEventType.RECOVER));
    } catch (Exception e) {
      LOG.error("Failed to recover application + " + appId, e);
      // Fail the application if it is a running application.
      if (!isApplicationInFinalState(appState.getState())) {
        rmContext.getDispatcher().getEventHandler().handle(
            new RMAppRejectedEvent(appId, e.getMessage()));
      }
      throw e;
{code}
- changes in TestWorkPreservingRMRestart
It was purposely done to force RM to fail if the queue is missing for the app and indicate admin to config the queue properly., Thanks for the review, Jian. Sorry for the delay in addressing the comments. 

Here is patch that moves the credential parsing to RMAppRecoveredTransition itself, it does make the code much cleaner. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12678528/yarn-2010-7.patch
  against trunk revision f1a149e.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions
                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5665//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5665//console

This message is automatically generated., Looks like TestRMAppTransitions tests are not initializing any tokens even though they are parametrized to run with security on. I ll work on fixing the tests, but it would be great to hear if we are on the right track. , Hi Karithik, thanks for updating.  a couple more things:
- Inside the catch, we may just return FAILED?
{code}
        } catch (Exception e) {
          LOG.warn("Unable to parse and renew delegation tokens.", e);
          throw new YarnRuntimeException(e);
        }
{code}
- I don’t think we can get ConnectException here, could you explain under what scenario, we get ConnectException
{code}
      } catch (ConnectException ce) {
        // Unable to connect to HDFS or ZK. Assuming this is a transient
        // issue, we should gracefully shutdown or transition to standby. If
        // the issue is permanent, there is not much YARN can do.
        rmContext.getDispatcher().getEventHandler().handle(
            new RMFatalEvent(RMFatalEventType.CONNECTION_FAILED, ce));
{code}, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12678687/yarn-2010-8.patch
  against trunk revision ed63b11.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5676//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5676//console

This message is automatically generated., The latest patch is a step back, closer to v6 patch. Fixing the test failures on v7 of the patch was more involved than I thought and was taking longer. So, in the interest of time, I would like to work on moving credential parsing to RMAppRecoveredTransition as part of a follow-up JIRA. 

bq. Inside the catch, we may just return FAILED?
This doesn't apply anymore. Will take a closer look in the follow-up JIRA.

bq. I don’t think we can get ConnectException here, could you explain under what scenario, we get ConnectException
The comments elaborate on potential reasons for ConnectException. The stack trace corresponding to one instance is here - https://issues.apache.org/jira/browse/YARN-2010?focusedCommentId=14164516&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14164516

, bq. The comments elaborate on potential reasons for ConnectException. The stack trace corresponding to one instance is here -
The stack trace should be saying renewHdfsToken on submission is failing.(Supposedly, DFS client should handle retry in case of ConnectException, why not?). IIUC, this {{RMAppManager#recoverApplication}} code-path is not doing any ZK operation. If we can handle the exception coming out of {{getDelegationTokenRenewer().addApplicationSync}} properly. For the purpose of this jira, we don't need the following change, since the problem of {{Any subsequent attempts to transition the RM to active fail because RMActiveServices is not INITED, as in the Standby case}} is already fixed in YARN-2588
{code}
        // Unable to connect to HDFS or ZK. Assuming this is a transient
        // issue, we should gracefully shutdown or transition to standby. If
        // the issue is permanent, there is not much YARN can do.
        rmContext.getDispatcher().getEventHandler().handle(
            new RMFatalEvent(RMFatalEventType.CONNECTION_FAILED, ce));
{code}
Also, the patch changed the behavior of YARN-2308. YARN-2308 forces RM to exist in case the queue is missing and indicate admin to config the queue properly. The patch changed the behavior to  move all apps belonging to the queue to FAILED state if queue is missing. we should not change this?, Updated patch doesn't handle ConnectException, and preserves the behavior introduced in YARN-2308 through a new QueueNotFoundException. , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12678760/yarn-2010-9.patch
  against trunk revision 5c0381c.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5682//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5682//console

This message is automatically generated., Give RM is synchronously starting and renewing the token synchronously, I don't quite understand why we have to catch the queue exception and stop RM asynchronously via events. I think it's fine to just let exception throw out and let RM stop.

After a closer look, RUNNING app on recovery will move to ACCEPTED state, ACCEPTED state is actually not handling RMAppRejectedEvent. I think doing the following will cause UnhandledEventException in RMApp state machine.
{code}
      application.handle(new RMAppEvent(appId, RMAppEventType.RECOVER));
    } catch (Exception e) {
      LOG.warn("Failed to recover application.", e);
      if (!isApplicationInFinalState(appState.getState())) {
        this.rmContext.getDispatcher().getEventHandler()
            .handle(new RMAppRejectedEvent(appId, e.getMessage()));
      }
{code}
We may still need to move {{addApplicationSync}} into RMAppRecoveredTransition. Please let me know your thoughts. thanks., bq. Give RM is synchronously starting and renewing the token synchronously, I don't quite understand why we have to catch the queue exception and stop RM asynchronously via events. I think it's fine to just let exception throw out and let RM stop.
This is not always on startup. Transitions to Active also go through this. In HA cases, we would want to transition to standby, no? 

bq. After a closer look, RUNNING app on recovery will move to ACCEPTED state, ACCEPTED state is actually not handling RMAppRejectedEvent.
Good point. What do you think of handling rejection in ACCEPTED as well? 

bq. We may still need to move addApplicationSync into RMAppRecoveredTransition.
I am not sure if this is necessarily related to the rest of the patch. It is definitely a code improvement. , Updated patch to address review comments. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679057/yarn-2010-10.patch
  against trunk revision 734eeb4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesFairScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.TestRMContainerImpl
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens
                  org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager
                  org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart
                  org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes
                  org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication
                  org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart
                  org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService
                  org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication
                  org.apache.hadoop.yarn.server.resourcemanager.TestAppManager

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5706//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/5706//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5706//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679057/yarn-2010-10.patch
  against trunk revision 734eeb4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService

                                      The following test timeouts occurred in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

org.apache.hadoop.yarn.server.resourcemanager.TestRMProxyUsersConf

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5707//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-YARN-Build/5707//artifact/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-resourcemanager.html
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5707//console

This message is automatically generated., The tests pass locally, the findbugs warning is to do with catching Exception instead of just IOException and InterruptedException in RMAppRecoveredTransition, [~kasha], I reviewed your patch.  and just made some edits on top of your patch, could you please take a look ? thx , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679113/yarn-2010-11.patch
  against trunk revision c5a46d4.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5717//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5717//console

This message is automatically generated., Jian - thanks for taking the time to look at this closely. The patch looks mostly good. However, this patch only catches DT renewal issues during app recovery. Any other errors that could be encountered in the remaining code paths are not handled gracefully. For instance, any errors during app.recoverAppAttempts can affect the health of the RM.

, I moved the app.recover part to the RMAppRecoveredTransition too., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679286/YARN-2010.12.patch
  against trunk revision 1eed102.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5723//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5723//console

This message is automatically generated., Fixe the test failures, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679312/YARN-2010.12.patch
  against trunk revision 1eed102.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5725//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5725//console

This message is automatically generated., Patch yarn-2010-13.patch adds a simple test to verify handling of app recovery with missing information in the store. , We may need to save the final state as FAILED too, updated the patch to do that. , New patch,   just added a timeout on the unit test, same patch, just cleaned up a little, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679381/YARN-2010.14.patch
  against trunk revision 99d7103.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

                  org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart

                                      The following test timeouts occurred in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager:

org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerQueueACLs

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5728//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5728//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679385/YARN-2010.16.patch
  against trunk revision 5bd3a56.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-YARN-Build/5730//testReport/
Console output: https://builds.apache.org/job/PreCommit-YARN-Build/5730//console

This message is automatically generated., The latest patch looks good to me. Thanks Jian for your patience through this.

+1. Checking this in., SUCCESS: Integrated in Hadoop-trunk-Commit #6445 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6445/])
YARN-2010. Handle app-recovery failures gracefully. (Jian He and Karthik Kambatla via kasha) (kasha: rev b2cd2698028118b6384904732dbf94942f644732)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueNotFoundException.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRecoverEvent.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java
, Glad to see this taken care of. Thanks Jian for reviewing and work on this. Thanks bc, Jason, Manoj, Rohith, Tsuyoshi and Vinod for chiming in. 

Just committed to trunk, branch-2 and branch-2.6., SUCCESS: Integrated in Hadoop-Yarn-trunk #734 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/734/])
YARN-2010. Handle app-recovery failures gracefully. (Jian He and Karthik Kambatla via kasha) (kasha: rev b2cd2698028118b6384904732dbf94942f644732)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueNotFoundException.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRecoverEvent.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1923 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1923/])
YARN-2010. Handle app-recovery failures gracefully. (Jian He and Karthik Kambatla via kasha) (kasha: rev b2cd2698028118b6384904732dbf94942f644732)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRecoverEvent.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueNotFoundException.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1948 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1948/])
YARN-2010. Handle app-recovery failures gracefully. (Jian He and Karthik Kambatla via kasha) (kasha: rev b2cd2698028118b6384904732dbf94942f644732)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java
* hadoop-yarn-project/CHANGES.txt
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRecoverEvent.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueNotFoundException.java
]