[hi [~xinxianyin]
Thanks for reporting this issue.
Can you attach the logs of this issue ? , bq. on the same node

I don't see what that has to do with:

bq. After analysis, we found the root cause is renewing HDFS tokens in the recovering process. The HDFS client created by the renewer would firstly try to connect to the original namenode, the result of which is time-out after 10~20s, and then the client tries to connect to the new namenode. The entire recovery cost 15*#apps seconds according our test.

What happens if the two NNs aren't on the same node as the RM?  Does the problem still exist?

(It should also be noted that running the NN and RM on the same node for other than trivial deployments has never been a recommended configuration.), Attach recovery log for 1 app. In the log, there's a gap(15s) between two log items printed at 02:39:48,998 and 02:40:04,025. Note that TokenRenewer for HIVE tokens is not found, but this has nothing to do with the gap. It's HDFS token renewing that caused the gap, the reason is just as the {{Description}} states., Sorry [~aw], i didn't make it clearly. "On the same node" means the original active RM and NN were running on the same node(node1). The standby RM and NN were running on other nodes. After node1 died, the HDFS token renewer would firstly try to connect to the NN on node1, but NN on node1 was not reachable. After the connection time-out, the HDFS token renewer then tries to connect to the original standy NN.
If the active NN and RM run on different nodes, the problem doesn't exist.
, Maybe we can fix this problem in the following way: the latter apps should learn the lesson given by the former apps, i.e., if one app find the original NN could not connect and then it connect to the new NN successfully, the latter apps should be aware of this to avoid repeating the failure. The token renewer creates a HDFS client when it tries to renew a HDFS token for an app, maybe the following apps could reuse the client?, bq.  "On the same node" means the original active RM and NN were running on the same node(node1)

OK, so yes, this still sounds like an irrelevant detail.  It appears the real problem here is that if the NN and the RM both go down at the same time (+regardless of location+), the RM doesn't use the prior knowledge of the NN being down to stream renewal tokens after the new NN is discovered., Yes, you're right [~aw]. , OK, just making sure there wasn't some weird thing going on where the problem really did exist because they happened to be on the same node.  That'd be very very bad. :D, This has been resolved by YARN-4041, so close it.]