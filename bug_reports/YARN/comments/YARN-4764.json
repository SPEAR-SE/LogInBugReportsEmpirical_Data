[Is this in trunk. Pls share more information with config details if possible. , Sorry. My bad. You are mentioning about suppressing exception to the user console end and now it's too verbose. Got it. :-) , Hi [~bibinchundatt]
I am trying to under stand the patch. Pls correct me if I am wrong.

Here user submitted an app to a non-existent queue. And this patch is trying to look for acl check for the user in a queue. So an AccessControlException will be thrown in this case. But for non-existent queue, do we need this exception.? I think it can be YarnException. , [~sunilg]

The problem over here is below

Application submission fails with NPE when Queue is not available in capacity-scheduler xml

*Analysis*

For checking permission of user for queue we are getting  CSQueue object for queue from submission context
As  explained in issue the CSQueue  for {{queue3}} will be {{null}} since its not configured in {{capacity-scheduler.xml}} and {{(CapacityScheduler) scheduler)           .getQueue(submissionContext.getQueue()).getPrivilegedEntity()}} will cause NPE. 

CSQueue csqueue = ((CapacityScheduler) scheduler).getQueue(queueName) so null is done over here too.

, [~sunilg]
For existing acl part of code only formatting is done. , Yes [~bibinchundatt]. Thanks for the analysis.

{code}
    if (!isRecovery && YarnConfiguration.isAclEnabled(conf)
        && scheduler instanceof CapacityScheduler &&
        !authorizer.checkPermission(new AccessRequest(
            ((CapacityScheduler) scheduler)
                .getQueue(submissionContext.getQueue()).getPrivilegedEntity(),
            userUgi, SchedulerUtils.toAccessType(QueueACL.SUBMIT_APPLICATIONS),
            submissionContext.getApplicationId().toString(),
            submissionContext.getApplicationName()))
{code}

When ACLs are enabled in cluster, as you mentioned an NPE will hit in above code since queue is not present. This exception is now thrown out. I think this handling is not very correct for handling non-existent queue in ACL scenario. 

Meantime in this patch, you are trying to handle this case explicitly and responding with Exception. There are 2 cases:
1. In case queue-mappings, we can submit w/o queue. So I think we might break some feature here now? Thoughts?
2. Do we need to send this as RPC remote exception from here?

Pls correct me if I am wrong.
, cc/ [~jianhe], | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 15s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red} 0m 0s {color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 0s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 26s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 28s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 18s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 36s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 8s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 21s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 26s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 25s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 25s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 26s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 16s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 32s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 12s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 15s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 19s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 23s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 71m 22s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.8.0_74. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 71m 55s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 18s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 160m 3s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_74 Failed junit tests | hadoop.yarn.server.resourcemanager.TestClientRMTokens |
|   | hadoop.yarn.server.resourcemanager.TestAMAuthorization |
| JDK v1.7.0_95 Failed junit tests | hadoop.yarn.server.resourcemanager.TestClientRMTokens |
|   | hadoop.yarn.server.resourcemanager.TestAMAuthorization |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ca8df7 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12791470/0001-YARN-4764.patch |
| JIRA Issue | YARN-4764 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 7a2fe0193530 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / cbd3132 |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_74 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/10709/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_74.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/10709/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-YARN-Build/10709/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_74.txt https://builds.apache.org/job/PreCommit-YARN-Build/10709/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/10709/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/10709/console |
| Powered by | Apache Yetus 0.3.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, {quote}
1. In case queue-mappings, we can submit w/o queue. So I think we might break some feature here now? Thoughts?
{quote}
scheduler mapping is configured CSQueue  shouldn't be null. Other scenario will wait for inputs..


{quote}
2. Do we need to send this as RPC remote exception from here?
{quote}
Should be {{YarnException}} as earlier also used to be {{YarnException}}


, {{CapacityScheduler#resolveReservationQueueName}} handles reservation cases. {{reservationID}} is considered there. So there can be cases where queue may not be there in CS, but reservationID may be valid and submission can go to that reservation queue. cc/[~jianhe], could you pls help to share your thoughts in this., IIUC, that logic is , if the queue does not exist, resolveReservationQueueName will still return the queueName, and later on in addApplication, the job will be marked as FAILED. , Do we know which patch broke this? We used to have a clear message for apps submitted to non-existing queues., IIUC its related to changes in YARN-4571, Thanks [~jianhe] for the clarification. Yes, APP_REJECTED exception will be thrown from {{CapacityScheduler#addApplication}} in this case.

If ACLs are enabled as per current code, then for "queue-not-exist" scenario we will not be creating an RMApp and directly AccessControlException will be thrown from {{createAndPopulateRMApp}}. But with ACLs disabled, RMApp will be created and then APP_REJECTED will be thrown back from scheduler (reservation/normal case).
So there is a behavioral change exists as per now and this is added in YARN-4522.

Quoting the "queue-not-exist" error in non-acl scenario
{noformat}
java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1457161239294_0002 to YARN : Application application_1457161239294_0002 submitted by user root to unknown queue: a1
{noformat}

I think we can make the behavior same in both cases.  Also by seeing below code snippet in {{createAndPopulateRMApp}}, 
{code}
    UserGroupInformation userUgi = UserGroupInformation.createRemoteUser(user);
    // Since FairScheduler queue mapping is done inside scheduler,
    // if FairScheduler is used and the queue doesn't exist, we should not
    // fail here because queue will be created inside FS. Ideally, FS queue
    // mapping should be done outside scheduler too like CS.
    // For now, exclude FS for the acl check.
    if (!isRecovery && YarnConfiguration.isAclEnabled(conf)
{code}

its better we do NOT do a "non-existent-queue" check here. So I think we can have the current check done in the patch inside below if condition. 
{code}
     if (!isRecovery && YarnConfiguration.isAclEnabled(conf)
        && scheduler instanceof CapacityScheduler) {
{code}
And if queue doesnt exist, then we can create a clear message like "Application application_1457161239294_0002 submitted by user root to unknown queue: a1" and throw YarnException directly. Thoughts?, Inline with above comments, I think the current approach in the patch looks fine., [~sunilg]
Thank you for confirming. 
[~jianhe]
Could you also please confirm., [~jianhe]

In current approach we are rejecting before RMApp is created if Acl is enabled and if acl is not enabled will create RMApp.
Should we have to keep it consistent with acl enabled and disabled. and make the app rejected after RMApp is created?? Similar to old behaviour.
, Attaching patch with testcase.
Skipping acl check when {{csqueue==null}} so that the behaviour will be same as old., [~bibinchundatt] and [~jianhe]
Is it better if we throw exception for CS in RMAppManager#createAndPopulateRMApp itself? Or any other advantages other than considering it as a Failed Application. May be an audit log can help in tracking such failures?, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 12m 38s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 40s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 25s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 28s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 19s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 33s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 6s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 21s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 25s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 27s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 26s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 12s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 19s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 23s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 68m 18s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.8.0_74. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 68m 0s {color} | {color:red} hadoop-yarn-server-resourcemanager in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 18s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 165m 0s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_74 Failed junit tests | hadoop.yarn.server.resourcemanager.TestClientRMTokens |
|   | hadoop.yarn.server.resourcemanager.TestAMAuthorization |
| JDK v1.7.0_95 Failed junit tests | hadoop.yarn.server.resourcemanager.TestClientRMTokens |
|   | hadoop.yarn.server.resourcemanager.TestAMAuthorization |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ca8df7 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12791775/0002-YARN-4764.patch |
| JIRA Issue | YARN-4764 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux cf2fa186d8c0 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / fd1c09b |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_74 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/10717/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_74.txt |
| unit | https://builds.apache.org/job/PreCommit-YARN-Build/10717/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-YARN-Build/10717/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.8.0_74.txt https://builds.apache.org/job/PreCommit-YARN-Build/10717/artifact/patchprocess/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/10717/testReport/ |
| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager |
| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/10717/console |
| Powered by | Apache Yetus 0.3.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, bq. Is it better if we throw exception for CS in RMAppManager#createAndPopulateRMApp itself?
I think it is fine to fail the createAndPopulateRMApp call itself., [~jianhe]

# The behaviour will be not be same as previous versions.
# If we are planning to add validation in {{createAndPopulateRMApp}} then cases like *invalid queue, submit to parent queue etc* need to be added in {{createAndPopulateRMApp}} same set of checks we are adding in too many places.

i think we should keep the behaviour as old and only the acl should be handled in {{createAndPopulateRMApp}}.
 , # Also only for capacity scheduler we will be doing validation in {{createAndPopulateRMApp}} .
i think {{APP_REJECT}}  to be a better approach., Thanks [~bibinchundatt] for the analysis and suggestions..

I will try summarizing the discussion so far. Agreeing to the fact that the current  behavior is inconsistent, we have to give similar way of handling for queue-non-existent scenario (with and w/o ACL). So we had 2 options.

1. For CS alone, queue-non-existent check could be done in {{createAndPopulateRMApp}}. Being said this, it will be common for apps with or w/o ACL enabled. This will make App to be rejected before even RMApp is created, hence audit logging is needed.
2. Or we could skip the ACL check if queue is non-existent and can pass to Scheduler inside so that it can send {{APP_REJECT}}. This will be inline with the old behavior. A minor drawback will be like, we know queue is not existing, and still we send scheduler for a know failure handling.

I had an offline talk with [~jianhe] also on this. May be we can go with Option 2 for now. This will make a consistent behavior. But we need to improve here. So I can raise an improvement ticket and all queue related validation check can be done in a new YarnScheduler api. We can see how much we can make it common for Fair and CS too.
Thoughts?
, [~sunilg]/[~jianhe]
Thank you for discussing with summarizing. So the currently implemented patch is fine rt?, Makes sense to me, thank you for the summarization !, Committed to trunk, branch-2,  thanks [~bibinchundatt] !
Thanks [~sunilg] for reviewing the patch !, FAILURE: Integrated in Hadoop-trunk-Commit #9444 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/9444/])
YARN-4764. Application submission fails when submitted queue is not (jianhe: rev 3c33158d1cb38ee4ab3baa21752a3cdf0bdc8ccc)
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java
* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java
, Just linking the broken jira.]