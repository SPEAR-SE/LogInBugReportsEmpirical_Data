[My findings and quick resolutions:
By default, Java 8 allocates extra virtual memory then Java 7. However, we can control the non-heap memory usage by limiting the maximum allowed values for some JVM  parameters  such as  "-XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256"

For M/R based job (such as Pig, Hive etc), user can pass the following  JVM -XX parameters as part of mapreduce.reduce.java.opts or mapreduce.map.java.opts
{noformat}
mapreduce.reduce.java.opts  '-XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m -Xmx1536m -Xms512m -Djava.net.preferIPv4Stack=true'
{noformat}

Similarly for Spark job, we need to pass the same parameters in the Spark AM/master and executor. Spark community is working on the ways to pass these type of parameters easily. In Spark-1.1.0, user can pass it for spark-cluster based job submission as follows. For general job submission, user has to wait until https://issues.apache.org/jira/browse/SPARK-4461 is released.
{noformat}
spark.driver.extraJavaOptions = -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m
{noformat}

For Spark executor, pass the following.
{noformat} 
spark.executor.extraJavaOptions = -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m
{noformat}

 These parameters can be set in conf/spark-defaults.conf as well., Hi [~kamrul], can you confirm that your settings of mapreduce.reduce.memory.mb irrespective of Java8? "2.1 GB virtual memory used" is suspicious in a sense that it looks {{vmem-pmem-ratio(2.1) * default mapreduce.reduce.memory.mb(1024)}}. However, the reducer is declared to  allocate for Java heap -Xmx1536m , let alone the total virtual C-Heap of the JVM. , Sorry [~jira.shegalov] for the late reply. The failure was coming from distcp command which uses 1GB as mapreduce.map.memory.mb. I think distcp is map-only job.

But in other cases, we used higher memory.mb (2GB) and got the similar exception with max 4.2 GB VM.
, I just hit this issue.
{code}
15/04/06 04:14:47 INFO mapreduce.Job: Task Id : attempt_1428293579539_0001_m_000003_0, Status : FAILED
Container [pid=7847,containerID=container_1428293579539_0001_01_000005] is running beyond virtual memory limits. Current usage: 123.5 MB of 1 GB physical memory used; 2.6 GB of 2.1 GB virtual memory used. Killing container.
{code}

* Change yarn.nodemanager.vmem-pmem-ratio to some higher value (4 or 5?)
* Turn yarn.nodemanager.vmem-check-enabled to false

Either way is fine for me., Is yarn.nodemanager.vmem-pmem-ratio and yarn.nodemanager.vmem-check-enabled are only supposed to be effective when set in yarn-site.xml or before a container is launched? I tried to set it via JobConf, and did not see an effect. Mohammad's suggestion tweaking jvm.opts worked though., set in yarn-site and read by the resource manager: it's a cluster-wide policy., Hey guys, I am facing the same issue with Hadoop 2.7.1 and java 7, I tried all the solutions above mentioned but could not resolve the problem.
, This issue is Java 8 only. If you are using Java7 and seeing this message, the allocated memory is too small for the task. I'm thinking  you should increase the allocated memory for the container to fix this issue., Attaching a patch to change the default value of "yarn.nodemanager.vmem-check-enabled" to false. I suppose almost all of Java8 users are hitting this problem, so I'd like to change the default value in trunk/branch-2/branch-2.8., [~ajisakaa] I don't think the workaround, turning vmem-check off, is acceptable. It's incompatible change as described on YARN-2225., Because of the discussion on YARN-2225, canceling the patch., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 9s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red} 0m 0s {color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 11s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 2s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 0s {color} | {color:green} trunk passed with JDK v1.8.0_72 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 14s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 36s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 1s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 28s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 10s {color} | {color:green} trunk passed with JDK v1.8.0_72 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 3m 40s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 11s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 51s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 57s {color} | {color:green} the patch passed with JDK v1.8.0_72 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 57s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 2m 13s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 2m 13s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 35s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 58s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 22s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 1s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 56s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 6s {color} | {color:green} the patch passed with JDK v1.8.0_72 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 3m 36s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 22s {color} | {color:green} hadoop-yarn-api in the patch passed with JDK v1.8.0_72. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 59s {color} | {color:green} hadoop-yarn-common in the patch passed with JDK v1.8.0_72. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 24s {color} | {color:green} hadoop-yarn-api in the patch passed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 12s {color} | {color:green} hadoop-yarn-common in the patch passed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 20s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 42m 14s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ca8df7 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12788024/HADOOP-11364.01.patch |
| JIRA Issue | HADOOP-11364 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  xml  |
| uname | Linux 5b2096da8f9d 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 8ed07bd |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_72 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/8631/testReport/ |
| modules | C:  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api   hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common  U: hadoop-yarn-project/hadoop-yarn |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/8631/console |
| Powered by | Apache Yetus 0.2.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, bq. It's incompatible change as described on YARN-2225.
Incompatible change can be done in trunk. I'll reopen YARN-2225 shortly., Thanks Tsuyoshi for the comment and sharing the jira information., In YARN-2225, Tsuyoshi commented:
bq. IMHO, I prefer to make the default value of the vmem ratio larger. How about closing this issue and doing it on another jira(or, moving HADOOP-11364 to YARN issue) since the addressing problem is different from this issue?

so I moved this issue to YARN. Let's make the default value of vmem ratio larger in this issue., true, but it's still traumatic: cluster performance can seriously suffer. I also expect management tools & their hadoop installations to restore the 2.6 value. Changing the ratio is the solution that would be viable in production with vmem checks enabled, Same issue with jdk1.8.0_60. By changing "yarn.nodemanager.vmem-pmem-ratio to some higher value (4 )" and Turn "yarn.nodemanager.vmem-check-enabled to false" no effect on container kills. With jdk1.7.0_67 no issues. , Hi Krishna, have you changed the configurations on all NodeManagers and restart all of them?, Yes. i did]