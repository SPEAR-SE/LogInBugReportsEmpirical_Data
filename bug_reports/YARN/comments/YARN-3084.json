[202, accepted, looks like the RM accepted it.

# does it appear in the queue of job submissions??
# what does the RM log say?, Hi,
first thanks for your quick reply...

1. Where can i see the queue? i looked for it but i still dont get how to get to it
2. I cant reach the logs...

The only clue I have regarding this job fails is:

Thats the only information i can see from the URL: http://192.168.38.133:8088/ws/v1/cluster/apps/application_1421661392788_0039
(the links there when i try to use them for the logs dont work....http://sandbox.hortonworks.com:8088/proxy/application_1421661392788_0039/ [i replace and insert my host ip]
and also the link for the container logs dont work..... http://sandbox.hortonworks.com:8042/node/containerlogs/container_1421661392788_0039_02_000001/dr.who
----------------------------------
<app>
<id>application_1421661392788_0039</id>
<user>dr.who</user>
<name>test_33</name>
<queue>default</queue>
<state>FAILED</state>
<finalStatus>FAILED</finalStatus>
<progress>0.0</progress>
<trackingUI>History</trackingUI>
<trackingUrl>
http://sandbox.hortonworks.com:8088/cluster/app/application_1421661392788_0039
</trackingUrl>
<diagnostics>
Application application_1421661392788_0039 failed 2 times due to AM Container for appattempt_1421661392788_0039_000002 exited with exitCode: 0 For more detailed output, check application tracking page:http://sandbox.hortonworks.com:8088/proxy/application_1421661392788_0039/Then, click on links to logs of each attempt. Diagnostics: Failing this attempt. Failing the application.
</diagnostics>
<clusterId>1421661392788</clusterId>
<applicationType>MAPREDUCE</applicationType>
<applicationTags>michael,pi example</applicationTags>
<startedTime>1421923561425</startedTime>
<finishedTime>1421923723426</finishedTime>
<elapsedTime>162001</elapsedTime>
<amContainerLogs>
http://sandbox.hortonworks.com:8042/node/containerlogs/container_1421661392788_0039_02_000001/dr.who
</amContainerLogs>
<amHostHttpAddress>sandbox.hortonworks.com:8042</amHostHttpAddress>
<allocatedMB>-1</allocatedMB>
<allocatedVCores>-1</allocatedVCores>
<runningContainers>-1</runningContainers>
<memorySeconds>200857</memorySeconds>
<vcoreSeconds>160</vcoreSeconds>
<preemptedResourceMB>0</preemptedResourceMB>
<preemptedResourceVCores>0</preemptedResourceVCores>
<numNonAMContainerPreempted>0</numNonAMContainerPreempted>
<numAMContainerPreempted>0</numAMContainerPreempted>
</app>, I ran the same but with a simplified job request:
{code}
{
  "application-id":"application_1424804952495_0004",
  "application-name":"seanpi2",
  "am-container-spec":
  {
    "commands":
    {
      "command":"hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar pi 2 2"
    }
  },
  "application-type":"YARN"
}
{code}, Apologies, didn't mean to hit submit.

I submitted with that job. Interestingly, the 'pi' runs and is successful but the parent job reports a failure.

Application application_1424804952495_0004 failed 2 times due to AM Container for appattempt_1424804952495_0004_000002 exited with exitCode: 0

Attaching resource manager logs as yarn-yarn-resourcemanager-sandbox.hortonworks.com.log, [~Xquery], [~seano] the REST API is analogous to the RPC API. It's meant to launch application masters, and not run the application itself. In the examples that you submitted, the RM allocated the container and would have run "hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar pi 2 2" on the NM. The RM then expects the application master to register with the RM. In your case no application master is launched and the attempt is marked as failed. After two attempts, the app itself is marked as failed.

In addition, specifically in your pi example, the code generates the input maps before submitting the job to YARN. This generation is done on the client side. If you wish to run the MapReduce job using the REST API, you need to write some code to generate the input maps.

If you're looking to just play around with the API, the distributed shell app might be a better option. There's a sample of the json at http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Applications_APISubmit_Application
, Hello Varun and Sean,

Thanks for your help! Finally some good progress :-)

Varun - I do understand and plan to write code. 
Still, I want remote execution only. Therefore, I still need 

assistance with the following steps:
1.	First, for launching the application master, I see I need to provide it in the request.
I tried to do it, but couldn’t find AppMaster.jar on the HDFS/local FS.
I assume there is AppMaster per YARN-based application (MR, Pig, Hive etc.).
Can you let me know how can I find/install/download such AppMaster jar?

2.	After I launched the application master, in order to run the map reduce remotely, I need to run another rest api request (I guess), but couldn’t find any example for it. Do you have REST API example of how to run map reduce using REST? (or an explained how to/steps)

3.	I didn’t understand - what does “input maps” means?
Also, if I want to run the application as user A password B, where I supposed to add my credentials and Identify; When I submit my map reduce job, isn’t yarn expects me to identify? 

Thanks,
Michael, [~Xquery] My apologies for not replying for so long. For some reason I didn't get any notification that you had replied to me. In response to your questions -

{quote{
1. First, for launching the application master, I see I need to provide it in the request. I tried to do it, but couldn’t find AppMaster.jar on the HDFS/local FS. I assume there is AppMaster per YARN-based application (MR, Pig, Hive etc.). Can you let me know how can I find/install/download such AppMaster jar?
{quote}

I would recommend using the DistributedShell jar. You can build it yourself from the Hadoop source code. The jar can be found at hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/target. Copy this jar to a location on HDFS that's readable by the user running the job. I've attached a sample json file which you can use to submit to the REST API. One thing you should note - please update the values for "size" and "timestamp" under the "AppMaster.jar" key. The value for "resource" should be the full path to the jar on HDFS.

The DistributedShell app runs a command on multiple machines and exits. The commands write their output to a local file and that output is aggregated at the end of the job if log aggregation is enabled.

In the json I've uploaded, copy the script you want to run to HDFS(making sure it's readable by the user submitting the job). Set the value of the key "DISTRIBUTEDSHELLSCRIPTTIMESTAMP" to the timestamp for this script(on HDFS), the value of the key "DISTRIBUTEDSHELLSCRIPTLEN" to the size of the script and the value of "DISTRIBUTEDSHELLSCRIPTLOCATION" to the location on HDFS.

The 'num_containers' parameter(part of the "command" key) is the number of containers you wish to launch.

{quote}
2. After I launched the application master, in order to run the map reduce remotely, I need to run another rest api request (I guess), but couldn’t find any example for it. Do you have REST API example of how to run map reduce using REST? (or an explained how to/steps)
{quote}

I don't have an example for running MapReduce using REST. The MapReduce client for YARN is a thick client which does a lot of calculations such as creating the splits for the map before it submits the job. You will have to implement that logic yourself if you wish to submit MapReduce jobs. You don't need to run any other API once you submit the job. The AppMaster is responsible for scheduling your mappers and reducers.

{quote}
Also, if I want to run the application as user A password B, where I supposed to add my credentials and Identify; When I submit my map reduce job, isn’t yarn expects me to identify? 
{quote}

Hadoop requires you to setup kerberos for secure mode. In secure mode, jobs are executed as the user who submitted the job. Credentials are picked up when you submit the job.

I'm going to close this issue since it doesn't seem like an issue with the REST API itself. If you have any further questions, we can discuss them offline.

]