[Thank you, [~shanekumpf@gmail.com] for raising this. [~haibochen], we had a logic in one of earlier patches in YARN-7064 to remove multiple reporting of issues from CGroupsResourceCalculator and we removed based on your advice to support debugging. What is your opinion about this suggestion? Do you think we should add back some filtering here? https://issues.apache.org/jira/browse/YARN-7064?focusedCommentId=16323135&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16323135

 , I still think we should not add back the filtering. The problem reported here is the effect, and I'd argue that the excessive log helped surface the cause, which is YARN-8035.

Any problem that is causing excessive log in CGroupResourceCalculator should be addressed, instead of being hidden., Thanks for the response [~haibochen]. I can appreciate that position and will close this issue.

I'll note that handling the exception called out in YARN-8035 doesn't fix this, the "The process vanished in the interim" exceptions continue to repeat every second. However, I agree that there is an underlying cause that filtering the exception doesn't solve. For relaunch, I think it makes sense to stop the monitoring thread and I'll look into doing so as part of YARN-7973.

 , Reopening after more testing. This is not limited to relaunch, I'm finding that I get these exceptions for every single application I run (MR pi, MR sleep, dshell, native services), even when the container completes successfully. I'd like to explore ways to eliminate this.

{code}
2018-03-31 12:09:48,082 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Container container_1522497044327_0004_01_000001 succeeded
2018-03-31 12:09:48,082 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1522497044327_0004_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS
2018-03-31 12:09:48,082 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1522497044327_0004_01_000001
2018-03-31 12:09:50,185 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator: Failed to parse 13187
org.apache.hadoop.yarn.exceptions.YarnException: The process vanished in the interim 13187
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator.processFile(CGroupsResourceCalculator.java:336)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator.readTotalProcessJiffies(CGroupsResourceCalculator.java:252)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator.updateProcessTree(CGroupsResourceCalculator.java:181)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator.updateProcessTree(CombinedResourceCalculator.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:457)
Caused by: java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn/container_1522497044327_0004_01_000001/cpuacct.stat (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator.processFile(CGroupsResourceCalculator.java:320)
	... 4 more
2018-03-31 12:09:50,186 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator: Failed to parse cgroups /sys/fs/cgroup/memory/hadoop-yarn/container_1522497044327_0004_01_000001/memory.memsw.usage_in_bytes
org.apache.hadoop.yarn.exceptions.YarnException: The process vanished in the interim 13187
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator.processFile(CGroupsResourceCalculator.java:336)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator.getMemorySize(CGroupsResourceCalculator.java:238)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator.updateProcessTree(CGroupsResourceCalculator.java:187)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator.updateProcessTree(CombinedResourceCalculator.java:52)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:457)
Caused by: java.io.FileNotFoundException: /sys/fs/cgroup/memory/hadoop-yarn/container_1522497044327_0004_01_000001/memory.usage_in_bytes (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator.processFile(CGroupsResourceCalculator.java:320)
	... 4 more
2018-03-31 12:09:50,186 DEBUG org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.audit: Resource usage of ProcessTree 13187 for container-id container_1522497044327_0004_01_000001: -1B of 1.5 GB physical memory used; -1B of 3.1 GB virtual memory used CPU:0.000000 CPU/core:0.000000
2018-03-31 12:09:50,220 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=hadoopuser	OPERATION=Container Finished - Succeeded	TARGET=ContainerImpl	RESULT=SUCCESS	APPID=application_1522497044327_0004	CONTAINERID=container_1522497044327_0004_01_000001
2018-03-31 12:09:50,220 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1522497044327_0004_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE
{code}, Thank you, [~shanekumpf@gmail.com]. How about hashing the stack trace of the exception and reporting it only, if it has not been seen before?, Thanks [~miklos.szegedi@cloudera.com] - For most applications I only see a single exception for each of the subsystems, like the output above, so I'm not sure that will address a bulk of these. I have a few ideas to test out and I'll report back soon with more detail.]