[If it tries to satisfy a reservation and fails to do so then that implies there are insufficient resources on the node to satisfy the request.  In this case it's trying to allocate a 50GB container, but unfortunately the logs don't show how much space is available on those nodes.  Since there are insufficient resources it re-reserves the node, and that causes subsequent scheduling on that node to be skipped due to the unsatisfied reservation.  If none of the nodes in the cluster can fit another 50GB container then that explains the delay.

If there are nodes in the cluster that can satisfy the 50GB request but it still fails to do so then that indicates either yarn.scheduler.capacity.reservations-continue-look-all-nodes=false (true by default) or there's a bug.  Recently there were some problems with reservations-continue-look-all-nodes that caused delayed scheduling, see YARN-4610.

, Screenshot of the nodes page showing available resources., Just attached a screenshot showing that there are multiple nodes that have hundreds of free gigabytes of RAM., Also, the "Tools -> Configuration" screen for this running cluster does not include an entry for reservations-continue-look-all-nodes, which I assume means it is at its default value., The screenshot also shows one app pending and one running -- I assume we're discussing the app that's already running since pending apps won't be scheduled at all until they become activated once AM resource and user limits allow it.

It's interesting that most of the nodes that have any containers at all don't have enough space for the container.  Is it expected that the containers for this app would cluster on just a few nodes like this?  If not then it's like the scheduler is somehow ignoring those, which would explain why it hangs once the remaining nodes fill up.  

We probably need some more logging to see exactly what's going on.  It would be helpful to turn on debug logging for org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue so we can see when each node heartbeats and can get more visibility into the scheduling for each node.  That can be dynamically enabled/disabled via the logLevel servlet at http://<RMwebaddr>/logLevel., I am talking about the pending app. It never leaves pending, and it is the one mentioned in the log lines above, appattempt_1462291866507_0025_000001, Here's the resourcemanager log file - I turned the level to DEBUG for a while near the end of the log file. It shows the activity related to the application stuck in pending., Apps are pending until they are activated.  Apps can be pending despite cluster resource availability depending upon how the queues and user limits are configured.  Given there's only one app running and one app pending, it's acting like the queue is only allowing one active app at a time.  If you go to the RM scheduler page and expand the details for the queue (click the triangle to the left of the queue bar) then it will show pertinent details like Max Application Master Resources, Used Application Master Resources, Max Application Master Resources Per User, Num Schedulable Applications, Num Non-Schedulable Applications, etc.  If it shows a non-zero number of non-schedulable applications then that's likely because adding the app would exceed the maximum application master resource limit for the user or queue.  The RM log would contain lines like "not starting application as amIfStarted exceeds amLimit" when that occurs.

Either way if the spread of containers across those nodes is not expected it would be good to get some debug logs for the LeafQueue., Attaching a screenshot of the scheduler screen - I do not believe there is any policy preventing this app from starting.

In addition, oftentimes we do have two or three or more applications running and starting without delay. It is only occasionally we see an inexplicably stuck one., 2 shedulable apps and zero non-schedulable apps are shown.

I have already attached DEBUG logs from the LeafQueue., Application 25 finally started after around two hours of delay.

The event that seemed to trigger it was the release of a container from the previously running application. It was able to use this slot to start, and immediately began using containers on the nodes it was previously ignoring as well.

Relevant portion of the resource manager log is attached., OK, so I see one of the empty nodes coming in but it's not using it.  It looks like it placed reservations for the appattempt_1462291866507_0025_000001 AM container on nodes 43-53 and 43-54, which don't have enough space.  Normally the reservations-continue-looking logic would fix this by releasing the reservations on the almost full nodes, so I'm wondering if this is another case of YARN-4610.  If this is easy to reproduce you could try disabling yarn.scheduler.capacity.reservations-continue-look-all-nodes to see if things get better.  It should be OK to disable if it's not common for users to run into their user limits with reservations.  The problem it was solving was a case where the user limits were exhausted making reservations and then a free node comes along -- in that case it needs to release reservations to avoid going over the user limit when allocating on the free node.
, Okay, I can try that. Looking over the linked issue, it should also be resolved at default settings in 2.7.3, correct?, Yes, if the problem is indeed the same as that reported in YARN-4610 then leaving reservations-continue-looking enabled by default in 2.7.3 or later will be fine.
, Nope, I set reservations-continue-look-all-nodes to false, and verified that the config screen showed it:
{code}<name>yarn.scheduler.capacity.reservations-continue-look-all-nodes</name>
<value>false</value>{code}

But I still have exactly the same hangup as before. Two apps schedulable, four nodes in the cluster with plenty of free resources, but still skipping because of reservations on busy nodes:
{code}
2016-05-05 21:09:58,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler (ResourceManager Event Processor): Trying to fulfill reservation for application application_1462468084916_0085 on node: ip-10-12-41-191.us-west-2.compute.internal:8041
2016-05-05 21:09:58,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue (ResourceManager Event Processor): Reserved container  application=application_1462468084916_0085 resource=<memory:50688, vCores:1> queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:1894464, vCores:33>, usedCapacity=0.7126589, absoluteUsedCapacity=0.7126589, numApps=2, numContainers=33 usedCapacity=0.7126589 absoluteUsedCapacity=0.7126589 used=<memory:1894464, vCores:33> cluster=<memory:2658304, vCores:704>
2016-05-05 21:09:58,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler (ResourceManager Event Processor): Skipping scheduling since node ip-10-12-41-191.us-west-2.compute.internal:8041 is reserved by application appattempt_1462468084916_0085_000001
{code}, Can you also double-check that the startup messages in the RM log show that the queues are configured with it off?  The conf screen always loads a fresh config, so it doesn't always reflect what is actually being used.  Look for "reservationsContinueLooking = " lines in the RM log when it dumps the queue config on startup.

I'll also look into reproducing it on our end.  This is a pretty simple setup with just two users and two total apps, so hopefully this is straightforward to replicate., Attached the queue config section of RM logs from startup, looks like the setting is false from the get-go., It seems some nodes are in decommissioning state from the [log|https://issues.apache.org/jira/secure/attachment/12802294/yarn-yarn-resourcemanager-ip-10-12-47-144.log.gz], probably we have some bugs to correctly show decommissioning nodes and related resources on web UI.

{code}
2016-05-04 21:00:10,182 INFO org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher (IPC Server handler 44 on 8025): Decommissioning Nodes: 
  ip-10-12-41-126.us-west-2.compute.internal 63833s fresh:41194s containers: 0          READY
  ip-10-12-36-61.us-west-2.compute.internal 62964s fresh:41194s containers: 0          READY
  ip-10-12-46-96.us-west-2.compute.internal 98343s fresh:98343s containers: 0          READY
  ...
{code}

IIRC, scheduler will not assign containers to decommissioning nodes, that could be the reason why your applications stay at ACCEPTED state., So we could be shooting ourselves in the foot by scaling our cluster up and down as needed?

But, this only applies to new jobs not starting - the nodes are used once the application does start..., + [~djp] since this may be related to node grace decommissioning., Hi [~milesc], DecommissioningNodesWatcher is a new class involving in YARN-4676 which is still in review process. Which branch your test is based on?, bq.  scheduler will not assign containers to decommissioning nodes, that could be the reason why your applications stay at ACCEPTED state.

When I saw those log messages I immediately thought that was the case, but I couldn't see any of the three completely empty nodes in the list of nodes that supposedly were decommissioning.  In addition the debug logs clearly show the nodes are heartbeating in, the nodes page shows the RM thinks the nodes have 256GB available, and as Miles mentioned the nodes are immediately used when the second app's AM finally starts.  Therefore I don't think this is related to node decommissioning unless the Amazon node decommissioning logic is very bizarre and somehow tied to when applications start.
, [~milesc], if you have it in this state again, would you mind enabling DEBUG for all of capacity scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity

I wasn't seeing enough information in LeafQueue to figure out why it's not scheduling on some of the nodes., Will do, happens pretty regularly, so should be in the next day or two., Attached capacity-scheduler-at-debug.log.gz, which has a few minutes of DEBUG level for the whole package right at the end of the log.

The stuck application is 0033, and there are three nodes with 100GB or more memory listed as available in the resource manager UI., Thanks [~milesc]! Still not quite enough. How about org.apache.hadoop.yarn.server.resourcemanager.scheduler? 

BTW. Thanks so much for helping to track this down.
, Okey dokey, here's another log - last 2 or three minutes have the whole scheduler at debug.

Stuck application in this case is 0041 and I did verify that there are many nodes with sufficient space.

Thank you so much for looking into this!, Thanks [~milesc]. This seems to be an Amazon emr thing (unless I'm misunderstanding the log messages). 

Here are the important pieces:

Every time the scheduler is trying to schedule on a node with sufficient room, it is bailing out claiming it's not on the right type of emr node:
{noformat}
# egrep -i "node being looked for|is excluded" whole-scheduler-at-debug.log
2016-05-11 00:55:46,818 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler (ResourceManager Event Processor): Node being looked for scheduling ip-10-12-40-239.us-west-2.compute.internal:8041 availableResource: <memory:241664, vCores:64>
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils (ResourceManager Event Processor): node ip-10-12-40-239.us-west-2.compute.internal with emrlabel:TASK is excluded to request with emrLabel:MASTER,CORE
{noformat}

And below you see it consider the 0041 application and everything looks promising until the node is excluded. This is an emr-specific check which is why it wasn't making a lot of sense as to how this could happen.
{noformat}
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue (ResourceManager Event Processor): pre-assignContainers for application application_1462722347496_0041
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue (ResourceManager Event Processor): User limit computation for ai2service in queue default userLimit=100 userLimitFactor=1.0 required: <memory:50688, vCores:1> consumed: <memory:101376, vCores:2> limit: <memory:4833280, vCores:1> queueCapacity: <memory:4833280, vCores:1> qconsumed: <memory:3107648, vCores:55> currentCapacity: <memory:4833280, vCores:1> activeUsers: 1 clusterCapacity: <memory:4833280, vCores:1280>
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt (ResourceManager Event Processor): showRequests: application=application_1462722347496_0041 headRoom=<memory:1725632, vCores:1> currentConsumption=0
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt (ResourceManager Event Processor): showRequests: application=application_1462722347496_0041 request={Priority: 0, Capability: <memory:50688, vCores:1>, # Containers: 1, Location: *, Relax Locality: true}
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue (ResourceManager Event Processor): needsContainers: app.#re-reserve=636 reserved=2 nodeFactor=0.20974576 minAllocFactor=0.99986756 starvation=251
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue (ResourceManager Event Processor): User limit computation for ai2service in queue default userLimit=100 userLimitFactor=1.0 required: <memory:50688, vCores:1> consumed: <memory:101376, vCores:2> limit: <memory:4833280, vCores:1> queueCapacity: <memory:4833280, vCores:1> qconsumed: <memory:3107648, vCores:55> currentCapacity: <memory:4833280, vCores:1> activeUsers: 1 clusterCapacity: <memory:4833280, vCores:1280>
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue (ResourceManager Event Processor): Headroom calculation for user ai2service:  userLimit=<memory:4833280, vCores:1> queueMaxAvailRes=<memory:4833280, vCores:1> consumed=<memory:101376, vCores:2> headroom=<memory:1725632, vCores:1>
2016-05-11 00:55:46,819 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils (ResourceManager Event Processor): node ip-10-12-40-239.us-west-2.compute.internal with emrlabel:TASK is excluded to request with emrLabel:MASTER,CORE
{noformat}

I suspect EMR is not wanting to schedule AMs on nodes that are more likely to go away (TASK nodes). Once it gets the AM running though, it takes off. 
 
Maybe someone from Amazon can chime-in?? cc [~danzhi]
, I am OOO so didn't really dive deep. EMR has logic to only schedule application master on CORE nodes by default (So to avoid SPOT instance termination if customer use SPOT for TASK nodes etc). I guess this might delay the start of application is all CORE slots are occupied.

This behavior could changed with "yarn.app.mapreduce.am.labels", which should have default value "CORE", but could be customized as "CORE,TASK" to allow MRAppMaster on task node., Okay, wow that is wonderful to know. We have a highly variable workload and make almost no use of HDFS, so we use a tiny number of CORE nodes and remove TASK nodes when things are idle to save on costs. We do not use spot instances at all (because of https://issues.apache.org/jira/browse/SPARK-14209)

I cannot seem to find any mention of this behavior in the EMR documentation, so it's a bit of a blindside.

Additionally, the Node Labels page of the Hadoop UI does not distinguish between spot and task, so I wasn't even aware labeling was going on.

I guess things are working as designed, so I'm sorry to take up all your time. Thanks very much for helping. I think I'll follow up with AWS and request a documentation fix.]