{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12829681","self":"https://issues.apache.org/jira/rest/api/2/issue/12829681","key":"YARN-3642","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2015-05-14T12:13:03.275+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri May 15 17:33:17 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_161227505_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-05-15T17:33:16.977+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-3642/watchers","watchCount":3,"isWatching":false},"created":"2015-05-13T20:46:09.531+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327585","id":"12327585","description":"2.7.0 release","name":"2.7.0","archived":false,"released":true,"releaseDate":"2015-04-20"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-05-15T17:33:17.026+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319322","id":"12319322","name":"resourcemanager"}],"timeoriginalestimate":null,"description":"There is an issue with Hadoop 2.7.0 when in distributed operation the datanode is unable to reach the yarn scheduler.  In our yarn-site.xml, we have defined this path to be:\n\n{code}\n   <property>\n      <name>yarn.resourcemanager.scheduler.address</name>\n      <value>qadoop-nn001.apsalar.com:8030</value>\n   </property>\n{code}\n\nBut when running an oozie job, the problem manifests when looking at the job logs for the yarn container.\nWe see logs similar to the following showing the connection problem:\n\n{quote}\nShowing 4096 bytes. Click here for full log\n[main] org.apache.hadoop.http.HttpServer2: Jetty bound to port 64065\n2015-05-13 17:49:33,930 INFO [main] org.mortbay.log: jetty-6.1.26\n2015-05-13 17:49:33,971 INFO [main] org.mortbay.log: Extract jar:file:/opt/local/hadoop/hadoop-2.7.0/share/hadoop/yarn/hadoop-yarn-common-2.7.0.jar!/webapps/mapreduce to /var/tmp/Jetty_0_0_0_0_64065_mapreduce____.1ayyhk/webapp\n2015-05-13 17:49:34,234 INFO [main] org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:64065\n2015-05-13 17:49:34,234 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Web app /mapreduce started at 64065\n2015-05-13 17:49:34,645 INFO [main] org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules\n2015-05-13 17:49:34,651 INFO [main] org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-05-13 17:49:34,652 INFO [Socket Reader #1 for port 38927] org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 38927\n2015-05-13 17:49:34,660 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2015-05-13 17:49:34,660 INFO [IPC Server listener on 38927] org.apache.hadoop.ipc.Server: IPC Server listener on 38927: starting\n2015-05-13 17:49:34,700 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: nodeBlacklistingEnabled:true\n2015-05-13 17:49:34,700 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: maxTaskFailuresPerNode is 3\n2015-05-13 17:49:34,700 INFO [main] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: blacklistDisablePercent is 33\n2015-05-13 17:49:34,775 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8030\n2015-05-13 17:49:35,820 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:36,821 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:37,823 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:38,824 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:39,825 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:40,826 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:41,827 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:42,828 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:43,829 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2015-05-13 17:49:44,830 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n{quote}\n\n\nTo prove the problem, we have patched the file:\n{code}\nhadoop-2.7.0/src/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/RMProxy.java\n{code}\n\nso that we now \"inject\" the yarn.resourcemanager.scheduler.address directly into the configuration.\n\nThe modified code looks like this:\n\n{code}\n  @Private\n  protected static <T> T createRMProxy(final Configuration configuration,\n      final Class<T> protocol, RMProxy instance) throws IOException {\n    YarnConfiguration conf = (configuration instanceof YarnConfiguration)\n        ? (YarnConfiguration) configuration\n        : new YarnConfiguration(configuration);\n    LOG.info(\"LEE: changing the conf to include yarn.resourcemanager.scheduler.address at 10.1.26.1\");\n    conf.set(\"yarn.resourcemanager.scheduler.address\", \"10.1.26.1\");\n    RetryPolicy retryPolicy = createRetryPolicy(conf);\n    if (HAUtil.isHAEnabled(conf)) {\n      RMFailoverProxyProvider<T> provider =\n          instance.createRMFailoverProxyProvider(conf, protocol);\n      return (T) RetryProxy.create(protocol, provider, retryPolicy);\n    } else {\n      InetSocketAddress rmAddress = instance.getRMAddress(conf, protocol);\n      LOG.info(\"LEE: Connecting to ResourceManager at \" + rmAddress);\n      T proxy = RMProxy.<T>getProxy(conf, protocol, rmAddress);\n      return (T) RetryProxy.create(protocol, proxy, retryPolicy);\n    }\n  }\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Hadoop2 yarn.resourcemanager.scheduler.address not loaded by RMProxy.java","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"yarn-site.xml:\n<configuration>\n\n   <property>\n      <name>yarn.nodemanager.aux-services</name>\n      <value>mapreduce_shuffle</value>\n   </property>\n\n   <property>\n      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n      <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.hostname</name>\n      <value>qadoop-nn001.apsalar.com</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.scheduler.address</name>\n      <value>qadoop-nn001.apsalar.com:8030</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.address</name>\n      <value>qadoop-nn001.apsalar.com:8032</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.webap.address</name>\n      <value>qadoop-nn001.apsalar.com:8088</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.resource-tracker.address</name>\n      <value>qadoop-nn001.apsalar.com:8031</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.admin.address</name>\n      <value>qadoop-nn001.apsalar.com:8033</value>\n   </property>\n\n   <property>\n      <name>yarn.log-aggregation-enable</name>\n      <value>true</value>\n   </property>\n\n   <property>\n      <description>Where to aggregate logs to.</description>\n      <name>yarn.nodemanager.remote-app-log-dir</name>\n      <value>/var/log/hadoop/apps</value>\n   </property>\n\n   <property>\n      <name>yarn.web-proxy.address</name>\n      <value>qadoop-nn001.apsalar.com:8088</value>\n   </property>\n\n</configuration>\n\n\ncore-site.xml:\n<configuration>\n\n   <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://qadoop-nn001.apsalar.com</value>\n   </property>\n\n   <property>\n      <name>hadoop.proxyuser.hdfs.hosts</name>\n      <value>*</value>\n   </property>\n\n   <property>\n      <name>hadoop.proxyuser.hdfs.groups</name>\n      <value>*</value>\n   </property>\n\n</configuration>\n\n\nhdfs-site.xml:\n<configuration>\n\n   <property>\n      <name>dfs.replication</name>\n      <value>2</value>\n   </property>\n\n   <property>\n      <name>dfs.namenode.name.dir</name>\n      <value>file:/hadoop/nn</value>\n   </property>\n\n   <property>\n      <name>dfs.datanode.data.dir</name>\n      <value>file:/hadoop/dn/dfs</value>\n   </property>\n\n   <property>\n      <name>dfs.http.address</name>\n      <value>qadoop-nn001.apsalar.com:50070</value>\n   </property>\n\n   <property>\n      <name>dfs.secondary.http.address</name>\n      <value>qadoop-nn002.apsalar.com:50090</value>\n   </property>\n\n</configuration>\n\n\nmapred-site.xml:\n<configuration>\n\n   <property> \n      <name>mapred.job.tracker</name> \n      <value>qadoop-nn001.apsalar.com:8032</value> \n   </property>\n\n   <property>\n      <name>mapreduce.framework.name</name>\n      <value>yarn</value>\n   </property>\n\n   <property>\n      <name>mapreduce.jobhistory.address</name>\n      <value>qadoop-nn001.apsalar.com:10020</value>\n      <description>the JobHistoryServer address.</description>\n   </property>\n\n   <property>  \n      <name>mapreduce.jobhistory.webapp.address</name>  \n      <value>qadoop-nn001.apsalar.com:19888</value>  \n      <description>the JobHistoryServer web address</description>\n   </property>\n\n</configuration>\n\n\nhbase-site.xml:\n<configuration>\n\n    <property> \n        <name>hbase.master</name> \n        <value>qadoop-nn001.apsalar.com:60000</value> \n    </property> \n\n    <property> \n        <name>hbase.rootdir</name> \n        <value>hdfs://qadoop-nn001.apsalar.com:8020/hbase</value> \n    </property> \n\n    <property> \n        <name>hbase.cluster.distributed</name> \n        <value>true</value> \n    </property> \n\n    <property>\n        <name>hbase.zookeeper.property.dataDir</name>\n        <value>/opt/local/zookeeper</value>\n    </property> \n\n    <property>\n        <name>hbase.zookeeper.property.clientPort</name>\n        <value>2181</value> \n    </property>\n\n    <property> \n        <name>hbase.zookeeper.quorum</name> \n        <value>qadoop-nn001.apsalar.com</value> \n    </property> \n\n    <property> \n        <name>zookeeper.session.timeout</name> \n        <value>180000</value> \n    </property> \n\n</configuration>\n","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12829681/comment/14543551","id":"14543551","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"I think this is related to /etc/hosts mapping. Does /etc/hosts mapping exits in all the machines of NodeManager for *qadoop-nn001.apsalar.com*? \nIn your changed code, you are setting for an ip which will work. Can you set hostname and try? It wont work I guess. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-14T12:13:03.275+0000","updated":"2015-05-14T12:13:03.275+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12829681/comment/14544450","id":"14544450","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"body":"Yes, we have the same /etc/hosts on all machines.\nWe also have internal DNS with those.\nThe problem is not name-resolution.\n\nOur yarn-site.conf is being read for all the other values.\nonly the yarn.resourcemanager.scheduler.address is not properly loaded into the conf object.\n\nI won't be able to do a hostname test (in place of the hard-code IP) until sometime late next week.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"created":"2015-05-14T22:10:45.964+0000","updated":"2015-05-14T22:10:45.964+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12829681/comment/14544459","id":"14544459","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"body":"Here is a copy of our common /etc/hosts:\n\n::1        localhost\n127.0.0.1  localhost loghost\n10.1.0.220      35d47c6c-97c4-429c-a189-aeb5349eea43    loghost\n127.0.0.1       pgbackup\n10.1.26.11      qadoop-batch.apsalar.com\n10.1.26.1       qadoop-nn001.apsalar.com\n10.1.26.2       qadoop-nn002.apsalar.com\n10.1.26.3       qadoop-jt001.apsalar.com\n10.1.26.4       qadoop-d001.apsalar.com\n10.1.26.5       qadoop-d002.apsalar.com\n10.1.26.6       qadoop-d003.apsalar.com\n10.1.26.7       qadoop-master1a.apsalar.com\n10.1.26.8       qadoop-shard1a.apsalar.com\n10.1.26.9       qadoop-shard2a.apsalar.com\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"created":"2015-05-14T22:15:44.094+0000","updated":"2015-05-14T22:15:44.094+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12829681/comment/14544462","id":"14544462","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"body":"Here is a copy of our yarn-site.xml:\n\n<?xml version=\"1.0\"?>\n<!--\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License. See accompanying LICENSE file.\n-->\n<configuration>\n\n   <property>\n      <name>yarn.nodemanager.aux-services</name>\n      <value>mapreduce_shuffle</value>\n   </property>\n\n   <property>\n      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n      <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.hostname</name>\n      <value>qadoop-nn001.apsalar.com</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.scheduler.address</name>\n      <value>qadoop-nn001.apsalar.com:8030</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.address</name>\n      <value>qadoop-nn001.apsalar.com:8032</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.webap.address</name>\n      <value>qadoop-nn001.apsalar.com:8088</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.resource-tracker.address</name>\n      <value>qadoop-nn001.apsalar.com:8031</value>\n   </property>\n\n   <property>\n      <name>yarn.resourcemanager.admin.address</name>\n      <value>qadoop-nn001.apsalar.com:8033</value>\n   </property>\n\n   <property>\n      <name>yarn.log-aggregation-enable</name>\n      <value>true</value>\n   </property>\n\n   <property>\n      <description>Where to aggregate logs to.</description>\n      <name>yarn.nodemanager.remote-app-log-dir</name>\n      <value>/var/log/hadoop/apps</value>\n   </property>\n\n   <property>\n      <name>yarn.web-proxy.address</name>\n      <value>qadoop-nn001.apsalar.com:8088</value>\n   </property>\n\n</configuration>\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"created":"2015-05-14T22:16:27.425+0000","updated":"2015-05-14T22:16:27.425+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12829681/comment/14544502","id":"14544502","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"body":"Yes, we have a proper /etc/hosts\nThat is not the problem.\n\nI am able to make it work by forcing the conf to have our yarn scheduler\naddress IP.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"created":"2015-05-14T22:37:40.970+0000","updated":"2015-05-14T22:37:40.970+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12829681/comment/14544920","id":"14544920","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"How many nodemanagers are running? If it more than 1 then I am thinking what would have happen in your case is yarn-site.xml never read by clent i.e oozi job but still you are able to submit the job because you might be submitting job from the local machine i.e where RM is running. So with default port job is able to submit , but when AppplicationManster is launched , it is launched in different machine where NodeManager is running. Since scheduler address is not loaded by any configuration, AM tries to connect default address i.e 0.0.0.0:8030 which never connect. \n\nI suggest that you can make sure your yarn-site.xml is loaded into classpath before submitting the job. So the AM gets the yarn.resourcemanager.scheduler.address and connect to RM. Otherway is explicitely set yarn.resourcemanager.scheduler.address  using job client.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-15T04:46:31.733+0000","updated":"2015-05-15T04:46:31.733+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12829681/comment/14545770","id":"14545770","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"body":"Rohith, Thank you very much for your help!  You are correct.  We are running 3 nodemanagers.  After adding our yarn-site.xml to the CLASSPATH, (and putting back original code) we are able to successfully submit and run an oozie job.  Our problem is solved.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Lee+Hounshell","name":"Lee Hounshell","key":"lee hounshell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lee Hounshell","active":true,"timeZone":"Etc/UTC"},"created":"2015-05-15T16:46:38.035+0000","updated":"2015-05-15T16:46:38.035+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12829681/comment/14545839","id":"14545839","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"Closing as Invalid.\n\nIf there is any queries or basic environment problems , I suggest to use user mailing lists to ask queries.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2015-05-15T17:33:17.014+0000","updated":"2015-05-15T17:33:17.014+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-3642/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2ep8f:"}}