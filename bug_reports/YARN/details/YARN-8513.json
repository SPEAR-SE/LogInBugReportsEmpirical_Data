{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13171211","self":"https://issues.apache.org/jira/rest/api/2/issue/13171211","key":"YARN-8513","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2018-07-13T08:40:52.523+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Sep 04 01:30:05 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-8513/watchers","watchCount":7,"isWatching":false},"created":"2018-07-10T17:28:27.859+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"14.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12341436","id":"12341436","description":"3.1.0 release","name":"3.1.0","archived":false,"released":true,"releaseDate":"2018-04-06"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12341762","id":"12341762","description":"2.9.1 release","name":"2.9.1","archived":false,"released":true,"releaseDate":"2018-05-03"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-09-04T02:25:54.268+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12327621","id":"12327621","name":"capacity scheduler"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12325004","id":"12325004","name":"yarn"}],"timeoriginalestimate":null,"description":"ResourceManager does not respond to any request when queue is near fully utilized sometimes. Sending SIGTERM won't stop RM, only SIGKILL can. After RM restart, it can recover running jobs and start accepting new ones.\r\n\r\n \r\n\r\nSeems like CapacityScheduler is in an infinite loop printing out the following log messages (more than 25,000 lines in a second):\r\n\r\n \r\n\r\n{{2018-07-10 17:16:29,227 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.99816763 absoluteUsedCapacity=0.99816763 used=<memory:16170624, vCores:1577> cluster=<memory:29441544, vCores:5792>}}\r\n{{2018-07-10 17:16:29,227 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Failed to accept allocation proposal}}\r\n{{2018-07-10 17:16:29,227 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1530619767030_1652_000001 container=null queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@14420943 clusterResource=<memory:29441544, vCores:5792> type=NODE_LOCAL requestedPartition=}}\r\n\r\n \r\n\r\nI encounter this problem several times after upgrading to YARN 2.9.1, while the same configuration works fine under version 2.7.3.\r\n\r\n \r\n\r\nYARN-4477 is an infinite loop bug in FairScheduler, not sure if this is a similar problem.\r\n\r\n ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12931725","id":"12931725","filename":"jstack-1.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-16T02:15:29.179+0000","size":176955,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12931725/jstack-1.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12931726","id":"12931726","filename":"jstack-2.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-16T02:15:28.887+0000","size":175502,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12931726/jstack-2.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12931727","id":"12931727","filename":"jstack-3.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-16T02:15:28.987+0000","size":173268,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12931727/jstack-3.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12931728","id":"12931728","filename":"jstack-4.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-16T02:15:29.078+0000","size":174265,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12931728/jstack-4.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12931729","id":"12931729","filename":"jstack-5.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-16T02:15:29.436+0000","size":177272,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12931729/jstack-5.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12931730","id":"12931730","filename":"top-during-lock.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-16T02:15:28.793+0000","size":2540,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12931730/top-during-lock.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12931731","id":"12931731","filename":"top-when-normal.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-16T02:15:29.506+0000","size":2540,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12931731/top-when-normal.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936151","id":"12936151","filename":"yarn3-jstack1.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T14:04:18.103+0000","size":154480,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12936151/yarn3-jstack1.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936152","id":"12936152","filename":"yarn3-jstack2.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T14:04:18.277+0000","size":154727,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12936152/yarn3-jstack2.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936153","id":"12936153","filename":"yarn3-jstack3.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T14:04:18.997+0000","size":228401,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12936153/yarn3-jstack3.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936154","id":"12936154","filename":"yarn3-jstack4.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T14:04:18.069+0000","size":155340,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12936154/yarn3-jstack4.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936155","id":"12936155","filename":"yarn3-jstack5.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T14:04:20.231+0000","size":156625,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12936155/yarn3-jstack5.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936158","id":"12936158","filename":"yarn3-resourcemanager.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T14:10:03.168+0000","size":816781,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12936158/yarn3-resourcemanager.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936157","id":"12936157","filename":"yarn3-top","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T14:05:24.543+0000","size":8982,"mimeType":"text/html","content":"https://issues.apache.org/jira/secure/attachment/12936157/yarn3-top"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"CapacityScheduler infinite loop when queue is near fully utilized","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Ubuntu 14.04.5 and 16.04.4\r\n\r\nYARN is configured with one label and 5 queues.","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16542719","id":"16542719","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yuanbo","name":"yuanbo","key":"yuanbo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yuanbo&avatarId=25581","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yuanbo&avatarId=25581","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yuanbo&avatarId=25581","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yuanbo&avatarId=25581"},"displayName":"Yuanbo Liu","active":true,"timeZone":"Asia/Shanghai"},"body":"[~cyfdecyf] Can you reproduce this issue and capture the stack of RM\r\n # jstack -F pid\r\n # top -H -p pid \r\n\r\nthen attach those info in this Jira so that we can figure out the cause of infinite loop here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yuanbo","name":"yuanbo","key":"yuanbo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yuanbo&avatarId=25581","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yuanbo&avatarId=25581","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yuanbo&avatarId=25581","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yuanbo&avatarId=25581"},"displayName":"Yuanbo Liu","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-13T08:40:52.523+0000","updated":"2018-07-13T08:40:52.523+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16543689","id":"16543689","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"body":"[~cyfdecyf], I couldn't find the error message on the latest codebase. Not sure if this still a problem in latest release (3.1.0). We have many fixes in the last several months for CapacityScheduler scheduling after YARN-5139, I believe many of them are not backported to 2.9.1. Could u check if the problem still exists in 3.1.0 if possible?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-07-13T20:51:41.618+0000","updated":"2018-07-13T20:51:41.618+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16544231","id":"16544231","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"[~yuanbo] I'll capture those info when I encounter this problem again next time. Currently I don't know the exact trigger condition for the problem, so have to wait for some time.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-14T14:39:45.433+0000","updated":"2018-07-14T14:39:45.433+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16544234","id":"16544234","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"[~leftnoteasy] Thanks for your suggestions. I'll deploy a test environment with 3.1.0 and test it with some of our production workloads. This may take quite some time but I'll report my findings when I've done the test.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-14T14:57:54.808+0000","updated":"2018-07-14T14:57:54.808+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16544742","id":"16544742","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"[~yuanbo] I've uploaded jstack and top log when the problem appeared yesterday.\r\n\r\njstack log is captured for 5 times thus 5 log files.\r\n\r\n[^top-during-lock.log] is captured when RM is not responding to requests.\r\n\r\n[^top-when-normal.log] is captured today and RM is running normally.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-16T02:38:52.967+0000","updated":"2018-07-16T02:39:24.882+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16551551","id":"16551551","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yuanbo","name":"yuanbo","key":"yuanbo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yuanbo&avatarId=25581","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yuanbo&avatarId=25581","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yuanbo&avatarId=25581","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yuanbo&avatarId=25581"},"displayName":"Yuanbo Liu","active":true,"timeZone":"Asia/Shanghai"},"body":"Sorry for the late response. Quite busy this week. I will go through the dump files today","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yuanbo","name":"yuanbo","key":"yuanbo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yuanbo&avatarId=25581","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yuanbo&avatarId=25581","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yuanbo&avatarId=25581","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yuanbo&avatarId=25581"},"displayName":"Yuanbo Liu","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-07-21T06:28:21.221+0000","updated":"2018-07-21T06:28:21.221+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16552536","id":"16552536","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"body":"We also met this problem in 2.9.1. It is caused by deadlock.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"created":"2018-07-23T09:20:25.767+0000","updated":"2018-07-23T09:21:17.202+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16580862","id":"16580862","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"We got infinite loops two times recently with 2.9.1, restarting ResourceManager fixed the issue again.\r\n\r\n \r\n\r\nAs the cause of the problem is still not clear, we have upgraded to Hadoop 3.1.0. I'll give further updates in case we encounter this issue again.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-15T09:00:29.251+0000","updated":"2018-08-15T09:00:29.251+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16582584","id":"16582584","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"[~leftnoteasy] We encounter the same problem twice today with Hadoop 3.1.0. ResourceManager log messages are the same as before, flushing out 20k lines every seconds during deadlock.\r\n\r\nOur current setup enables ResourceManager and NameNode HA, killing the active ResourceManager with SIGKILL will turn other standby RM to active state, and the new active RM can work properly.\r\n\r\nApplicationMaster running on our cluster is a slightly modified version of distributedshell (modified from Hadoop 2.7.1). When updating to Hadoop 3.1.0, we fixed all deprecated APIs and didn't make other modifications. During the RM deadlock, I killed the AM (having very low CPU usage) which is being reported in the log message, but RM still flushes the same log messages. If the problem is caused by our AM, killing the problematic AM may turn RM to a normal state. But seems like it's the internal deadlock in RM causing the problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-16T14:08:16.735+0000","updated":"2018-08-16T14:08:16.735+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16583026","id":"16583026","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"body":"[~cyfdecyf], \r\n\r\nCould u upload logs/jstacks for 3.1.0 deployment? We can help to take a look at it. It will be better if you can enable DEBUG log of {{org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity}} for a short while.\r\n\r\ncc: [~sunil.govind@gmail.com], [~cheersyang]. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-16T20:26:23.733+0000","updated":"2018-08-16T20:26:23.733+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16583270","id":"16583270","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"[~leftnoteasy] Thanks for your help. I'll enable capacity scheduler DEBUG log and upload logs/jstacks when the problem occurs again.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-17T02:11:34.548+0000","updated":"2018-08-17T02:11:34.548+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16584787","id":"16584787","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"New jstack/top and RM logs are uploaded and prefixed with yarn3. We upgraded to Hadoop 3.1.1 yesterday and encounter this problem several times.\r\n\r\nThe problem seems reproducible when one queue is near fully utilized. Killing current active RM can not solve the problem. We have to kill some jobs in the fully utilized queue in order to submit new jobs.\r\n\r\nDebug log shows that CapacityScheduler repeatedly trying to schedule on a specific node, but as queue resource has exceeded resource limit, allocation proposal won't be accepted. top command shows only one thread with near 100% CPU usage, strace shows this thread is the one trying to do the allocation and flushing out logs.\r\n\r\nI've tried to dig into source code, but can't find out why RM repeatedly trying to schedule on a specific node.\r\n\r\nSome notes about our setup:\r\n\r\n* 3 partitions: default, sim, gpu\r\n* 4 queues: dev & mkt (10% capacity, max 90%), dev-daily & mkt-daily (40% capacity, max 100%)\r\n* preemption disabled","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T14:29:40.399+0000","updated":"2018-08-18T14:29:40.399+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16584825","id":"16584825","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cheersyang","name":"cheersyang","key":"cheersyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cheersyang&avatarId=23772","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cheersyang&avatarId=23772","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cheersyang&avatarId=23772","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cheersyang&avatarId=23772"},"displayName":"Weiwei Yang","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~cyfdecyf]\r\n\r\nFrom the RM log you uploaded, in 1 sec, there are 70 times of\r\n{code:java}\r\n Trying to schedule on node: rndcl58.rt.com, available: <memory:120769, vCores:28>\r\n{code}\r\nFrom jstack files, they both have\r\n{code:java}\r\nThread 328918: (state = IN_JAVA)\r\n...\r\norg.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet, boolean) @bci=50, line=1647 (Compiled frame) - org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId, boolean) @bci=102, line=1417 (Compiled frame) - org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode) @bci=110, line=1258 (Compiled frame)\r\n{code}\r\nso they are triggered by nodeUpdate (HB). Looks like the RM dispatcher is flooded with NM HBs? I think we need more info, what's the size of your cluster and what is the time interval of the NM HB?\r\n\r\nThanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cheersyang","name":"cheersyang","key":"cheersyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cheersyang&avatarId=23772","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cheersyang&avatarId=23772","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cheersyang&avatarId=23772","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cheersyang&avatarId=23772"},"displayName":"Weiwei Yang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-18T15:52:36.746+0000","updated":"2018-08-18T15:52:36.746+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16585036","id":"16585036","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"[~cheersyang] Thanks for looking into this issue.\r\n\r\nThe log message file is truncated (because mosts are repeated), the whole log size in a second is 60MB and contains about 5300 lines of \"Trying to schedule on node\".\r\n\r\nCluster size:\r\n\r\n* default partition: 79 NM\r\n* sim: 45 NM\r\n* gpu: 0 NM (we still have Hadoop 2.9.1 running and some nodes haven't join the new version of Hadoop)\r\n\r\nNM HB interval is not changed and is the default values 1000ms.\r\n\r\nI looked at some NMs CPU usage when the infinite loop happens, but didn't see high CPU usage of NM process.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-19T06:36:43.850+0000","updated":"2018-08-19T06:39:45.104+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16585359","id":"16585359","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cheersyang","name":"cheersyang","key":"cheersyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cheersyang&avatarId=23772","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cheersyang&avatarId=23772","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cheersyang&avatarId=23772","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cheersyang&avatarId=23772"},"displayName":"Weiwei Yang","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks for the info [~cyfdecyf]. To work more efficiently on this issue, lets collaborate on Slack. I created a channel {{yarn-8513}} and please use [this link|https://join.slack.com/t/yarn-group/shared_invite/enQtNDE5NTA0ODk2NTE3LTljNWE2MzdjMWU5NGUyYTJkYjI0YmJhNzc4NDg5MTMwMWRiZDI1OGE2YWI4ZTE1MzQ2MjBkYWRjNDk0MjJhMTY] to join.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cheersyang","name":"cheersyang","key":"cheersyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cheersyang&avatarId=23772","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cheersyang&avatarId=23772","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cheersyang&avatarId=23772","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cheersyang&avatarId=23772"},"displayName":"Weiwei Yang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-20T02:33:54.134+0000","updated":"2018-08-20T02:33:54.134+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16586811","id":"16586811","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cheersyang","name":"cheersyang","key":"cheersyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cheersyang&avatarId=23772","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cheersyang&avatarId=23772","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cheersyang&avatarId=23772","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cheersyang&avatarId=23772"},"displayName":"Weiwei Yang","active":true,"timeZone":"Asia/Shanghai"},"body":"Discussed with [~cyfdecyf] in the slack channel, it looks like this issue was caused by the greedy container assignments per HB mechanism, for some reason, it never stop trying assign new containers for this particular node in a while loop. I suggested to add following config to work-around\r\n{noformat}\r\n“yarn.scheduler.capacity.per-node-heartbeat.multiple-assignments-enable”=“true”\r\n“yarn.scheduler.capacity.per-node-heartbeat.maximum-container-assignments”=“10”\r\n{noformat}\r\nAt the mean time, [~cyfdecyf] is trying to apply this config changes to their cluster to see if that helps, and I am trying to reproduce this issue locally.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cheersyang","name":"cheersyang","key":"cheersyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cheersyang&avatarId=23772","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cheersyang&avatarId=23772","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cheersyang&avatarId=23772","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cheersyang&avatarId=23772"},"displayName":"Weiwei Yang","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-21T02:24:25.945+0000","updated":"2018-08-21T02:24:25.945+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16586854","id":"16586854","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"body":"Interesting, [~cheersyang], \r\n\r\nI can only think about reservation allocation causes the issue, but given we already have logic below, it should not happen:\r\n\r\n{code} \r\n    // And it should not be a reserved container\r\n    if (assignment.getAssignmentInformation().getNumReservations() > 0) {\r\n      return false;\r\n    }\r\n{code} \r\n\r\nWe should be able to see what kind of allocation causes the issue, or is it possible that CSAssignment indicate allocation happens but actually it doesn't.\r\n\r\nWhat is the {{maximum-container-assignments}} settings now?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-21T03:09:36.285+0000","updated":"2018-08-21T03:37:08.258+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16588238","id":"16588238","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"body":"[~leftnoteasy] My original config did not have the two config options specified so should be using the default values.\r\n\r\nCurrently I have applied the configuration suggested by [~cheersyang], so maximum-container-assignments is 10 now.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cyfdecyf","name":"cyfdecyf","key":"cyfdecyf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Yufei","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-08-22T01:57:36.155+0000","updated":"2018-08-22T01:57:36.155+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16591337","id":"16591337","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"body":"I also check the logic for printing out of the error \"Failed to accept allocation proposal\".\r\n\r\nallocateContainersToNode\r\n\t-- if canAllocateMore // (decided by yarn.scheduler.capacity.per-node-heartbeat.maximum-container-assignments)\r\n\t\t-- allocateContainersToNode\r\n\t\t -- allocateContainerOnSingleNode\r\n\t\t  -- allocateOrReserveNewContainers\r\n\t\t   -- submitResourceCommitRequest\r\n\t\t     -- tryCommit\r\n\t\t       -- LOG.info(\"Failed to accept allocation proposal\");\r\n\r\n\r\n\r\nIt seems this error is indeed caused by infinit number of of container requests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-24T08:42:41.962+0000","updated":"2018-08-24T08:42:41.962+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16591878","id":"16591878","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"body":"[~hustnn], what is the cause of \"Failed to accept allocation proposal\"? You should be able to get this from DEBUG log. Once we understand why allocation proposal got rejected, we can understand why such loop happens.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-24T16:37:18.594+0000","updated":"2018-08-24T16:37:18.594+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16593141","id":"16593141","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"body":"OK. I will try.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-27T02:46:53.196+0000","updated":"2018-08-27T02:46:53.196+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16601171","id":"16601171","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Card","name":"Card","key":"card","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34058","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"},"displayName":"Janus","active":true,"timeZone":"America/New_York"},"body":"[~leftnoteasy] Can reproduce as follow:\r\n\r\n1.Specify 2 queues:\r\n    - queue1:\r\n        - capacity: 68\r\n        - maximum-capacity: 100\r\n    - queue2:\r\n        - capacity: 32\r\n        - maximum-capacity: 60\r\n2. Submit a big spark job, make sure this job use resource higher than queue's capacity(68%), like 90% of the cluster's resource.\r\n    pyspark --num-executor=100 --executor-memory=20g --queue=queue1\r\n3. Submit another big spark job. \r\n    pyspark --num-executor=100 --executor-memory=20g --queue=queue2\r\n4.Then the resource manager log will show the \"Failed to accept allocation proposal\".\r\n\r\nAnd if I change the maximum-capacity of queue2 to 100 or -1, the log stops flushing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Card","name":"Card","key":"card","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34058","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"},"displayName":"Janus","active":true,"timeZone":"America/New_York"},"created":"2018-09-02T10:26:18.358+0000","updated":"2018-09-02T10:26:18.358+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16601223","id":"16601223","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"body":"Thanks. I will try it tomorrow. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"created":"2018-09-02T14:06:43.935+0000","updated":"2018-09-02T14:06:57.131+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16602323","id":"16602323","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"body":"Interesting, it must be caused by CS allocation doesn't fully consider queue maximum resource in some cases. Tried to look at related code, hasn't figured out root case yet. \r\n\r\nCS allocation phase relies on the logic of ResourceLimits passed by upper level component (Parent of queues, queue of apps, etc.). Under some corner cases, the ResourceLimits passed in could be larger than accurate. \r\n\r\n[~Card], could u enable DEBUG log of org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity, and rerun the test? (or you can click \"dump DEBUG log\" in CS web UI) It gonna be helpful if you can get a few seconds DEBUG log for our troubleshooting when the infinite loop happens.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-09-03T16:40:34.685+0000","updated":"2018-09-03T16:40:34.685+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16602326","id":"16602326","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"body":"And btw, I found a comment in LeafQueue:\r\n{code:java}\r\nprivate void updateCurrentResourceLimits(\r\n    ResourceLimits currentResourceLimits, Resource clusterResource) {\r\n  // TODO: need consider non-empty node labels when resource limits supports\r\n  // node labels\r\n  // Even if ParentQueue will set limits respect child's max queue capacity,\r\n  // but when allocating reserved container, CapacityScheduler doesn't do\r\n  // this. So need cap limits by queue's max capacity here.\r\n  this.cachedResourceLimitsForHeadroom =\r\n      new ResourceLimits(currentResourceLimits.getLimit());\r\n  Resource queueMaxResource = getEffectiveMaxCapacityDown(\r\n      RMNodeLabelsManager.NO_LABEL, minimumAllocation);\r\n  this.cachedResourceLimitsForHeadroom.setLimit(Resources.min(\r\n      resourceCalculator, clusterResource, queueMaxResource,\r\n      currentResourceLimits.getLimit()));\r\n}{code}\r\nI can remember a little bit when I wrote the code: YARN-3243 fixed an issue which ParentQueue's max capacity could be violated. I didn't consider node label max capacity because at that time per-queue per-label capacities support has some issues. I believe the issue should be fixed in later patches, but it is worth to check if we need any other fixes. \r\n\r\n[~Card], does this happen when node label is being used or not? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-09-03T16:46:52.327+0000","updated":"2018-09-03T16:46:52.327+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16602507","id":"16602507","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"body":"I also tested. This happens when node label is not used. Is it caused by the resource request which is already rejected but still recovered? So it never ends.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"created":"2018-09-04T00:41:48.026+0000","updated":"2018-09-04T01:31:03.573+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13171211/comment/16602522","id":"16602522","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"body":"Debug dump:\r\n{code:java}\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Processing xxx-test-cluster04:45454 of type STATUS_UPDATE\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.event.AsyncDispatcher: Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: nodeUpdate: xxx-test-cluster04:45454 cluster capacity: <memory:1351680, vCores:240>\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Node being looked for scheduling xxx-test-cluster04:45454 availableResource: <memory:82944, vCores:77>\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Trying to schedule on node: xxx-test-cluster04, available: <memory:82944, vCores:77>\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Trying to assign containers to child-queue of root\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: Check assign to queue: root nodePartition: , usedResources: <memory:1095680, vCores:8>, clusterResources: <memory:1351680, vCores:240>, currentUsedCapacity: 0.81060606, max-capacity: 1.0\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: printChildQueues - queue: root child-queues: root.dwusedCapacity=(1.1842697),  label=(*)root.devusedCapacity=(0.016571993),  label=(*)\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Trying to assign to queue: root.dev stats: dev: capacity=0.32, absoluteCapacity=0.32, usedResources=<memory:7168, vCores:1>, usedCapacity=0.016571993, absoluteUsedCapacity=0.0053030304, numApps=1, numContainers=1\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignContainers: partition= #applications=1\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: Check assign to queue: dev nodePartition: , usedResources: <memory:7168, vCores:1>, clusterResources: <memory:1351680, vCores:240>, currentUsedCapacity: 0.0053030304, max-capacity: 0.6\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: userLimit is fetched. userLimit=<memory:432640, vCores:77>, userSpecificUserLimit=<memory:432640, vCores:77>, schedulingMode=RESPECT_PARTITION_EXCLUSIVITY, partition=\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Headroom calculation for user work:  userLimit=<memory:432640, vCores:77> queueMaxAvailRes=<memory:811008, vCores:144> consumed=<memory:7168, vCores:1> partition=\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: pre-assignContainers for application application_1535930391687_0019\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: userLimit is fetched. userLimit=<memory:432640, vCores:77>, userSpecificUserLimit=<memory:432640, vCores:77>, schedulingMode=RESPECT_PARTITION_EXCLUSIVITY, partition=\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: showRequests: application=application_1535930391687_0019 headRoom=<memory:425472, vCores:76> currentConsumption=7168\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalitySchedulingPlacementSet:         Request={AllocationRequestId: 0, Priority: 1, Capability: <memory:360448, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: null, Node Label Expression: }\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: assignContainers: node=xxx-test-cluster04 application=application_1535930391687_0019 priority=1 pendingAsk=<per-allocation-resource=<memory:360448, vCores:2>,repeat=3> type=OFF_SWITCH\r\n2018-09-03 11:44:11,175 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: Reserved container  application=application_1535930391687_0019 resource=<memory:360448, vCores:2> queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@65ed660 cluster=<memory:1351680, vCores:240>\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: post-assignContainers for application application_1535930391687_0019\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: userLimit is fetched. userLimit=<memory:432640, vCores:77>, userSpecificUserLimit=<memory:432640, vCores:77>, schedulingMode=RESPECT_PARTITION_EXCLUSIVITY, partition=\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: showRequests: application=application_1535930391687_0019 headRoom=<memory:425472, vCores:76> currentConsumption=7168\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalitySchedulingPlacementSet:         Request={AllocationRequestId: 0, Priority: 1, Capability: <memory:360448, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: null, Node Label Expression: }\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Assigned to queue: root.dev stats: dev: capacity=0.32, absoluteCapacity=0.32, usedResources=<memory:7168, vCores:1>, usedCapacity=0.016571993, absoluteUsedCapacity=0.0053030304, numApps=1, numContainers=1 --> <memory:360448, vCores:2>, OFF_SWITCH\r\n2018-09-03 11:44:11,175 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.81060606 absoluteUsedCapacity=0.81060606 used=<memory:1095680, vCores:8> cluster=<memory:1351680, vCores:240>\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: ParentQ=root assignedSoFarInThisIteration=<memory:360448, vCores:2> usedCapacity=0.81060606 absoluteUsedCapacity=0.81060606\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Try to commit allocation proposal=New org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest:\r\n         RESERVED=[(Application=appattempt_1535930391687_0019_000001; Node=xxx-test-cluster04:45454; Resource=<memory:360448, vCores:2>)]\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: userLimit is fetched. userLimit=<memory:432640, vCores:77>, userSpecificUserLimit=<memory:432640, vCores:77>, schedulingMode=RESPECT_PARTITION_EXCLUSIVITY, partition=\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Headroom calculation for user work:  userLimit=<memory:432640, vCores:77> queueMaxAvailRes=<memory:811008, vCores:144> consumed=<memory:7168, vCores:1> partition=\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: Used resource=<memory:1095680, vCores:8> exceeded maxResourceLimit of the queue =<memory:1351680, vCores:240>\r\n2018-09-03 11:44:11,175 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Failed to accept allocation proposal\r\n2018-09-03 11:44:11,175 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Assigned maximum number of off-switch containers: 1, assignments so far: resource:<memory:360448, vCores:2>; type:OFF_SWITCH; excessReservation:null; applicationid:null; skipped:NONE; fulfilled reservation:false; allocations(count/resource):0/<memory:0, vCores:0>; reservations(count/resource):1/<memory:360448, vCores:2>\r\n2018-09-03 11:44:11,287 DEBUG org.apache.hadoop.ipc.Server:  got #68890\r\n2018-09-03 11:44:11,287 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 30 on 8031: Call#68890 Retry#0 org.apache.hadoop.yarn.server.api.ResourceTrackerPB.nodeHeartbeat from 10.65.205.151:60900 for RpcKind RPC_PROTOCOL_BUFFER\r\n2018-09-03 11:44:11,287 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:work (auth:SIMPLE) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.ipc.Server: Served: nodeHeartbeat, queueTime= 1 procesingTime= 0\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.event.AsyncDispatcher: Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Processing xxx-test-cluster03:45454 of type STATUS_UPDATE\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 30 on 8031: responding to Call#68890 Retry#0 org.apache.hadoop.yarn.server.api.ResourceTrackerPB.nodeHeartbeat from 10.65.205.151:60900\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.event.AsyncDispatcher: Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 30 on 8031: responding to Call#68890 Retry#0 org.apache.hadoop.yarn.server.api.ResourceTrackerPB.nodeHeartbeat from 10.65.205.151:60900 Wrote 42 bytes.\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: nodeUpdate: xxx-test-cluster03:45454 cluster capacity: <memory:1351680, vCores:240>\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Node being looked for scheduling xxx-test-cluster03:45454 availableResource: <memory:90112, vCores:78>\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Trying to schedule on node: xxx-test-cluster03, available: <memory:90112, vCores:78>\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Trying to assign containers to child-queue of root\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: Check assign to queue: root nodePartition: , usedResources: <memory:1095680, vCores:8>, clusterResources: <memory:1351680, vCores:240>, currentUsedCapacity: 0.81060606, max-capacity: 1.0\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: printChildQueues - queue: root child-queues: root.dwusedCapacity=(1.1842697),  label=(*)root.devusedCapacity=(0.016571993),  label=(*)\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Trying to assign to queue: root.dev stats: dev: capacity=0.32, absoluteCapacity=0.32, usedResources=<memory:7168, vCores:1>, usedCapacity=0.016571993, absoluteUsedCapacity=0.0053030304, numApps=1, numContainers=1\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignContainers: partition= #applications=1\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: Check assign to queue: dev nodePartition: , usedResources: <memory:7168, vCores:1>, clusterResources: <memory:1351680, vCores:240>, currentUsedCapacity: 0.0053030304, max-capacity: 0.6\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: userLimit is fetched. userLimit=<memory:432640, vCores:77>, userSpecificUserLimit=<memory:432640, vCores:77>, schedulingMode=RESPECT_PARTITION_EXCLUSIVITY, partition=\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Headroom calculation for user work:  userLimit=<memory:432640, vCores:77> queueMaxAvailRes=<memory:811008, vCores:144> consumed=<memory:7168, vCores:1> partition=\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: pre-assignContainers for application application_1535930391687_0019\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: userLimit is fetched. userLimit=<memory:432640, vCores:77>, userSpecificUserLimit=<memory:432640, vCores:77>, schedulingMode=RESPECT_PARTITION_EXCLUSIVITY, partition=\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: showRequests: application=application_1535930391687_0019 headRoom=<memory:425472, vCores:76> currentConsumption=7168\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalitySchedulingPlacementSet:         Request={AllocationRequestId: 0, Priority: 1, Capability: <memory:360448, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: null, Node Label Expression: }\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: assignContainers: node=xxx-test-cluster03 application=application_1535930391687_0019 priority=1 pendingAsk=<per-allocation-resource=<memory:360448, vCores:2>,repeat=3> type=OFF_SWITCH\r\n2018-09-03 11:44:11,288 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: Reserved container  application=application_1535930391687_0019 resource=<memory:360448, vCores:2> queue=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator@65ed660 cluster=<memory:1351680, vCores:240>\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: post-assignContainers for application application_1535930391687_0019\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: userLimit is fetched. userLimit=<memory:432640, vCores:77>, userSpecificUserLimit=<memory:432640, vCores:77>, schedulingMode=RESPECT_PARTITION_EXCLUSIVITY, partition=\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: showRequests: application=application_1535930391687_0019 headRoom=<memory:425472, vCores:76> currentConsumption=7168\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalitySchedulingPlacementSet:         Request={AllocationRequestId: 0, Priority: 1, Capability: <memory:360448, vCores:2>, # Containers: 3, Location: *, Relax Locality: true, Execution Type Request: null, Node Label Expression: }\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Assigned to queue: root.dev stats: dev: capacity=0.32, absoluteCapacity=0.32, usedResources=<memory:7168, vCores:1>, usedCapacity=0.016571993, absoluteUsedCapacity=0.0053030304, numApps=1, numContainers=1 --> <memory:360448, vCores:2>, OFF_SWITCH\r\n2018-09-03 11:44:11,288 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.81060606 absoluteUsedCapacity=0.81060606 used=<memory:1095680, vCores:8> cluster=<memory:1351680, vCores:240>\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: ParentQ=root assignedSoFarInThisIteration=<memory:360448, vCores:2> usedCapacity=0.81060606 absoluteUsedCapacity=0.81060606\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Try to commit allocation proposal=New org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest:\r\n         RESERVED=[(Application=appattempt_1535930391687_0019_000001; Node=xxx-test-cluster03:45454; Resource=<memory:360448, vCores:2>)]\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: userLimit is fetched. userLimit=<memory:432640, vCores:77>, userSpecificUserLimit=<memory:432640, vCores:77>, schedulingMode=RESPECT_PARTITION_EXCLUSIVITY, partition=\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Headroom calculation for user work:  userLimit=<memory:432640, vCores:77> queueMaxAvailRes=<memory:811008, vCores:144> consumed=<memory:7168, vCores:1> partition=\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: Used resource=<memory:1095680, vCores:8> exceeded maxResourceLimit of the queue =<memory:1351680, vCores:240>\r\n2018-09-03 11:44:11,288 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Failed to accept allocation proposal\r\n2018-09-03 11:44:11,288 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Assigned maximum number of off-switch containers: 1, assignments so far: resource:<memory:360448, vCores:2>; type:OFF_SWITCH; excessReservation:null; applicationid:null; skipped:NONE; fulfilled reservation:false; allocations(count/resource):0/<memory:0, vCores:0>; reservations(count/resource):1/<memory:360448, vCores:2>\r\n2018-09-03 11:44:11,700 DEBUG org.apache.hadoop.ipc.Server:  got #440\r\n2018-09-03 11:44:11,700 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8032: Call#440 Retry#0 org.apache.hadoop.yarn.api.ApplicationClientProtocolPB.getApplicationReport from 10.65.205.148:48970 for RpcKind RPC_PROTOCOL_BUFFER\r\n2018-09-03 11:44:11,700 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:work (auth:SIMPLE) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)\r\n2018-09-03 11:44:11,700 DEBUG org.apache.hadoop.yarn.server.security.ApplicationACLsManager: Verifying access-type VIEW_APP for work (auth:SIMPLE) on application application_1535930391687_0019 owned by work\r\n2018-09-03 11:44:11,701 DEBUG org.apache.hadoop.ipc.Server: Served: getApplicationReport, queueTime= 0 procesingTime= 1\r\n2018-09-03 11:44:11,701 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8032: responding to Call#440 Retry#0 org.apache.hadoop.yarn.api.ApplicationClientProtocolPB.getApplicationReport from 10.65.205.148:48970\r\n2018-09-03 11:44:11,701 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 2 on 8032: responding to Call#440 Retry#0 org.apache.hadoop.yarn.api.ApplicationClientProtocolPB.getApplicationReport from 10.65.205.148:48970 Wrote 358 bytes.\r\n2018-09-03 11:44:11,989 DEBUG org.apache.hadoop.ipc.Server:  got #3118\r\n2018-09-03 11:44:11,990 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 27 on 8032: Call#3118 Retry#0 org.apache.hadoop.yarn.api.ApplicationClientProtocolPB.getApplicationReport from 10.65.205.148:48370 for RpcKind RPC_PROTOCOL_BUFFER\r\n2018-09-03 11:44:11,990 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:work (auth:SIMPLE) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)\r\n2018-09-03 11:44:11,990 DEBUG org.apache.hadoop.yarn.server.security.ApplicationACLsManager: Verifying access-type VIEW_APP for work (auth:SIMPLE) on application application_1535930391687_0012 owned by work\r\n2018-09-03 11:44:11,990 DEBUG org.apache.hadoop.ipc.Server: Served: getApplicationReport, queueTime= 1 procesingTime= 0\r\n2018-09-03 11:44:11,990 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 27 on 8032: responding to Call#3118 Retry#0 org.apache.hadoop.yarn.api.ApplicationClientProtocolPB.getApplicationReport from 10.65.205.148:48370\r\n2018-09-03 11:44:11,990 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 27 on 8032: responding to Call#3118 Retry#0 org.apache.hadoop.yarn.api.ApplicationClientProtocolPB.getApplicationReport from 10.65.205.148:48370 Wrote 361 bytes.\r\n2018-09-03 11:44:12,005 DEBUG org.apache.hadoop.ipc.Server:  got #502725\r\n2018-09-03 11:44:12,005 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8031: Call#502725 Retry#0 org.apache.hadoop.yarn.server.api.ResourceTrackerPB.nodeHeartbeat from 10.65.205.150:38836 for RpcKind RPC_PROTOCOL_BUFFER\r\n2018-09-03 11:44:12,005 DEBUG org.apache.hadoop.security.UserGroupInformation: PrivilegedAction as:work (auth:SIMPLE) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.ipc.Server: Served: nodeHeartbeat, queueTime= 1 procesingTime= 0\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8031: responding to Call#502725 Retry#0 org.apache.hadoop.yarn.server.api.ResourceTrackerPB.nodeHeartbeat from 10.65.205.150:38836\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.yarn.event.AsyncDispatcher: Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.ipc.Server: IPC Server handler 31 on 8031: responding to Call#502725 Retry#0 org.apache.hadoop.yarn.server.api.ResourceTrackerPB.nodeHeartbeat from 10.65.205.150:38836 Wrote 42 bytes.\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Processing xxx-test-cluster02:45454 of type STATUS_UPDATE\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.yarn.event.AsyncDispatcher: Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: nodeUpdate: xxx-test-cluster02:45454 cluster capacity: <memory:1351680, vCores:240>\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Node being looked for scheduling xxx-test-cluster02:45454 availableResource: <memory:82944, vCores:77>\r\n2018-09-03 11:44:12,006 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Trying to schedule on node: xxx-test-cluster02, available: <memory:82944, vCores:77>{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hustnn","name":"hustnn","key":"hustnn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"niu","active":true,"timeZone":"Etc/UTC"},"created":"2018-09-04T01:30:05.108+0000","updated":"2018-09-04T02:25:54.257+0000"}],"maxResults":27,"total":27,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-8513/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3vqhj:"}}