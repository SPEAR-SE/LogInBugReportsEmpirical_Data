{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13087646","self":"https://issues.apache.org/jira/rest/api/2/issue/13087646","key":"YARN-6831","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-07-17T21:35:47.509+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jul 18 17:11:04 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-6831/watchers","watchCount":2,"isWatching":false},"created":"2017-07-17T17:29:17.679+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=haibochen","name":"haibochen","key":"haibochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haibo Chen","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-03-20T18:18:46.424+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319323","id":"12319323","name":"nodemanager"}],"timeoriginalestimate":null,"description":"While reviewing YARN-6706, Karthik pointed out a few issues for improvment in ContainerScheduler\n\n*Make ResourceUtilizationTracker pluggable. That way, we could use a different tracker when oversubscription is enabled.\n\n*ContainerScheduler\n  ##Why do we need maxOppQueueLength given queuingLimit?\n  ##Is there value in splitting runningContainers into runningGuaranteed and runningOpportunistic?\n  ##getOpportunisticContainersStatus method implementation feels awkward. How about capturing the state in the field here, and have metrics etc. pull from here?\n  ##startContainersFromQueue: Local variable resourcesAvailable is unnecessary\n\n*OpportunisticContainersStatus\n  ##Let us clearly differentiate between allocated, used and utilized. Maybe, we should rename current Used methods to Allocated?\n  ##I prefer either full name Opportunistic (in method) or Opp (shortest name that makes sense). Opport is neither short nor fully descriptive.\n  ##Have we considered folding ContainerQueuingLimit class into this?\n\nWe decided to move the issues into this follow up jira to keep YARN-6706 moving forward to unblock oversubscription work.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12342758","id":"12342758","name":"3.2.0","archived":false,"released":false}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Miscellaneous refactoring changes of ContainScheduler ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=haibochen","name":"haibochen","key":"haibochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haibo Chen","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[{"id":"13087784","key":"YARN-6835","self":"https://issues.apache.org/jira/rest/api/2/issue/13087784","fields":{"summary":"Remove runningContainers from ContainerScheduler","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=haibochen","name":"haibochen","key":"haibochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haibo Chen","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13087646/comment/16090609","id":"16090609","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=asuresh","name":"asuresh","key":"asuresh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun Suresh","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for raising this [~haibochen] / [~kasha]. Some thoughts:\n\nbq. Why do we need maxOppQueueLength given queuingLimit?\nSo, maxOppQueueLength is more like an *active* limit. The CS (ContainerScheduler) will not admit any more containers than that value. While the queuingLimit is more *reactive* and dynamically calculated by the RM and passed down to the NM in a HB response. The RM constantly calculates the mean/median of the queueLengths on all nodes and it tells the NM to shed containers from the queue if it is too high. I agree that the *maxOppQueueLength* can probably be removed though. But given your observation in YARN-6706 that test cases depends on this, my opinion is that we will keep it, and put a very high value by default - and mark it as VisibileForTesting only.\n\nbq. Is there value in splitting runningContainers into runningGuaranteed and runningOpportunistic ?\nHmm… I was actually thinking of removing the *runningContainers* itself. It was introduced to keep track of all running containers (containers whose state is running) AND those that have been scheduled but not yet running. I think it may be better to encapsulate that as a proper container state, something like *SCHEDULED_TO_RUN* via a proper transition.\nAdding more data structures might be problematic later on, since we can hit minor race conditions when transferring containers from runningGuaranteed to running Opportunistic (during promotion) and vice-versa (during demotion) if we are not careful about synchronization etc. Also, given the fact that a NM will not run more than say a couple of 100 containers, it might be better to just iterate over all the containers when the scheduler needs to make a decision.\nAnother problem with keeping a separate map is during NM recovery, we have to populate this specifically. we don’t do that for running containers now either – but I was thinking if we removed the *runningContainers* map, we wont have to (we already have a state called *QUEUED* in the NMStateStore which can be used to set the correct state in the recovered container)\n\nbq. getOpportunisticContainersStatus method implementation feels awkward..\nKind of agree with you there, don’t recall exactly why we did it like that… think it was to not have to create a new instance of the status at every heart beat. \n\nbq. Have we considered folding ContainerQueuingLimit class into this\nMy first instinct is to keep it separate. Don’t think we should mix the Queuing aspect of the Container Scheduler with the ExecutionType aspect. Also, one is part of the NM heartbeat request and the other comes back as response.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=asuresh","name":"asuresh","key":"asuresh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun Suresh","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-17T21:35:47.509+0000","updated":"2017-07-17T22:33:35.758+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13087646/comment/16090832","id":"16090832","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=asuresh","name":"asuresh","key":"asuresh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun Suresh","active":true,"timeZone":"America/Los_Angeles"},"body":"Do take a look at YARN-6835 where i've posted an initial patch removing *runningContainers*","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=asuresh","name":"asuresh","key":"asuresh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun Suresh","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-18T00:01:54.905+0000","updated":"2017-07-18T00:01:54.905+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13087646/comment/16091823","id":"16091823","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=asuresh","name":"asuresh","key":"asuresh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun Suresh","active":true,"timeZone":"America/Los_Angeles"},"body":"I was thinking about removing *maxOppQueueLength* which led me to think about the following.\nIn YARN-5972, we are trying to get the NM to pause an opportunistic container instead of killing it. Both cgroup freezer and windows job objects implement freezing in the following way:\nWhen a process is frozen, it's cpu share is reduced to 0 and its working set remains in memory as long as there is no external memory pressure. If the OS can't keep the frozen process in memory, it's memory is swapped out to disk and restored when the process is thawed. This implies that the number of paused containers is limited to the total swap space on the NM. This should be another local NM config, maybe something like *maxConsumedOpportunisticResources* which places an additional limit on number of running opportunistic containers.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=asuresh","name":"asuresh","key":"asuresh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arun Suresh","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-18T17:11:04.625+0000","updated":"2017-07-18T17:11:04.625+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-6831/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3hlxr:"}}