{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13023311","self":"https://issues.apache.org/jira/rest/api/2/issue/13023311","key":"YARN-5936","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-11-29T18:32:51.373+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Nov 13 19:13:17 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":0,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-5936/watchers","watchCount":14,"isWatching":false},"created":"2016-11-25T06:55:51.584+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":60,"aggregatetimeoriginalestimate":60,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12331976","id":"12331976","description":"2.7.1 release","name":"2.7.1","archived":false,"released":true,"releaseDate":"2015-07-06"}],"issuelinks":[{"id":"12498971","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12498971","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"inwardIssue":{"id":"13058790","key":"YARN-6384","self":"https://issues.apache.org/jira/rest/api/2/issue/13058790","fields":{"summary":"Add configuration property to increase max CPU usage when strict-resource-usage is true with cgroups","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-11-13T19:13:17.104+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319323","id":"12319323","name":"nodemanager"}],"timeoriginalestimate":60,"description":"When using LinuxContainer, the setting that \"yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage\" is true could assure scheduling fairness with the cpu bandwith of cgroup. But the cpu bandwidth of cgroup would lead to bad performance in our experience. \n    Without cpu bandwidth of cgroup, cpu.share of cgroup is our only way to assure scheduling fairness, but it is not completely effective. For example, There are two container that have same vcore(means same cpu.share), one container is single-threaded, the other container is multi-thread. the multi-thread will have more CPU time, It's unreasonable!\n    Here is my test case, I submit two distributedshell application. And two commmand are below:\n{code}\nhadoop jar share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar org.apache.hadoop.yarn.applications.distributedshell.Client -jar share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar -shell_script ./run.sh  -shell_args 10 -num_containers 1 -container_memory 1024 -container_vcores 1 -master_memory 1024 -master_vcores 1 -priority 10\n\nhadoop jar share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar org.apache.hadoop.yarn.applications.distributedshell.Client -jar share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar -shell_script ./run.sh  -shell_args 1  -num_containers 1 -container_memory 1024 -container_vcores 1 -master_memory 1024 -master_vcores 1 -priority 10\n{code}\n     here show the cpu time of the two container:\n{code}\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n15448 yarn      20   0 9059592  28336   9180 S 998.7  0.1  24:09.30 java\n15026 yarn      20   0 9050340  27480   9188 S 100.0  0.1   3:33.97 java\n13767 yarn      20   0 1799816 381208  18528 S   4.6  1.2   0:30.55 java\n   77 root      rt   0       0      0      0 S   0.3  0.0   0:00.74 migration/1   \n{code}\n    We find the cpu time of Muliti-Thread are ten times than the cpu time of Single-Thread, though the two container have same cpu.share.\n\nnotes:\nrun.sh\n{code} \n\tjava -cp /home/yarn/loop.jar:$CLASSPATH loop.loop $1\t\n{code} \nloop.java\n{code} \npackage loop;\npublic class loop {\n\tpublic static void main(String[] args) {\n\t\t// TODO Auto-generated method stub\n\t\tint loop = 1;\n\t\tif(args.length>=1) {\n\t\t\tSystem.out.println(args[0]);\n\t\t\tloop = Integer.parseInt(args[0]);\n\t\t}\n\t\tfor(int i=0;i<loop;i++){\n\t\t\tSystem.out.println(\"start thread \" + i);\n\t\t\tnew Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\t// TODO Auto-generated method stub\n\t\t\t\t\tint j=0;\n\t\t\t\t\twhile(true){j++;}\n\t\t\t\t}\n\t\t\t}).start();\n\t\t}\n\t}\n}\n{code}","customfield_10010":null,"timetracking":{"originalEstimate":"1m","remainingEstimate":"1m","originalEstimateSeconds":60,"remainingEstimateSeconds":60},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12331976","id":"12331976","description":"2.7.1 release","name":"2.7.1","archived":false,"released":true,"releaseDate":"2015-07-06"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":60,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"when cpu strict mode is closed, yarn couldn't assure scheduling fairness between containers","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":60,"percent":0},"customfield_12311024":null,"environment":"CentOS7.1","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":60,"percent":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15706129","id":"15706129","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you for reporting this issue [~zhengchenyu]!\nWhen you mentioned \"But the cpu bandwidth of cgroup would lead to bad performance in our experience.\", do you mean that it is due to the design that it limits the CPU usage to the vCore share affecting overall utilization, or do you mean that the container got less resources than what was assigned to it? In other words, is this a remark of the strict CPU cgroup design or the implementation? Thank you!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-11-29T18:32:51.373+0000","updated":"2016-11-29T18:32:51.373+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15708991","id":"15708991","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cheersyang","name":"cheersyang","key":"cheersyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cheersyang&avatarId=23772","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cheersyang&avatarId=23772","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cheersyang&avatarId=23772","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cheersyang&avatarId=23772"},"displayName":"Weiwei Yang","active":true,"timeZone":"Asia/Shanghai"},"body":"Hello [~zhengchenyu]\n\nHow many cores your system has, can you attach the output of top command that contains each core usages (by type 1 after top)?\n\nIt looks like it was not fair because your system still has plenty of resources, so each container gets whatever it needs to run happily.  It only maintains fairness relatively. \n\nAccording to the [cgroups document|https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-cpu.html]\n\nbq. cpu.shares contains an integer value that specifies a relative share of CPU time available to the tasks in a cgroup ... Note that shares of CPU time are distributed per all CPU cores on multi-core systems. Even if a cgroup is limited to less than 100% of CPU on a multi-core system, it may use 100% of each individual CPU core.\n\nWhat you are asking for (container level strict limit) looks like YARN-810, unfortunately it has been left out there for quite a long time already.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cheersyang","name":"cheersyang","key":"cheersyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cheersyang&avatarId=23772","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cheersyang&avatarId=23772","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cheersyang&avatarId=23772","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cheersyang&avatarId=23772"},"displayName":"Weiwei Yang","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-11-30T16:11:57.227+0000","updated":"2016-11-30T16:11:57.227+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15721247","id":"15721247","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"body":"I think two reason both affect the performance, but I can't evaluate which is the major reason. \nFirst，The linux kernel source code of cpu bandwidth will add too many timer, and add more function to be called. \nSecondly, limit utilization ratio will lead to bad performance.\nClosing the cpu bandwith limit is inevitabe. Here I only wanna to a idea that keep justice when only use cpu share.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"created":"2016-12-05T05:06:23.986+0000","updated":"2016-12-05T05:06:23.986+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15721341","id":"15721341","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"body":"You didn't catch my meaning! In fact, I knew the reason of the unfairness. \nThe processes and threads has the same level of scheduling. In Linux-3.10, task are scheduled by a red–black tree, and update the red–black tree periodically or manually(by other calling). the left-most is next task to be scheduled. The red-black tree is update by this formula:\n    curr->vruntime+=delta_exec*nice_0_load/curr->load.weight\nhere curr->load.weight is just cpu.share. So Mulit Thread obtain more cpu than Single-Thread because of more child thread are participating the scheduler. \n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"created":"2016-12-05T06:05:58.034+0000","updated":"2016-12-05T06:05:58.034+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15755361","id":"15755361","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you, for the reply [~zhengchenyu]!\n{quote}\nFirst，The linux kernel source code of cpu bandwidth will add too many timer, and add more function to be called. \nSecondly, limit utilization ratio will lead to bad performance.\n{quote}\nI did an experiment with he following cpu heavy app:\n{code}\n//a.c\nint main() {\n  int i;\n  int j = 0;\n  for (i = 0; i < 1000000000; ++i) {\n    j++;\n  }\n  return j & 1;\n}\n{code}\nI ran it in parallel in a single cgroup, multiple cgroups and multipre cgroups with CPU throttling enabled on a single CPU.\n{code}\nfor j in `seq 1 10`; do export i=$j;sh -c 'time ./a.out&'; done\nfor j in `seq 1 10`; do export i=$j;sh -c 'echo $$ >/cgroup/cpu/$i/tasks;echo -1 >/cgroup/cpu/$i/cpu.cfs_quota_us;time ./a.out&'; done\nfor j in `seq 1 10`; do export i=$j;sh -c 'echo $$ >/cgroup/cpu/$i/tasks;echo 10000 >/cgroup/cpu/$i/cpu.cfs_quota_us;time ./a.out&'; done\n{code}\nThe runtime in the first case (no cgroups) was 24.7154, the second (no group throttle) was 24.6907 seconds on average, the runtime in the latter case was 24.7469 respectively.\nThe difference less than 0.25% in these cases. I ran it a few more times and I received very similar numbers.\nThis means to me that what you are seeing is the utilization drop, if the container group limits the CPU usage and not an inefficiency in the Linux kernel.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-16T19:52:54.485+0000","updated":"2016-12-16T19:52:54.485+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15760010","id":"15760010","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"body":"Because your program is a single-thread. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"created":"2016-12-19T03:15:34.857+0000","updated":"2016-12-19T03:15:34.857+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15761877","id":"15761877","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"body":"I run 10 of them in parallel.\n{quote}\nFirst，The linux kernel source code of cpu bandwidth will add too many timer, and add more function to be called. \n{quote}\nAre you saying you see this only if an app is running multiple threads and not multiple processes?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-19T18:13:56.755+0000","updated":"2016-12-19T18:13:56.755+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15762998","id":"15762998","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"body":"Indeed, I see some 6% performance loss in the second case above compared to the first. This is when I move away from the root cgroup and use the hierarchy but do not use the limit, yet. This happens, when I am running 10 processes in different cgroups with 100 threads each.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-20T02:33:06.744+0000","updated":"2016-12-20T02:33:06.744+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15763020","id":"15763020","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"body":"But your \"time\" command is only related to it's own program, every program is single thread. \nMy question is below:\n     two process that has different numbers of threads has different ability of schedule, though they have the same cpu.share. \nI know the reason, but I don't have a proper suggestion to avoid this problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"created":"2016-12-20T02:42:31.368+0000","updated":"2016-12-20T02:42:31.368+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15763042","id":"15763042","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"body":"I don't care the performance loss of cpu bandwith, because I know it must happen. I only wanna know the method of keep fair without cpu bandwith.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"created":"2016-12-20T02:55:01.490+0000","updated":"2016-12-20T02:55:01.490+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/15806611","id":"15806611","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"body":"In the latest test I used 100 threads per program, I just did not share the code. They run in parallel, so the sum of time command results measures, whether the whole set spent time in additional CPU cycles other than the activity loop. The reason I checked, is to ask whether you like a solution that uses {{cpu.cfs_quota_us}}.\nI could imagine a dynamic cfs algorithm like the following.\nA timer callback with a certain period could do:\n{code}\nif CPU is saturated\n  for each container\n    if previous usage > fair share\n      limit to fair share\nelse\n  release all limits\n{code}\nIt has drawbacks. It only works with saturated CPU, when not much time is spent waiting on I/O. It has a delay, since it works on historic data. This means also that it adds some utilization loss, which can be larger with multiple cores. On the other hand, it provides the requested fairness, when the CPU is saturated.\nDoes your node have multiple cores? The algorithm may not help much in that case. For example there are 8 cores. One container runs 8 threads, one container runs 2 threads. The fair share requested is 50%-50%. Without throttling the two containers will share 80%-20%. Even, if we set the fair share by throttling, when the cores are saturated, the usage will be 50%/25% when the quota is applied, so there is a utilization loss for a period. Now then, the algorithm may get more complicated...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-01-07T03:13:08.183+0000","updated":"2017-01-07T03:13:08.183+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/16248592","id":"16248592","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"body":"We open cpu strict mode, because  the container can't use the free resource. But we close cpu strict mode, it's not fair for every container. If i set the cpu bandwidth of cgroup dynamically, I think this problem is solved.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhengchenyu","name":"zhengchenyu","key":"zhengchenyu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zhengchenyu","active":true,"timeZone":"Asia/Harbin"},"created":"2017-11-11T16:58:07.897+0000","updated":"2017-11-11T16:58:07.897+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13023311/comment/16250021","id":"16250021","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"body":"Another option for the future is the use of the cgroup pids subsystem on newer kernels. The main reason fairness is not enforced in non-strict mode, is that it allows the container to run as many threads with the same cgroup and weight as needed. You can limit the amount of threads with the pids namespace, so that the effective overall container weight becomes weight*pids_limit. The drawback of this approach is that it limits multitasking and the number of launcher processes. The possible ideal value of pids_limit is the <number of cores>/<desired thread count>, so that we do not starve single threaded containers.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miklos.szegedi%40cloudera.com","name":"miklos.szegedi@cloudera.com","key":"miklos.szegedi@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miklos.szegedi%40cloudera.com&avatarId=32342"},"displayName":"Miklos Szegedi","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-13T19:13:17.104+0000","updated":"2017-11-13T19:13:17.104+0000"}],"maxResults":13,"total":13,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-5936/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i36rsn:"}}