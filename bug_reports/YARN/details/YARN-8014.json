{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13143583","self":"https://issues.apache.org/jira/rest/api/2/issue/13143583","key":"YARN-8014","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2018-03-08T15:45:34.119+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Mar 08 16:02:21 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_1459759_*|*_4_*:*_1_*:*_43046_*|*_5_*:*_2_*:*_384680","customfield_12312321":null,"resolutiondate":"2018-03-08T16:02:21.029+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-8014/watchers","watchCount":2,"isWatching":false},"created":"2018-03-08T15:30:53.577+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340356","id":"12340356","name":"2.8.2","archived":false,"released":true,"releaseDate":"2017-10-24"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-03-08T16:02:21.055+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319322","id":"12319322","name":"resourcemanager"}],"timeoriginalestimate":null,"description":"A graceful shutdown & then startup of a NodeManager process using YARN/HDFS v2.8.2 seems to successfully place the Node back into RUNNING state. However, ResouceManager appears to keep the Node also in SHUTDOWN state.\r\n\r\n \r\n\r\n*Steps To Reproduce:*\r\n\r\n1. SSH to host running NodeManager.\r\n 2. Switch-to UserID that NodeManager is running as (hadoop).\r\n 3. Execute cmd: /opt/hadoop/sbin/yarn-daemon.sh stop nodemanager\r\n 4. Wait for NodeManager process to terminate gracefully.\r\n 5. Confirm Node is in SHUTDOWN state via: [http://rb01rm01.local:8088/cluster/nodes]\r\n 6. Execute cmd: /opt/hadoop/sbin/yarn-daemon.sh stop nodemanager\r\n 7. Confirm Node is in RUNNING state via: [http://rb01rm01.local:8088/cluster/nodes]\r\n\r\n \r\n\r\n*Investigation:*\r\n 1. Review contents of ResourceManager + NodeManager log-files:\r\n\r\n+ResourceManager log-[file:+|file:///+]\r\n 2018-03-08 08:15:44,085 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: Node with node id : rb0101.local:43892 has shutdown, hence unregistering the node.\r\n 2018-03-08 08:15:44,092 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Deactivating Node rb0101.local:43892 as it is now SHUTDOWN\r\n 2018-03-08 08:15:44,092 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: rb0101.local:43892 Node Transitioned from RUNNING to SHUTDOWN\r\n 2018-03-08 08:15:44,093 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Removed node rb0101.local:43892 cluster capacity: <memory:110592, vCores:54>\r\n 2018-03-08 08:16:08,915 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node rb0101.local(cmPort: 42627 httpPort: 8042) registered with capability: <memory:12288, vCores:6>, assigned nodeId rb0101.local:42627\r\n 2018-03-08 08:16:08,916 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: rb0101.local:42627 Node Transitioned from NEW to RUNNING\r\n 2018-03-08 08:16:08,916 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Added node rb0101.local:42627 cluster capacity: <memory:122880, vCores:60>\r\n 2018-03-08 08:16:34,826 WARN org.apache.hadoop.ipc.Server: Large response size 2976014 for call Call#428958 Retry#0 org.apache.hadoop.yarn.api.ApplicationClientProtocolPB.getApplications from 192.168.1.100:44034\r\n\r\n \r\n\r\n+NodeManager log-[file:+|file:///+]\r\n 2018-03-08 08:00:14,500 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 10720046250, Total Deleted: 0, Public\r\n Deleted: 0, Private Deleted: 0\r\n 2018-03-08 08:10:14,498 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 10720046250, Total Deleted: 0, Public\r\n Deleted: 0, Private Deleted: 0\r\n 2018-03-08 08:15:44,048 ERROR org.apache.hadoop.yarn.server.nodemanager.NodeManager: RECEIVED SIGNAL 15: SIGTERM\r\n 2018-03-08 08:15:44,101 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Successfully Unregistered the Node rb0101.local:43892 with ResourceManager.\r\n 2018-03-08 08:15:44,114 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042\r\n 2018-03-08 08:15:44,226 INFO org.apache.hadoop.ipc.Server: Stopping server on 43892\r\n 2018-03-08 08:15:44,232 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 43892\r\n 2018-03-08 08:15:44,237 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder\r\n 2018-03-08 08:15:44,239 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService: org.apache.hadoop.yarn.server.nodemanager.containermanager.logag\r\n gregation.LogAggregationService waiting for pending aggregation during exit\r\n 2018-03-08 08:15:44,242 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.Cont\r\n ainersMonitorImpl is interrupted. Exiting.\r\n 2018-03-08 08:15:44,284 INFO org.apache.hadoop.ipc.Server: Stopping server on 8040\r\n 2018-03-08 08:15:44,285 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8040\r\n 2018-03-08 08:15:44,285 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder\r\n 2018-03-08 08:15:44,287 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Public cache exiting\r\n 2018-03-08 08:15:44,289 WARN org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl: org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is interrupted. Exiting.\r\n 2018-03-08 08:15:44,294 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NodeManager metrics system...\r\n 2018-03-08 08:15:44,295 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NodeManager metrics system stopped.\r\n 2018-03-08 08:15:44,296 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NodeManager metrics system shutdown complete.\r\n 2018-03-08 08:15:44,297 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: SHUTDOWN_MSG:\r\n /************************************************************\r\n SHUTDOWN_MSG: Shutting down NodeManager at rb0101.local/192.168.1.101\r\n ************************************************************/\r\n 2018-03-08 08:16:01,905 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: STARTUP_MSG:\r\n /************************************************************\r\n STARTUP_MSG: Starting NodeManager\r\n STARTUP_MSG: user = hadoop\r\n STARTUP_MSG: host = rb0101.local/192.168.1.101\r\n STARTUP_MSG: args = []\r\n STARTUP_MSG: version = 2.8.2\r\n STARTUP_MSG: classpath = blahblahblah (truncated for size-purposes)\r\n STARTUP_MSG: build = Unknown -r Unknown; compiled by 'root' on 2017-09-14T18:22Z\r\n STARTUP_MSG: java = 1.8.0_144\r\n ************************************************************/\r\n 2018-03-08 08:16:01,918 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\r\n 2018-03-08 08:16:03,202 INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the\r\n node health script runner.\r\n 2018-03-08 08:16:03,321 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class\r\n org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\r\n 2018-03-08 08:16:03,322 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for c\r\n lass org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\r\n 2018-03-08 08:16:03,323 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType\r\n for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService\r\n 2018-03-08 08:16:03,323 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apa\r\n che.hadoop.yarn.server.nodemanager.containermanager.AuxServices\r\n 2018-03-08 08:16:03,324 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for\r\n class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\r\n 2018-03-08 08:16:03,324 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType f\r\n or class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\r\n 2018-03-08 08:16:03,347 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.y\r\n arn.server.nodemanager.containermanager.ContainerManagerImpl\r\n 2018-03-08 08:16:03,348 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.s\r\n erver.nodemanager.NodeManager\r\n 2018-03-08 08:16:03,402 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\r\n 2018-03-08 08:16:03,484 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n 2018-03-08 08:16:03,484 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NodeManager metrics system started\r\n 2018-03-08 08:16:03,561 INFO org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl: Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4b8729f\r\n f\r\n 2018-03-08 08:16:03,564 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType f\r\n or class org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService\r\n 2018-03-08 08:16:03,565 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploa\r\n dEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\r\n 2018-03-08 08:16:03,565 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: AMRMProxyService is disabled\r\n 2018-03-08 08:16:03,566 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: per directory file limit = 8192\r\n 2018-03-08 08:16:03,621 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: usercache path : [file:/space/hadoop/tmp/nm-local-dir/usercache_|file:///space/hadoop/tmp/nm-local-dir/usercache_]\r\n DEL_1520518563569\r\n 2018-03-08 08:16:03,667 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : [file:/space/hadoop/tmp/nm-local-dir/usercache_DEL_1520518563569/user1|file:///space/hadoop/tmp/nm-local-dir/usercache_DEL_1520518563569/user1]\r\n 2018-03-08 08:16:03,667 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : [file:/space/hadoop/tmp/nm-local-dir/usercache_DEL_1520518563569/user2|file:///space/hadoop/tmp/nm-local-dir/usercache_DEL_1520518563569/user2]\r\n 2018-03-08 08:16:03,668 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : [file:/space/hadoop/tmp/nm-local-dir/usercache_DEL_1520518563569/user3|file:///space/hadoop/tmp/nm-local-dir/usercache_DEL_1520518563569/user3]\r\n 2018-03-08 08:16:03,681 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting path : [file:/space/hadoop/tmp/nm-local-dir/usercache_DEL_1520518563569/user4|file:///space/hadoop/tmp/nm-local-dir/usercache_DEL_1520518563569/user4]\r\n 2018-03-08 08:16:03,739 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\r\n 2018-03-08 08:16:03,793 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Adding auxiliary service mapreduce_shuffle, \"mapreduce_shuffle\"\r\n 2018-03-08 08:16:03,826 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@1187c9e8\r\n 2018-03-08 08:16:03,826 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Using ResourceCalculatorProcessTree : null\r\n 2018-03-08 08:16:03,827 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Physical memory check enabled: true\r\n 2018-03-08 08:16:03,827 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Virtual memory check enabled: true\r\n 2018-03-08 08:16:03,832 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\r\n 2018-03-08 08:16:03,841 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Nodemanager resources: memory set to 12288MB.\r\n 2018-03-08 08:16:03,841 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Nodemanager resources: vcores set to 6.\r\n 2018-03-08 08:16:03,846 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=12288 virtual-memory=25805 virtual-cores=6\r\n 2018-03-08 08:16:03,850 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor\r\n 2018-03-08 08:16:03,908 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\r\n 2018-03-08 08:16:03,932 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 42627\r\n 2018-03-08 08:16:04,153 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\r\n 2018-03-08 08:16:04,153 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Blocking new container-requests as container manager rpc server is still starting.\r\n 2018-03-08 08:16:04,154 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting\r\n 2018-03-08 08:16:04,154 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 42627: starting\r\n 2018-03-08 08:16:04,166 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Updating node address : rb0101.local:42627\r\n 2018-03-08 08:16:04,183 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\r\n 2018-03-08 08:16:04,184 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8040\r\n 2018-03-08 08:16:04,191 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\r\n 2018-03-08 08:16:04,191 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting\r\n 2018-03-08 08:16:04,191 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8040: starting\r\n 2018-03-08 08:16:04,192 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Localizer started on port 8040\r\n 2018-03-08 08:16:04,312 INFO org.apache.hadoop.mapred.IndexCache: IndexCache created with max memory = 10485760\r\n 2018-03-08 08:16:04,330 INFO org.apache.hadoop.mapred.ShuffleHandler: mapreduce_shuffle listening on port 13562\r\n 2018-03-08 08:16:04,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: ContainerManager started at rb0101.local/192.168.1.101:42627\r\n 2018-03-08 08:16:04,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: ContainerManager bound to 0.0.0.0/0.0.0.0:0\r\n 2018-03-08 08:16:04,340 INFO org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer: Instantiating NMWebApp at 0.0.0.0:8042\r\n 2018-03-08 08:16:04,427 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\r\n 2018-03-08 08:16:04,436 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n 2018-03-08 08:16:04,442 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\r\n 2018-03-08 08:16:04,450 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n 2018-03-08 08:16:04,461 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\r\n 2018-03-08 08:16:04,462 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n 2018-03-08 08:16:04,462 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n 2018-03-08 08:16:04,462 INFO org.apache.hadoop.security.HttpCrossOriginFilterInitializer: CORS filter not enabled. Please set hadoop.http.cross-origin.enabled to 'true' to enable it\r\n 2018-03-08 08:16:04,465 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /node/*\r\n 2018-03-08 08:16:04,465 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*\r\n 2018-03-08 08:16:04,843 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules\r\n 2018-03-08 08:16:04,846 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8042\r\n 2018-03-08 08:16:04,846 INFO org.mortbay.log: jetty-6.1.26\r\n 2018-03-08 08:16:04,877 INFO org.mortbay.log: Extract jar:[file:/opt/hadoop-2.8.2/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar!/webapps/node|file:///opt/hadoop-2.8.2/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar!/webapps/node] to /tmp/Jetty_0_0_0_0_8042_node____19tj0x/webapp\r\n 2018-03-08 08:16:08,355 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:8042\r\n 2018-03-08 08:16:08,356 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app node started at 8042\r\n 2018-03-08 08:16:08,473 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : rb0101.local:42627\r\n 2018-03-08 08:16:08,498 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at rb01rm01.local/192.168.1.100:8031\r\n 2018-03-08 08:16:08,613 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\r\n 2018-03-08 08:16:08,621 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\r\n 2018-03-08 08:16:08,934 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -2086472604\r\n 2018-03-08 08:16:08,938 INFO org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -426187560\r\n 2018-03-08 08:16:08,939 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as rb0101.local:42627 with total resource of <memory:12288, vCores:6>\r\n 2018-03-08 08:16:08,939 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Notifying ContainerManager to unblock new container-requests\r\n 2018-03-08 08:26:04,174 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 0, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0\r\n 2018-03-08 08:36:04,170 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 0, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0\r\n 2018-03-08 08:46:04,170 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 0, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0\r\n\r\n2. Listing all of YARN's Nodes, we can see it was returned to the RUNNING state. However, when listing all nodes, it shows the node in 2 states; RUNNING and SHUTDOWN:\r\n\r\n[hadoop@rb01rm01 logs]$ /opt/hadoop/bin/yarn node -list -all\r\n 18/03/08 09:20:33 INFO client.RMProxy: Connecting to ResourceManager at rb01rm01.local/192.168.1.100:8032\r\n 18/03/08 09:20:34 INFO client.AHSProxy: Connecting to Application History server at rb01rm01.local/192.168.1.100:10200\r\n Total Nodes:11\r\n Node-Id Node-State Node-Http-Address Number-of-Running-Containers\r\n rb0106.local:44160 RUNNING rb0106.local:8042 0\r\n rb0105.local:32832 RUNNING rb0105.local:8042 0\r\n rb0101.local:42627 RUNNING rb0101.local:8042 0\r\n rb0108.local:38209 RUNNING rb0108.local:8042 0\r\n rb0107.local:34306 RUNNING rb0107.local:8042 0\r\n rb0102.local:43063 RUNNING rb0102.local:8042 0\r\n rb0103.local:42374 RUNNING rb0103.local:8042 0\r\n rb0109.local:37455 RUNNING rb0109.local:8042 0\r\n rb0110.local:36690 RUNNING rb0110.local:8042 0\r\n rb0104.local:33268 RUNNING rb0104.local:8042 0\r\n rb0101.local:43892 SHUTDOWN rb0101.local:8042 0\r\n [hadoop@rb01rm01 logs]$\r\n\r\n[hadoop@rb01rm01 logs]$ /opt/hadoop/bin/yarn node -list -states RUNNING\r\n 18/03/08 09:20:55 INFO client.RMProxy: Connecting to ResourceManager at rb01rm01.local/192.168.1.100:8032\r\n 18/03/08 09:20:56 INFO client.AHSProxy: Connecting to Application History server at rb01rm01.local/192.168.1.100:10200\r\n Total Nodes:10\r\n Node-Id Node-State Node-Http-Address Number-of-Running-Containers\r\n rb0106.local:44160 RUNNING rb0106.local:8042 0\r\n rb0105.local:32832 RUNNING rb0105.local:8042 0\r\n rb0101.local:42627 RUNNING rb0101.local:8042 0\r\n rb0108.local:38209 RUNNING rb0108.local:8042 0\r\n rb0107.local:34306 RUNNING rb0107.local:8042 0\r\n rb0102.local:43063 RUNNING rb0102.local:8042 0\r\n rb0103.local:42374 RUNNING rb0103.local:8042 0\r\n rb0109.local:37455 RUNNING rb0109.local:8042 0\r\n rb0110.local:36690 RUNNING rb0110.local:8042 0\r\n rb0104.local:33268 RUNNING rb0104.local:8042 0\r\n [hadoop@rb01rm01 logs]$ /opt/hadoop/bin/yarn node -list -states SHUTDOWN\r\n 18/03/08 09:21:01 INFO client.RMProxy: Connecting to ResourceManager at rb01rm01.local/192.168.1.100:8032\r\n 18/03/08 09:21:01 INFO client.AHSProxy: Connecting to Application History server at rb01rm01.local/192.168.1.100:10200\r\n Total Nodes:0\r\n Node-Id Node-State Node-Http-Address Number-of-Running-Containers\r\n [hadoop@rb01rm01 logs]$\r\n\r\n3. ResourceManager however, does not list Node rb0101.local as SHUTDOWN when specifically requesting list of Nodes in SHUTDOWN state:\r\n\r\n[hadoop@rb01rm01 bin]$ /opt/hadoop/bin/yarn node -list -states SHUTDOWN\r\n 18/03/08 08:28:23 INFO client.RMProxy: Connecting to ResourceManager at rb01rm01.local/v.x.y.z:8032\r\n 18/03/08 08:28:24 INFO client.AHSProxy: Connecting to Application History server at rb01rm01.local/v.x.y.z:10200\r\n Total Nodes:0\r\n Node-Id Node-State Node-Http-Address Number-of-Running-Containers\r\n [hadoop@rb01rm01 bin]$","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"YARN ResourceManager Lists A NodeManager As RUNNING & SHUTDOWN Simultaneously","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Tepsic","name":"Tepsic","key":"tepsic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Tepsic","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Tepsic","name":"Tepsic","key":"tepsic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Tepsic","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13143583/comment/16391417","id":"16391417","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"I believe this is an artifact of the NM appearing to be two separate instances of nodemanagers.  Note that the NM port changed between the two instances.  It originally was rb0101.local:43892 but became rb0101.local:42627 after it was restarted.  That explains why the node shows up twice when listing all nodes.  The RM did not understand that the newly joining NM at port 42627 was supposed to be the same one that was at port 43892.  The RM does not preclude multiple NMs running at the same node, and indeed that's how the mini clusters used for unit tests can run multiple NMs with only one host.\r\n\r\nIt is surprising that the shutdown NM instance does not appear when explicitly asking for nodes in the shutdown state.  I suspect somewhere in the RM's bookkeeping it is dropping the port distinction and the RUNNING instance ends up superceding the SHUTDOWN one for that query.\r\n\r\nSimplest workaround for this is to use a fixed port for the NM.  Then the RM will understand that the new node joining is the same node that left previously.  That also has the benefit of precluding an accidental double-startup of an NM on a node which is not going to go well if not configured intentionally for that scenario.  Both NMs will think they have control of the node's resources and end up using far more resources on the node than intended.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2018-03-08T15:45:34.119+0000","updated":"2018-03-08T15:45:34.119+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13143583/comment/16391423","id":"16391423","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Tepsic","name":"Tepsic","key":"tepsic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Tepsic","active":true,"timeZone":"Etc/UTC"},"body":"This behavior seems to be caused by the lack of property *yarn.nodemanager.address* inside yarn-site.xml files of NodeManagers.\r\n\r\n\r\nWhen explicitly defining that, this behavior does-not occur:\r\n\r\n <property>\r\n   <name>yarn.nodemanager.address</name>\r\n   <value>rb0101.local:9999</value>\r\n</property>","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Tepsic","name":"Tepsic","key":"tepsic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Tepsic","active":true,"timeZone":"Etc/UTC"},"created":"2018-03-08T15:51:07.885+0000","updated":"2018-03-08T15:51:07.885+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13143583/comment/16391438","id":"16391438","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Tepsic","name":"Tepsic","key":"tepsic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Tepsic","active":true,"timeZone":"Etc/UTC"},"body":"This could be caused by buildNodeId( ), as the Port # it generates appears to be random when yarn.nodemanager.address is not defined in a NodeManager's yarn-site.xml.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Tepsic","name":"Tepsic","key":"tepsic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Evan Tepsic","active":true,"timeZone":"Etc/UTC"},"created":"2018-03-08T15:58:38.445+0000","updated":"2018-03-08T15:59:24.267+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13143583/comment/16391444","id":"16391444","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"body":"Closing as invalid","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2018-03-08T16:02:21.050+0000","updated":"2018-03-08T16:02:21.050+0000"}],"maxResults":4,"total":4,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-8014/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3r1tj:"}}