{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12952323","self":"https://issues.apache.org/jira/rest/api/2/issue/12952323","key":"YARN-4852","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-03-22T10:12:13.906+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Nov 03 15:17:10 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4852/watchers","watchCount":16,"isWatching":false},"created":"2016-03-22T06:20:54.044+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327197","id":"12327197","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-11-03T15:17:10.411+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319322","id":"12319322","name":"resourcemanager"}],"timeoriginalestimate":null,"description":"Resource Manager went out of memory (max heap size: 8 GB, CMS GC) and shut down itself. \n\n\n\nHeap dump analysis reveals that 1200 instances of RMNodeImpl class hold 86% of memory. When digging  deeper, there are around 0.5 million objects of UpdatedContainerInfo (nodeUpdateQueue inside RMNodeImpl). This in turn contains around 1.7 million objects of YarnProtos$ContainerIdProto, ContainerStatusProto, ApplicationAttemptIdProto, ApplicationIdProto each of which retain around 1 GB heap.\n\nBack to Back Full GC kept on happening. GC wasn't able to recover any heap and went OOM. JVM dumped the heap before quitting. We analyzed the heap. \n\nRM's usual heap usage is around 4 GB but it suddenly spiked to 8 GB in 20 mins time and went OOM.\n\nThere are no spike in job submissions, container numbers at the time of issue occurrence. \n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12794938","id":"12794938","filename":"threadDump.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-23T06:06:30.948+0000","size":203850,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12794938/threadDump.log"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Resource Manager Ran Out of Memory","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15206113","id":"15206113","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"[~slukog] Can you give more information to verify why there was immediate glitch\n# Any NM got restarted? If so how many and how many containers were running in each NM.?\n# Was there RM heavily loaded or any deadlock in scheduler where most of the node heart beat was not processed by scheduler?\n# Do you have Jstack report for RM  while memory is increasing?\n\nThese container status are cleared from nodeUpdateQueue when node heartbeat is processed by scheduler. If there is any issue/slow from scheduler, node status events would pile up. This would increase nodeUpdateQueue size and might cause OOM.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-22T10:12:13.906+0000","updated":"2016-03-22T10:32:43.384+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15206164","id":"15206164","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"body":"Hi [~rohithsharma],\n\n* During the sudden heap size increase we didn't notice any NM going down/restarted. But once the heap reached it's max we noticed a couple of NMs going down. This might be due to RM being in OOM state.\n{quote}Any NM got restarted? If so how many and how many containers were running in each NM.?{quote}\n\n* Not sure if CapacityScheduler was in deadlock but can see from thread dump that three IPC Handler threads were waiting on *CapacityScheduler.getQueueInfo* and the *Resource Manager Event Processor* Thread was printing the log line \"Null Container Completed\". There are millions of such log lines are seen in ResourceManager during the time of issue occurrence.\n{quote}Was there RM heavily loaded or any deadlock in scheduler where most of the node heart beat was not processed by scheduler?{quote}\n\n* Will attach the thread dump in a while\n{quote}Do you have Jstack report for RM while memory is increasing?{quote}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-22T10:57:52.036+0000","updated":"2016-03-22T10:57:52.036+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15206334","id":"15206334","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"body":"The cause for the OOM looks like huge number of sudden logs of \"Null container completed\". Do we know in what scenarios we will get this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-22T13:22:23.216+0000","updated":"2016-03-22T13:22:23.216+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15206613","id":"15206613","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=templedf","name":"templedf","key":"templedf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=templedf&avatarId=24879","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=templedf&avatarId=24879","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=templedf&avatarId=24879","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=templedf&avatarId=24879"},"displayName":"Daniel Templeton","active":true,"timeZone":"America/Los_Angeles"},"body":"[~slukog], could you post an excerpt from the log so we can see the exact log messages?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=templedf","name":"templedf","key":"templedf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=templedf&avatarId=24879","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=templedf&avatarId=24879","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=templedf&avatarId=24879","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=templedf&avatarId=24879"},"displayName":"Daniel Templeton","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-22T15:49:34.176+0000","updated":"2016-03-22T15:49:34.176+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15206761","id":"15206761","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"body":"Sure. Here are the log messages that are seen in RM logs before it goes OOM. There are millions of such entries.\n\n2016-03-22 20:04:40,443 INFO  capacity.CapacityScheduler (CapacityScheduler.java:completedContainer(1190)) - Null container completed...\n2016-03-22 20:04:40,443 INFO  capacity.CapacityScheduler (CapacityScheduler.java:completedContainer(1190)) - Null container completed...\n2016-03-22 20:04:40,443 INFO  capacity.CapacityScheduler (CapacityScheduler.java:completedContainer(1190)) - Null container completed...\n2016-03-22 20:04:40,443 INFO  capacity.CapacityScheduler (CapacityScheduler.java:completedContainer(1190)) - Null container completed...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-22T16:47:05.829+0000","updated":"2016-03-22T16:47:05.829+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15206763","id":"15206763","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"body":"Thread dump attached ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-22T16:48:09.521+0000","updated":"2016-03-22T16:48:09.521+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15206873","id":"15206873","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"body":"Suspect you may run into YARN-3487, in which case CS lock is hold and the UpdatedContainerInfo gets piled up.\n[~slukog], how often do you see this ?  would you like to patch YARN-3487 and give it a try ?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-22T17:47:26.620+0000","updated":"2016-03-22T17:47:26.620+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15207936","id":"15207936","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"body":"Hi [~jianhe],\n\nThe previous thread dump may be looking like we've hit [YARN-3487|https://issues.apache.org/jira/browse/YARN-3487], but that was extracted from the heap dump. I'm now attaching the thread dump taken when the second time the issue had occurred(removing the old one I attached before which is not complete). There the CS thread(Resource Manager Event Processor) which is supposed to consume from UpdatedContainerInfo is not in blocked state. Still the queue filled up and the issue recurred. Any pointers here? One common observation is there were huge number of log lines I mentioned above both times when the issue occurred.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-23T06:05:40.827+0000","updated":"2016-03-23T06:05:40.827+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15208172","id":"15208172","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"Looking at your attached threadump, I feel root cause for your issue is YARN-3487. May be you can try if it is recurring regularly.\n\nFrom the thread dump,\nI see that there are 8 threads are waiting for CS lock out of 7 are {{CapacityScheduler.getQueueInf}} which are called from validating resource request either during application submission for AM resource request OR for AM heartbeat request. \nAt this time, nodeUpdate is holding the CS lock. This would take few mills to process containers status if more are there.\n{code}\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainer(CapacityScheduler.java:1190)\n\t- locked <0x00000005d4cfe5c8> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:951)\n\t- locked <0x00000005d4cfe5c8> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler)\n{code}\n\n\nIn larger cluster what can happen is if more ApplicationsMaster are running concurrently and application submission rate is very high, then significantly nodeUpdate will be blocked for obtaining CS lock. The reason for blocking is YARN-3487. So if more NodeManagers are there then time consumed to process each node update increase which internally pill up the container status and might be causing oom.\n\nJust for an info,  How many NodeManagers are there in cluster? How many AM are running concurrently and How many tasks per job? what is the job submission rate? \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-23T10:18:05.581+0000","updated":"2016-03-23T10:18:05.581+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15208204","id":"15208204","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"body":"Agreed 7 threads are waiting to lock CapacityScheduler.getQueueInfo. What is the impact if these many threads are waiting on this lock in application submission phase? Will it be the cause for RMNodeImpl.nodeUpdateQueue piling up? If yes then YARN-3487 will fix the issue. Else there should be some other reason - like the consumer thread of the queue(RMNodeImpl.nodeUpdateQueue) which is ResourceManager Event processor stuck at something that it is not draining the queue.\n\nAlso the thread which is doing nodeUpdate(ResourceManager Event processor) is not in blocked state. It is still runnable. \n\nThere are around 1200 NMs in the cluster. 93 apps were running when issue occurred. The number of containers allocated were 17803 and pending were 63422. Job submission rate was roughly 6 per minute.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-23T10:37:20.314+0000","updated":"2016-03-23T10:37:20.314+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15208246","id":"15208246","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"To be more clear, \n*Flow-1* : Each AM heart beat or application submission try to acquire CS lock. In your cluster, 93 apps running concurrently would send resource request in  AM heartbeat to RM. These many AM's heartbeat are race to obtain CS lock. \n\n*Flow-2* And other hand, scheduler event process thread dispatches events one by one. So at any point of time, only one nodeUpdate event is processed.This nodeUpdate event try to acquire a CS lock which is also in race ( From your thread dump, nodeUpdate has acquired the CS lock as I mentioned previous comment).\n\nConsider worst case where always AM heart beat is getting chance to acquire CS lock, then nodeUpdate call would be delayed. As I said scheduler event processor process an event one by one, other node update events will be piled up. Note that scheduler node status event is triggered from RMNodeIMpl. Delay in scheduler event processing does not block NodeManagers heartbeat. So NodeManager keep sending node heart beat and updating the RMNodeImpl#nodeUpdateQueue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-23T11:12:53.363+0000","updated":"2016-03-23T11:12:53.363+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15208283","id":"15208283","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"I was justifying how without YARN-3487 might cause oom. There could be other reason causing for nodeUpdate queue pill up which need to be analysed. For leaving out a suspect of YARN-3487, apply the patch in the cluster. If issue occur again it is easy to focus on particular area.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-23T11:32:39.106+0000","updated":"2016-03-23T11:32:39.106+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15208301","id":"15208301","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"body":"Thanks [~rohithsharma], this gives some perspective about the starvation of Scheduler Event Processor Thread. May be YARN-3487 would bring down the probability of this issue. \n\nIt took more than 30 minutes for the heap to double and go OOM. So Scheduler Event Processor would have got to process at least some nodeUpdate events. But heap was on growing state continuously and never came down. That's why I am not fully convinced that YARN-3487 would solve the issue. By the way what is the hearbeat interval from AM to RM in which it will acquire the CS lock.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-23T11:56:18.722+0000","updated":"2016-03-23T11:56:18.722+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15208314","id":"15208314","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"bq. By the way what is the hearbeat interval from AM to RM in which it will acquire the CS lock.\nMRAppMaster heartbeat is 1sec default. And CS lock is aquired only if there are ask resource request in heartbeat.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-23T12:10:12.827+0000","updated":"2016-03-23T12:10:12.827+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15209520","id":"15209520","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sharadag","name":"sharadag","key":"sharadag","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sharad Agarwal","active":true,"timeZone":"Etc/UTC"},"body":"[~rohithsharma] the slowness in schedulers still does not explain the built up of UpdatedContainerInfo to be 0.5 million objects in a short span. UpdatedContainerInfo should only be created in case of newly launched/completed containers. \nLooking at the code at RMNodeImpl.StatusUpdateWhenHealthyTransition  (branch 2.6.0)\n{code}\n // Process running containers\n        if (remoteContainer.getState() == ContainerState.RUNNING) {\n          if (!rmNode.launchedContainers.contains(containerId)) {\n            // Just launched container. RM knows about it the first time.\n            rmNode.launchedContainers.add(containerId);\n            newlyLaunchedContainers.add(remoteContainer);\n          }\n        } else {\n          // A finished container\n          rmNode.launchedContainers.remove(containerId);\n          completedContainers.add(remoteContainer);\n        }\n      }\n      if(newlyLaunchedContainers.size() != 0 \n          || completedContainers.size() != 0) {\n        rmNode.nodeUpdateQueue.add(new UpdatedContainerInfo\n            (newlyLaunchedContainers, completedContainers));\n      }\n{code}\n\nAbove UpdatedContainerInfo is seemed to be getting created each time there is a completed containers in the container status (it is not checking if from previous update this has already been created). Wouldn't this lead to lot of duplicates UpdatedContainerInfo objects and further putting stress on the scheduler unnecessarily.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sharadag","name":"sharadag","key":"sharadag","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sharad Agarwal","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-24T00:55:56.364+0000","updated":"2016-03-24T00:55:56.364+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15209535","id":"15209535","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sharadag","name":"sharadag","key":"sharadag","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sharad Agarwal","active":true,"timeZone":"Etc/UTC"},"body":"Further analysis shows that we are seeing exceptionally high log lines of \"Null container completed...\", somewhere in between 100k to 200k every minute. This could be related to lot of duplicate UpdatedContainerInfo objects for completed containers.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sharadag","name":"sharadag","key":"sharadag","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sharad Agarwal","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-24T01:12:30.891+0000","updated":"2016-03-24T01:12:30.891+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15209583","id":"15209583","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"Thanks for bringing out duplicated container status stored in UpdatedContainerInfo. This makes to think of ticket YARN-2997 which is already solved.\n\nScenario is NM keeps the containers in NMContext as long as RM sends notification to NM in response to remove from NM. Every heart beat these(pendingCompletedContainers) container status is sent to RM which could be duplicated!!  But from RM , while creating UpdatedContainerInfo validation is not done for duplicated entries. This is keep accumulating when there is slow in scheduler event processing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-24T02:05:07.209+0000","updated":"2016-03-24T02:05:07.209+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15209609","id":"15209609","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"Adding to above point, since NM->RM is push design, already sent containers are not supposed to send again unless there is RESYNC command from RM. So it should be a bug from NodeManager","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-24T02:24:57.472+0000","updated":"2016-03-24T02:24:57.472+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15209689","id":"15209689","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sharadag","name":"sharadag","key":"sharadag","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sharad Agarwal","active":true,"timeZone":"Etc/UTC"},"body":"Thanks Rohith. Should we consider adding duplicate check in the RM side as well for completed containers as we are doing for launched ones. This will make it more full proof and eliminate scenarious like resync etc where NM might still send duplicates.\n we can open a new ticket for the same.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sharadag","name":"sharadag","key":"sharadag","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sharad Agarwal","active":true,"timeZone":"Etc/UTC"},"created":"2016-03-24T03:51:53.752+0000","updated":"2016-03-24T03:51:53.752+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15209715","id":"15209715","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"I will raise a new ticket for this. Thanks:-)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-24T04:18:41.358+0000","updated":"2016-03-24T04:18:41.358+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15209726","id":"15209726","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"Raised a JIRA YARN-4862 for handling duplicated container status check.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-03-24T04:31:06.990+0000","updated":"2016-03-24T04:31:06.990+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12952323/comment/15633063","id":"15633063","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"[~sharadag] [~slukog] As per analysis in earlier discussions, YARN-4862 was identified as reason for OOM. \nShould this JIRA can be resolved since YARN-4862 patch got committed? Does YARN-4862 need to backported to Hadoop-2.6?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-11-03T15:17:10.411+0000","updated":"2016-11-03T15:17:10.411+0000"}],"maxResults":22,"total":22,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4852/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2v08n:"}}