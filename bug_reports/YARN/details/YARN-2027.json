{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12712951","self":"https://issues.apache.org/jira/rest/api/2/issue/12712951","key":"YARN-2027","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-05-08T22:11:33.864+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sat May 17 22:41:55 UTC 2014","customfield_12310420":"391267","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-2027/watchers","watchCount":14,"isWatching":false},"created":"2014-05-07T21:39:55.237+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326142","id":"12326142","description":"2.4.0 release","name":"2.4.0","archived":false,"released":true,"releaseDate":"2014-04-07"}],"issuelinks":[{"id":"12408762","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12408762","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12662451","key":"YARN-1042","self":"https://issues.apache.org/jira/rest/api/2/issue/12662451","fields":{"summary":"add ability to specify affinity/anti-affinity in container requests","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}},{"id":"12486851","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12486851","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13021599","key":"YARN-5907","self":"https://issues.apache.org/jira/rest/api/2/issue/13021599","fields":{"summary":"[Umbrella] [YARN-1042] add ability to specify affinity/anti-affinity in container requests","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/2","id":"2","description":"A new feature of the product, which has yet to be developed.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype","name":"New Feature","subtask":false,"avatarId":21141}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-11-17T21:36:29.942+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319322","id":"12319322","name":"resourcemanager"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12319325","id":"12319325","name":"scheduler"}],"timeoriginalestimate":null,"description":"YARN appears to be ignoring host-level ContainerRequests.\n\nI am creating a container request with code that pretty closely mirrors the DistributedShell code:\n\n{code}\n  protected def requestContainers(memMb: Int, cpuCores: Int, containers: Int) {\n    info(\"Requesting %d container(s) with %dmb of memory\" format (containers, memMb))\n    val capability = Records.newRecord(classOf[Resource])\n    val priority = Records.newRecord(classOf[Priority])\n    priority.setPriority(0)\n    capability.setMemory(memMb)\n    capability.setVirtualCores(cpuCores)\n    // Specifying a host in the String[] host parameter here seems to do nothing. Setting relaxLocality to false also doesn't help.\n    (0 until containers).foreach(idx => amClient.addContainerRequest(new ContainerRequest(capability, null, null, priority)))\n  }\n{code}\n\nWhen I run this code with a specific host in the ContainerRequest, YARN does not honor the request. Instead, it puts the container on an arbitrary host. This appears to be true for both the FifoScheduler and the CapacityScheduler.\n\nCurrently, we are running the CapacityScheduler with the following settings:\n\n{noformat}\n<configuration>\n\n  <property>\n    <name>yarn.scheduler.capacity.maximum-applications</name>\n    <value>10000</value>\n    <description>\n      Maximum number of applications that can be pending and running.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>\n    <value>0.1</value>\n    <description>\n      Maximum percent of resources in the cluster which can be used to run\n      application masters i.e. controls number of concurrent running\n      applications.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.resource-calculator</name>\n    <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>\n    <description>\n      The ResourceCalculator implementation to be used to compare\n      Resources in the scheduler.\n      The default i.e. DefaultResourceCalculator only uses Memory while\n      DominantResourceCalculator uses dominant-resource to compare\n      multi-dimensional resources such as Memory, CPU etc.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.root.queues</name>\n    <value>default</value>\n    <description>\n      The queues at the this level (root is the root queue).\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.root.default.capacity</name>\n    <value>100</value>\n    <description>Samza queue target capacity.</description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>\n    <value>1</value>\n    <description>\n      Default queue user limit a percentage from 0.0 to 1.0.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>\n    <value>100</value>\n    <description>\n      The maximum capacity of the default queue.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.root.default.state</name>\n    <value>RUNNING</value>\n    <description>\n      The state of the default queue. State can be one of RUNNING or STOPPED.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>\n    <value>*</value>\n    <description>\n      The ACL of who can submit jobs to the default queue.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>\n    <value>*</value>\n    <description>\n      The ACL of who can administer jobs on the default queue.\n    </description>\n  </property>\n\n  <property>\n    <name>yarn.scheduler.capacity.node-locality-delay</name>\n    <value>40</value>\n    <description>\n      Number of missed scheduling opportunities after which the CapacityScheduler\n      attempts to schedule rack-local containers.\n      Typically this should be set to number of nodes in the cluster, By default is setting\n      approximately number of nodes in one rack which is 40.\n    </description>\n  </property>\n\n</configuration>\n{noformat}\n\nDigging into the code a bit (props to [~jghoman] for finding this), we have a theory as to why this is happening. It looks like RMContainerRequestor.addContainerReq adds three resource requests per container request: data-local, rack-local, and any:\n\n{code}\nprotected void addContainerReq(ContainerRequest req) {\n  // Create resource requests\n  for (String host : req.hosts) {\n    // Data-local\n    if (!isNodeBlacklisted(host)) {\n      addResourceRequest(req.priority, host, req.capability);\n    }      \n  }\n\n  // Nothing Rack-local for now\n  for (String rack : req.racks) {\n    addResourceRequest(req.priority, rack, req.capability);\n  }\n\n  // Off-switch\n  addResourceRequest(req.priority, ResourceRequest.ANY, req.capability);\n}\n{code}\n\nThe addResourceRequest method, in turn, calls addResourceRequestToAsk, which in turn calls ask.add(remoteRequest):\n\n{code}\nprivate void addResourceRequestToAsk(ResourceRequest remoteRequest) {\n  // because objects inside the resource map can be deleted ask can end up \n  // containing an object that matches new resource object but with different\n  // numContainers. So exisintg values must be replaced explicitly\n  if(ask.contains(remoteRequest)) {\n    ask.remove(remoteRequest);\n  }\n  ask.add(remoteRequest);    \n}\n{code}\n\nThe problem is that the \"ask\" variable is a TreeSet:\n\n{code}\nprivate final Set<ResourceRequest> ask = new TreeSet<ResourceRequest>(\n    new org.apache.hadoop.yarn.api.records.ResourceRequest.ResourceRequestComparator());\n{code}\n\nThe ResourceRequestComparator sorts the TreeSet according to:\n\n{code}\npublic int compare(ResourceRequest r1, ResourceRequest r2) {\n\n  // Compare priority, host and capability\n  int ret = r1.getPriority().compareTo(r2.getPriority());\n  if (ret == 0) {\n    String h1 = r1.getResourceName();\n    String h2 = r2.getResourceName();\n    ret = h1.compareTo(h2);\n  }\n  if (ret == 0) {\n    ret = r1.getCapability().compareTo(r2.getCapability());\n  }\n  return ret;\n}\n{code}\n\nThe first thing to note is that our resource requests all have the same priority, so the TreeSet is really sorted by resource name (host/rack). The resource names that are added as part of addContainerReq are host, rack, and any, which is denoted as \"\\*\" (see above). The problem with this is that the TreeSet is going to sort the resource requests with the \"\\*\" request first, even if the host request was added first in addContainerReq.\n\n{code}\n> import java.util.TreeSet\n> val set = new TreeSet[String]\n\nset: java.util.TreeSet[String] = []\n\n> set.add(\"eat1-app\")\n> set\n\nres3: java.util.TreeSet[String] = [eat1-app]\n\n> set.add(\"*\")\n> set\n\nres5: java.util.TreeSet[String] = [*, eat1-app]\n{code}\n\nFrom here on out, it seems to me that anything interacting with the \"ask\" TreeSet (including the allocation requests) will be using the most general resource request, not the most specific.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"391487","customfield_12312823":null,"summary":"YARN ignores host-specific resource requests","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"RHEL 6.1\nYARN 2.4","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13992900","id":"13992900","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Dug into this a bit more. Not entirely convinced that the TreeSet stuff is actually an issue anymore. RMContainerRequestor.makeRemoteRequest calls:\n\n{code}\n      allocateResponse = scheduler.allocate(allocateRequest);\n{code}\n\nIf you drill down through the capacity scheduler, into SchedulerApplicationAttempt and AppSchedulingInfo, you'll eventually see that AppSchedulingInfo.updateResourceRequests simply adds the items in \"ask\" into a map based on priority. The order in which these asks come in seem to always be with ANY first (see above), so updatePendingResources will always be true, but this doesn't seem harmful.\n\nAnyway, any ideas why YARN is ignoring host requests?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-08T16:33:33.830+0000","updated":"2014-05-08T16:33:33.830+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13993156","id":"13993156","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"So, running this request with memMb=3584, cpuCores=1, containers=32:\n\n{code}\n  protected def requestContainers(memMb: Int, cpuCores: Int, containers: Int) {\n    info(\"Requesting %d container(s) with %dmb of memory\" format (containers, memMb)) \n    val capability = Records.newRecord(classOf[Resource]) \n    val priority = Records.newRecord(classOf[Priority])\n    priority.setPriority(0)\n    capability.setMemory(memMb)\n    capability.setVirtualCores(cpuCores)\n    def getHosts = {\n      val hosts = getNextRoundRobinHosts\n      System.err.println(hosts.toList)\n      hosts \n    }\n    (0 until containers).foreach(idx => amClient.addContainerRequest(new ContainerRequest(capability, getHosts, List(\"/default-rack\").toArray[String], priority, false)))\n  } \n{code}\n\nPrints this in the AM logs:\n\n{noformat}\nList(eat1-app857, eat1-app873, eat1-app880)\nList(eat1-app854, eat1-app864, eat1-app872)\nList(eat1-app852, eat1-app873, eat1-app880)\nList(eat1-app854, eat1-app880, eat1-app867)\nList(eat1-app875, eat1-app852, eat1-app873)\nList(eat1-app875, eat1-app852, eat1-app872)\nList(eat1-app873, eat1-app859, eat1-app880)\nList(eat1-app854, eat1-app873, eat1-app864)\nList(eat1-app852, eat1-app874, eat1-app875)\nList(eat1-app864, eat1-app859, eat1-app880)\nList(eat1-app874, eat1-app872, eat1-app875)\nList(eat1-app874, eat1-app873, eat1-app864)\nList(eat1-app873, eat1-app859, eat1-app858)\nList(eat1-app874, eat1-app873, eat1-app854)\nList(eat1-app867, eat1-app880, eat1-app872)\nList(eat1-app859, eat1-app875, eat1-app880)\nList(eat1-app875, eat1-app872, eat1-app864)\nList(eat1-app875, eat1-app867, eat1-app852)\nList(eat1-app857, eat1-app852, eat1-app867)\nList(eat1-app872, eat1-app854, eat1-app858)\nList(eat1-app852, eat1-app872, eat1-app858)\nList(eat1-app880, eat1-app873, eat1-app857)\nList(eat1-app859, eat1-app871, eat1-app874)\nList(eat1-app880, eat1-app874, eat1-app865)\nList(eat1-app867, eat1-app873, eat1-app875)\nList(eat1-app857, eat1-app858, eat1-app852)\nList(eat1-app857, eat1-app867, eat1-app873)\nList(eat1-app857, eat1-app871, eat1-app854)\nList(eat1-app874, eat1-app865, eat1-app873)\nList(eat1-app852, eat1-app880, eat1-app858)\nList(eat1-app875, eat1-app873, eat1-app871)\nList(eat1-app854, eat1-app880, eat1-app865)\n{noformat}\n\nWith DEBUG logging in the RM logs (with no other job on the grid), I see:\n\n{noformat}\n21:18:02,958 DEBUG AppSchedulingInfo:135 - update: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: *, Relax Locality: false}\n21:18:02,958 DEBUG ActiveUsersManager:68 - User my-job-name added to activeUsers, currently: 1\n21:18:02,959 DEBUG CapacityScheduler:704 - allocate: post-update\n21:18:02,959 DEBUG SchedulerApplicationAttempt:328 - showRequests: application=application_1399581102453_0003 headRoom=<memory:736256, vCores:0> currentConsumption=1024\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app875, Relax Locality: true}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app857, Relax Locality: true}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 11, Location: eat1-app880, Relax Locality: true}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app854, Relax Locality: true}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: /default-rack, Relax Locality: true}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app858, Relax Locality: true}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: *, Relax Locality: false}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app874, Relax Locality: true}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app872, Relax Locality: true}\n21:18:02,959 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app859, Relax Locality: true}\n21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 13, Location: eat1-app873, Relax Locality: true}\n21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app864, Relax Locality: true}\n21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app865, Relax Locality: true}\n21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app852, Relax Locality: true}\n21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app867, Relax Locality: true}\n21:18:02,960 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app871, Relax Locality: true}\n21:18:02,960 DEBUG CapacityScheduler:709 - allocate: applicationAttemptId=appattempt_1399581102453_0003_000001 #ask=16\n21:18:03,009 DEBUG AsyncDispatcher:164 - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE\n21:18:03,009 DEBUG RMNodeImpl:373 - Processing eat1-app863:35408 of type STATUS_UPDATE\n21:18:03,009 DEBUG AsyncDispatcher:164 - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE\n21:18:03,009 DEBUG CapacityScheduler:754 - nodeUpdate: eat1-app863:35408 clusterResources: <memory:737280, vCores:360>\n21:18:03,009 DEBUG CapacityScheduler:785 - Node being looked for scheduling eat1-app863:35408 availableResource: <memory:49152, vCores:24>\n21:18:03,009 DEBUG CapacityScheduler:828 - Trying to schedule on node: eat1-app863, available: <memory:49152, vCores:24>\n21:18:03,009 DEBUG ParentQueue:559 - Trying to assign containers to child-queue of root\n21:18:03,009 DEBUG ParentQueue:690 - printChildQueues - queue: root child-queues: root.default(0.0013888889),\n21:18:03,009 DEBUG ParentQueue:652 - Trying to assign to queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:1024, vCores:1>usedCapacity=0.0013888889, absoluteUsedCapacity=0.0013888889, numApps=1, numContainers=1\n21:18:03,009 DEBUG LeafQueue:807 - assignContainers: node=eat1-app863 #applications=1\n21:18:03,009 DEBUG LeafQueue:826 - pre-assignContainers for application application_1399581102453_0003\n21:18:03,009 DEBUG SchedulerApplicationAttempt:328 - showRequests: application=application_1399581102453_0003 headRoom=<memory:736256, vCores:0> currentConsumption=1024\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app875, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app857, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 11, Location: eat1-app880, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app854, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: /default-rack, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app858, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: *, Relax Locality: false}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app874, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app872, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app859, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 13, Location: eat1-app873, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app864, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app865, Relax Locality: true}\n21:18:03,010 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app852, Relax Locality: true}\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app867, Relax Locality: true}\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app871, Relax Locality: true}\n21:18:03,011 DEBUG LeafQueue:1062 - User limit computation for my-job-name in queue default userLimit=100 userLimitFactor=1.0 required: <memory:3584, vCores:1> consumed: <memory:1024, vCores:1> limit: <memory:737280, vCores:1> queueCapacity: <memory:737280, vCores:1> qconsumed: <memory:1024, vCores:1> currentCapacity: <memory:737280, vCores:1> activeUsers: 1 clusterCapacity: <memory:737280, vCores:360>\n21:18:03,011 DEBUG LeafQueue:995 - Headroom calculation for user my-job-name:  userLimit=<memory:737280, vCores:1> queueMaxCap=<memory:737280, vCores:1> consumed=<memory:1024, vCores:1> headroom=<memory:736256, vCores:0>\n21:18:03,011 DEBUG LeafQueue:914 - post-assignContainers for application application_1399581102453_0003\n21:18:03,011 DEBUG SchedulerApplicationAttempt:328 - showRequests: application=application_1399581102453_0003 headRoom=<memory:736256, vCores:0> currentConsumption=1024\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app875, Relax Locality: true}\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app857, Relax Locality: true}\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 11, Location: eat1-app880, Relax Locality: true}\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app854, Relax Locality: true}\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: /default-rack, Relax Locality: true}\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app858, Relax Locality: true}\n21:18:03,011 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 32, Location: *, Relax Locality: false}\n21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 7, Location: eat1-app872, Relax Locality: true}\n21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app859, Relax Locality: true}\n21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 13, Location: eat1-app873, Relax Locality: true}\n21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 5, Location: eat1-app864, Relax Locality: true}\n21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app865, Relax Locality: true}\n21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app852, Relax Locality: true}\n21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 6, Location: eat1-app867, Relax Locality: true}\n21:18:03,012 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 3, Location: eat1-app871, Relax Locality: true}\n21:18:03,012 DEBUG ParentQueue:657 - Assigned to queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:1024, vCores:1>usedCapacity=0.0013888889, absoluteUsedCapacity=0.0013888889, numApps=1, numContainers=1 --> <memory:0, vCores:0>, NODE_LOCAL\n{noformat}\n\nThe logs go on for quite a while like this, but this is the general gist of it. This is very confusing, since I made 32 single requests all with different random hosts, a hard-coded /default-rack, and relaxLocality set to false. I'm seeing all kinds of crazy requests in the logs.\n\nI'm assuming the reason for the expansion in the number of requests is because of the AMRMClientImpl's expansion in addContainerRequest:\n\n{code}\n    if (req.getNodes() != null) {\n      HashSet<String> dedupedNodes = new HashSet<String>(req.getNodes());\n      if(dedupedNodes.size() != req.getNodes().size()) {\n        Joiner joiner = Joiner.on(',');\n        LOG.warn(\"ContainerRequest has duplicate nodes: \"\n            + joiner.join(req.getNodes()));        \n      }\n      for (String node : dedupedNodes) {\n        addResourceRequest(req.getPriority(), node, req.getCapability(), req,\n            true);\n      }\n    }\n\n    for (String rack : dedupedRacks) {\n      addResourceRequest(req.getPriority(), rack, req.getCapability(), req,\n          true);\n    }\n\n    // Ensure node requests are accompanied by requests for\n    // corresponding rack\n    for (String rack : inferredRacks) {\n      addResourceRequest(req.getPriority(), rack, req.getCapability(), req,\n          req.getRelaxLocality());\n    }\n\n    // Off-switch\n    addResourceRequest(req.getPriority(), ResourceRequest.ANY, \n                    req.getCapability(), req, req.getRelaxLocality());\n{code}\n\nStill, the first request that is logged from the RM is this one:\n\n{noformat}\n21:18:03,058 DEBUG SchedulerApplicationAttempt:332 - showRequests: application=application_1399581102453_0003 request={Priority: 0, Capability: <memory:3584, vCores:1>, # Containers: 9, Location: eat1-app875, Relax Locality: true}\n{noformat}\n\nSure enough, I see 9 containers running on eat1-app875. Why is the RM behaving in this way? I made 32 SINGLE requests, each with DIFFERENT hosts, with relaxLocality OFF, but it seems that either the client or the scheduler is bunching them together without my consent.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-08T21:46:19.143+0000","updated":"2014-05-08T21:46:19.143+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13993186","id":"13993186","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kkasravi","name":"kkasravi","key":"kkasravi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kam Kasravi","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Chris\n\nTake a look at YARN-1412 which describes related failures to allocate a container on a specific host. I've also had a very similar experience with host specific resource requests with relaxLocality set to false failing. I'm thinking of creating a new type of scheduler 'LocalityScheduler' which would allow specific hosts to be permanently reserved until released by the AM. Also take a look at YARN-371 - another instance of where host specific allocation fails and the related discussion.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kkasravi","name":"kkasravi","key":"kkasravi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kam Kasravi","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-08T22:11:33.864+0000","updated":"2014-05-08T22:11:33.864+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13993620","id":"13993620","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for pointing this out. Bummer! The LocalityScheduler idea crossed my mind last night, as well. It still seems to me that the correct solution is to properly patch the RM (or AMRMClient) to work.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-09T14:57:24.384+0000","updated":"2014-05-09T14:57:24.384+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13994652","id":"13994652","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandyr","name":"sandyr","key":"sandyr","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sandy Ryza","active":true,"timeZone":"America/Los_Angeles"},"body":"YARN doesn't guarantee any node-locality unless you specify strictLocality=true in your ContainerRequest.  The FIFO scheduler does not even make an attempt at node-locality.  For the Capacity Scheduler, you need to set yarn.scheduler.capacity.node-locality-delay, which I believes specifies a number of scheduling opportunities to pass on before accepting a non-local container.  Apparently it's not included in the Capacity Scheduler doc - http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html.  The Fair Scheduler equivalent is documented here, but it works a little bit differently - http://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/FairScheduler.html.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandyr","name":"sandyr","key":"sandyr","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sandy Ryza","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-11T19:36:52.800+0000","updated":"2014-05-11T19:36:52.800+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13995262","id":"13995262","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"body":"Was the relaxLocality flag set to false in order to make a hard constraint for the node? \nOr is the jira stating that even soft locality constraints (where YARN is allowed to relax the locality from node to rack to *) is also not working? Soft locality would need delay scheduling to be enabled and that needs the configs that Sandy mentioned.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-12T17:14:06.320+0000","updated":"2014-05-12T17:14:06.320+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13997708","id":"13997708","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"relaxLocality was set to false.\n\n{noformat}\n    (0 until containers).foreach(idx => amClient.addContainerRequest(new ContainerRequest(capability, getHosts, List(\"/default-rack\").toArray[String], priority, false)))\n{noformat}\n\nThe last false in that parameter list is relaxLocality.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-14T16:12:46.651+0000","updated":"2014-05-14T16:12:46.651+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13997750","id":"13997750","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandyr","name":"sandyr","key":"sandyr","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sandy Ryza","active":true,"timeZone":"America/Los_Angeles"},"body":"Including a rack in your request will allow containers to go anywhere on the rack, even when relaxLocality is set to false.\n\nFrom the AMRMClient.ContainerRequest doc: \"If locality relaxation is disabled, then only within the same request, a node and its rack may be specified together. This allows for a specific rack with a preference for a specific node within that rack.\"\n\nSo try passing in the rack list as null instead of List(\"/default-rack\").toArray[String].\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandyr","name":"sandyr","key":"sandyr","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sandy Ryza","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-14T16:51:51.886+0000","updated":"2014-05-14T16:51:51.886+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13998912","id":"13998912","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"body":"Yes. If strict node locality is needed then the rack should not be specified. If the rack is specified then it will allow relaxing locality up to the rack but no further.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-15T17:05:56.158+0000","updated":"2014-05-15T17:05:56.158+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13999584","id":"13999584","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhiguohong","name":"zhiguohong","key":"zhiguohong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Zhiguo","active":true,"timeZone":"Asia/Chongqing"},"body":"I did it in YARN-1974 to specify nodes on which the containers should be allocated(for fair and capacity scheduler), and it works both in unit test and in our real cluster.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhiguohong","name":"zhiguohong","key":"zhiguohong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hong Zhiguo","active":true,"timeZone":"Asia/Chongqing"},"created":"2014-05-16T04:14:02.128+0000","updated":"2014-05-16T04:14:02.128+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/13999943","id":"13999943","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"K, feel free to close.\n\nI'm fairly sure that I tried a host with a null rack during testing and it didn't work, but it might have been on the FIFO scheduler. Either way, we've figured out a workaround to our problem, and [~zhiguohong] has verified functionality on a real cluster, so I'm OK with closing this ticket out.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-16T16:20:12.626+0000","updated":"2014-05-16T16:20:12.626+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12712951/comment/14000910","id":"14000910","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kkasravi","name":"kkasravi","key":"kkasravi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kam Kasravi","active":true,"timeZone":"America/Los_Angeles"},"body":"I tried this on a 5 node cluster in AWS EC2 instances. I set relaxLocality to false. Hadoop-2.2. Capacity Scheudler. Did not work. I tried setting rack to null. I tried allocating 3 container request as well as just 1 container request.\n3 container requests:\n        containers += new ContainerRequest(capability, broker.nodes, null, pri, false)\n        containers += new ContainerRequest(capability, null, Seq[String](\"/default-rack\").toArray[String], pri, false)\n        containers += new ContainerRequest(capability, null, null, pri, false)\n\n1 container request:\n        containers += new ContainerRequest(capability, broker.nodes, null, pri, false)\n\n\nwhere broker.nodes is an array of one FQDM host. Did not work. I can try Hong Zhiguo's distributed shell patch on the same AWS cluster and report findings. I'll run run hadoop-2.4 instead of hadoop-2.2.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kkasravi","name":"kkasravi","key":"kkasravi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kam Kasravi","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-17T22:41:55.455+0000","updated":"2014-05-17T22:41:55.455+0000"}],"maxResults":12,"total":12,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-2027/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1vdzz:"}}