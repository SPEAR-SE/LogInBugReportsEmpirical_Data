{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12901849","self":"https://issues.apache.org/jira/rest/api/2/issue/12901849","key":"YARN-4216","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-10-01T13:13:46.268+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sat Nov 14 05:00:26 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4216/watchers","watchCount":6,"isWatching":false},"created":"2015-10-01T10:09:54.834+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":["oct16-medium"],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-10-27T18:46:01.042+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12322905","id":"12322905","name":"log-aggregation","description":"Log aggregation feature"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12319323","id":"12319323","name":"nodemanager"}],"timeoriginalestimate":null,"description":"Steps to reproduce\n\n# Start 2 nodemanagers  with NM recovery enabled\n# Submit pi job with 20 maps \n# Once 5 maps gets completed in NM 1 stop NM (yarn daemon stop nodemanager)\n(Logs of all completed container gets aggregated to HDFS)\n# Now start  the NM1 again and wait for job completion\n*The newly assigned container logs on NM1 are not shown*\n\n*hdfs log dir state*\n\n# When logs are aggregated to HDFS during stop its with NAME (localhost_38153)\n# On log aggregation after starting NM the newly assigned container logs gets uploaded with name  (localhost_38153.tmp) \n\nHistory server the logs are now shown for new task attempts\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12772349","id":"12772349","filename":"0001-YARN-4216.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-11-14T04:11:53.326+0000","size":10486,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12772349/0001-YARN-4216.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12764574","id":"12764574","filename":"NMLog","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-01T10:14:03.465+0000","size":318388,"mimeType":"text/html","content":"https://issues.apache.org/jira/secure/attachment/12764574/NMLog"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12764573","id":"12764573","filename":"ScreenshotFolder.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-01T10:10:58.406+0000","size":123192,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12764573/ScreenshotFolder.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12764575","id":"12764575","filename":"yarn-site.xml","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-01T10:14:43.813+0000","size":2990,"mimeType":"text/xml","content":"https://issues.apache.org/jira/secure/attachment/12764575/yarn-site.xml"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Container logs not shown for newly assigned containers  after NM  recovery","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14939791","id":"14939791","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"The problem is the NM is thinking it is being torn down _not_ for a restart and is trying to clean up.  From the NM log:\n{noformat}\n2015-10-01 14:58:40,688 ERROR org.apache.hadoop.yarn.server.nodemanager.NodeManager: RECEIVED SIGNAL 15: SIGTERM\n2015-10-01 14:58:40,720 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Successfully Unregistered the Node localhost:38153 with ResourceManager.\n2015-10-01 14:58:40,731 INFO org.mortbay.log: Stopped SelectChannelConnector@127.0.0.1:8042\n2015-10-01 14:58:40,836 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Applications still running : [application_1443685464627_0007]\n2015-10-01 14:58:40,836 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Waiting for Applications to be Finished\n2015-10-01 14:58:40,837 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1443685464627_0007 transitioned from RUNNING to FINISHING_CONTAINERS_WAIT\n2015-10-01 14:58:40,837 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1443685464627_0007_01_000014 transitioned from RUNNING to KILLING\n2015-10-01 14:58:40,837 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1443685464627_0007_01_000001 transitioned from RUNNING to KILLING\n{noformat}\n\nFor a proper recovery the NM should not be trying to kill containers.  Part of the issue here is having the NM distinguish a shutdown that will be restarted from a shutdown that won't be restarted.  In the former it should _not_ kill containers since the restart will recover them.  For the latter it _should_ kill containers since there won't be an NM around later to control them.  See YARN-1362 for more details.\n\nDoes this problem occur if you stop the NM with kill -9 before restarting?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2015-10-01T13:13:46.268+0000","updated":"2015-10-01T13:13:46.268+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14939830","id":"14939830","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"body":"[~jlowe]\nThe problem doesnt happen when NM is killed using kill -9 before restarting.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-01T13:41:25.184+0000","updated":"2015-10-01T13:41:25.184+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14939839","id":"14939839","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Then I think this is largely a problem of the NM thinking it is being shutdown for a non-recovery case.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2015-10-01T13:47:42.108+0000","updated":"2015-10-01T13:47:42.108+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14939924","id":"14939924","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"body":"So {{yarn --daemon stop nodemanager}} should be considered as a recoverable case rt  and only when decommission of NM is done should have uploaded the logs on stop ?. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-01T14:55:07.340+0000","updated":"2015-10-01T14:55:07.340+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14939931","id":"14939931","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Not necessarily.  A nodemanager could also be shutting down due to an uncaught exception, crash, etc. or an admin could be shutting down a nodemanager without an intention of restarting it.  That's why YARN-1362 was done, so we can explicitly tell the nodemanager whether or not the NM is under supervision and likely to restart.  If the NM is not under supervision then kill -9 should be used for the restart scenario and yarn --daemon stop nodemanager used for shutting it down.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2015-10-01T15:00:31.114+0000","updated":"2015-10-01T15:00:31.114+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14943113","id":"14943113","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"body":"{quote}\nThat's why YARN-1362 was done, so we can explicitly tell the nodemanager whether or not the NM is under supervision and likely to restart.\n{quote}\n*yarn.nodemanager.recovery.supervised=false* in my current setup.\nIn this case as i understand from above comment i am supposed to set *yarn.nodemanager.recovery.supervised* as true to inform restart is under supervision.\n \n[~jlowe] so should i close this jira ??","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-05T09:21:24.954+0000","updated":"2015-10-05T09:21:24.954+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14943124","id":"14943124","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"body":"[Document|https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/NodeManagerRestart.html] doesn't mention  about the  *yarn.nodemanager.recovery.supervised* . Should i update doc?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-05T09:31:04.848+0000","updated":"2015-10-05T09:31:04.848+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14943368","id":"14943368","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Yes, the document should be updated to cover that property.  Did you try setting that property to true, and does it solve your issue?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2015-10-05T13:40:44.965+0000","updated":"2015-10-05T13:40:44.965+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14943574","id":"14943574","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"body":"When yarn.nodemanager.recovery.supervised=true and  nodemanager stoppped abort aggregation is called\n\n2015-10-05 20:17:20,634 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl: *Aborting log aggregation for application_1444056058955_0002*\n{noformat}\n2015-10-05 20:17:20,634 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder\n2015-10-05 20:17:20,634 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService: org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit\n2015-10-05 20:17:20,634 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl: Aborting log aggregation for application_1444056058955_0002\n2015-10-05 20:17:20,634 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl: Aggregation did not complete for application application_1444056058955_0002\n2015-10-05 20:17:20,639 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.\n2015-10-05 20:17:20,664 INFO org.apache.hadoop.ipc.Server: Stopping server on 8040\n2015-10-05 20:17:20,665 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8040\n2015-10-05 20:17:20,665 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder\n2015-10-05 20:17:20,665 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Public cache exiting\n2015-10-05 20:17:20,665 WARN org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl: org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is interrupted. Exiting.\n2015-10-05 20:17:20,671 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NodeManager metrics system...\n2015-10-05 20:17:20,674 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NodeManager metrics system stopped.\n{noformat}\n\nContainer logs are not cleaned up and uploaded to HDFS on stop\n\nBut decommision + nm restart while application is running should cause  the same  log missing scenario as per {{LogAggregationService#stopAggregators}}\n\n{code}\n boolean supervised = getConfig().getBoolean(\n        YarnConfiguration.NM_RECOVERY_SUPERVISED,\n        YarnConfiguration.DEFAULT_NM_RECOVERY_SUPERVISED);\n    // if recovery on restart is supported then leave outstanding aggregations\n    // to the next restart\n    boolean shouldAbort = context.getNMStateStore().canRecover()\n        && !context.getDecommissioned() && supervised;\n    // politely ask to finish\n    for (AppLogAggregator aggregator : appLogAggregators.values()) {\n      if (shouldAbort) {\n        aggregator.abortLogAggregation();\n      } else {\n        aggregator.finishLogAggregation();\n      }\n    }\n{code}\n'","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-05T16:00:52.390+0000","updated":"2015-10-05T16:00:52.390+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14943781","id":"14943781","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"The container logs should not be uploaded on NM stop if we are doing recovery.  That is intentional.  Decommission + nm restart doesn't make sense to me.  Either we are decommissioning a node and don't expect it to return, or we are going to restart it and expect it to return shortly.  For the former, we want the NM to linger a bit to try to finish log aggregation.  For the latter it should not.\n\nIf we are decommissioning the node then context.getDecommissioned() in the boolean clause above should be true which means shouldAbort would be false.  That means it should not do the same thing as a shutdown under supervision.  My apologies if I'm missing something.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2015-10-05T18:15:34.691+0000","updated":"2015-10-05T18:15:34.691+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14944524","id":"14944524","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"body":"{quote}\n That is intentional. Decommission + nm restart doesn't make sense to me. Either we are decommissioning a node and don't expect it to return, or we are going to restart it and expect it to return shortly.\n{quote}\nFor *rolling upgrade* the same scenarios can happen *( decommmision (logs upload) --> upgrade --> start NM --> new container assignment --> on finish log upload )* and container log loss happens. Append logs during aggregation could be one solution in this case rt?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-10-06T05:20:49.483+0000","updated":"2015-10-06T05:20:49.483+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/14945020","id":"14945020","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"If we're decommissioning a node then we're not doing a rolling upgrade of it.  Decomm of a node should kill all of the containers on the node, upload the logs, then shutdown the node.  That's not a rolling upgrade since we lose work.  It may be rolling in the sense that we can go through the nodes in a serial fashion, but since work is being lost at each step it's significantly different than the rolling upgrade with work-preserving restart.\n\nWhat we're talking about here is reinsertion of a previously decomm'd node that ends up running containers for an application that already had logs aggregated which is slightly different than the JIRA title which implies work-preserving restart.  Having the NM append the new logs would be a reasonable approach to try to avoid log loss, although there's the problem of active readers for the logs.  If we're appending then we can end up with partially written logs at the end when readers come along to parse the logs.  We'd either have to live with that possibility or have the NM copy the existing logs to the .tmp file before appending the new logs then atomically replacing the previous logs with the new version.  Not all filesystems support atomic replace, but HDFS can do it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2015-10-06T13:28:07.475+0000","updated":"2015-10-06T13:28:07.475+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/15005147","id":"15005147","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"body":"Attaching patch for review","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bibinchundatt","name":"bibinchundatt","key":"bibinchundatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bibinchundatt&avatarId=29912","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bibinchundatt&avatarId=29912","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bibinchundatt&avatarId=29912","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bibinchundatt&avatarId=29912"},"displayName":"Bibin A Chundatt","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-11-14T04:11:53.329+0000","updated":"2015-11-14T04:11:53.329+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12901849/comment/15005159","id":"15005159","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 5s {color} | {color:blue} docker + precommit patch detected. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 2m 56s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 51s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 46s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 28s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 52s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 26s {color} | {color:green} trunk passed {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 1m 18s {color} | {color:red} hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common in trunk has 3 extant Findbugs warnings. {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 47s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 54s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 50s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 52s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 52s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 48s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 48s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 25s {color} | {color:red} Patch generated 1 new checkstyle issues in hadoop-yarn-project/hadoop-yarn (total was 33, now 33). {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 53s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 26s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 29s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 45s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 54s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 1m 49s {color} | {color:green} hadoop-yarn-common in the patch passed with JDK v1.8.0_60. {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 8m 34s {color} | {color:green} hadoop-yarn-server-nodemanager in the patch passed with JDK v1.8.0_60. {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 2m 2s {color} | {color:green} hadoop-yarn-common in the patch passed with JDK v1.7.0_79. {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 8m 59s {color} | {color:green} hadoop-yarn-server-nodemanager in the patch passed with JDK v1.7.0_79. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 23s {color} | {color:green} Patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 41m 45s {color} | {color:black} {color} |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker | Client=1.7.1 Server=1.7.1 Image:test-patch-base-hadoop-date2015-11-14 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12772349/0001-YARN-4216.patch |\n| JIRA Issue | YARN-4216 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux f2c0dee1eebe 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/patchprocess/apache-yetus-fa12328/precommit/personality/hadoop.sh |\n| git revision | trunk / 47c79a2 |\n| findbugs | v3.0.0 |\n| findbugs | https://builds.apache.org/job/PreCommit-YARN-Build/9689/artifact/patchprocess/branch-findbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-common-warnings.html |\n| checkstyle | https://builds.apache.org/job/PreCommit-YARN-Build/9689/artifact/patchprocess/diff-checkstyle-hadoop-yarn-project_hadoop-yarn.txt |\n| JDK v1.7.0_79  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/9689/testReport/ |\n| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn |\n| Max memory used | 227MB |\n| Powered by | Apache Yetus   http://yetus.apache.org |\n| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/9689/console |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2015-11-14T05:00:26.287+0000","updated":"2015-11-14T05:00:26.287+0000"}],"maxResults":14,"total":14,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4216/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2mhrr:"}}