{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13117726","self":"https://issues.apache.org/jira/rest/api/2/issue/13117726","key":"YARN-7474","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-11-13T17:27:07.369+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Nov 14 13:05:34 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_263213115_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2017-11-14T13:05:34.256+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-7474/watchers","watchCount":4,"isWatching":false},"created":"2017-11-11T11:58:41.196+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332791","id":"12332791","description":"2.7.2 release","name":"2.7.2","archived":false,"released":true,"releaseDate":"2016-01-25"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-11-14T13:05:56.503+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12322906","id":"12322906","name":"fairscheduler","description":"Fair Scheduler"}],"timeoriginalestimate":null,"description":"Hadoop Version: *2.7.2*\r\nMy Yarn cluster have *(1100TB,368vCores)*  totallly with 15 nodemangers . \r\nMy cluster use fair-scheduler and I have 4 queues for different kinds of jobs:\r\n \r\n{quote}\r\n<allocations>\r\n    <queue name=\"queue1\">\r\n       <minResources>100000 mb, 30 vcores</minResources>\r\n       <maxResources>422280 mb, 132 vcores</maxResources>\r\n       <maxAMShare>0.5f</maxAMShare>\r\n       <fairSharePreemptionTimeout>9000000000</fairSharePreemptionTimeout>\r\n       <minSharePreemptionTimeout>9000000000</minSharePreemptionTimeout>\r\n       <maxRunningApps>50</maxRunningApps>\r\n    </queue>\r\n    <queue name=\"queue2\">\r\n       <minResources>25000 mb, 20 vcores</minResources>\r\n       <maxResources>600280 mb, 150 vcores</maxResources>\r\n       <maxAMShare>0.6f</maxAMShare>\r\n       <fairSharePreemptionTimeout>9000000000</fairSharePreemptionTimeout>\r\n       <minSharePreemptionTimeout>9000000000</minSharePreemptionTimeout>\r\n       <maxRunningApps>50</maxRunningApps>\r\n    </queue>\r\n    <queue name=\"queue3\">\r\n       <minResources>100000 mb, 30 vcores</minResources>\r\n       <maxResources>647280 mb, 132 vcores</maxResources>\r\n       <maxAMShare>0.8f</maxAMShare>\r\n       <fairSharePreemptionTimeout>9000000000</fairSharePreemptionTimeout>\r\n       <minSharePreemptionTimeout>9000000000</minSharePreemptionTimeout>\r\n       <maxRunningApps>50</maxRunningApps>\r\n    </queue>\r\n  \r\n    <queue name=\"queue4\">\r\n       <minResources>80000 mb, 20 vcores</minResources>\r\n       <maxResources>120000 mb, 30 vcores</maxResources>\r\n       <maxAMShare>0.5f</maxAMShare>\r\n       <fairSharePreemptionTimeout>9000000000</fairSharePreemptionTimeout>\r\n       <minSharePreemptionTimeout>9000000000</minSharePreemptionTimeout>\r\n       <maxRunningApps>50</maxRunningApps>\r\n     </queue>\r\n</allocations>\r\n {quote}\r\n\r\nfrom about 9:00 am, all new-coming applications get stuck for nearly 5 hours, but the cluster resource usage is about *(600GB,120vCores)*，it means，the cluster resource is still *sufficient*.\r\n*The resource usage of the whole yarn cluster AND of each single queue stay unchanged for 5 hours*, really strange. Obviously , if it a resource insufficiency problem , it's impossible that used resource of all queues didn't have any change for 5 hours. So , I is the problem of ResourceManager.\r\n\r\nSince my cluster scale is not large, only 15 nodes with 1100G memory ,I exclude the possibility showed in [YARN-4618].\r\n \r\nbesides that , all the running applications seems never finished, the Yarn RM seems static ,the RM log  have no more state change logs about running applications，except for the log about more and more application is submitted and become ACCEPTED, but never from ACCEPTED to RUNNING.\r\n*The resource usage of the whole yarn cluster AND of each single queue stay unchanged for 5 hours*, really strange.\r\nThe cluster seems like a zombie.\r\n \r\nI haved checked the ApplicationMaster log of some running but stucked application , \r\n \r\n {quote}\r\n2017-11-11 09:04:55,896 INFO [IPC Server handler 0 on 42899] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Getting task report for MAP job_1507795051888_183385. Report-size will be 4\r\n2017-11-11 09:04:55,957 INFO [IPC Server handler 0 on 42899] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Getting task report for REDUCE job_1507795051888_183385. Report-size will be 0\r\n2017-11-11 09:04:56,037 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\r\n2017-11-11 09:04:56,061 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1507795051888_183385: ask=6 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:109760, vCores:25> knownNMs=15\r\n2017-11-11 13:58:56,736 INFO [IPC Server handler 0 on 42899] org.apache.hadoop.mapreduce.v2.app.client.MRClientService: Kill job job_1507795051888_183385 received from appuser (auth:SIMPLE) at 10.120.207.11\r\n {quote}\r\n \r\nYou can see that at  *2017-11-11 09:04:56,061* It send resource request to ResourceManager but RM allocate zero containers. Then ,no more logs  for 5 hours. At  13:58， I have to kill it manually.\r\n \r\nAfter 5 hours , I kill some pending applications and then everything recovered，remaining cluster resources can be allocated again, ResourceManager seems  to be alive again.\r\n \r\nI have exclude the possibility of  the restriction of maxRunningApps and maxAMShare config because they will just affect a single queue, but my problem is that whole yarn cluster application get stuck.\r\n \r\n \r\n \r\nAlso , I exclude the possibility of a  resourcemanger  full gc problem because I check that with gcutil，no full gc happened , resource manager memory is OK.\r\n \r\nSo , anyone could give me some suggestions?\r\n ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12897516","id":"12897516","filename":"rm.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-14T08:30:40.437+0000","size":21525476,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12897516/rm.log"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Yarn resourcemanager stop allocating container when cluster resource is sufficient ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13117726/comment/16249853","id":"16249853","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=templedf","name":"templedf","key":"templedf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=templedf&avatarId=24879","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=templedf&avatarId=24879","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=templedf&avatarId=24879","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=templedf&avatarId=24879"},"displayName":"Daniel Templeton","active":true,"timeZone":"America/Los_Angeles"},"body":"There are severals reasons why that could happen.  [~yufeigu], any suggestions?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=templedf","name":"templedf","key":"templedf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=templedf&avatarId=24879","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=templedf&avatarId=24879","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=templedf&avatarId=24879","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=templedf&avatarId=24879"},"displayName":"Daniel Templeton","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-13T17:27:07.369+0000","updated":"2017-11-13T17:27:07.369+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13117726/comment/16249880","id":"16249880","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yufeigu","name":"yufeigu","key":"yufeigu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yufeigu&avatarId=31240","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yufeigu&avatarId=31240","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yufeigu&avatarId=31240","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yufeigu&avatarId=31240"},"displayName":"Yufei Gu","active":true,"timeZone":"America/Los_Angeles"},"body":"Seems like an deadlock or an infinity loop in RM. Please check YARN-4477.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yufeigu","name":"yufeigu","key":"yufeigu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yufeigu&avatarId=31240","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yufeigu&avatarId=31240","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yufeigu&avatarId=31240","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yufeigu&avatarId=31240"},"displayName":"Yufei Gu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-13T17:52:11.123+0000","updated":"2017-11-13T17:52:11.123+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13117726/comment/16251082","id":"16251082","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"body":"[~yufeigu] [~templedf] Big thanks for you reply.\r\nI noticed that the  the bug mentioned at [YARN-4477|https://issues.apache.org/jira/browse/YARN-4477]  is just for hadoop 2.8.0 or higher, but my hadoop version is 2.7.2. I have already checked my 2.7.2 source code , there didn't exists the method *reservationExceedsThreshold()* metioned there.\r\nWould you please give me more suggestions?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-14T08:27:30.368+0000","updated":"2017-11-14T08:27:30.368+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13117726/comment/16251092","id":"16251092","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"body":"[~yufeigu] [~templedf] I have attatched my ResourceManager log during the time problem occurs.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-14T08:33:07.162+0000","updated":"2017-11-14T08:33:07.162+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13117726/comment/16251093","id":"16251093","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"body":"Below is some explanation  about the log:\r\n\r\nBefore 2017-11-11 09:04, everything seems OK , application submitted and become runnings state.\r\n\r\nAt about 2017-11-11 09:04:47, no more applications become RUNNING , all the new-comming applications keep in pending state, and the already-running applications seem never finished.\r\n\r\nAt about 2017-11-11 13:58, namely about 5 hours after when problem occurs , I killed some applications，and yarn seems alive again. You can see that many pending applications' state become running , everything seems OK.\r\n\r\nDuring the problem , Yarn's cluster resource usage is about half of the total YARN cluster resources and keeped unchanged , abosolutely unchanged, it seems static and dead.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-14T08:34:45.009+0000","updated":"2017-11-14T08:34:45.009+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13117726/comment/16251226","id":"16251226","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"body":"[~yufeigu] [~templedf]\r\n\r\nFrom the ResourceManager log，I see:\r\nAt 09:04 when the problem start to occur , all NodeManagers in my yarn cluster has just been reserved ,below is the result of grepping the *Making reservation* from the log:\r\n\r\n{code:java}\r\n2017-11-11 09:00:30,343 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.106 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:30,346 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.105 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:30,401 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.84 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:30,412 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.85 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:30,535 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.102 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:30,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.86 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:30,824 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.108 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:30,865 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.104 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:30,991 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.103 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:31,232 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.107 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:31,249 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.101 app_id=application_1507795051888_183354\r\n2017-11-11 09:00:34,547 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183358\r\n2017-11-11 09:01:06,277 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183342\r\n2017-11-11 09:01:16,525 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183342\r\n2017-11-11 09:01:25,348 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183342\r\n2017-11-11 09:01:28,351 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183342\r\n2017-11-11 09:02:29,658 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183342\r\n2017-11-11 09:04:14,788 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183376\r\n2017-11-11 09:04:26,307 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183380\r\n2017-11-11 09:04:51,200 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: Making reservation: node=10.120.117.100 app_id=application_1507795051888_183383\r\n{code}\r\n\r\nSo, I guess if it is caused by a reservation deadlock , which means ,  all nodes has been reserved , these reserved containers cannot be turned to allocated , and new-coming application cannot make reservation anymore so they are all pending, thus , my yarn cluster become dead.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-14T10:48:30.833+0000","updated":"2017-11-14T10:48:30.833+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13117726/comment/16251347","id":"16251347","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"body":"Finally I get the reason.\r\nA submitted application's container has make reservations on all NodeManagers , which make all NodeManagers become unavailable.\r\n\r\nI think I have configured the *yarn.scheduler.maximum-allocation-mb* far too big (about half of *yarn.nodemanager.resource.memory-mb*) so that it is possible that a bad-configured application's containers will make reservation on all nodes and can never switched to allocated ,namely result in a deadlock.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wuchang1989","name":"wuchang1989","key":"wuchang1989","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=wuchang1989&avatarId=34378","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wuchang1989&avatarId=34378","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wuchang1989&avatarId=34378","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wuchang1989&avatarId=34378"},"displayName":"wuchang","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-14T13:05:34.299+0000","updated":"2017-11-14T13:05:56.491+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-7474/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3mo5r:"}}