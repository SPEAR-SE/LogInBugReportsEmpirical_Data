{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12767057","self":"https://issues.apache.org/jira/rest/api/2/issue/12767057","key":"YARN-3054","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2015-01-14T16:33:41.208+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Mar 14 02:47:30 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_51479045816_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2016-08-31T03:45:02.371+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-3054/watchers","watchCount":9,"isWatching":false},"created":"2015-01-13T08:00:56.677+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327197","id":"12327197","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"issuelinks":[{"id":"12479125","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12479125","type":{"id":"12310010","name":"Incorporates","inward":"is part of","outward":"incorporates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310010"},"inwardIssue":{"id":"12946005","key":"YARN-4752","self":"https://issues.apache.org/jira/rest/api/2/issue/12946005","fields":{"summary":"FairScheduler should preempt for a ResourceRequest and all preempted containers should be on the same node","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12494270","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12494270","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13043082","key":"MAPREDUCE-6849","self":"https://issues.apache.org/jira/rest/api/2/issue/13043082","fields":{"summary":"Preemption by container priority can be problematic for MapReduce","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-02-14T21:39:08.972+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12322906","id":"12322906","name":"fairscheduler","description":"Fair Scheduler"}],"timeoriginalestimate":null,"description":"Preemption policy is related with schedule policy now. Using comparator of schedule policy to find preemption candidate cannot guarantee a subset of containers never be preempted. And this may cause tasks to be preempted periodically before they finish. So job cannot make any progress. \n\nI think preemption in YARN should got below assurance:\n1. Mapreduce jobs can get additional resources when others are idle;\n2. Mapreduce jobs for one user in one queue can still progress with its min share when others preempt resources back.\n\nMaybe always preempt the latest app and container can get this? ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Preempt policy in FairScheduler may cause mapreduce job never finish","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=peng.zhang","name":"peng.zhang","key":"peng.zhang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10441","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10441","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10441","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10441"},"displayName":"Peng Zhang","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=peng.zhang","name":"peng.zhang","key":"peng.zhang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10441","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10441","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10441","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10441"},"displayName":"Peng Zhang","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12767057/comment/14277174","id":"14277174","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ywskycn","name":"ywskycn","key":"ywskycn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wei Yan","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi, [~peng.zhang]. Firstly, FairScheduler will check whether the usage is over fairness.\n{code}\n  private boolean preemptContainerPreCheck() {\n    return parent.getPolicy().checkIfUsageOverFairShare(getResourceUsage(),\n        getFairShare());\n  }\n{code}\n\nbq. Mapreduce jobs can get additional resources when others are idle.\nI'm not sure what your \"idle\" meaning here. But in YARN, one queue can take the over-fairshare resource, if the resources are not used by other queues. And in FairScheduler, each queue has \"steady\" fairshare and \"dynamic\" fairshare. For example, if we have two queues (Q1 and Q2), both with weight 1. So Q1's steady share is 50%, and Q2 is also 50%. Assume only Q1 has jobs and no job submitted to Q2, Q1's dynamic fairness is 100% and Q2 is 0. The dynamic fairshare calculation only considers \"active\" queues.\n\nbq. Mapreduce jobs for one user in one queue can still progress with its min share when others preempt resources back.\nAs I said above, each queue is guaranted with minshare and fairshare. That means, some jobs can still move on. We cannot assign a minshare to each job. Otherwise, the job with multiple concurrent jobs may take over the cluster.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ywskycn","name":"ywskycn","key":"ywskycn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wei Yan","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-14T16:33:41.208+0000","updated":"2015-01-14T16:33:41.208+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12767057/comment/15186463","id":"15186463","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"body":"[~peng.zhang] - the problem reported is not quite clear, mind elaborating it further? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-09T04:07:48.336+0000","updated":"2016-03-09T04:07:48.336+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12767057/comment/15186492","id":"15186492","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=peng.zhang","name":"peng.zhang","key":"peng.zhang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10441","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10441","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10441","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10441"},"displayName":"Peng Zhang","active":true,"timeZone":"Asia/Shanghai"},"body":"Preemption happened on low priority container, and for MapReduce reduce task got higher priority than map task for scheduling first, but it has data dependency on map task. \nSo preempt map task which has lower priority may cause job progress never proceed.\n\nDetailed scenario described as below: \n1. assume 10 resources in cluster(map and reduce task request the same amount of memory and cpu, 1 resource per task), two queues(q1 and q2). \n2. q1 has one job and get all resources when q2 is idle.\n3. job in q1 has 5 map tasks and 5 reduce tasks.\n4. when q2 get new job, job in q1 will be preempted, and 5 containers will be preempted.\n5. according to container preemption policy, all map tasks with lower priority will be preempted (all progress for these tasks are lost)\n6. after container preemption, job in q1 get new resource headroom, and decide new ratio between map and reduce tasks, and then AM preempt reduce tasks for map tasks. so 5 reduce tasks are killed and new 5 map tasks start.\n7. when q2 is idle, job in q1 will then get 5 extra resources and new 5 reduce tasks start. \nThis is back to phase 3. and this may happens periodically(maybe because map tasks for job in q1 run for a long time), map tasks cannot finish before container is preempted. So job cannot make any progress.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=peng.zhang","name":"peng.zhang","key":"peng.zhang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10441","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10441","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10441","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10441"},"displayName":"Peng Zhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-09T04:52:08.665+0000","updated":"2016-03-09T04:52:08.665+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12767057/comment/15191538","id":"15191538","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~peng.zhang]. I understand it now. \n\nThe elegant way of handling this would be to have a preemption priority or even a preemption cost per container, which is different from the priority that is used for allocation. That is a larger conversation to be had. Let us move this out of this umbrella and look at it for both schedulers together. \n\nThat said, I would expect MapReduce to realize that pending mappers are blocked on waiting reducers and resolve this. MAPREDUCE-6302 and co. attempt to fix this, so you shouldn't see issues with job completion itself. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-11T21:51:39.321+0000","updated":"2016-03-11T21:51:39.321+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12767057/comment/15191554","id":"15191554","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"body":"Also, the approach being proposed on YARN-4752 doesn't take container priority into consideration anymore. So, we shouldn't systemic bias towards preempting low priority containers. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-11T22:00:09.748+0000","updated":"2016-03-11T22:00:09.748+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12767057/comment/15191556","id":"15191556","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"body":"Also, the approach being proposed on YARN-4752 doesn't take container priority into consideration anymore. So, we shouldn't systemic bias towards preempting low priority containers. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-11T22:01:57.783+0000","updated":"2016-03-11T22:01:57.783+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12767057/comment/15192683","id":"15192683","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=peng.zhang","name":"peng.zhang","key":"peng.zhang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10441","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10441","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10441","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10441"},"displayName":"Peng Zhang","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks [~kasha]\nI agreed with \"have a preemption priority or even a preemption cost per container\". \n\nAnd in my temporary fix, I preempt latest scheduled container instead of low or high priority containers. \nI think this will make containers for a mount of resources(at least steady fair share) steady. So MapReduce job progress will proceed. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=peng.zhang","name":"peng.zhang","key":"peng.zhang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10441","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10441","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10441","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10441"},"displayName":"Peng Zhang","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-14T02:47:30.471+0000","updated":"2016-03-14T02:47:30.471+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-3054/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i24aof:"}}