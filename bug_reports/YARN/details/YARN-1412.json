{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12679255","self":"https://issues.apache.org/jira/rest/api/2/issue/12679255","key":"YARN-1412","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2013-11-15T05:59:04.010+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri May 09 14:53:11 UTC 2014","customfield_12310420":"358620","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-1412/watchers","watchCount":15,"isWatching":false},"created":"2013-11-14T17:54:12.017+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12325051","id":"12325051","description":"2.2.0 release","name":"2.2.0","archived":false,"released":true,"releaseDate":"2013-10-15"}],"issuelinks":[{"id":"12449112","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12449112","type":{"id":"10001","name":"dependent","inward":"is depended upon by","outward":"depends upon","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10001"},"inwardIssue":{"id":"12715428","key":"SLIDER-82","self":"https://issues.apache.org/jira/rest/api/2/issue/12715428","fields":{"summary":"Support ANTI_AFFINITY_REQUIRED option","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/3","id":"3","description":"A task that needs to be done.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype","name":"Task","subtask":false,"avatarId":21148}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-11-18T19:26:37.712+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"Summary of the problem: \n\n If I pass the node on which I want container and set relax locality default which is true, I don't get back the container on the node specified even if the resources are available on the node. It doesn't matter if I set rack or not.\n\nHere is the snippet of the code that I am using\n\nAMRMClient<ContainerRequest> amRmClient =  AMRMClient.createAMRMClient();;\n    String host = \"h1\";\n    Resource capability = Records.newRecord(Resource.class);\n    capability.setMemory(memory);\n    nodes = new String[] {host};\n    // in order to request a host, we also have to request the rack\n    racks = new String[] {\"/default-rack\"};\n     List<ContainerRequest> containerRequests = new ArrayList<ContainerRequest>();\n    List<ContainerId> releasedContainers = new ArrayList<ContainerId>();\n    containerRequests.add(new ContainerRequest(capability, nodes, racks, Priority.newInstance(priority)));\n    if (containerRequests.size() > 0) {\n      LOG.info(\"Asking RM for containers: \" + containerRequests);\n      for (ContainerRequest cr : containerRequests) {\n        LOG.info(\"Requested container: {}\", cr.toString());\n        amRmClient.addContainerRequest(cr);\n      }\n    }\n\n    for (ContainerId containerId : releasedContainers) {\n      LOG.info(\"Released container, id={}\", containerId.getId());\n      amRmClient.releaseAssignedContainer(containerId);\n    }\n    return amRmClient.allocate(0);\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"358910","customfield_12312823":null,"summary":"Allocating Containers on a particular Node in Yarn","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"centos, Hadoop 2.2.0","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13822794","id":"13822794","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"body":"I looked at AMRMClientImpl, I see that the addContainerRequest(T) method is setting true for dedupedNodes and dedupedRacks in Line 361 and 367.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-14T19:22:31.884+0000","updated":"2013-11-14T19:22:31.884+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13823323","id":"13823323","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"body":"Can you please report if the same is observed when no rack is set. ie. only specific node is requested without any fallback.\nCan you please attach RM logs along with container id of the container returned by the RM that did not match expectations. Were ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-15T05:59:04.010+0000","updated":"2013-11-15T05:59:04.010+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13823902","id":"13823902","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"body":"Here is the synopsis of the various combinations\n\nNode_Set\t Rack_Set\tRelax locality\t\nYes\t         No\t                FALSE\t      I  get back on the node, but then fallback doesn't work\nYes\t         No\t                TRUE\tI don't get back the correct node\nYes\t         Yes\t                T/F   \tI don't get back the correct node\n\n\nI am attaching the logs when Node is Yes and Rack is False and Relax is true. The containers for which it is not working is container_1384534729839_0001_01_000002 and container_1384534729839_0001_01_000004\n\n2013-11-15 09:00:38,116 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(335)) - showRequests: application=application_1384534729839_0001 headRoom=<memory:9091072, vCores:0> currentConsumption=2048\n2013-11-15 09:00:38,116 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 0, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: /default-rack, Relax Locality: true}\n2013-11-15 09:00:38,116 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 0, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true}\n2013-11-15 09:00:38,116 IPC Server handler 43 on 8031 DEBUG security.UserGroupInformation (UserGroupInformation.java:logPrivilegedAction(1513)) - PrivilegedAction as:hadoop (auth:SIMPLE) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2042)\n2013-11-15 09:00:38,116 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 0, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: node10.morado.com, Relax Locality: true}\n2013-11-15 09:00:38,117 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(335)) - showRequests: application=application_1384534729839_0001 headRoom=<memory:9091072, vCores:0> currentConsumption=2048\n2013-11-15 09:00:38,117 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 1, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true}\n2013-11-15 09:00:38,117 AsyncDispatcher event handler DEBUG event.AsyncDispatcher (AsyncDispatcher.java:dispatch(125)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE\n2013-11-15 09:00:38,117 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(335)) - showRequests: application=application_1384534729839_0001 headRoom=<memory:9091072, vCores:0> currentConsumption=2048\n2013-11-15 09:00:38,117 AsyncDispatcher event handler DEBUG rmnode.RMNodeImpl (RMNodeImpl.java:handle(354)) - Processing node6.morado.com:39327 of type STATUS_UPDATE\n2013-11-15 09:00:38,117 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 2, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: /default-rack, Relax Locality: true}\n2013-11-15 09:00:38,117 AsyncDispatcher event handler DEBUG event.AsyncDispatcher (AsyncDispatcher.java:dispatch(125)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE\n2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 2, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true}\n2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG fica.FiCaSchedulerApp (FiCaSchedulerApp.java:showRequests(339)) - showRequests: application=application_1384534729839_0001 request={Priority: 2, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: node18.morado.com, Relax Locality: true}\n2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:computeUserLimit(1056)) - User limit computation for gaurav in queue default userLimit=100 userLimitFactor=1.0 required: <memory:8192, vCores:1> consumed: <memory:2048, vCores:1> limit: <memory:9093120, vCores:1> queueCapacity: <memory:9093120, vCores:1> qconsumed: <memory:2048, vCores:1> currentCapacity: <memory:9093120, vCores:1> activeUsers: 1 clusterCapacity: <memory:9093120, vCores:296>\n2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:computeUserLimitAndSetHeadroom(989)) - Headroom calculation for user gaurav:  userLimit=<memory:9093120, vCores:1> queueMaxCap=<memory:9093120, vCores:1> consumed=<memory:2048, vCores:1> headroom=<memory:9091072, vCores:0>\n2013-11-15 09:00:38,118 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:assignContainer(1306)) - assignContainers: node=node8.morado.com application=1 priority=0 request={Priority: 0, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true} type=OFF_SWITCH\n2013-11-15 09:00:38,119 ResourceManager Event Processor DEBUG security.BaseContainerTokenSecretManager (BaseContainerTokenSecretManager.java:createPassword(90)) - Creating password for container_1384534729839_0001_01_000002 for user container_1384534729839_0001_01_000002 (auth:SIMPLE) to be run on NM node8.morado.com:51530\n2013-11-15 09:00:38,119 ResourceManager Event Processor DEBUG security.ContainerTokenIdentifier (ContainerTokenIdentifier.java:write(112)) - Writing ContainerTokenIdentifier to RPC layer: org.apache.hadoop.yarn.security.ContainerTokenIdentifier@77c5b2de\n2013-11-15 09:00:38,120 ResourceManager Event Processor DEBUG security.ContainerTokenIdentifier (ContainerTokenIdentifier.java:write(112)) - Writing ContainerTokenIdentifier to RPC layer: org.apache.hadoop.yarn.security.ContainerTokenIdentifier@77c5b2de\n2013-11-15 09:00:38,120 ResourceManager Event Processor DEBUG scheduler.AppSchedulingInfo (AppSchedulingInfo.java:allocate(377)) - allocate: applicationId=application_1384534729839_0001 container=container_1384534729839_0001_01_000002 host=node8.morado.com:51530\n2013-11-15 09:00:38,120 ResourceManager Event Processor DEBUG scheduler.AppSchedulingInfo (AppSchedulingInfo.java:allocate(265)) - allocate: user: gaurav, memory: <memory:8192, vCores:1>\n2013-11-15 09:00:38,120 ResourceManager Event Processor DEBUG rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(208)) - Processing container_1384534729839_0001_01_000002 of type START\n2013-11-15 09:00:38,120 ResourceManager Event Processor INFO  rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(220)) - container_1384534729839_0001_01_000002 Container Transitioned from NEW to ALLOCATED\n2013-11-15 09:00:38,146 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:computeUserLimit(1056)) - User limit computation for gaurav in queue default userLimit=100 userLimitFactor=1.0 required: <memory:8192, vCores:1> consumed: <memory:18432, vCores:3> limit: <memory:9093120, vCores:1> queueCapacity: <memory:9093120, vCores:1> qconsumed: <memory:18432, vCores:3> currentCapacity: <memory:9093120, vCores:1> activeUsers: 1 clusterCapacity: <memory:9093120, vCores:296>\n2013-11-15 09:00:38,146 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:computeUserLimitAndSetHeadroom(989)) - Headroom calculation for user gaurav:  userLimit=<memory:9093120, vCores:1> queueMaxCap=<memory:9093120, vCores:1> consumed=<memory:18432, vCores:3> headroom=<memory:9074688, vCores:-2>\n2013-11-15 09:00:38,146 ResourceManager Event Processor DEBUG capacity.LeafQueue (LeafQueue.java:assignContainer(1306)) - assignContainers: node=node7.morado.com application=1 priority=2 request={Priority: 2, Capability: <memory:8192, vCores:1>, # Containers: 1, Location: *, Relax Locality: true} type=OFF_SWITCH\n2013-11-15 09:00:38,147 ResourceManager Event Processor DEBUG security.BaseContainerTokenSecretManager (BaseContainerTokenSecretManager.java:createPassword(90)) - Creating password for container_1384534729839_0001_01_000004 for user container_1384534729839_0001_01_000004 (auth:SIMPLE) to be run on NM node7.morado.com:36087\n2013-11-15 09:00:38,147 ResourceManager Event Processor DEBUG security.ContainerTokenIdentifier (ContainerTokenIdentifier.java:write(112)) - Writing ContainerTokenIdentifier to RPC layer: org.apache.hadoop.yarn.security.ContainerTokenIdentifier@2f566b7d\n2013-11-15 09:00:38,147 ResourceManager Event Processor DEBUG security.ContainerTokenIdentifier (ContainerTokenIdentifier.java:write(112)) - Writing ContainerTokenIdentifier to RPC layer: org.apache.hadoop.yarn.security.ContainerTokenIdentifier@2f566b7d\n2013-11-15 09:00:38,147 ResourceManager Event Processor DEBUG scheduler.AppSchedulingInfo (AppSchedulingInfo.java:allocate(377)) - allocate: applicationId=application_1384534729839_0001 container=container_1384534729839_0001_01_000004 host=node7.morado.com:36087\n2013-11-15 09:00:38,148 ResourceManager Event Processor DEBUG scheduler.ActiveUsersManager (ActiveUsersManager.java:deactivateApplication(94)) - User gaurav removed from activeUsers, currently: 0\n2013-11-15 09:00:38,148 ResourceManager Event Processor DEBUG scheduler.AppSchedulingInfo (AppSchedulingInfo.java:allocate(265)) - allocate: user: gaurav, memory: <memory:8192, vCores:1>\n2013-11-15 09:00:38,148 ResourceManager Event Processor DEBUG rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(208)) - Processing container_1384534729839_0001_01_000004 of type START\n2013-11-15 09:00:38,148 ResourceManager Event Processor INFO  rmcontainer.RMContainerImpl (RMContainerImpl.java:handle(220)) - container_1384534729839_0001_01_000004 Container Transitioned from NEW to ALLOCATED\n\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-15T18:22:00.505+0000","updated":"2013-11-15T18:22:00.505+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13828340","id":"13828340","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"body":"What is the value of the following configuration? \nyarn.scheduler.capacity.node-locality-delay\n\nIt looks like you are being hit by a bug that will happen with small number of container requests.\n\nLeafQueue.assignContainersOnNode()\nLooks like if rackLocalityDelay is not met, even then the scheduler falls back to off-switch assignment. The delay calculation for off-switch assignment is basically (#different-locations/#nodes-in-cluster)*#containers < #node-heartbeats-without-assignment. In your case, if you have 20 nodes in all, (2/20)*1 == 0.1. So the moment we skip 1 node (waiting for locality delay) we end up assigning an off-switch container to the request.\n\nTry the following, set the node locality delay mentioned at the beginning to the number of nodes in the cluster. Then instead of asking for 1 container at pri 0, ask for 20 containers, each for a specific node, rack=false, relax=true. The above off-switch locality delay will become 20/20*1 == 20 missed assignments.\nIf you see correct assignments then the above theory is correct about the bug.\n\nBtw, what you are trying to do (node=specific, rack=null and relaxLocality=true) is the default behavior of existing schedulers. They will always try to relax locality to rack and then off-switch by default. So you dont need to explicitly code for it. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-21T00:35:42.640+0000","updated":"2013-11-21T00:35:42.640+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13828368","id":"13828368","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"body":" yarn.scheduler.capacity.node-locality-delay is set to 50 and the size of cluster is 36. \nSince this property affects the cluster, not sure if it is right thing to depend on this property for container allocation\n\nFor the nature of our application we are requesting containers with incremental priorities so we have 1 container per priority, so we can't request for multiple containers at pri 0 and there are some applications were total number of containers are less than the cluster size. \n\nWe want what you mentioned \"Btw, what you are trying to do (node=specific, rack=null and relaxLocality=true) is the default behavior of existing schedulers. They will always try to relax locality to rack and then off-switch by default. So you dont need to explicitly code for it. \"... But since it is not working we are trying to code it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-21T01:22:19.543+0000","updated":"2013-11-21T01:22:19.543+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13828412","id":"13828412","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"body":"I mentioned allocating multiple containers at the same priority as an experiment to check if the above theory is correct or not.\n\nbq. But since it is not working we are trying to code it.\nI am saying that your code (yes, null, true) is the same as the default. The behavior that you will get (correct or buggy as it is now) will be the same in both cases.\n\nbq. yarn.scheduler.capacity.node-locality-delay is set to 50 and the size of cluster is 36. \nThe max value of this should be the number of nodes in the cluster. Higher than that has no effect.\n\nBtw, are all nodes in the same rack?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bikassaha","name":"bikassaha","key":"bikassaha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=bikassaha&avatarId=29845","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=bikassaha&avatarId=29845","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=bikassaha&avatarId=29845","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=bikassaha&avatarId=29845"},"displayName":"Bikas Saha","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-11-21T02:27:00.530+0000","updated":"2013-11-21T02:27:00.530+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13829264","id":"13829264","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"body":"Yes All the nodes are on the same Rack.\n\nHere is the experiment that I did to verify the theory\n1. Cluster size: 36 nodes\n2. yarn.scheduler.capacity.node-locality-delay is set to 36\n3. Asked for 36 containers with priority 0\n4. I requested containers with (node=yes, rack=yes,relax-locality=true)\n\nBut I still see that the containers are allocated on different nodes.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-21T19:40:44.958+0000","updated":"2013-11-21T19:40:44.958+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13833001","id":"13833001","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"body":"Any updates on this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gaurav.gopi123","name":"gaurav.gopi123","key":"gaurav.gopi123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gaurav gupta","active":true,"timeZone":"Etc/UTC"},"created":"2013-11-26T20:20:23.322+0000","updated":"2013-11-26T20:20:23.322+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13844750","id":"13844750","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thw","name":"thw","key":"thw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Thomas Weise","active":true,"timeZone":"America/Los_Angeles"},"body":"We implemented it in the AM, tracking resource requests made for a specific host with relaxLocality=false and then, if they are not filled by the scheduler after n heartbeats, dropping host constraint and switching to relaxLocality=true. We would prefer to leave this to YARN with the combination of specific host and relaxLocality=true, but it does not work.\n\nThe requirement is not unique to our application, and instead of handling it in user land it would be great to see this working as expected in future YARN versions.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thw","name":"thw","key":"thw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Thomas Weise","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-12-10T22:25:46.565+0000","updated":"2013-12-10T22:25:46.565+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12679255/comment/13993616","id":"13993616","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Seeing this as well (YARN-2027).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-09T14:53:11.554+0000","updated":"2014-05-09T14:53:11.554+0000"}],"maxResults":10,"total":10,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-1412/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1ptkn:"}}