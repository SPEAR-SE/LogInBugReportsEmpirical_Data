{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12945117","self":"https://issues.apache.org/jira/rest/api/2/issue/12945117","key":"YARN-4741","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-02-27T02:58:05.533+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Mar 01 07:49:25 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4741/watchers","watchCount":19,"isWatching":false},"created":"2016-02-26T23:43:12.154+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327197","id":"12327197","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-03-01T07:49:25.316+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319322","id":"12319322","name":"resourcemanager"}],"timeoriginalestimate":null,"description":"We had a pretty major incident with the RM where it was continually flooded with RMNodeFinishedContainersPulledByAMEvents in the async dispatcher event queue.\n\nIn our setup, we had the RM HA or stateful restart *disabled*, but NM work-preserving restart *enabled*. Due to other issues, we did a cluster-wide NM restart.\n\nSome time during the restart (which took multiple hours), we started seeing the async dispatcher event queue building. Normally it would log 1,000. In this case, it climbed all the way up to tens of millions of events.\n\nWhen we looked at the RM log, it was full of the following messages:\n{noformat}\n2016-02-18 01:47:29,530 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Invalid event FINISHED_CONTAINERS_PULLED_BY_AM on Node  worker-node-foo.bar.net:8041\n2016-02-18 01:47:29,535 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Can't handle this event at current state\n2016-02-18 01:47:29,535 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Invalid event FINISHED_CONTAINERS_PULLED_BY_AM on Node  worker-node-foo.bar.net:8041\n2016-02-18 01:47:29,538 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Can't handle this event at current state\n2016-02-18 01:47:29,538 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Invalid event FINISHED_CONTAINERS_PULLED_BY_AM on Node  worker-node-foo.bar.net:8041\n{noformat}\n\nAnd that node in question was restarted a few minutes earlier.\n\nWhen we inspected the RM heap, it was full of RMNodeFinishedContainersPulledByAMEvents.\n\nSuspecting the NM work-preserving restart, we disabled it and did another cluster-wide rolling restart. Initially that seemed to have helped reduce the queue size, but the queue built back up to several millions and continued for an extended period. We had to restart the RM to resolve the problem.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12790563","id":"12790563","filename":"nm.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sjlee0","name":"sjlee0","key":"sjlee0","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"},"displayName":"Sangjin Lee","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-29T21:25:19.897+0000","size":4346949,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12790563/nm.log"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"RM is flooded with RMNodeFinishedContainersPulledByAMEvents in the async dispatcher event queue","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sjlee0","name":"sjlee0","key":"sjlee0","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"},"displayName":"Sangjin Lee","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sjlee0","name":"sjlee0","key":"sjlee0","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"},"displayName":"Sangjin Lee","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12945117/comment/15170145","id":"15170145","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sjlee0","name":"sjlee0","key":"sjlee0","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"},"displayName":"Sangjin Lee","active":true,"timeZone":"America/Los_Angeles"},"body":"I do see the node in question trying to get in sync with the RM with the applications it thinks it still owns. The trigger might be related to that. Still, it's not clear why the queue was still flooded with those events even after the *second* restart that disabled the NM work-preserving restart.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sjlee0","name":"sjlee0","key":"sjlee0","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"},"displayName":"Sangjin Lee","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-26T23:58:07.138+0000","updated":"2016-02-26T23:58:07.138+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12945117/comment/15170307","id":"15170307","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"body":"I am  not pretty sure whether it is same YARN-3990. Based on the affect version I am suspecting it might be a same issue. On the other hand, looking into event type, it may be new issue also.\nAnyway [~sjlee0] can  you cross verify the fix of YARN-3990 is present in your cluster? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rohithsharma","name":"rohithsharma","key":"rohithsharma","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rohith Sharma K S","active":true,"timeZone":"Australia/Sydney"},"created":"2016-02-27T02:58:05.533+0000","updated":"2016-02-27T02:58:05.533+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12945117/comment/15171341","id":"15171341","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandflee","name":"sandflee","key":"sandflee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"sandflee","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi,[~sjlee0], \n1,  does the num of FINISHED_CONTAINERS_PULLED_BY_AM log equal to the num of active applications?\n2,  does NM received the RSYNC command from RM while restart?  could you paste the status changes of one NM?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandflee","name":"sandflee","key":"sandflee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"sandflee","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-02-29T03:01:21.874+0000","updated":"2016-02-29T03:01:21.874+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12945117/comment/15171378","id":"15171378","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandflee","name":"sandflee","key":"sandflee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"sandflee","active":true,"timeZone":"Asia/Shanghai"},"body":"one race condition may cause the \"Invalid event FINISHED_CONTAINERS_PULLED_BY_AM\".\n1,  RMNode A  finished(LOST or REBOOTED), rm removes it from rmContext.activeNodes\n2,  scheduler complete the container running on node A. To AM container, RM will send FINISHED_CONTAINERS_PULLED_BY_AM event to RMNode A\n3,  normally RMNode A is removed in step 1.  but when node A register at this time, new RMNode of A may process this event at NEW state.\n\nbut the main problem may not applying YARN-3990 and YARN-3896.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandflee","name":"sandflee","key":"sandflee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"sandflee","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-02-29T03:45:46.014+0000","updated":"2016-02-29T03:45:46.014+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12945117/comment/15172650","id":"15172650","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sjlee0","name":"sjlee0","key":"sjlee0","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"},"displayName":"Sangjin Lee","active":true,"timeZone":"America/Los_Angeles"},"body":"I attached the node manager log. It's pretty much the entirety of the log from the start until after it's past the point of these events happening for this node in the RM. The only thing I removed is a section early in the log that lists all the localization service recovering files.\n\nUnfortunately I no longer have the RM log for this episode.\n\nWe do not have YARN-3990 or YARN-3896 applied. Although we should get them in any case, I'm not sure if those are related to the issue we're seeing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sjlee0","name":"sjlee0","key":"sjlee0","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=sjlee0&avatarId=16831","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=sjlee0&avatarId=16831","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=sjlee0&avatarId=16831","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=sjlee0&avatarId=16831"},"displayName":"Sangjin Lee","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-29T21:34:19.751+0000","updated":"2016-02-29T21:34:19.751+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12945117/comment/15173395","id":"15173395","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandflee","name":"sandflee","key":"sandflee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"sandflee","active":true,"timeZone":"Asia/Shanghai"},"body":"without the fix of YARN-3990 and YARN-3896, our rm was flooded by node up/down events, and node is synced.  and have the same output in NM.\n{quote}\n2016-02-18 01:39:43,217 WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Node is out of sync with ResourceManager, hence resyncing.\n2016-02-18 01:39:43,217 WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Message from ResourceManager: Too far behind rm response id:100314 nm response id:0\n{quote}\n\nthings may like that:\n1,  nm restarted,  ResourceTrackerService send a NodeReconnectEvent to reset response id to 0,\n2,  nodeHeartBeat is processed before NodeReconnectEvent is handled(dispatcher is flooded by RMAppNodeUpateEvent),  RM send sync command to NM for mismatch of response id,\n3,  rmNode comes to REBOOT status, and remove it from rmContext.activeNodes\n4,  nm register, create a new rmNode, added to  rmContext.activeNodes and send NodeStartEvent\n5,  scheduler  complete the container running on node,   to AM container, will send FINISHED_CONTAINERS_PULLED_BY_AM event to RMNode , but the RMNode is in NEW state, couldn't handle FINISHED_CONTAINERS_PULLED_BY_AM.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sandflee","name":"sandflee","key":"sandflee","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"sandflee","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-03-01T07:49:25.316+0000","updated":"2016-03-01T07:49:25.316+0000"}],"maxResults":6,"total":6,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4741/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2tuqv:"}}