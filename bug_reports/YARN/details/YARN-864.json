{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12653975","self":"https://issues.apache.org/jira/rest/api/2/issue/12653975","key":"YARN-864","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2013-06-20T18:24:07.290+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jul 18 19:30:11 UTC 2013","customfield_12310420":"334252","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_2424907252_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2013-07-18T19:30:11.667+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-864/watchers","watchCount":17,"isWatching":false},"created":"2013-06-20T17:55:04.449+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"3.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12324429","id":"12324429","description":"maintenance release on branch-2.0-alpha","name":"2.0.5-alpha","archived":false,"released":false,"releaseDate":"2013-06-06"}],"issuelinks":[{"id":"12372315","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12372315","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12647870","key":"YARN-688","self":"https://issues.apache.org/jira/rest/api/2/issue/12647870","fields":{"summary":"Containers not cleaned up when NM received SHUTDOWN event from NodeStatusUpdater","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2013-07-18T19:30:11.685+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319323","id":"12319323","name":"nodemanager"}],"timeoriginalestimate":null,"description":"Hey Guys,\n\nI'm running YARN 2.0.5-alpha with CGroups and stateful RM turned on, and I'm seeing containers getting leaked by the NMs. I'm not quite sure what's going on -- has anyone seen this before? I'm concerned that maybe it's a mis-understanding on my part about how YARN's lifecycle works.\n\nWhen I look in my AM logs for my app (not an MR app master), I see:\n\n2013-06-19 05:34:22 AppMasterTaskManager [INFO] Got an exit code of -100. This means that container container_1371141151815_0008_03_000002 was killed by YARN, either due to being released by the application master or being 'lost' due to node failures etc.\n2013-06-19 05:34:22 AppMasterTaskManager [INFO] Released container container_1371141151815_0008_03_000002 was assigned task ID 0. Requesting a new container for the task.\n\nThe AM has been running steadily the whole time. Here's what the NM logs say:\n\n{noformat}\n05:34:59,783  WARN AsyncDispatcher:109 - Interrupted Exception while stopping\njava.lang.InterruptedException\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Thread.join(Thread.java:1143)\n        at java.lang.Thread.join(Thread.java:1196)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:107)\n        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)\n        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:209)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:336)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:61)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n        at java.lang.Thread.run(Thread.java:619)\n05:35:00,314  WARN ContainersMonitorImpl:463 - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.\n05:35:00,434  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371141151815_0006_01_001598\n05:35:00,434  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371141151815_0008_03_000002\n05:35:00,434  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n05:35:00,434  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n{noformat}\n\nAnd, if I look on the machine that's running container_1371141151815_0008_03_000002, I see:\n\n{noformat}\n$ ps -ef | grep container_1371141151815_0008_03_000002\ncriccomi  5365 27915 38 Jun18 ?        21:35:05 /export/apps/jdk/JDK-1_6_0_21/bin/java -cp /path-to-yarn-data-dir/usercache/criccomi/appcache/application_1371141151815_0008/container_1371141151815_0008_03_000002/...\n{noformat}\n\nThe same holds true for container_1371141151815_0006_01_001598. When I look in the container logs, it's just happily running. No kill signal appears to be sent, and no error appears.\n\nLastly, the RM logs show no major events around the time of the leak (5:35am). I am able to reproduce this simply by waiting about 12 hours, or so, and it seems to have started happening after I switched over to CGroups and LCE, and turned on stateful RM (using file system).\n\nAny ideas what's going on?\n\nThanks!\nChris","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12589105","id":"12589105","filename":"rm-log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-21T16:39:22.173+0000","size":12680,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12589105/rm-log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12589287","id":"12589287","filename":"YARN-864.1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-23T04:34:31.668+0000","size":3415,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12589287/YARN-864.1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12589482","id":"12589482","filename":"YARN-864.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-24T21:08:44.179+0000","size":3700,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12589482/YARN-864.2.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"334578","customfield_12312823":null,"summary":"YARN NM leaking containers with CGroups","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"YARN 2.0.5-alpha with patches applied for YARN-799 and YARN-600.","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13689481","id":"13689481","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"body":"In the log, NodeManager.stop() is getting called. Know why this is happening? You can check the RM logs. Things have changed a bit from 2.0.5 to 2.1.0 so I've to look at the old code.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-20T18:24:07.290+0000","updated":"2013-06-20T18:24:07.290+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13689491","id":"13689491","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Vinod,\n\nI have no idea why it would be called.\n\nThe ps tree shows the NM running since June 13 (this log trace is from the 19th).\n\n{noformat}\n$ ps -ef | grep Node\napp      27915 27655  2 Jun13 ?        04:01:20 /export/apps/jdk/JDK-1_6_0_21/bin/java -Dproc_nodemanager...\n{noformat}\n\nCheers,\nChris","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-20T18:28:25.309+0000","updated":"2013-06-20T18:28:25.309+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13689493","id":"13689493","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"BTW- my NM/RM are running on WARN right now, in logs. I'm going to switch to INFO and see if there's more detail.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-20T18:29:32.921+0000","updated":"2013-06-20T18:29:32.921+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13690458","id":"13690458","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Attaching RM logs. They show that the NM was marked as lost.\n\n09:36:05,614  INFO AbstractLivelinessMonitor:111 - Expired:my-host-name:45454 Timed out after 60 secs\n09:36:05,616  INFO RMNodeImpl:493 - Deactivating Node my-host-name:45454 as it is now LOST\n09:36:05,617  INFO RMNodeImpl:346 - my-host-name:45454 Node Transitioned from RUNNING to LOST\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-21T16:39:22.176+0000","updated":"2013-06-21T16:39:22.176+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13690461","id":"13690461","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Vinod,\n\nSo here's what I know:\n\n1. The RM is marking the NM as lost, and telling the NM to shut down.\n2. The NM is trying to shut down, but appears unable to kill all containers:\n\n{noformat}\n09:37:46,132  INFO NodeStatusUpdaterImpl:365 - Node is out of sync with ResourceManager, hence rebooting.\n09:37:47,932  INFO NodeManager:219 - Containers still running on shutdown: [container_1371756883564_0001_01_000003, container_1371756883564_0002_01_000002, container_1371756883564_0003_01_000005, container_1371756883564_0004_01_000001, container_1371756883564_0004_01_000002]\n09:37:47,955  INFO NMAuditLogger:89 - USER=jhoman       IP=172.18.146.129       OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1371756883564_0004    CONTAINERID=container_1371756883564_0004_01_000001\n09:37:47,970  INFO NodeManager:226 - Waiting for containers to be killed\n09:37:47,970  WARN Server:997 - IPC Server Responder, call org.apache.hadoop.yarn.api.ContainerManagerPB.stopContainer from 172.18.146.129:46200: output error\n09:37:47,973  INFO Container:835 - Container container_1371756883564_0004_01_000001 transitioned from RUNNING to KILLING\n09:37:47,989  INFO ContainerLaunch:300 - Cleaning up container container_1371756883564_0004_01_000001\n09:37:47,989  INFO Server:1797 - IPC Server handler 6 on 45454 caught an exception\njava.nio.channels.ClosedChannelException\n        at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:126)\n        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)\n        at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2200)\n        at org.apache.hadoop.ipc.Server.access$2000(Server.java:113)\n        at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:946)\n        at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1012)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1790)\n09:37:48,837  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 29897 for container-id container_1371756883564_0003_01_000005: 4.2 GB of 6.1 GB physical memory used; 5.4 GB of 49 GB virtual memory used\n09:37:48,863  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 28967 for container-id container_1371756883564_0004_01_000002: 16.4 GB of 19.3 GB physical memory used; 17.0 GB of 154 GB virtual memory used\n09:37:48,904  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27555 for container-id container_1371756883564_0001_01_000003: 601.9 MB of 768 MB physical memory used; 1.1 GB of 6 GB virtual memory used\n09:37:48,928  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27559 for container-id container_1371756883564_0002_01_000002: 731.7 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used\n09:37:48,953  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 28890 for container-id container_1371756883564_0004_01_000001: 146.5 MB of 512 MB physical memory used; 726.6 MB of 4 GB virtual memory used\n09:37:51,995  INFO NodeManager:242 - Done waiting for containers to be killed. Still alive: [container_1371756883564_0001_01_000003, container_1371756883564_0002_01_000002, container_1371756883564_0003_01_000005, container_1371756883564_0004_01_000001, container_1371756883564_0004_01_000002]\n09:37:52,054  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 29897 for container-id container_1371756883564_0003_01_000005: 4.2 GB of 6.1 GB physical memory used; 5.4 GB of 49 GB virtual memory used\n09:37:52,991  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl is stopped.\n09:37:53,095  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 28967 for container-id container_1371756883564_0004_01_000002: 16.4 GB of 19.3 GB physical memory used; 17.0 GB of 154 GB virtual memory used\n09:37:53,184  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27555 for container-id container_1371756883564_0001_01_000003: 602.0 MB of 768 MB physical memory used; 1.1 GB of 6 GB virtual memory used\n09:37:53,271  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27559 for container-id container_1371756883564_0002_01_000002: 731.7 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used\n09:37:53,310  WARN AsyncDispatcher:109 - Interrupted Exception while stopping\njava.lang.InterruptedException\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Thread.join(Thread.java:1143)\n        at java.lang.Thread.join(Thread.java:1196)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:107)\n        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)\n        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:209)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:336)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:61)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n        at java.lang.Thread.run(Thread.java:619)\n09:37:53,318  INFO AbstractService:113 - Service:Dispatcher is stopped.\n09:37:53,759  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 28890 for container-id container_1371756883564_0004_01_000001: 146.5 MB of 512 MB physical memory used; 726.6 MB of 4 GB virtual memory used\n09:37:54,148  INFO log:67 - Stopped SelectChannelConnector@0.0.0.0:9999\n09:37:54,250  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer is stopped.\n09:37:54,250  INFO Server:2060 - Stopping server on 45454\n09:37:54,452  INFO Server:654 - Stopping IPC Server listener on 45454\n09:37:54,493  INFO Server:796 - Stopping IPC Server Responder\n09:37:55,179  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler is stopped.\n09:37:55,626  WARN LinuxContainerExecutor:245 - Exit code from container is : 137\n09:37:55,987  INFO AbstractService:113 - Service:Dispatcher is stopped.\n09:37:55,988  WARN ContainersMonitorImpl:463 - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.\n09:37:55,988  INFO AbstractService:113 - Service:containers-monitor is stopped.\n09:37:55,988  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices is stopped.\n09:37:55,989  INFO AbstractService:113 - Service:containers-launcher is stopped.\n09:37:55,989  INFO Server:2060 - Stopping server on 4344\n09:37:56,028  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371756883564_0004_01_000002\n09:37:56,028  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n09:37:56,107  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371756883564_0003_01_000005\n09:37:56,169  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371756883564_0001_01_000003\n09:37:56,169  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n09:37:56,107  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371756883564_0002_01_000002\n09:37:56,169  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n09:37:56,170  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n09:37:56,176  INFO Server:654 - Stopping IPC Server listener on 4344\n09:37:56,199  INFO Server:796 - Stopping IPC Server Responder\n09:37:56,327  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker is stopped.\n09:37:56,327  INFO ResourceLocalizationService:689 - Public cache exiting\n09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService is stopped.\n09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl is stopped.\n09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is stopped.\n09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService is stopped.\n09:37:56,328  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService is stopped.\n09:37:56,440  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.\n09:37:56,440  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeManager is stopped.\n09:37:56,441  INFO MetricsSystemImpl:200 - Stopping NodeManager metrics system...\n09:37:56,441  INFO MetricsSystemImpl:206 - NodeManager metrics system stopped.\n09:37:56,442  INFO MetricsSystemImpl:572 - NodeManager metrics system shutdown complete.\n{noformat}\n\n3. The NM reboots itself:\n\n09:37:56,441  INFO MetricsSystemImpl:206 - NodeManager metrics system stopped.\n09:37:56,442  INFO MetricsSystemImpl:572 - NodeManager metrics system shutdown complete.\n09:37:56,442  INFO NodeManager:304 - Rebooting the node manager.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-21T16:43:19.704+0000","updated":"2013-06-21T16:43:19.704+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13690506","id":"13690506","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"body":"This might be related to YARN-688. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-21T17:39:13.205+0000","updated":"2013-06-21T17:39:13.205+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13690523","id":"13690523","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Jian,\n\nInteresting. Do you have a re-based patch for 2.0.5-alpha? I'd be willing to patch and try it out.\n\nCheers,\nChris","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-21T17:58:11.208+0000","updated":"2013-06-21T17:58:11.208+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13690602","id":"13690602","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Chris\nUploaded two patches there one for 2.0.5-alpha and one for latest trunk. Things have changed a lot since alpha, not sure that works on alpha. One big change is that NM reboot behavior is changed to resync.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-21T18:59:11.509+0000","updated":"2013-06-21T18:59:11.509+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13690784","id":"13690784","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Jian,\n\nI'm testing YARN 2.0.5-alpha with YARN-799, YARN-600, and YARN-688. I'm going to let it run over the weekend. Failures normally start happening within 5-6 hours. I'll keep you posted.\n\nCheers,\nChris","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-21T22:02:07.154+0000","updated":"2013-06-21T22:02:07.154+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13690935","id":"13690935","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ojoshi","name":"ojoshi","key":"ojoshi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ojoshi&avatarId=16634","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ojoshi&avatarId=16634","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ojoshi&avatarId=16634","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ojoshi&avatarId=16634"},"displayName":"Omkar Vinit Joshi","active":true,"timeZone":"America/Los_Angeles"},"body":"Also it would be very useful if you can share NM logs around the time when container was lost.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ojoshi","name":"ojoshi","key":"ojoshi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ojoshi&avatarId=16634","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ojoshi&avatarId=16634","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ojoshi&avatarId=16634","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ojoshi&avatarId=16634"},"displayName":"Omkar Vinit Joshi","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-22T00:17:56.804+0000","updated":"2013-06-22T00:17:56.804+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13691294","id":"13691294","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Guys,\n\nContainer leaking still seems to be happening. [~ojoshi], here's the logs you asked for:\n\n{noformat}\n10:28:38,753  INFO NodeStatusUpdaterImpl:365 - Node is out of sync with ResourceManager, hence rebooting.\n10:28:40,306  INFO NMAuditLogger:89 - USER=criccomi     IP=172.18.146.129       OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1371849977601_0001    CONTAINERID=container_1371849977601_0001_02_000001\n10:28:40,345  INFO NodeManager:229 - Containers still running on shutdown: [container_1371849977601_0001_02_000001, container_1371849977601_0001_02_000003, container_1371849977601_0002_02_000003, container_1371849977601_0003_01_000004, container_1371849977601_0004_01_000002]\n10:28:40,355  INFO Container:835 - Container container_1371849977601_0001_02_000001 transitioned from RUNNING to KILLING\n10:28:40,375  INFO ContainerLaunch:300 - Cleaning up container container_1371849977601_0001_02_000001\n10:28:40,376  INFO NodeManager:236 - Waiting for containers to be killed\n10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diagnostics: \"Container killed by the ApplicationMaster.\\n\", exit_status: -1000,\n10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:40,377  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:41,378  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diagnostics: \"Container killed by the ApplicationMaster.\\n\", exit_status: -1000,\n10:28:41,378  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:41,378  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:41,378  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:41,379  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:41,555  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 4230 for container-id container_1371849977601_0001_02_000001: 161.0 MB of 512 MB physical memory used; 726.2 MB of 4 GB virtual memory used\n10:28:41,802  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 4324 for container-id container_1371849977601_0001_02_000003: 522.9 MB of 768 MB physical memory used; 1.1 GB of 6 GB virtual memory used\n10:28:41,844  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 5717 for container-id container_1371849977601_0002_02_000003: 608.3 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used10:28:41,869  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 26908 for container-id container_1371849977601_0004_01_000002: 16.4 GB of 19.3 GB physical memory used; 17.0 GB of 154 GB virtual memory used\n10:28:41,896  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27868 for container-id container_1371849977601_0003_01_000004: 4.2 GB of 6.1 GB physical memory used; 5.4 GB of 49 GB virtual memory used\n10:28:42,186  WARN LinuxContainerExecutor:245 - Exit code from container is : 137\n10:28:42,382  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diagnostics: \"Container killed by the ApplicationMaster.\\nContainer killed on request. Exit code is 137\\n\", exit_status: -1000,\n10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diag\nnostics: \"\", exit_status: -1000,\n10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diag\nnostics: \"\", exit_status: -1000,\n10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diag\nnostics: \"\", exit_status: -1000,\n10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diag\nnostics: \"\", exit_status: -1000,\n10:28:43,389  INFO Container:835 - Container container_1371849977601_0001_02_000001 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL\n10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diag\nnostics: \"Container killed by the ApplicationMaster.\\nContainer killed on request. Exit code is 137\\n\", exit_status: 137,\n10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diag\nnostics: \"\", exit_status: -1000,\n10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diag\nnostics: \"\", exit_status: -1000,\n10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diag\nnostics: \"\", exit_status: -1000,\n10:28:42,383  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diag\nnostics: \"\", exit_status: -1000,\n10:28:43,389  INFO Container:835 - Container container_1371849977601_0001_02_000001 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL\n10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 1, }, state: C_RUNNING, diag\nnostics: \"Container killed by the ApplicationMaster.\\nContainer killed on request. Exit code is 137\\n\", exit_status: 137,\n10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 1, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 2, cluster_timestamp: 1371849977601, }, attemptId: 2, }, id: 3, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 3, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 4, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:43,390  INFO NodeStatusUpdaterImpl:265 - Sending out status for container: container_id {, app_attempt_id {, application_id {, id: 4, cluster_timestamp: 1371849977601, }, attemptId: 1, }, id: 2, }, state: C_RUNNING, diagnostics: \"\", exit_status: -1000,\n10:28:43,448  INFO NMAuditLogger:89 - USER=criccomi     OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1371849977601_0001    CONTAINERID=container_1371849977601_0001_02_000001\n10:28:43,468  INFO LinuxContainerExecutor:308 - Deleting absolute path : /path/to/yarn-data/usercache/criccomi/appcache/application_1371849977601_0001/container_1371849977601_0001_02_000001\n10:28:43,481  INFO LinuxContainerExecutor:318 -  -- DEBUG -- deleteAsUser: [/path/to/yarn/i001/bin/container-executor, criccomi, 3, /path/to/yarn-data/usercache/criccomi/appcache/application_1371849977601_0001/container_1371849977601_0001_02_000001]\n10:28:43,556  INFO Container:835 - Container container_1371849977601_0001_02_000001 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE\n10:28:43,666  INFO Application:321 - Removing container_1371849977601_0001_02_000001 from application application_1371849977601_0001\n10:28:44,391  INFO NodeManager:253 - Done waiting for containers to be killed. Still alive: [container_1371849977601_0001_02_000001, container_1371849977601_0001_02_000003, container_1371849977601_0002_02_000003, container_1371849977601_0003_01_000004, container_1371849977601_0004_01_000002]\n10:28:44,559  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl is stopped.\n10:28:44,861  WARN AsyncDispatcher:109 - Interrupted Exception while stopping\njava.lang.InterruptedException\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Thread.join(Thread.java:1143)\n        at java.lang.Thread.join(Thread.java:1196)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:107)\n        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)\n        at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:219)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:347)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:61)\n\t\t\t\t        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)\n\t\t\t\t        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\t\t\t\t        at java.lang.Thread.run(Thread.java:619)\n10:28:45,162  INFO AbstractService:113 - Service:Dispatcher is stopped.\n10:28:44,913  INFO ContainersMonitorImpl:347 - Stopping resource-monitoring for container_1371849977601_0001_02_000001\n10:28:45,196  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 4324 for container-id container_1371849977601_0001_02_000003: 522.9 MB of 768 MB physical memory used; 1.1 GB of 6 GB virtual memory used\n10:28:45,230  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 5717 for container-id container_1371849977601_0002_02_000003: 608.3 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used\n10:28:45,266  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 26908 for container-id container_1371849977601_0004_01_000002: 16.4 GB of 19.3 GB physical memory used; 17.0 GB of 154 GB virtual memory used\n10:28:45,291  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 27868 for container-id container_1371849977601_0003_01_000004: 4.2 GB of 6.1 GB physical memory used; 5.4 GB of 49 GB virtual memory used\n10:28:45,608  INFO log:67 - Stopped SelectChannelConnector@0.0.0.0:9999\n10:28:45,709  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer is stopped.\n10:28:45,709  INFO Server:2060 - Stopping server on 45454\n10:28:45,718  INFO Server:654 - Stopping IPC Server listener on 45454\n10:28:45,759  INFO Server:796 - Stopping IPC Server Responder\n10:28:45,977  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler is stopped.\n10:28:45,978  INFO AbstractService:113 - Service:Dispatcher is stopped.\n10:28:45,978  WARN ContainersMonitorImpl:463 - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.\n10:28:45,978  INFO AbstractService:113 - Service:containers-monitor is stopped.\n10:28:45,978  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices is stopped.\n10:28:45,979  INFO AbstractService:113 - Service:containers-launcher is stopped.\n10:28:45,979  INFO Server:2060 - Stopping server on 4344\n10:28:45,995  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371849977601_0002_02_000003\n10:28:45,996  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n10:28:46,076  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371849977601_0003_01_000004\n10:28:46,076  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n10:28:46,076  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371849977601_0004_01_000002\n10:28:46,076  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n10:28:46,109  WARN CgroupsLCEResourcesHandler:166 - Unable to delete cgroup at: /cgroup/cpu/hadoop-yarn/container_1371849977601_0001_02_000003\n10:28:46,109  WARN ContainerLaunch:247 - Failed to launch container.\njava.io.IOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:205)\n        at org.apache.hadoop.util.Shell.run(Shell.java:129)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:322)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:230)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:242)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n10:28:46,125  INFO Server:796 - Stopping IPC Server Responder\n10:28:46,125  INFO Server:654 - Stopping IPC Server listener on 4344\n10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker is stopped.\n10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService is stopped.\n10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl is stopped.\n10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is stopped.\n10:28:46,383  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService is stopped.\n10:28:46,384  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService is stopped.\n10:28:46,384  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.\n10:28:46,384  INFO AbstractService:113 - Service:org.apache.hadoop.yarn.server.nodemanager.NodeManager is stopped.\n10:28:46,384  INFO MetricsSystemImpl:200 - Stopping NodeManager metrics system...\n10:28:46,385  INFO MetricsSystemImpl:206 - NodeManager metrics system stopped.\n10:28:46,385  INFO MetricsSystemImpl:572 - NodeManager metrics system shutdown complete.\n10:28:46,385  INFO NodeManager:315 - Rebooting the node manager.\n{noformat}\n\nCheers,\nChris","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-23T00:01:54.775+0000","updated":"2013-06-23T00:01:54.775+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13691295","id":"13691295","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Wondering if this is because of YARN-495? I applied YARN-688, but YARN-495 didn't apply easily to the 2.0.5-alpha branch, so I didn't use it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-23T00:03:28.125+0000","updated":"2013-06-23T00:03:28.125+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13691343","id":"13691343","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"body":"patch for NM clean up containers on SHUTDOWN and REBOOT event.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-23T04:34:31.671+0000","updated":"2013-06-23T04:34:31.671+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13691344","id":"13691344","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Chris\nYes, the log shows its on REBOOT event. The earlier patch only takes care of SHUTDOWN event, uploaded a new patch for that.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-23T04:38:07.804+0000","updated":"2013-06-23T04:38:07.804+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13692138","id":"13692138","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Jian,\n\nAwesome. I've patched and started the cluster with YARN-600, YARN-799, and YARN-864. I'll keep you posted.\n\nCheers,\nChris","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-24T16:54:34.076+0000","updated":"2013-06-24T16:54:34.076+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13692296","id":"13692296","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Jian,\n\nWith your patch applied, the new error (in the NM) is:\n\n{noformat}\n19:33:36,741  INFO NodeStatusUpdaterImpl:365 - Node is out of sync with ResourceManager, hence rebooting.\n19:33:36,764  INFO ContainersMonitorImpl:399 - Memory usage of ProcessTree 14751 for container-id container_1372091455469_0002_01_000002: 779.3 MB of 1.3 GB physical memory used; 1.6 GB of 10 GB virtual memory used\n19:33:37,239  INFO NodeManager:315 - Rebooting the node manager.\n19:33:37,261  INFO NodeManager:229 - Containers still running on shutdown: [container_1372091455469_0002_01_000002]\n19:33:37,278 FATAL AsyncDispatcher:137 - Error in dispatcher thread\norg.apache.hadoop.metrics2.MetricsException: Metrics source JvmMetrics already exists!\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:126)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:107)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:217)\n\tat org.apache.hadoop.metrics2.source.JvmMetrics.create(JvmMetrics.java:79)\n\tat org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics.create(NodeManagerMetrics.java:49)\n\tat org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics.create(NodeManagerMetrics.java:45)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.<init>(NodeManager.java:75)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.createNewNodeManager(NodeManager.java:357)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.reboot(NodeManager.java:316)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:348)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.handle(NodeManager.java:61)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:619)\n{noformat}\n\nFor the record, you can reproduce this yourself by:\n\n1. Start a YARN RM and NM.\n2. Run a YARN job on the cluster that uses at least one container.\n3. Run kill -STOP <NM PID> on the NM.\n4. Wait 65 seconds (enough for the NM to time out).\n5. Run kill -CONT <NM PID>\n\nYou will see the NM trigger a reboot since it's out of sync with the RM.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-24T19:37:01.565+0000","updated":"2013-06-24T19:37:01.565+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13692388","id":"13692388","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Chris \nthat failure was due to reboot starts even before stop fully completes.\nUploaded a new patch, tested locally. let me know if that works, thx","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-24T21:12:39.298+0000","updated":"2013-06-24T21:12:39.298+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13692476","id":"13692476","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Jian,\n\nI re-deployed my test cluster with YARN-600, YARN-799, and your latest patch (.2.patch) from YARN-864. I simulated the timeout using kill -STOP (as described above), and your patch worked! :)\n\nI'm going to let the cluster run for 24h before declaring victory, but this looks promising. I'll follow up tomorrow, when I know more.\n\nCheers,\nChris","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-24T22:31:40.561+0000","updated":"2013-06-24T22:31:40.561+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13693216","id":"13693216","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Guys,\n\nThanks for all the help. The cluster has been stable after applying YARN-864.2.patch.\n\nJian, is this a patch that's already incorporated into 2.1.0-beta, or has it not yet been applied to anything yet?\n\nCheers,\nChris","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=criccomini","name":"criccomini","key":"criccomini","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=criccomini&avatarId=27141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=criccomini&avatarId=27141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=criccomini&avatarId=27141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=criccomini&avatarId=27141"},"displayName":"Chris Riccomini","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-25T18:03:07.267+0000","updated":"2013-06-25T18:03:07.267+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13693528","id":"13693528","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Chris\nYARN-495 was already committed which deals with containers cleanup in RESYNC case (REBOOT is changed to RESYNC in that patch).\nYARN-688 which takes care of the SHUTDOWN event still needs sometime to get in.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jianhe","name":"jianhe","key":"jianhe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jian He","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-06-26T00:13:50.268+0000","updated":"2013-06-26T00:13:50.268+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12653975/comment/13712696","id":"13712696","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"body":"Given Jian's update, I'm closing this as duplicate of YARN-688.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-07-18T19:30:11.683+0000","updated":"2013-07-18T19:30:11.683+0000"}],"maxResults":21,"total":21,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-864/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1lnr3:"}}