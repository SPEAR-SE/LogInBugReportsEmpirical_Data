{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12910438","self":"https://issues.apache.org/jira/rest/api/2/issue/12910438","key":"YARN-4331","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2015-11-04T19:11:05.747+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Nov 09 14:01:12 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_414803365_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-11-09T14:01:59.983+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4331/watchers","watchCount":7,"isWatching":false},"created":"2015-11-04T18:48:36.643+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12331976","id":"12331976","description":"2.7.1 release","name":"2.7.1","archived":false,"released":true,"releaseDate":"2015-07-06"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-11-09T14:02:00.004+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319323","id":"12319323","name":"nodemanager"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12325004","id":"12325004","name":"yarn"}],"timeoriginalestimate":null,"description":"We are seeing a lot of orphaned containers running in our production clusters.\nI tried to simulate this locally on my machine and can replicate the issue by killing nodemanager.\nI'm running Yarn 2.7.1 with RM state stored in zookeeper and deploying samza jobs.\nSteps:\n{quote}1. Deploy a job \n2. Issue a kill -9 signal to nodemanager \n3. We should see the AM and its container running without nodemanager\n4. AM should die but the container still keeps running\n5. Restarting nodemanager brings up new AM and container but leaves the orphaned container running in the background\n{quote}\nThis is effectively causing double processing of data.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Restarting NodeManager leaves orphaned containers","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=josephfrancis","name":"josephfrancis","key":"josephfrancis","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joseph Francis","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=josephfrancis","name":"josephfrancis","key":"josephfrancis","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joseph Francis","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12910438/comment/14990201","id":"14990201","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Note that the killing of the nodemanager itself with SIGKILL should not cause the containers to be killed in itself.  Instead the problem seems to be that when the nodemanager restarts it is either failing to reacquire the containers that were running or it reacquires them and the RM fails to tell the NM to kill them when it re-registers.  Updating the summary accordingly.  Also by \"the AM and its container\" I assume you mean the application master and some other container that the AM launched.  Please correct me if I'm wrong.  \n\nIs work-preserving nodemanager restart enabled on this cluster?  Without it nodemanagers cannot track containers that were previously running, so it will not be able to reacquire them and kill them.  If they don't exit on their own then they will \"leak\" and continue running outside of YARN's knowledge.  If that feature is not enabled on the nodemanager then this behavior is expected, since killing it with SIGKILL gave the nodemanager no chance to perform any container cleanup on its own.\n\nIf restart is enabled on the nodemanager then this behavior could be correct if the application running told the RM that containers should not be killed when AM attempts fail.  In that case the container should be left running and its up to the AM to reacquire it via some means.  (I believe the RM does provide a bit of help there in the AM-RM protocol.)\n\nIf the containers were supposed to be killed when the AM attempt failed then we need to figure out which of the two possibilities above is the problem.  Could you look in the NM logs and see if it said it was able to reacquire the previously running containers before it was killed?  If it didn't then we need to figure out why, and log snippets around the restart/recovery would be a big help.  If it did reacquire the containers and register to the RM with those containers then apparently the RM didn't tell the NM to kill the undesired containers.  In that case the log from the RM side around the time the NM re-registered would be helpful.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2015-11-04T19:11:05.747+0000","updated":"2015-11-04T19:11:05.747+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12910438/comment/14991575","id":"14991575","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=josephfrancis","name":"josephfrancis","key":"josephfrancis","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joseph Francis","active":true,"timeZone":"Etc/UTC"},"body":"[~jlowe] Thanks for your comments, very helpful.\nyarn.resourcemanager.work-preserving-recovery.enabled is indeed set to false. The reason we have set it to false is because we run samza jobs on the yarn cluster and they don't work well with this feature turned on (https://issues.apache.org/jira/browse/SAMZA-750).\n\nApologies for my ignorance in this area, but if the application master (AM) is dead, shouldn't it be responsibility of the container to kill itself? I'd imagine every container should be required to heartbeat to its application master and killing itself if it misses a few?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=josephfrancis","name":"josephfrancis","key":"josephfrancis","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joseph Francis","active":true,"timeZone":"Etc/UTC"},"created":"2015-11-05T11:56:50.459+0000","updated":"2015-11-05T11:56:50.459+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12910438/comment/14991729","id":"14991729","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"SAMZA-750 is discussing RM restart, but this is NM restart.  They are related but mostly independent features, and one can be enabled without the other.  Check if yarn.nodemanager.recovery.enabled=true on that node.  If you want to support rolling upgrades of the entire YARN cluster they both need to be enabled, but if you simply want to restart/upgrade a NodeManager independent of the ResourceManager then you can turn on nodemanager restart without resourcemanager restart.  NodeManager restart should be mostly invisible to applications except for interruptions in the auxiliary services on that node (e.g.: shuffle handler).\n\nbq. if the application master (AM) is dead, shouldn't it be responsibility of the container to kill itself?\n\nThat is completely application framework dependent and not the responsibility of YARN.  A container is completely under the control of the application (i.e.: user code) and doesn't have to have any YARN code in it at all.  Theoretically one could write an application entirely in C or Go or whatever that generates compatible protocol buffers and adheres to the YARN RPC protocol semantics.  No YARN code would be running at all for that application or in any of its containers at that point.  (I know of no such applications, but it is theoretically possible.)\n\nAlso it is not a requirement that containers have an umbilical connection to the ApplicationMaster.  That choice is up to the application, and some applications don't do this (like the distributed shell sample YARN application).  MapReduce is an application framework that does have an umbilical connection, but if there's a bug in that app where tasks don't properly recognize the umbilical was severed then that's a bug in the app and not a bug in YARN.  Once the nodemanager died on the node, YARN lost all ability to control containers on that node.  If the container decides not to exit then that's an issue with the app more than an issue with YARN.  There's not much YARN can do about it since YARN's actor on that node is no longer present.\n\nIf NM restart is not enabled then the nodemanager should _not_ be killed with SIGKILL.  Simply kill it with SIGTERM and the nodemanager should attempt to kill all containers before shutting down.  Killing the NM with SIGKILL is normally only done when performing a work-preserving restart on the NM, and that requres that yarn.nodemanager.recovery.enabled=true on that node to function properly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2015-11-05T14:28:19.155+0000","updated":"2015-11-05T14:28:19.155+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12910438/comment/14996559","id":"14996559","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=josephfrancis","name":"josephfrancis","key":"josephfrancis","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joseph Francis","active":true,"timeZone":"Etc/UTC"},"body":"[~jlowe] Setting yarn.nodemanager.recovery.enabled=true does solve the issue with orphaned containers.\nNote that the SIGKILL was only done locally to emulate few production issues we had that caused nodemanagers to fall over.\nThanks very much for your clear explanation!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=josephfrancis","name":"josephfrancis","key":"josephfrancis","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Joseph Francis","active":true,"timeZone":"Etc/UTC"},"created":"2015-11-09T14:01:12.221+0000","updated":"2015-11-09T14:01:12.221+0000"}],"maxResults":4,"total":4,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4331/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2nyef:"}}