{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12941655","self":"https://issues.apache.org/jira/rest/api/2/issue/12941655","key":"YARN-4730","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2016-02-24T21:57:40.241+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Feb 25 06:39:27 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_84784689_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2016-02-25T06:39:27.732+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4730/watchers","watchCount":2,"isWatching":false},"created":"2016-02-24T07:06:23.082+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-02-25T06:39:27.768+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12322906","id":"12322906","name":"fairscheduler","description":"Fair Scheduler"}],"timeoriginalestimate":null,"description":"On a big cluster with Total Cluster Resource of 10TB, 3000 cores and Fair Sheduler having 230 queues and total 60000 jobs run a day. [ all 230 queues are very critical and hence the minResource is same for all]. On this case, when a Spark Job is run on queue A and which occupies the entire cluster resource and does not release any resource, another job submitted into queue B and preemption is getting only the Fair Share which is <10TB , 3000> / 230 = <45 GB , 13 cores> which is very less fair share for a queue.shared by many applications. \n\nThe Preemption should get the instantaneous fair Share, that is <10TB, 3000> / 2 (active queues) = 5TB and 1500 cores, so that the first job won't hog the entire cluster resource and also the subsequent jobs run fine.\n\nThis issue is only when the number of queues are very high. In case of less number of queues, Preemption getting Fair Share would be suffice as the fair share will be high. But in case of too many number of queues, Preemption should try to get the instantaneous Fair Share.\n\nNote: Configuring optimal maxResources to 230 queues is difficult and also putting constraint for the queues using maxResource will leave  cluster resource idle most of the time.\n        There are 1000s of Spark Jobs, so asking each user to restrict the number of executors is also difficult.\n\nPreempting Instantaneous Fair Share will help to overcome the above issues.\n\n          \n\n\n\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"YARN preemption based on instantaneous fair share","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Prabhu+Joseph","name":"Prabhu Joseph","key":"prabhu joseph","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prabhu Joseph","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Prabhu+Joseph","name":"Prabhu Joseph","key":"prabhu joseph","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prabhu Joseph","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12941655/comment/15163873","id":"15163873","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"body":"IIRR, FairScheduler preemption is based on instantaneous fairshare. The steady fairshare is used only for WebUI purposes. \n\nIn your case, I would think minshare preemption kicks in because you specify min resources for all queues. Isn't it expected that all queues are getting the same resources the sum of which is cluster resources? Do you expect allocations different from minshare? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kasha","name":"kasha","key":"kasha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-02-24T21:57:40.241+0000","updated":"2016-02-24T21:57:40.241+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12941655/comment/15166817","id":"15166817","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Prabhu+Joseph","name":"Prabhu Joseph","key":"prabhu joseph","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prabhu Joseph","active":true,"timeZone":"Etc/UTC"},"body":"YARN-2026","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Prabhu+Joseph","name":"Prabhu Joseph","key":"prabhu joseph","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prabhu Joseph","active":true,"timeZone":"Etc/UTC"},"created":"2016-02-25T06:39:27.757+0000","updated":"2016-02-25T06:39:27.757+0000"}],"maxResults":2,"total":2,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-4730/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2t9dj:"}}