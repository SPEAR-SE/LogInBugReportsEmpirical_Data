{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12701893","self":"https://issues.apache.org/jira/rest/api/2/issue/12701893","key":"YARN-1842","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-03-17T13:46:33.008+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Dec 16 08:46:03 UTC 2014","customfield_12310420":"380233","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-1842/watchers","watchCount":13,"isWatching":false},"created":"2014-03-17T13:22:51.198+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12325256","id":"12325256","description":"2.3.0 release","name":"2.3.0","archived":false,"released":true,"releaseDate":"2014-02-20"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-12-16T08:46:03.909+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319322","id":"12319322","name":"resourcemanager"}],"timeoriginalestimate":null,"description":"Report of the RM raising a stack trace [https://gist.github.com/matyix/9596735] during AM-initiated shutdown. The AM could just swallow this and exit, but it could be a sign of a race condition YARN-side, or maybe just in the RM client code/AM dual signalling the shutdown. \n\nI haven't replicated this myself; maybe the stack will help track down the problem. Otherwise: what is the policy YARN apps should adopt for AM's handling errors on shutdown? go straight to an exit(-1)?","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12635368","id":"12635368","filename":"hoyalogs.tar.gz","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=keyki","name":"keyki","key":"keyki","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Krisztian Horvath","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-18T18:58:02.164+0000","size":19905,"mimeType":"application/x-gzip","content":"https://issues.apache.org/jira/secure/attachment/12635368/hoyalogs.tar.gz"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"380517","customfield_12312823":null,"summary":"InvalidApplicationMasterRequestException raised during AM-requested shutdown","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13937784","id":"13937784","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"stack \n{code}\n2014-03-17 10:41:31,833 [AMRM Callback Handler Thread] INFO  HoyaAppMaster.yarn - Shutdown Request received\n2014-03-17 10:41:31,841 [AMRM Callback Handler Thread] INFO  impl.AMRMClientAsyncImpl - Shutdown requested. Stopping callback.\n2014-03-17 10:41:32,841 [main] INFO  appmaster.HoyaAppMaster - Triggering shutdown of the AM: Shutdown requested from RM\n2014-03-17 10:41:32,842 [main] INFO  appmaster.HoyaAppMaster - Process has exited with exit code 0 mapped to 0 -ignoring\n2014-03-17 10:41:32,843 [main] INFO  state.AppState - Releasing 1 containers\n2014-03-17 10:41:32,843 [main] INFO  appmaster.HoyaAppMaster - Application completed. Signalling finish to RM\n2014-03-17 10:41:32,843 [main] INFO  appmaster.HoyaAppMaster - Unregistering AM status=FAILED message=Shutdown requested from RM\n2014-03-17 10:41:32,855 [main] INFO  appmaster.HoyaAppMaster - Failed to unregister application: org.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException: Application doesn't exist in cache appattempt_1395049102171_0001_000001\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:329)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster(ApplicationMasterService.java:288)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.finishApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:75)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:97)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)\n \norg.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException: Application doesn't exist in cache appattempt_1395049102171_0001_000001\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:329)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster(ApplicationMasterService.java:288)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.finishApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:75)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:97)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)\n \n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:408)\n\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.finishApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:94)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy21.finishApplicationMaster(Unknown Source)\n\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.unregisterApplicationMaster(AMRMClientImpl.java:310)\n\tat org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl.unregisterApplicationMaster(AMRMClientAsyncImpl.java:157)\n\tat org.apache.hoya.yarn.appmaster.HoyaAppMaster.finish(HoyaAppMaster.java:763)\n\tat org.apache.hoya.yarn.appmaster.HoyaAppMaster.createAndRunCluster(HoyaAppMaster.java:627)\n\tat org.apache.hoya.yarn.appmaster.HoyaAppMaster.runService(HoyaAppMaster.java:362)\n\tat org.apache.hadoop.yarn.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:178)\n\tat org.apache.hadoop.yarn.service.launcher.ServiceLauncher.launchServiceRobustly(ServiceLauncher.java:397)\n\tat org.apache.hadoop.yarn.service.launcher.ServiceLauncher.launchServiceAndExit(ServiceLauncher.java:328)\n\tat org.apache.hadoop.yarn.service.launcher.ServiceLauncher.serviceMain(ServiceLauncher.java:532)\n\tat org.apache.hoya.yarn.appmaster.HoyaAppMaster.main(HoyaAppMaster.java:1470)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException): Application doesn't exist in cache appattempt_1395049102171_0001_000001\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:329)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster(ApplicationMasterService.java:288)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.finishApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:75)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:97)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)\n \n\tat org.apache.hadoop.ipc.Client.call(Client.java:1406)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1359)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy20.finishApplicationMaster(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.finishApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:91)\n\t... 17 more\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2014-03-17T13:24:42.316+0000","updated":"2014-03-17T13:24:42.316+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13937809","id":"13937809","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"Wondering if this is a case where the NM or AM somehow failed to heartbeat and expired from the RM's point of view.  At that point the RM will ask the NM to kill all containers when it resyncs and will have cleaned up the bookkeeping on the AM (hence an unknown app attempt).  The RM log should shed some light on what happened there.\n\nNormally when an AM is told to \"go away\" by the RM there will be a subsequent AM attempt following it up (assuming there are app attempt retries left).  In those cases the AM attempt should leave without causing any damage to subsequent attempts (e.g.: don't cleanup staging areas and prevent subsequent attempts from launching).  However if the attempt is the last one then it should go ahead and perform any normal shutdown cleanup as there will not be any subsequent attempts to clean up the mess.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2014-03-17T13:46:33.008+0000","updated":"2014-03-17T13:46:33.008+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13939658","id":"13939658","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=keyki","name":"keyki","key":"keyki","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Krisztian Horvath","active":true,"timeZone":"Etc/UTC"},"body":"The original exception occurred on osx with hadoop 2.3.0, but after trying on debian with hadoop 2.2.0 the result is the same. I attached some logs. \nThe Flume agent starts with a provider creating the shell command to exec.\n\nOther than that, the application should run in the requested container, right? Because it seems that it is running outside of it (setting the yarn.memory to 8mb and the jvm heap size of the application a much larger and works fine).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=keyki","name":"keyki","key":"keyki","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Krisztian Horvath","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-18T19:03:56.611+0000","updated":"2014-03-18T19:03:56.611+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13939720","id":"13939720","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"-even if you start up a process with lots of RAM, it still runs in a container unless you tell YARN to enforce memory limits:\n\n{code}\n  <property>\n    <description>Whether physical memory limits will be enforced for\n      containers.\n    </description>\n    <name>yarn.nodemanager.pmem-check-enabled</name>\n    <value>true</value>\n  </property>\n  <property>\n    <name>yarn.nodemanager.vmem-check-enabled</name>\n    <value>true</value>\n  </property>\n  {code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2014-03-18T20:18:30.854+0000","updated":"2014-03-18T20:18:30.854+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13939798","id":"13939798","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=keyki","name":"keyki","key":"keyki","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Krisztian Horvath","active":true,"timeZone":"Etc/UTC"},"body":"I'm curious whether this bug is causing the application to hang up after container shutdown.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=keyki","name":"keyki","key":"keyki","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Krisztian Horvath","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-18T21:09:06.659+0000","updated":"2014-03-18T21:09:06.659+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13940705","id":"13940705","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matyix","name":"matyix","key":"matyix","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Janos Matyas","active":true,"timeZone":"Europe/Berlin"},"body":"Hi,\n\nThis seems to be an issue on OS/X and Debian only. We have just tried on CentOS (for automatic Hoya install on CentOS feel free to use this script - https://github.com/sequenceiq/hadoop-docker/blob/master/hoya-centos-install.sh) and it works fine launching HBase containers. \n\nAlso we have tried our custom Apache Flume provider (https://github.com/sequenceiq/hoya) and it works well - launching and stoping containers as supposed. \n\nA quick note: on Debian and OS/X there are different exceptions if you launch the containers using IP address or localhost (hoya create hbase --role master 1 --role worker 1 --manager localhost:8032 --filesystem     hdfs://localhost:9000 --image hdfs://localhost:9000/hbase.tar.gz --appconf file:///tmp/hoya-master/hoya-core/src/main/resources/org/apache/hoya/providers/hbase/conf --zkhosts localhost)\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matyix","name":"matyix","key":"matyix","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Janos Matyas","active":true,"timeZone":"Europe/Berlin"},"created":"2014-03-19T17:17:12.758+0000","updated":"2014-03-19T17:17:12.758+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13977446","id":"13977446","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wavenger","name":"wavenger","key":"wavenger","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brian Murphy","active":true,"timeZone":"Etc/UTC"},"body":"Hey there,\n\nWe are seeing this bug occur while shutting down Samza containers as well. We are running Hadoop 2.3.0 on Ubuntu 12.10. The container hangs indefinitely in the KILLING state.\n\nHere is the stack trace:\n\n{code}\n2014-04-22 20:25:08 SamzaAppMaster$ [ERROR] Error occured in amClient's callback\norg.apache.samza.SamzaException: Received a reboot signal from the RM, so throwing an exception to reboot the AM.\n\tat org.apache.samza.job.yarn.SamzaAppMasterLifecycle.onReboot(SamzaAppMasterLifecycle.scala:59)\n\tat org.apache.samza.job.yarn.SamzaAppMaster$$anonfun$onShutdownRequest$1.apply(SamzaAppMaster.scala:136)\n\tat org.apache.samza.job.yarn.SamzaAppMaster$$anonfun$onShutdownRequest$1.apply(SamzaAppMaster.scala:136)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.samza.job.yarn.SamzaAppMaster$.onShutdownRequest(SamzaAppMaster.scala:136)\n\tat org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:285)\n2014-04-22 20:25:09 ELContextCleaner [INFO] javax.el.BeanELResolver purged\n2014-04-22 20:25:09 ContextHandler [INFO] stopped o.e.j.w.WebAppContext{/,jar:file:/mnt/data/hadoop/yarn/usercache/brian/appcache/application_1397507485520_0040/filecache/10/samza-job-package-0.7.0-dist.tar.gz/lib/samza-yarn_2.10-0.7.0.jar!/scalate}\n2014-04-22 20:25:10 ELContextCleaner [INFO] javax.el.BeanELResolver purged\n2014-04-22 20:25:10 ContextHandler [INFO] stopped o.e.j.w.WebAppContext{/,jar:file:/mnt/data/hadoop/yarn/usercache/brian/appcache/application_1397507485520_0040/filecache/10/samza-job-package-0.7.0-dist.tar.gz/lib/samza-yarn_2.10-0.7.0.jar!/scalate}\n2014-04-22 20:25:10 SamzaAppMasterLifecycle [INFO] Shutting down.\n2014-04-22 20:25:10 SamzaAppMaster$ [WARN] Listener org.apache.samza.job.yarn.SamzaAppMasterLifecycle@3c9ead34 failed to shutdown.\norg.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException: Application doesn't exist in cache appattempt_1397507485520_0040_000001\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:329)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster(ApplicationMasterService.java:288)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.finishApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:75)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:97)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wavenger","name":"wavenger","key":"wavenger","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brian Murphy","active":true,"timeZone":"Etc/UTC"},"created":"2014-04-22T21:15:59.452+0000","updated":"2014-04-22T21:15:59.452+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13977601","id":"13977601","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey folks, can someone who can reproduce it with ease upload both RM as well as the AM logs?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinodkv","name":"vinodkv","key":"vinodkv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinod Kumar Vavilapalli","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-04-22T23:10:29.067+0000","updated":"2014-04-22T23:10:29.067+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13977886","id":"13977886","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"body":"Took a look at this, I'm wondering if it's caused by this case\n1) Client asked kill application, \n2) After RM transferred application's state to killed, and before AM container actually killed by NM, the AM asked to finish application\nSince the RMAppAttempt already called AMS.unregisterAttempt, the attempt will be cleaned from cache, thus the InvalidApplicationMasterRequestException will be raised.\n\nI guess this after reading log uploaded by [~keyki], \nStill pretty good in following log,\n{code}\n2014-03-18 19:36:50,802 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1395167286771_0002 State change from ACCEPTED to RUNNING\n2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1395167286771_0002_01_000002 Container Transitioned from NEW to ALLOCATED\n2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=keyki\tOPERATION=AM Allocated Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1395167286771_0002\tCONTAINERID=container_1395167286771_0002_01_000002\n2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1395167286771_0002_01_000002 of capacity <memory:1024, vCores:1> on host localhost:56214, which currently has 2 containers, <memory:2048, vCores:2> used and <memory:6144, vCores:6> available\n2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignedContainer application=application_1395167286771_0002 container=Container: [ContainerId: container_1395167286771_0002_01_000002, NodeId: localhost:56214, NodeHttpAddress: localhost:8042, Resource: <memory:1024, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: 127.0.0.1:56214 }, ] containerId=container_1395167286771_0002_01_000002 queue=default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:1024, vCores:1>usedCapacity=0.125, absoluteUsedCapacity=0.125, numApps=1, numContainers=1 usedCapacity=0.125 absoluteUsedCapacity=0.125 used=<memory:1024, vCores:1> cluster=<memory:8192, vCores:8>\n2014-03-18 19:36:52,534 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting assigned queue: root.default stats: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:2048, vCores:2>usedCapacity=0.25, absoluteUsedCapacity=0.25, numApps=1, numContainers=2\n2014-03-18 19:36:52,535 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.25 absoluteUsedCapacity=0.25 used=<memory:2048, vCores:2> cluster=<memory:8192, vCores:8>\n2014-03-18 19:36:52,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1395167286771_0002_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\n2014-03-18 19:36:53,536 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1395167286771_0002_01_000002 Container Transitioned from ACQUIRED to RUNNING\n{code}\n\nClient asked kill application, and AMS.unregisterAttempt called, attempt will be removed from AMS cache\n{code}\n2014-03-18 19:38:50,427 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=keyki\tIP=37.139.29.192\tOPERATION=Kill Application Request\tTARGET=ClientRMService\tRESULT=SUCCESS\tAPPID=application_1395167286771_0002\n2014-03-18 19:38:50,427 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing info for app: application_1395167286771_0002\n2014-03-18 19:38:50,427 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1395167286771_0002 State change from RUNNING to KILLED\n2014-03-18 19:38:50,428 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1395167286771_0002_000001\n{code}\n\nAfter that, AM asked finishApplication, but unfortunately, attempt is already removed from cache\n{code}\n2014-03-18 19:38:51,397 ERROR org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AppAttemptId doesnt exist in cache appattempt_1395167286771_0002_000001\n2014-03-18 19:38:52,415 ERROR org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Application doesn't exist in cache appattempt_1395167286771_0002_000001\n{code}\n\nI'm not sure if it's possible in current Hoya design, please correct me if I was wrong. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-04-23T06:18:03.381+0000","updated":"2014-04-23T06:18:03.381+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13977987","id":"13977987","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"The code to work in the AM is still there in the slider project, so it still exists","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2014-04-23T08:29:59.112+0000","updated":"2014-04-23T08:29:59.112+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13978318","id":"13978318","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"body":"So I think we should improve the message reported to AM when FinishApplicationRequest received, and application is already finished/killed/failed in RM side. Just throwing \"Application doesn't exist in cache\" exception doesn't make sense to user.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftnoteasy","name":"leftnoteasy","key":"leftnoteasy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=leftnoteasy&avatarId=18647","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=leftnoteasy&avatarId=18647","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=leftnoteasy&avatarId=18647","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=leftnoteasy&avatarId=18647"},"displayName":"Wangda Tan","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-04-23T15:40:09.271+0000","updated":"2014-04-23T15:40:09.271+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/13978415","id":"13978415","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matyix","name":"matyix","key":"matyix","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Janos Matyas","active":true,"timeZone":"Europe/Berlin"},"body":"\nWe had the same issues on OSX (during dev) and Debian server(s), but since we switched to CentOS it works fine (Hadoop 2.3, Hoya 0.13, HBase 0.98, Zookeeper 3.3.6).\n\nI was hoping to find that this is some env/os related issue, and I have built a new docker image(s) starting from the same Dockerfile we use with CentOS but I wasn't able to reproduce it on Ubuntu. You can get the Ubuntu based Hoya image from https://github.com/matyix/hoya-docker-ubuntu or the CentOS one from https://github.com/sequenceiq/hoya-docker if you'd like to try it.\n\nFor us the problem was coming when we were freezing HBase or Flume clusters (custom provider) with Hoya ( hoya freeze hbase --manager localhost:8032 --filesystem hdfs://localhost:9000).\n\nI will try it on Debian tomorrow, but I think I will need to skim through the code and see what we have changed to get rid of this issue.\n\nWe are migrating this to Slider but as Steve mentioned the code in AM is still there ...\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=matyix","name":"matyix","key":"matyix","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Janos Matyas","active":true,"timeZone":"Europe/Berlin"},"created":"2014-04-23T16:39:29.788+0000","updated":"2014-04-23T16:39:29.788+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/14244176","id":"14244176","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fs","name":"fs","key":"fs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Falk Scheerschmidt","active":true,"timeZone":"Europe/Berlin"},"body":"Same problem here with Hadoop 2.6 and Zookeeper 3.4 on Debian. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fs","name":"fs","key":"fs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Falk Scheerschmidt","active":true,"timeZone":"Europe/Berlin"},"created":"2014-12-12T14:24:20.307+0000","updated":"2014-12-12T14:24:20.307+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701893/comment/14247949","id":"14247949","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fs","name":"fs","key":"fs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Falk Scheerschmidt","active":true,"timeZone":"Europe/Berlin"},"body":"It seems to be a Linux Kernel problem, see SAMZA-498","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fs","name":"fs","key":"fs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Falk Scheerschmidt","active":true,"timeZone":"Europe/Berlin"},"created":"2014-12-16T08:46:03.909+0000","updated":"2014-12-16T08:46:03.909+0000"}],"maxResults":14,"total":14,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-1842/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1tij3:"}}