{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12737970","self":"https://issues.apache.org/jira/rest/api/2/issue/12737970","key":"YARN-2476","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2014-08-30T13:07:20.068+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Oct 03 04:34:03 UTC 2014","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_2937800545_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-10-03T04:32:46.943+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-2476/watchers","watchCount":7,"isWatching":false},"created":"2014-08-30T04:29:26.446+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["ha","high-availability","resourcemanager"],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326697","id":"12326697","description":"2.4.1 bug-fix release","name":"2.4.1","archived":false,"released":true,"releaseDate":"2014-06-30"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-10-03T04:34:03.967+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319322","id":"12319322","name":"resourcemanager"}],"timeoriginalestimate":null,"description":"RM HA is configured with 2 RMs. Used FileSystemRMStateStore.\n\nFairscheduler allocation file is configured in yarn-site.xml:\n<property>\n  <name>yarn.scheduler.fair.allocation.file</name>\n  <value>/opt/mapr/hadoop/hadoop-2.4.1/etc/hadoop/allocation-pools.xml</value>\n</property>\n\nFS allocation-pools.xml:\n<?xml version=\"1.0\"?>\n<allocations>\n   <queue name=\"dev\">\n      <minResources>10000 mb,10vcores</minResources>\n          <maxResources>19000 mb,100vcores</maxResources>\n          <maxRunningApps>5525</maxRunningApps>\n          <weight>4.5</weight>\n          <schedulingPolicy>fair</schedulingPolicy>\n          <fairSharePreemptionTimeout>3600</fairSharePreemptionTimeout>\n   </queue>\n   <queue name=\"default\">\n      <minResources>10000 mb,10vcores</minResources>\n          <maxResources>19000 mb,100vcores</maxResources>\n          <maxRunningApps>5525</maxRunningApps>\n          <weight>1.5</weight>\n          <schedulingPolicy>fair</schedulingPolicy>\n          <fairSharePreemptionTimeout>3600</fairSharePreemptionTimeout>\n   </queue>\n    <defaultMinSharePreemptionTimeout>600</defaultMinSharePreemptionTimeout>\n    <fairSharePreemptionTimeout>600</fairSharePreemptionTimeout>\n</allocations>\n\n\n    Submitted 10 sleep jobs to a FS queue using the command:\n    hadoop jar hadoop-mapreduce-examples-2.4.1-mapr-4.0.1-SNAPSHOT.jar sleep\n    -Dmapreduce.job.queuename=root.dev  -m 10 -r 10 -mt 10000 -rt 10000\n\n    All the jobs were submitted by the same user, with the same priority and to the\n    same queue. No other jobs were running in the cluster. Jobs started executing\n    in the order in which they were submitted (jobs 6 to 10 were active, while 11\n    to 15 were waiting):\n    root@perfnode131:/opt/mapr/hadoop/hadoop-2.4.1/logs# yarn application -list\n    Total number of applications (application-types: [] and states: [SUBMITTED,ACCEPTED, RUNNING]):10\n    Application-Id      Application-Name        Application-Type User           Queue                   State             Final-State Progress                        Tracking-URL\n    application_1408572781346_0012             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0% N/A\n    application_1408572781346_0014             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0% N/A\n    application_1408572781346_0011             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0% N/A\n    application_1408572781346_0010             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5% http://perfnode132:52799\n    application_1408572781346_0008             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5% http://perfnode131:33766\n    application_1408572781346_0009             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5% http://perfnode132:50964\n    application_1408572781346_0007             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5% http://perfnode134:52966\n    application_1408572781346_0015             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0% N/A\n    application_1408572781346_0006             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 9.5% http://perfnode134:34094\n    application_1408572781346_0013             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0%  N/A\n\n\n    Stopped RM1. There was a failover and RM2 became active. But the jobs seem to\n    have started in a different order:\n    root@perfnode131:~/scratch/raw_rm_logs_fs_hang# yarn application -list\n    14/08/21 07:26:13 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2\n    Total number of applications (application-types: [] and states: [SUBMITTED,ACCEPTED, RUNNING]):10\n    Application-Id      Application-Name        Application-Type User           Queue                   State             Final-State Progress                        Tracking-URL\n    application_1408572781346_0012             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5%http://perfnode134:59351\n    application_1408572781346_0014             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5%http://perfnode132:37866\n    application_1408572781346_0011             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5%http://perfnode131:59744\n    application_1408572781346_0010             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0%N/A\n    application_1408572781346_0008             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0%N/A\n    application_1408572781346_0009             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0%N/A\n    application_1408572781346_0007             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0%N/A\n    application_1408572781346_0015             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5%http://perfnode134:39754\n    application_1408572781346_0006             Sleep job               MAPREDUCE userA        root.dev                ACCEPTED               UNDEFINED 0%N/A\n    application_1408572781346_0013             Sleep job               MAPREDUCE userA        root.dev                 RUNNING               UNDEFINED 5%http://perfnode132:34714\n\n\n\nThe problem is this:\n- The jobs that were previously in RUNNING state moved to ACCEPTED after failover.\n- The jobs that were previously in ACCEPTED state moved to RUNNING after failover.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Apps are scheduled in random order after RM failover","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=smarella","name":"smarella","key":"smarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santosh Marella","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=smarella","name":"smarella","key":"smarella","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Santosh Marella","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Linux","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737970/comment/14116368","id":"14116368","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ozawa","name":"ozawa","key":"ozawa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ozawa&avatarId=21740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ozawa&avatarId=21740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ozawa&avatarId=21740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ozawa&avatarId=21740"},"displayName":"Tsuyoshi Ozawa","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for your report, Santosh. IIUC, it's because RM restart Phase 1 only supports re-submitting jobs after failover. Once RM restart Phase 2(YARN-556) is done, states of the jobs are preserved. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ozawa","name":"ozawa","key":"ozawa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ozawa&avatarId=21740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ozawa&avatarId=21740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ozawa&avatarId=21740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ozawa&avatarId=21740"},"displayName":"Tsuyoshi Ozawa","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-08-30T13:07:20.068+0000","updated":"2014-08-30T13:07:20.068+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737970/comment/14116843","id":"14116843","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kkambatl","name":"kkambatl","key":"kkambatl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":false,"timeZone":"America/Los_Angeles"},"body":"As Tsuyoshi said, this is the expected behavior today. The apps are re-submitted after restart/failover. Because we are re-submitting programmatically, the jobs all come in roughly at the same time, and any of them could be scheduled. If you want strictly FIFO behavior, you can always set the policy for that queue to be FIFO. Once work-preserving RM restart is done, we wouldn't kill the running AM and resubmit the app and the order before the restart/failover is preserved. \n\nOrthogonal - FileSystemRMStateStore is good for RM restart, but can have fencing issues with RM HA. We recommend using ZKRMStateStore for RM HA. \n\nI propose closing this as a duplicate (is part of) of YARN-556.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kkambatl","name":"kkambatl","key":"kkambatl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Karthik Kambatla","active":false,"timeZone":"America/Los_Angeles"},"created":"2014-08-31T18:06:04.152+0000","updated":"2014-08-31T18:06:04.152+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737970/comment/14157671","id":"14157671","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ozawa","name":"ozawa","key":"ozawa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ozawa&avatarId=21740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ozawa&avatarId=21740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ozawa&avatarId=21740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ozawa&avatarId=21740"},"displayName":"Tsuyoshi Ozawa","active":true,"timeZone":"America/Los_Angeles"},"body":"Closing this issue as a duplicated issue of (is part of) of YARN-556. Please feel free to reopen this issue if you have any comments. Thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ozawa","name":"ozawa","key":"ozawa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=ozawa&avatarId=21740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=ozawa&avatarId=21740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=ozawa&avatarId=21740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=ozawa&avatarId=21740"},"displayName":"Tsuyoshi Ozawa","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-10-03T04:34:03.967+0000","updated":"2014-10-03T04:34:03.967+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-2476/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1zijj:"}}