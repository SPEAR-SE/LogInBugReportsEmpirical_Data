{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12844806","self":"https://issues.apache.org/jira/rest/api/2/issue/12844806","key":"YARN-3921","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2015-07-13T21:57:19.048+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Jul 13 22:20:25 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_2000670_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-07-13T22:21:18.861+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-3921/watchers","watchCount":2,"isWatching":false},"created":"2015-07-13T21:47:58.217+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12331976","id":"12331976","description":"2.7.1 release","name":"2.7.1","archived":false,"released":true,"releaseDate":"2015-07-06"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-07-13T22:21:18.883+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Prior to enabling Kerberos on the Hadoop cluster, I am able to run a simple MapReduce example as the Linux user 'tdatuser':\n{code}\niripiri1:~ # su tdatuser\n    tdatuser@piripiri1:/root> yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples-2.*.jar pi 16 10000\n    Number of Maps  = 16\n    Samples per Map = 10000\n    Wrote input for Map #0\n    Wrote input for Map #1\n    Wrote input for Map #2\n    Wrote input for Map #3\n    Wrote input for Map #4\n    Wrote input for Map #5\n    Wrote input for Map #6\n    Wrote input for Map #7\n    Wrote input for Map #8\n    Wrote input for Map #9\n    Wrote input for Map #10\n    Wrote input for Map #11\n    Wrote input for Map #12\n    Wrote input for Map #13\n    Wrote input for Map #14\n    Wrote input for Map #15\n    Starting Job\n    15/07/13 17:02:31 INFO impl.TimelineClientImpl: Timeline service address: http:/                                   s/v1/timeline/\n    15/07/13 17:02:31 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to\n    15/07/13 17:02:31 INFO input.FileInputFormat: Total input paths to process : 16\n    15/07/13 17:02:31 INFO mapreduce.JobSubmitter: number of splits:16\n    15/07/13 17:02:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_14\n    15/07/13 17:02:32 INFO impl.YarnClientImpl: Submitted application application_14\n    15/07/13 17:02:32 INFO mapreduce.Job: The url to track the job: http://piripiri3                                   cation_1436821014431_0003/\n    15/07/13 17:02:32 INFO mapreduce.Job: Running job: job_1436821014431_0003\n    15/07/13 17:05:50 INFO mapreduce.Job: Job job_1436821014431_0003 running in uber mode : false\n    15/07/13 17:05:50 INFO mapreduce.Job:  map 0% reduce 0%\n    15/07/13 17:05:56 INFO mapreduce.Job:  map 6% reduce 0%\n    15/07/13 17:06:00 INFO mapreduce.Job:  map 13% reduce 0%\n    15/07/13 17:06:01 INFO mapreduce.Job:  map 38% reduce 0%\n    15/07/13 17:06:05 INFO mapreduce.Job:  map 44% reduce 0%\n    15/07/13 17:06:07 INFO mapreduce.Job:  map 63% reduce 0%\n    15/07/13 17:06:09 INFO mapreduce.Job:  map 69% reduce 0%\n    15/07/13 17:06:11 INFO mapreduce.Job:  map 75% reduce 0%\n    15/07/13 17:06:12 INFO mapreduce.Job:  map 81% reduce 0%\n    15/07/13 17:06:13 INFO mapreduce.Job:  map 81% reduce 25%\n    15/07/13 17:06:14 INFO mapreduce.Job:  map 94% reduce 25%\n    15/07/13 17:06:16 INFO mapreduce.Job:  map 100% reduce 31%\n    15/07/13 17:06:17 INFO mapreduce.Job:  map 100% reduce 100%\n    15/07/13 17:06:17 INFO mapreduce.Job: Job job_1436821014431_0003 completed successfully\n    15/07/13 17:06:17 INFO mapreduce.Job: Counters: 49\n            File System Counters\n                    FILE: Number of bytes read=358\n                    FILE: Number of bytes written=2249017\n                    FILE: Number of read operations=0\n                    FILE: Number of large read operations=0\n                    FILE: Number of write operations=0\n                    HDFS: Number of bytes read=4198\n                    HDFS: Number of bytes written=215\n                    HDFS: Number of read operations=67\n                    HDFS: Number of large read operations=0\n                    HDFS: Number of write operations=3\n            Job Counters\n                    Launched map tasks=16\n                    Launched reduce tasks=1\n                    Data-local map tasks=16\n                    Total time spent by all maps in occupied slots (ms)=160498\n                    Total time spent by all reduces in occupied slots (ms)=27302\n                    Total time spent by all map tasks (ms)=80249\n                    Total time spent by all reduce tasks (ms)=13651\n                    Total vcore-seconds taken by all map tasks=80249\n                    Total vcore-seconds taken by all reduce tasks=13651\n                    Total megabyte-seconds taken by all map tasks=246524928\n                    Total megabyte-seconds taken by all reduce tasks=41935872\n            Map-Reduce Framework\n                    Map input records=16\n                    Map output records=32\n                    Map output bytes=288\n                    Map output materialized bytes=448\n                    Input split bytes=2310\n                    Combine input records=0\n                    Combine output records=0\n                    Reduce input groups=2\n                    Reduce shuffle bytes=448\n                    Reduce input records=32\n                    Reduce output records=0\n                    Spilled Records=64\n                    Shuffled Maps =16\n                    Failed Shuffles=0\n                    Merged Map outputs=16\n                    GC time elapsed (ms)=1501\n                    CPU time spent (ms)=13670\n                    Physical memory (bytes) snapshot=13480296448\n                    Virtual memory (bytes) snapshot=72598511616\n                    Total committed heap usage (bytes)=12508463104\n            Shuffle Errors\n                    BAD_ID=0\n                    CONNECTION=0\n                    IO_ERROR=0\n                    WRONG_LENGTH=0\n                    WRONG_MAP=0\n                    WRONG_REDUCE=0\n            File Input Format Counters\n                    Bytes Read=1888\n            File Output Format Counters\n                    Bytes Written=97\n    Job Finished in 226.813 seconds\n    Estimated value of Pi is 3.14127500000000000000\n{code}\n\nHowever, after enabling Kerberos, the job fails:\n{code}\ntdatuser@piripiri1:/root> kinit -kt /etc/security/keytabs/tdatuser.headless.keytab tdatuser\n    tdatuser@piripiri1:/root> yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples-2.*.jar pi 16 10000\n    Number of Maps  = 16\n    Samples per Map = 10000\n    Wrote input for Map #0\n    Wrote input for Map #1\n    Wrote input for Map #2\n    Wrote input for Map #3\n    Wrote input for Map #4\n    Wrote input for Map #5\n    Wrote input for Map #6\n    Wrote input for Map #7\n    Wrote input for Map #8\n    Wrote input for Map #9\n    Wrote input for Map #10\n    Wrote input for Map #11\n    Wrote input for Map #12\n    Wrote input for Map #13\n    Wrote input for Map #14\n    Wrote input for Map #15\n    Starting Job\n    15/07/13 17:27:05 INFO impl.TimelineClientImpl: Timeline service address: http://piripiri1.labs.teradata.com:8188/ws/v1/timeline/\n    15/07/13 17:27:05 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 140 for tdatuser on ha-hdfs:PIRIPIRI\n    15/07/13 17:27:05 INFO security.TokenCache: Got dt for hdfs://PIRIPIRI; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:PIRIPIRI, Ident: (HDFS_DELEGATION_TOKEN token 140 for tdatuser)\n    15/07/13 17:27:06 INFO input.FileInputFormat: Total input paths to process : 16\n    15/07/13 17:27:06 INFO mapreduce.JobSubmitter: number of splits:16\n    15/07/13 17:27:06 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1436822321287_0007\n    15/07/13 17:27:06 INFO mapreduce.JobSubmitter: Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:PIRIPIRI, Ident: (HDFS_DELEGATION_TOKEN token 140 for tdatuser)\n    15/07/13 17:27:06 INFO impl.YarnClientImpl: Submitted application application_1436822321287_0007\n    15/07/13 17:27:06 INFO mapreduce.Job: The url to track the job: http://piripiri2.labs.teradata.com:8088/proxy/application_1436822321287_0007/\n    15/07/13 17:27:06 INFO mapreduce.Job: Running job: job_1436822321287_0007\n    15/07/13 17:27:09 INFO mapreduce.Job: Job job_1436822321287_0007 running in uber mode : false\n    15/07/13 17:27:09 INFO mapreduce.Job:  map 0% reduce 0%\n    15/07/13 17:27:09 INFO mapreduce.Job: Job job_1436822321287_0007 failed with state FAILED due to: Application application_1436822321287_0007 failed 2 times due to AM Container for appattempt_1436822321287_0007_000002 exited with  exitCode: -1000\n    For more detailed output, check application tracking page:http://piripiri2.labs.teradata.com:8088/cluster/app/application_1436822321287_0007Then, click on links to logs of each attempt.\n    Diagnostics: Application application_1436822321287_0007 initialization failed (exitCode=255) with output: main : command provided 0\n    main : run as user is tdatuser\n    main : requested yarn user is tdatuser\n    Can't create directory /data1/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data2/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data3/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data4/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data5/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data6/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data7/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data8/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data9/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data10/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data11/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Can't create directory /data12/hadoop/yarn/local/usercache/tdatuser/appcache/application_1436822321287_0007 - Permission denied\n    Did not create any app directories\n\n    Failing this attempt. Failing the application.\n    15/07/13 17:27:09 INFO mapreduce.Job: Counters: 0\n    Job Finished in 4.748 seconds\n    java.io.FileNotFoundException: File does not exist: hdfs://PIRIPIRI/user/tdatuser/QuasiMonteCarlo_1436822823095_2120947622/out/reduce-out\n            at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309)\n            at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n            at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n            at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n            at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1752)\n            at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1776)\n            at org.apache.hadoop.examples.QuasiMonteCarlo.estimatePi(QuasiMonteCarlo.java:314)\n            at org.apache.hadoop.examples.QuasiMonteCarlo.run(QuasiMonteCarlo.java:354)\n            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n            at org.apache.hadoop.examples.QuasiMonteCarlo.main(QuasiMonteCarlo.java:363)\n            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n            at java.lang.reflect.Method.invoke(Method.java:483)\n            at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n            at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n            at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n            at java.lang.reflect.Method.invoke(Method.java:483)\n            at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n            at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n{code}\n\nAs seen above there are many \"Can't create directory... Permission denied errors\" related to the local usercache directory for the 'tdatuser'.\n\nPrior to enabling Kerberos, the contents of a usercache directory was as follows:\n{code}\npiripiri4:~ # ls -l /data1/hadoop/yarn/local/usercache/\n    total 0\n    drwxr-xr-x 3 yarn hadoop 21 Jul 13 16:59 ambari-qa\n    drwxr-x--- 4 yarn hadoop 37 Jul 13 17:00 tdatuser\n{code}\n\nAfter enabling Kerberos the contents are:\n{code}\npiripiri4:~ # ls -l /data1/hadoop/yarn/local/usercache/\n    total 0\n    drwxr-s--- 4 ambari-qa hadoop 37 Jul 13 17:21 ambari-qa\n    drwxr-x--- 4 yarn      hadoop 37 Jul 13 17:00 tdatuser\n{code}\n\nIt appears that the owner of the usercache directory for the 'ambari-qa' user was updated, but the 'tdatuser' directory was not. \n\nIs this expected behavior, and is there a recommended work-around for this issue? ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Permission denied errors for local usercache directories when attempting to run MapReduce job on Kerberos enabled cluster ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zmarsh13","name":"zmarsh13","key":"zmarsh13","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Zack Marsh","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zmarsh13","name":"zmarsh13","key":"zmarsh13","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Zack Marsh","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"sles11sp3","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12844806/comment/14625438","id":"14625438","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"AFAIK, YARN won't change the permissions on the work dirs when you switch modes.  The assumption is the ops folks/tools will handle this as part of the transition.\n\namabari-qa's dir changing seems to be more related to something else (are these machines being managed via ambari and, like a naughty child, ambari is putting these where they don't belong?) given that you didn't say that a job belonging to the user ambari-qa job was run...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2015-07-13T21:57:19.048+0000","updated":"2015-07-13T21:57:37.307+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12844806/comment/14625448","id":"14625448","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zmarsh13","name":"zmarsh13","key":"zmarsh13","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Zack Marsh","active":true,"timeZone":"Etc/UTC"},"body":"Yes this cluster is being manager by Ambari, and yes there were jobs belonging to the user ambari-qa ran before and after enabling Kerberos.\n\nGiven what you said, it sounds like the fix for this issue is just changing the ownersip on these usercache directories and the directories/files within to the appropriate user.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zmarsh13","name":"zmarsh13","key":"zmarsh13","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Zack Marsh","active":true,"timeZone":"Etc/UTC"},"created":"2015-07-13T22:03:08.965+0000","updated":"2015-07-13T22:03:08.965+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12844806/comment/14625455","id":"14625455","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"Yeah. In general when switching from non-K to K, people move to LinuxContainerExecutor first.  When they do that, a general going over of all permissions is done first since things like temp dirs tend to require rwx+sticky.  LCE is what is almost certainly forcing the permissions failures in your jobs.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2015-07-13T22:06:12.480+0000","updated":"2015-07-13T22:06:12.480+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12844806/comment/14625463","id":"14625463","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"So, this is probably a bug in Ambari, really.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2015-07-13T22:10:32.495+0000","updated":"2015-07-13T22:10:32.495+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12844806/comment/14625480","id":"14625480","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zmarsh13","name":"zmarsh13","key":"zmarsh13","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Zack Marsh","active":true,"timeZone":"Etc/UTC"},"body":"Okay, thanks for the responses. I've  created AMBARI-12402 for this issue (I don't think I have the permissions to move this issue to the Ambari project).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zmarsh13","name":"zmarsh13","key":"zmarsh13","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Zack Marsh","active":true,"timeZone":"Etc/UTC"},"created":"2015-07-13T22:20:25.202+0000","updated":"2015-07-13T22:20:25.202+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-3921/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2h7in:"}}