{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13160456","self":"https://issues.apache.org/jira/rest/api/2/issue/13160456","key":"YARN-8326","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12313722","id":"12313722","key":"YARN","name":"Hadoop YARN","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12313722&avatarId=15135","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12313722&avatarId=15135","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12313722&avatarId=15135","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12313722&avatarId=15135"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12342758","id":"12342758","name":"3.2.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12342982","id":"12342982","description":"3.1.1 Release","name":"3.1.1","archived":false,"released":true,"releaseDate":"2018-08-07"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2018-05-21T18:56:02.023+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Jun 29 15:45:23 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_3008892904_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_18456639","customfield_12312321":null,"resolutiondate":"2018-06-22T23:28:17.308+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-8326/watchers","watchCount":13,"isWatching":false},"created":"2018-05-18T22:32:27.804+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"3.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12341435","id":"12341435","description":"3.0.0 GA release","name":"3.0.0","archived":false,"released":true,"releaseDate":"2017-12-13"}],"issuelinks":[{"id":"12537501","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12537501","type":{"id":"12310560","name":"Problem/Incident","inward":"is caused by","outward":"causes","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310560"},"inwardIssue":{"id":"12988950","key":"YARN-5366","self":"https://issues.apache.org/jira/rest/api/2/issue/12988950","fields":{"summary":"Improve handling of the Docker container life cycle","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanekumpf%40gmail.com","name":"shanekumpf@gmail.com","key":"shanekumpf@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shane Kumpf","active":true,"timeZone":"America/Denver"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-06-29T15:48:47.628+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12325004","id":"12325004","name":"yarn"}],"timeoriginalestimate":null,"description":"Hi,  I am running testcases on Yarn 2.6 and Yarn 3.0 and found out the performance seems like twice slower on Yarn 3.0, and the performance would get even slower if we acquire more containers.   I looked at the node manager logs on 2.6 vs 3.0.   Here is what I find below.  \r\n\r\nOn 2.6 ,  this is a life cycle of a specific container,  from beginning to end, it takes about{color:#14892c}  8 seconds{color} (9:53:50 to 9:53:58). \r\n\r\n!image-2018-05-18-15-20-33-839.png!\r\n\r\nOn 3.0: the life cycle of a specific container looks like this,  it takes{color:#d04437} 20 seconds{color} to finish the same job.  (9:51:44 to 9:52:04)\r\n\r\n!image-2018-05-18-15-22-30-948.png!\r\n\r\n It seems like on 3.0, it spends an extra 5 seconds on monitor.ContinaerMonitorImpl  (marked in {color:#d04437}red{color}) which doesn't happen in 2.6,  and also after the job is done, and the container is exiting,  on 3.0, it took 5 seconds to do that (9:51:59 to 9:52:04)  which on 2.6, it only took less than 1/.2 of the time. (9: 53:56 to 9:53:58).  \r\n\r\n   Since we are running the same unit testcases and usually acquire more than 4 containers,  therefore, when it addess up all these extra seconds, it became a huge performance issue.  On 2.6, the unittest runs 7 hours whilc on 3.0, the same unitests runs 11 hours.  I was told this performance delay might be caused by Hadoop’s new monitoring system Timeline service v2.  Could someone take a look of this?   Thanks for any help on this!!","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12924212","id":"12924212","filename":"image-2018-05-18-15-20-33-839.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hlhuang%40us.ibm.com","name":"hlhuang@us.ibm.com","key":"hlhuang@us.ibm.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34045","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"},"displayName":"Hsin-Liang Huang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-18T22:20:35.163+0000","size":395934,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12924212/image-2018-05-18-15-20-33-839.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12924211","id":"12924211","filename":"image-2018-05-18-15-22-30-948.png","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hlhuang%40us.ibm.com","name":"hlhuang@us.ibm.com","key":"hlhuang@us.ibm.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34045","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"},"displayName":"Hsin-Liang Huang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-18T22:22:32.561+0000","size":598817,"mimeType":"image/png","content":"https://issues.apache.org/jira/secure/attachment/12924211/image-2018-05-18-15-22-30-948.png"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12928790","id":"12928790","filename":"YARN-8326.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanekumpf%40gmail.com","name":"shanekumpf@gmail.com","key":"shanekumpf@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shane Kumpf","active":true,"timeZone":"America/Denver"},"created":"2018-06-22T17:37:55.032+0000","size":1563,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12928790/YARN-8326.001.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Yarn 3.0 seems runs slower than Yarn 2.6","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hlhuang%40us.ibm.com","name":"hlhuang@us.ibm.com","key":"hlhuang@us.ibm.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34045","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"},"displayName":"Hsin-Liang Huang","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hlhuang%40us.ibm.com","name":"hlhuang@us.ibm.com","key":"hlhuang@us.ibm.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34045","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"},"displayName":"Hsin-Liang Huang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"This is the yarn-site.xml for 3.0. \r\n\r\n \r\n\r\n<configuration>\r\n\r\n<property>\r\n <name>hadoop.registry.dns.bind-port</name>\r\n <value>5353</value>\r\n </property>\r\n\r\n<property>\r\n <name>hadoop.registry.dns.domain-name</name>\r\n <value>hwx.site</value>\r\n </property>\r\n\r\n<property>\r\n <name>hadoop.registry.dns.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>hadoop.registry.dns.zone-mask</name>\r\n <value>255.255.255.0</value>\r\n </property>\r\n\r\n<property>\r\n <name>hadoop.registry.dns.zone-subnet</name>\r\n <value>172.17.0.0</value>\r\n </property>\r\n\r\n<property>\r\n <name>manage.include.files</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.acl.enable</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.admin.acl</name>\r\n <value>yarn</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.client.nodemanager-connect.max-wait-ms</name>\r\n <value>60000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.client.nodemanager-connect.retry-interval-ms</name>\r\n <value>10000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.http.policy</name>\r\n <value>HTTP_ONLY</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.log-aggregation-enable</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.log-aggregation.retain-seconds</name>\r\n <value>2592000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.log.server.url</name>\r\n <value>[http://xxxxxx:19888/jobhistory/logs|http://whiny2.fyre.ibm.com:19888/jobhistory/logs]</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.log.server.web-service.url</name>\r\n <value>[http://xxxxxx:8188/ws/v1/applicationhistory|http://whiny2.fyre.ibm.com:8188/ws/v1/applicationhistory]</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.node-labels.enabled</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.node-labels.fs-store.retry-policy-spec</name>\r\n <value>2000, 500</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.node-labels.fs-store.root-dir</name>\r\n <value>/system/yarn/node-labels</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.address</name>\r\n <value>0.0.0.0:45454</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.admin-env</name>\r\n <value>MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.aux-services</name>\r\n <value>mapreduce_shuffle,spark2_shuffle,timeline_collector</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>\r\n <value>org.apache.hadoop.mapred.ShuffleHandler</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.aux-services.spark2_shuffle.class</name>\r\n <value>org.apache.spark.network.yarn.YarnShuffleService</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.aux-services.spark2_shuffle.classpath</name>\r\n <value>/usr/spark2/aux/*</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\r\n <value>org.apache.spark.network.yarn.YarnShuffleService</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.aux-services.timeline_collector.class</name>\r\n <value>org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.bind-host</name>\r\n <value>0.0.0.0</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.container-executor.class</name>\r\n <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.container-metrics.unregister-delay-ms</name>\r\n <value>60000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.container-monitor.interval-ms</name>\r\n <value>3000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.delete.debug-delay-sec</name>\r\n <value>0</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>\r\n <value>90</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb</name>\r\n <value>1000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>\r\n <value>0.25</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.health-checker.interval-ms</name>\r\n <value>135000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.health-checker.script.timeout-ms</name>\r\n <value>60000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.linux-container-executor.group</name>\r\n <value>hadoop</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.local-dirs</name>\r\n <value>/hadoop/yarn/local</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.log-aggregation.compression-type</name>\r\n <value>gz</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.log-aggregation.debug-enabled</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.log-aggregation.num-log-files-per-app</name>\r\n <value>30</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds</name>\r\n <value>3600</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.log-dirs</name>\r\n <value>/hadoop/yarn/log</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.log.retain-seconds</name>\r\n <value>604800</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.pmem-check-enabled</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.recovery.dir</name>\r\n <value>/var/log/hadoop-yarn/nodemanager/recovery-state</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.recovery.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.recovery.supervised</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.remote-app-log-dir</name>\r\n <value>/app-logs</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.remote-app-log-dir-suffix</name>\r\n <value>logs</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.resource-plugins</name>\r\n <value></value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices</name>\r\n <value>auto</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.resource-plugins.gpu.docker-plugin</name>\r\n <value>nvidia-docker-v1</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidiadocker-\r\n v1.endpoint</name>\r\n <value>[http://localhost:3476/v1.0/docker/cli]</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.resource-plugins.gpu.path-to-discovery-executables</name>\r\n <value></value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.resource.cpu-vcores</name>\r\n <value>6</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.resource.memory-mb</name>\r\n <value>12288</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.resource.percentage-physical-cpu-limit</name>\r\n <value>80</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.runtime.linux.allowed-runtimes</name>\r\n <value>default,docker</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.runtime.linux.docker.allowed-container-networks</name>\r\n <value>host,none,bridge</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.runtime.linux.docker.capabilities</name>\r\n <value>\r\n CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,\r\n SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.runtime.linux.docker.default-container-network</name>\r\n <value>host</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.runtime.linux.docker.privileged-containers.acl</name>\r\n <value></value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.vmem-check-enabled</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.vmem-pmem-ratio</name>\r\n <value>2.1</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.nodemanager.webapp.cross-origin.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.address</name>\r\n <value>xxxxxxx:8050</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.admin.address</name>\r\n <value>xxxxxx:8141</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.am.max-attempts</name>\r\n <value>2</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.bind-host</name>\r\n <value>0.0.0.0</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.connect.max-wait.ms</name>\r\n <value>900000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.connect.retry-interval.ms</name>\r\n <value>30000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.fs.state-store.retry-policy-spec</name>\r\n <value>2000, 500</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.fs.state-store.uri</name>\r\n <value> </value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.ha.enabled</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.hostname</name>\r\n <value>xxxxxxxx</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval</name>\r\n <value>15000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor</name>\r\n <value>1</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round</name>\r\n <value>0.25</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.nodes.exclude-path</name>\r\n <value>/etc/hadoop/conf/yarn.exclude</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.recovery.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.resource-tracker.address</name>\r\n <value>xxxxxxx:8025</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.scheduler.address</name>\r\n <value>xxxxxxxx:8030</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.scheduler.class</name>\r\n <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.scheduler.monitor.enable</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.state-store.max-completed-applications</name>\r\n <value>${yarn.resourcemanager.max-completed-applications}</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.store.class</name>\r\n <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size</name>\r\n <value>10</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.system-metrics-publisher.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.webapp.address</name>\r\n <value>xxxxxx:8088</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.webapp.cross-origin.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.webapp.https.address</name>\r\n <value>wxxxxxx:8090</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.work-preserving-recovery.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms</name>\r\n <value>10000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.zk-acl</name>\r\n <value>world:anyone:rwcda</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.zk-address</name>\r\n <value>xxxxxx:2181,xxxxxx:2181,xxxxxx:2181</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.zk-num-retries</name>\r\n <value>1000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.zk-retry-interval-ms</name>\r\n <value>1000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.zk-state-store.parent-path</name>\r\n <value>/rmstore</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.resourcemanager.zk-timeout-ms</name>\r\n <value>10000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.rm.system-metricspublisher.emit-container-events</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.scheduler.capacity.ordering-policy.priority-utilization.underutilized-preemption.enabled</name>\r\n <value>false</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.scheduler.maximum-allocation-mb</name>\r\n <value>12288</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.scheduler.maximum-allocation-vcores</name>\r\n <value>6</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.scheduler.minimum-allocation-mb</name>\r\n <value>64</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.scheduler.minimum-allocation-vcores</name>\r\n <value>1</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.service.framework.path</name>\r\n <value>/yarn/service-dep.tar.gz</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.system-metricspublisher.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.address</name>\r\n <value>xxxxxx:10200</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.bind-host</name>\r\n <value>0.0.0.0</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.client.max-retries</name>\r\n <value>30</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.client.retry-interval-ms</name>\r\n <value>1000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.active-dir</name>\r\n <value>/ats/active/</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.app-cache-size</name>\r\n <value>10</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds</name>\r\n <value>3600</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.done-dir</name>\r\n <value>/ats/done/</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes</name>\r\n <value></value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.group-id-plugin-classpath</name>\r\n <value></value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.retain-seconds</name>\r\n <value>604800</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.scan-interval-seconds</name>\r\n <value>60</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.entity-group-fs-store.summary-store</name>\r\n <value>org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.generic-application-history.store-class</name>\r\n <value>org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.hbase-schema.prefix</name>\r\n <value>prod.</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.hbase.configuration.file</name>\r\n <value>[file:///etc/yarn-hbase/conf/hbase-site.xml]</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.hbase.coprocessor.jar.hdfs.location</name>\r\n <value>[file:///hadoop-yarn-client/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor.jar|file:///usr/hdp/current/hadoop-yarn-client/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor.jar]</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.http-authentication.simple.anonymous.allowed</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.http-authentication.type</name>\r\n <value>simple</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.http-cross-origin.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.leveldb-state-store.path</name>\r\n <value>/hadoop/yarn/timeline</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.leveldb-timeline-store.path</name>\r\n <value>/hadoop/yarn/timeline</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.leveldb-timeline-store.read-cache-size</name>\r\n <value>104857600</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size</name>\r\n <value>10000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size</name>\r\n <value>10000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms</name>\r\n <value>300000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.reader.webapp.address</name>\r\n <value>xxxxxx:8198</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.reader.webapp.https.address</name>\r\n <value>xxxxxx:8199</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.recovery.enabled</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.state-store-class</name>\r\n <value>org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.store-class</name>\r\n <value>org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.ttl-enable</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.ttl-ms</name>\r\n <value>2678400000</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.version</name>\r\n <value>2.0</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.versions</name>\r\n <value>1.5f,2.0f</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.webapp.address</name>\r\n <value>xxxxxx:8188</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.timeline-service.webapp.https.address</name>\r\n <value>xxxxxx:8190</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.webapp.api-service.enable</name>\r\n <value>true</value>\r\n </property>\r\n\r\n<property>\r\n <name>yarn.webapp.ui2.enable</name>\r\n <value>true</value>\r\n </property>\r\n\r\n</configuration>","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16482905","id":"16482905","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"This appears to be introduced by YARN-5662 by turning on container monitor from false to true.  This feature is used by opportunistic container scheduling and pre-emption to gather statics of the containers to make scheduling decisions.  You can disable this feature by:\r\n\r\n{code}\r\n    <property>\r\n      <name>yarn.nodemanager.container-monitor.enabled</name>\r\n      <value>false</value>\r\n    </property>\r\n{code}\r\n\r\nOr reduce the stats collection time from 3 seconds to 300 milliseconds (use more system resources, but faster scheduling):\r\n\r\n{code}\r\n    <property>\r\n      <name>yarn.nodemanager.container-monitor.interval-ms</name>\r\n      <value>300</value>\r\n    </property>\r\n{code}\r\n\r\nTimer optimization might be possible to the work done in YARN-2883.  The queuing and scheduling of containers is based on monitoring thread information.  If it takes several seconds to wait for information to become available before next container is scheduled, then it can introduced artificial delay to rapidly launching containers.  The timer value can not be smaller than certain value otherwise monitoring/container forking both will tax cpu resources too much.  If your workloads take less time than container scheduling/launch, then you might need to revisit how to decrease the containers to launch, and increase the work to run in containers.  [~hlhuang@us.ibm.com] Can you confirm that those settings changes the benchmark result?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-21T18:56:02.023+0000","updated":"2018-05-21T18:57:14.655+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16483116","id":"16483116","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~hlhuang@us.ibm.com] Does the same log entries show up?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-21T22:03:00.197+0000","updated":"2018-05-21T22:03:00.197+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16488013","id":"16488013","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hlhuang%40us.ibm.com","name":"hlhuang@us.ibm.com","key":"hlhuang@us.ibm.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34045","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"},"displayName":"Hsin-Liang Huang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi  [~eyang]\r\n\r\n   I ran the sample job,\r\n\r\n{color:#14892c}time hadoop jar /usr/hdp/3.0.0.0-829/hadoop-yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.3.0.0.0-829.jar Client -classpath simple-yarn-app-1.1.0.jar -cmd \"java com.hortonworks.simpleyarnapp.ApplicationMaster /bin/date 8\"{color}\r\n\r\nwith the changed settings, it still ran 15 seconds compared to 6 or 7 seconds in 2.6 environment.  So I am not sure if the significant performance role that these two  monitoring setting would play in this. The major issue could still be in the exiting container that in 3.0 environment is much slower than 2.6 environment.  Can someone from yarn team look into this? This is a general yarn application performance issue in 3.0. \r\n\r\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hlhuang%40us.ibm.com","name":"hlhuang@us.ibm.com","key":"hlhuang@us.ibm.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34045","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"},"displayName":"Hsin-Liang Huang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-23T20:46:25.071+0000","updated":"2018-05-23T21:19:03.144+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16488263","id":"16488263","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hlhuang%40us.ibm.com","name":"hlhuang@us.ibm.com","key":"hlhuang@us.ibm.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34045","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"},"displayName":"Hsin-Liang Huang","active":true,"timeZone":"America/Los_Angeles"},"body":"Here is more detail information from node manager log that compares between Hadoop 3.0 and 2.6.  They are both running on 4 node cluster with 3 data nodes with same machine power/cpu/memory and same type of job.   I picked only one node to compare the container cycle. \r\n\r\n*1. On 3.0.*  when I request 8 containers to run on 3 data nodes,  I picked the second node to examine the log:\r\n\r\nthis job used 2 containers in this node:\r\n\r\n \r\n\r\ncontainer *container_e04_1527109836290_0004_01_000002*  on application application_1527109836290_0004  (from container succeeded to Stopping container (from blue to red line) took about *4 seconds*)\r\n\r\n \r\n\r\n152231 2018-05-23 15:04:45,541 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(1059)) - Start request for container_e04_1527109836290_0004_01_000002 by user hlhuang\r\n\r\n152232 2018-05-23 15:04:45,657 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(1127)) - Creating a new application reference for app application_1527109836290_0004\r\n\r\n152233 2018-05-23 15:04:45,658 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(632)) - Application application_1527109836290_0004 transitioned from NEW to INITING\r\n\r\n152234 2018-05-23 15:04:45,658 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(446)) - Adding container_e04_1527109836290_0004_01_000002 to application application_1527109836290_0004\r\n\r\n152235 2018-05-23 15:04:45,658 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(632)) - Application application_1527109836290_0004 transitioned from INITING to RUNNING\r\n\r\n152236 2018-05-23 15:04:45,659 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000002 transitioned from NEW to SCHEDULED\r\n\r\n152237 2018-05-23 15:04:45,659 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_INIT for appId application_1527109836290_0004\r\n\r\n152238 2018-05-23 15:04:45,659 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e04_1527109836290_0004_01_000002\r\n\r\n152239 2018-05-23 15:04:45,660 INFO  scheduler.ContainerScheduler (ContainerScheduler.java:startContainer(503)) - Starting container [container_e04_1527109836290_0004_01_000002]\r\n\r\n152246 2018-05-23 15:04:45,965 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000002 transitioned from SCHEDULED to RUNNING\r\n\r\n152247 2018-05-23 15:04:45,965 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStartMonitoringContainer(941)) - Starting resource-monitoring for container_e04_1527109836290_0004_01_000002\r\n\r\n{color:#205081}152250 2018-05-23 15:04:46,002 INFO  launcher.ContainerLaunch (ContainerLaunch.java:handleContainerExitCode(512)) - Container container_e04_1527109836290_0004_01_000002 succeeded{color}\r\n\r\n \r\n\r\n152251 2018-05-23 15:04:46,003 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\r\n\r\n152252 2018-05-23 15:04:46,003 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1527109836290_0004_01_000002\r\n\r\n152254 2018-05-23 15:04:48,132 INFO  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(794)) - Deleting absolute path : /hadoop/yarn/local/usercache/hlhuang/appcache/application_1527109836290_0004/container_e04_1527109836290_0004_01_000002\r\n\r\n152256 2018-05-23 15:04:48,133 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\r\n\r\n152258 2018-05-23 15:04:49,171 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:removeOrTrackCompletedContainersFromContext(682)) - Removed completed containers from NM context: [container_e04_1527109836290_0004_01_000002]\r\n\r\n152260 2018-05-23 15:04:50,289 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(489)) - Removing container_e04_1527109836290_0004_01_000002 from application application_1527109836290_0004\r\n\r\n{color:#d04437}152261 2018-05-23 15:04:50,290 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStopMonitoringContainer(932)) - Stopping resource-monitoring for container_e04_1527109836290_0004_01_000002{color}\r\n\r\n152263 2018-05-23 15:04:50,290 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e04_1527109836290_0004_01_000002\r\n\r\n152262 2018-05-23 15:04:50,290 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_STOP for appId application_1527109836290_0004\r\n\r\n \r\n\r\n \r\n\r\ncontainer *container_e04_1527109836290_0004_01_000006* on application application_1527109836290_0004   (from container succeeded to Stopping container (from blue to red line) took about 4 seconds)\r\n\r\n \r\n\r\n152240 2018-05-23 15:04:45,876 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(1059)) - Start request for container_e04_1527109836290_0004_01_000006 by user hlhuang\r\n\r\n152241 2018-05-23 15:04:45,879 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(446)) - Adding container_e04_1527109836290_0004_01_000006 to application application_1527109836290_0004\r\n\r\n152242 2018-05-23 15:04:45,879 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000006 transitioned from NEW to SCHEDULED\r\n\r\n152243 2018-05-23 15:04:45,882 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_INIT for appId application_1527109836290_0004\r\n\r\n152244 2018-05-23 15:04:45,883 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e04_1527109836290_0004_01_000006\r\n\r\n152245 2018-05-23 15:04:45,883 INFO  scheduler.ContainerScheduler (ContainerScheduler.java:startContainer(503)) - Starting container [container_e04_1527109836290_0004_01_000006]\r\n\r\n152248 2018-05-23 15:04:45,985 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000006 transitioned from SCHEDULED to RUNNING\r\n\r\n152249 2018-05-23 15:04:45,985 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStartMonitoringContainer(941)) - Starting resource-monitoring for container_e04_1527109836290_0004_01_000006\r\n\r\n{color:#205081}152253 2018-05-23 15:04:46,013 INFO  launcher.ContainerLaunch (ContainerLaunch.java:handleContainerExitCode(512)) - Container container_e04_1527109836290_0004_01_000006 succeeded{color}\r\n\r\n \r\n\r\n152255 2018-05-23 15:04:48,133 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000006 transitioned from RUNNING to EXITED_WITH_SUCCESS\r\n\r\n152257 2018-05-23 15:04:48,133 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1527109836290_0004_01_000006\r\n\r\n152259 2018-05-23 15:04:50,289 INFO  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(794)) - Deleting absolute path : /hadoop/yarn/local/usercache/hlhuang/appcache/application_1527109836290_0004/container_e04_1527109836290_0004_01_000006\r\n\r\n152264 2018-05-23 15:04:50,290 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1527109836290_0004_01_000006 transitioned from EXITED_WITH_SUCCESS to DONE\r\n\r\n152265 2018-05-23 15:04:50,291 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(489)) - Removing container_e04_1527109836290_0004_01_000006 from application application_1527109836290_0004\r\n\r\n152266 2018-05-23 15:04:50,291 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:onStopMonitoringContainer(932)) - Stopping resource-monitoring for container_e04_1527109836290_0004_01_000006\r\n\r\n152267 2018-05-23 15:04:50,291 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event CONTAINER_STOP for appId application_1527109836290_0004\r\n\r\n{color:#d04437}152268 2018-05-23 15:04:50,291 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e04_1527109836290_0004_01_000006{color}\r\n\r\n \r\n\r\n{color:#14892c}Below is something that probably needs to look at too.  You can see the previous line is 152269, and next line is 152270, and in between there is a 5 seconds gap!!!  i.e. after containers are removed till application transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP  took 5 seconds!{color}\r\n\r\n \r\n\r\n*152269 2018-05-23 15:04:51,301* INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:removeOrTrackCompletedContainersFromContext(682)) - Removed completed containers from NM context: [container_e04_1527109836290_0004_01_000006]\r\n\r\n*152270 2018-05-23 15:04:56,315* INFO  application.ApplicationImpl (ApplicationImpl.java:handle(632)) - Application application_1527109836290_0004 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\r\n\r\n152271 2018-05-23 15:04:56,316 INFO  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:deleteAsUser(794)) - Deleting absolute path : /hadoop/yarn/local/usercache/hlhuang/appcache/application_1527109836290_0004\r\n\r\n152272 2018-05-23 15:04:56,316 INFO  containermanager.AuxServices (AuxServices.java:handle(220)) - Got event APPLICATION_STOP for appId application_1527109836290_0004\r\n\r\n152273 2018-05-23 15:04:56,316 INFO  shuffle.ExternalShuffleBlockResolver (ExternalShuffleBlockResolver.java:applicationRemoved(186)) - Application application_1527109836290_0004 removed, cleanupLocalDirs = false\r\n\r\n152274 2018-05-23 15:04:56,320 INFO  impl.TimelineV2ClientImpl (TimelineV2ClientImpl.java:stop(542)) - Stopping TimelineClient.\r\n\r\n \r\n\r\n \r\n\r\n*2.  On 2.6.*  when I request 8 containers to run on 3 data nodes,  I picked the second node to examine the log:\r\n\r\nthis job used 3 containers in this node:\r\n\r\n \r\n\r\nApplication application_1526961340393_0012,  container *container_e11_1526961340393_0012_01_000003* (from container succeeded to Stopping container (from blue to red line) took less than *1 seconds*)\r\n\r\n \r\n\r\n991154 2018-05-23 15:41:26,904 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(810)) - Start request for container_e11_1526961340393_0012_01_000003 by user hlhuang\r\n\r\n991155 2018-05-23 15:41:26,908 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(850)) - Creating a new application reference for app application_1526961340393_0012\r\n\r\n991156 2018-05-23 15:41:26,908 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1526961340393_0012 transitioned from NEW to INITING\r\n\r\n991157 2018-05-23 15:41:26,909 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(304)) - Adding container_e11_1526961340393_0012_01_000003 to application application_1526961340393_0012\r\n\r\n \r\n\r\n991158 2018-05-23 15:41:26,914 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1526961340393_0012 transitioned from INITING to RUNNING\r\n\r\n991159 2018-05-23 15:41:26,915 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000003 transitioned from NEW to LOCALIZED\r\n\r\n991160 2018-05-23 15:41:26,915 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_INIT for appId application_1526961340393_0012\r\n\r\n991161 2018-05-23 15:41:26,915 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(192)) - Initializing container container_e11_1526961340393_0012_01_000003\r\n\r\n991162 2018-05-23 15:41:26,915 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e11_1526961340393_0012_01_000003\r\n\r\n991163 2018-05-23 15:41:26,968 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000003 transitioned from LOCALIZED to RUNNING\r\n\r\n991164 2018-05-23 15:41:26,968 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n991165 2018-05-23 15:41:26,968 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\r\n\r\n{color:#205081}991166 2018-05-23 15:41:26,993 INFO  launcher.ContainerLaunch (ContainerLaunch.java:call(384)) - Container container_e11_1526961340393_0012_01_000003 succeeded{color}\r\n\r\n \r\n\r\n991167 2018-05-23 15:41:26,994 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS\r\n\r\n991168 2018-05-23 15:41:26,994 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(541)) - Cleaning up container container_e11_1526961340393_0012_01_000003\r\n\r\n991169 2018-05-23 15:41:26,995 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n991170 2018-05-23 15:41:27,021 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE\r\n\r\n991171 2018-05-23 15:41:27,021 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(347)) - Removing container_e11_1526961340393_0012_01_000003 from application application_1526961340393_0012\r\n\r\n991172 2018-05-23 15:41:27,021 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\r\n\r\n991173 2018-05-23 15:41:27,035 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_STOP for appId application_1526961340393_0012\r\n\r\n991174 2018-05-23 15:41:27,035 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(198)) - Stopping container container_e11_1526961340393_0012_01_000003\r\n\r\n{color:#d04437}991175 2018-05-23 15:41:27,036 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e11_1526961340393_0012_01_000003{color}\r\n\r\n \r\n\r\nApplication application_1526961340393_0012,  container *container_e11_1526961340393_0012_01_000005* (from container succeeded to Stopping container (from blue to red line) took less than *1 seconds*)\r\n\r\n \r\n\r\n991176 2018-05-23 15:41:27,152 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(810)) - Start request for container_e11_1526961340393_0012_01_000005 by user hlhuang\r\n\r\n991177 2018-05-23 15:41:27,154 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(304)) - Adding container_e11_1526961340393_0012_01_000005 to application application_1526961340393_0012\r\n\r\n \r\n\r\n991178 2018-05-23 15:41:27,154 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000005 transitioned from NEW to LOCALIZED\r\n\r\n991179 2018-05-23 15:41:27,155 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_INIT for appId application_1526961340393_0012\r\n\r\n991180 2018-05-23 15:41:27,155 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(192)) - Initializing container container_e11_1526961340393_0012_01_000005\r\n\r\n991181 2018-05-23 15:41:27,155 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e11_1526961340393_0012_01_000005\r\n\r\n991182 2018-05-23 15:41:27,205 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000005 transitioned from LOCALIZED to RUNNING\r\n\r\n991183 2018-05-23 15:41:27,205 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\r\n\r\n991184 2018-05-23 15:41:27,205 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n{color:#205081}991185 2018-05-23 15:41:27,227 INFO  launcher.ContainerLaunch (ContainerLaunch.java:call(384)) - Container container_e11_1526961340393_0012_01_000005 succeeded{color}\r\n\r\n \r\n\r\n991186 2018-05-23 15:41:27,228 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000005 transitioned from RUNNING to EXITED_WITH_SUCCESS\r\n\r\n991187 2018-05-23 15:41:27,228 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(541)) - Cleaning up container container_e11_1526961340393_0012_01_000005\r\n\r\n991188 2018-05-23 15:41:27,228 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n991189 2018-05-23 15:41:27,258 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n991190 2018-05-23 15:41:27,268 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000005 transitioned from EXITED_WITH_SUCCESS to DONE\r\n\r\n991191 2018-05-23 15:41:27,268 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(347)) - Removing container_e11_1526961340393_0012_01_000005 from application application_1526961340393_0012\r\n\r\n991192 2018-05-23 15:41:27,268 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\r\n\r\n991193 2018-05-23 15:41:27,268 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_STOP for appId application_1526961340393_0012\r\n\r\n991194 2018-05-23 15:41:27,268 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(198)) - Stopping container container_e11_1526961340393_0012_01_000005\r\n\r\n{color:#d04437}991195 2018-05-23 15:41:27,268 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e11_1526961340393_0012_01_000005{color}\r\n\r\n \r\n\r\nApplication application_1526961340393_0012,  *container container_e11_1526961340393_0012_01_000008* (from container succeeded to Stopping container (from blue to red line) took less than *1 seconds*)\r\n\r\n991196 2018-05-23 15:41:27,340 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:startContainerInternal(810)) - Start request for container_e11_1526961340393_0012_01_000008 by user hlhuang\r\n\r\n991197 2018-05-23 15:41:27,350 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(304)) - Adding container_e11_1526961340393_0012_01_000008 to application application_1526961340393_0012\r\n\r\n991198 2018-05-23 15:41:27,351 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000008 transitioned from NEW to LOCALIZED\r\n\r\n991199 2018-05-23 15:41:27,351 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_INIT for appId application_1526961340393_0012\r\n\r\n991200 2018-05-23 15:41:27,351 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(192)) - Initializing container container_e11_1526961340393_0012_01_000008\r\n\r\n991201 2018-05-23 15:41:27,352 INFO  yarn.YarnShuffleService (YarnShuffleService.java:initializeContainer(289)) - Initializing container container_e11_1526961340393_0012_01_000008\r\n\r\n991202 2018-05-23 15:41:27,397 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000008 transitioned from LOCALIZED to RUNNING\r\n\r\n991203 2018-05-23 15:41:27,397 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\r\n\r\n991204 2018-05-23 15:41:27,397 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n{color:#205081}991205 2018-05-23 15:41:27,440 INFO  launcher.ContainerLaunch (ContainerLaunch.java:call(384)) - Container container_e11_1526961340393_0012_01_000008 succeeded{color}\r\n\r\n \r\n\r\n991206 2018-05-23 15:41:27,440 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000008 transitioned from RUNNING to EXITED_WITH_SUCCESS\r\n\r\n991207 2018-05-23 15:41:27,440 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(541)) - Cleaning up container container_e11_1526961340393_0012_01_000008\r\n\r\n991208 2018-05-23 15:41:27,441 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n991209 2018-05-23 15:41:27,471 INFO  container.ContainerImpl (ContainerImpl.java:handle(1163)) - Container container_e11_1526961340393_0012_01_000008 transitioned from EXITED_WITH_SUCCESS to DONE\r\n\r\n991210 2018-05-23 15:41:27,471 INFO  application.ApplicationImpl (ApplicationImpl.java:transition(347)) - Removing container_e11_1526961340393_0012_01_000008 from application application_1526961340393_0012\r\n\r\n991211 2018-05-23 15:41:27,471 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:isEnabled(222)) - Neither virutal-memory nor physical-memory monitoring is needed. Not running the monitor-thread\r\n\r\n991212 2018-05-23 15:41:27,471 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event CONTAINER_STOP for appId application_1526961340393_0012\r\n\r\n991213 2018-05-23 15:41:27,471 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(198)) - Stopping container container_e11_1526961340393_0012_01_000008\r\n\r\n{color:#d04437}991214 2018-05-23 15:41:27,471 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopContainer(295)) - Stopping container container_e11_1526961340393_0012_01_000008{color}\r\n\r\n{color:#d04437} {color}\r\n\r\n \r\n\r\n{color:#14892c}Now compare with 3.0 above,  from Removed completed containers (line 991215) to transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP (line 991218), it only took 1 seconds for 2.6.{color}\r\n\r\n{color:#14892c} {color}\r\n\r\n \r\n\r\n*991215 2018-05-23 15:41:27,472* INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:removeOrTrackCompletedContainersFromContext(553)) - Removed completed containers from NM context: [container_e11_1526961340393_0012_01_000005, container_e11_1526961340393_0012_01_000003]\r\n\r\n991216 2018-05-23 15:41:27,485 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n991217 2018-05-23 15:41:27,699 INFO  runtime.DelegatingLinuxContainerRuntime (DelegatingLinuxContainerRuntime.java:pickContainerRuntime(67)) - Using container runtime: DefaultLinuxContainerRuntime\r\n\r\n*991218 2018-05-23 15:41:28,473* INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1526961340393_0012 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\r\n\r\n991219 2018-05-23 15:41:28,474 INFO  containermanager.AuxServices (AuxServices.java:handle(215)) - Got event APPLICATION_STOP for appId application_1526961340393_0012\r\n\r\n991220 2018-05-23 15:41:28,474 INFO  yarn.YarnShuffleService (YarnShuffleService.java:stopApplication(179)) - Stopping application application_1526961340393_0012\r\n\r\n991221 2018-05-23 15:41:28,475 INFO  shuffle.ExternalShuffleBlockResolver (ExternalShuffleBlockResolver.java:applicationRemoved(206)) - Application application_1526961340393_0012 removed, cleanupLocalDirs = false\r\n\r\n991222 2018-05-23 15:41:28,475 INFO  shuffle.ExternalShuffleBlockResolver (ExternalShuffleBlockResolver.java:applicationRemoved(186)) - Application application_1526961340393_0012 removed, cleanupLocalDirs = false\r\n\r\n991223 2018-05-23 15:41:28,475 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1526961340393_0012 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\r\n\r\n991224 2018-05-23 15:41:28,475 INFO  loghandler.NonAggregatingLogHandler (NonAggregatingLogHandler.java:handle(167)) - Scheduling Log Deletion for application: application_1526961340393_0012, with delay of 604800 seconds\r\n\r\n  \r\n\r\nAbove logs are from changing the yarn.nodemanager.container-monitor.interval-ms to 300 only.  I removed the monitoring to false property.\r\n\r\n \r\n\r\nTo summarize,   I believe there are two places that need to be investigated:\r\n\r\n1. on each container, between container succeeded to stopping container, it takes 4 seconds for 3.0, and only 1 second for 2.6\r\n\r\n2. from 'Removed completed containers' to 'application transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP', 3.0 takes a wopping 5 seconds and only 1 second for 2.6.\r\n\r\n \r\n\r\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hlhuang%40us.ibm.com","name":"hlhuang@us.ibm.com","key":"hlhuang@us.ibm.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34045","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"},"displayName":"Hsin-Liang Huang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-05-24T00:57:19.781+0000","updated":"2018-05-24T00:57:19.781+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16514439","id":"16514439","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"The root cause for the stopping delay coming from clean up containers:\r\n\r\n{code}\r\n    final int sleepMsec = 100;\r\n    int msecLeft = 2000;\r\n    if (pidFilePath != null) {\r\n      File file = new File(getExitCodeFile(pidFilePath.toString()));\r\n      while (!file.exists() && msecLeft >= 0) {\r\n        try {\r\n          Thread.sleep(sleepMsec);\r\n        } catch (InterruptedException e) {\r\n        }\r\n        msecLeft -= sleepMsec;\r\n      }\r\n      if (msecLeft < 0) {\r\n        if (LOG.isDebugEnabled()) {\r\n          LOG.debug(\"Timeout while waiting for the exit code file:  \"\r\n              + file.getAbsolutePath());\r\n        }\r\n      }\r\n    }\r\n{code}\r\n\r\nThis was committed as part of YARN-5366 for making sure that exit code file exists, or we waited sufficient amount of time for the file to appear on disk before clean up operation happens.  [~shanekumpf@gmail.com] is there a better way to solve this problem without using the waiting logic?  Should this part of logic apply to non-docker containers?\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-06-15T22:21:46.184+0000","updated":"2018-06-15T22:30:21.295+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16514769","id":"16514769","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanekumpf%40gmail.com","name":"shanekumpf@gmail.com","key":"shanekumpf@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shane Kumpf","active":true,"timeZone":"America/Denver"},"body":"That code should only come into play if the exit code file isn't written and at most would add 2 seconds. Any idea what is delaying the exit code file? Do debug logs show the timeout message? (e.g. Timeout while waiting for the exit code file..)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanekumpf%40gmail.com","name":"shanekumpf@gmail.com","key":"shanekumpf@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shane Kumpf","active":true,"timeZone":"America/Denver"},"created":"2018-06-16T12:46:17.636+0000","updated":"2018-06-16T12:46:17.636+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16516072","id":"16516072","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"[~shanekumpf@gmail.com] In line [802|https://github.com/apache/hadoop/blob/fba9d7cd746cd7b659d2fd9d2bfa23266be9009b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java#L804] of ContainerLaunch.java, the exit_code file is deleted.  By the time that YARN-5366 sleeping loop is encountered, the file does not exist.  This is the reason that 2 second sleep always happens.  I mentioned in YARN-5366, I did not find sleep and retry docker inspect logic in YARN-5366, and I added in YARN-7654 line [1671|https://github.com/apache/hadoop/blame/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c#L1671] to have the retry logic for entry_point based docker container.  I think the two seconds sleep logic can be removed from ContainerLaunch.java. Thoughts?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-06-18T17:43:00.761+0000","updated":"2018-06-18T17:43:00.761+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16517066","id":"16517066","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanekumpf%40gmail.com","name":"shanekumpf@gmail.com","key":"shanekumpf@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shane Kumpf","active":true,"timeZone":"America/Denver"},"body":"Thanks for the analysis, [~eyang]. Based on your findings, it does appear this should be removed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanekumpf%40gmail.com","name":"shanekumpf@gmail.com","key":"shanekumpf@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shane Kumpf","active":true,"timeZone":"America/Denver"},"created":"2018-06-19T13:24:27.179+0000","updated":"2018-06-19T13:24:27.179+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16517330","id":"16517330","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tanping","name":"tanping","key":"tanping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tanping Wang","active":true,"timeZone":"Etc/UTC"},"body":"[~shanekumpf@gmail.com]  When do you think you can have the patch uploaded?  This is causing a big slow down for 3.0 Yarn applications.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tanping","name":"tanping","key":"tanping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tanping Wang","active":true,"timeZone":"Etc/UTC"},"created":"2018-06-19T17:15:30.332+0000","updated":"2018-06-19T17:15:30.332+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16520718","id":"16520718","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 22s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 23m 42s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m  9s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 29s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 42s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 46s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m  7s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  9s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 18m  0s{color} | {color:green} hadoop-yarn-server-nodemanager in the patch passed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 21s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black} 68m  8s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:abb62dd |\r\n| JIRA Issue | YARN-8326 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12928790/YARN-8326.001.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux cd2f8bf00ab6 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 55fad6a |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_171 |\r\n| findbugs | v3.1.0-RC1 |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-YARN-Build/21079/testReport/ |\r\n| Max. process+thread count | 459 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager |\r\n| Console output | https://builds.apache.org/job/PreCommit-YARN-Build/21079/console |\r\n| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-06-22T19:32:48.377+0000","updated":"2018-06-22T19:32:48.377+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16520890","id":"16520890","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you for the patch, [~shanekumpf@gmail.com].  I committed this to branch-3.1 and trunk.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-06-22T23:28:17.330+0000","updated":"2018-06-22T23:28:17.330+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16520898","id":"16520898","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14469 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14469/])\nYARN-8326.  Removed exit code file check for launched container.         (eyang: rev 8a32bc39eb210fca8052c472601e24c2446b4cc2)\n* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2018-06-22T23:37:45.163+0000","updated":"2018-06-22T23:37:45.163+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16520939","id":"16520939","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanekumpf%40gmail.com","name":"shanekumpf@gmail.com","key":"shanekumpf@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shane Kumpf","active":true,"timeZone":"America/Denver"},"body":"Thank you for reporting the issue, [~hlhuang@us.ibm.com], and thanks for the analysis and commit, [~eyang]!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shanekumpf%40gmail.com","name":"shanekumpf@gmail.com","key":"shanekumpf@gmail.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shane Kumpf","active":true,"timeZone":"America/Denver"},"created":"2018-06-23T01:44:32.453+0000","updated":"2018-06-23T01:44:32.453+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16521437","id":"16521437","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Jenkins build Hadoop-precommit-ozone-acceptance #20 (See [https://builds.apache.org/job/Hadoop-precommit-ozone-acceptance/20/])\nYARN-8326.  Removed exit code file check for launched container.         (eyang: [https://github.com/apache/hadoop/commit/8a32bc39eb210fca8052c472601e24c2446b4cc2])\n* (edit) hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2018-06-24T08:05:04.431+0000","updated":"2018-06-24T08:05:04.431+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13160456/comment/16527840","id":"16527840","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dchuyko","name":"dchuyko","key":"dchuyko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dchuyko&avatarId=33801","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dchuyko&avatarId=33801","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dchuyko&avatarId=33801","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dchuyko&avatarId=33801"},"displayName":"Dmitry Chuyko","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for fixing this! We at BellSoft also noticed major regression between Hadoop 3.0.1 and 3.1 and tracked that down to YARN-5366. Below there are some details someone may find interesting.\r\n\r\nWe run benchmarks like Terasort and DFSIO on a setup that uses Yarn but not Docker. Mainly on ARM64 machines. For example it is terasort of 32 M records which is relatively short running (minute scale). So on Cavium ThunderX2 the slowdown is ~2x, on ThunderX it's ~1.5 and also we made x86 setup which also shown ~1.5x slowdown. It happened with JDK 8,9,10 and 11.\r\n\r\nWe plot running JVM processes on a single timeline and take into account their CPU utilization. Consider plots before and after the change:\r\n\r\nBefore\r\nhttps://raw.githubusercontent.com/dchuyko/hadoop/86bdc68a142e19c8eff083daa2de0d11257aae12/terasort-32M-good.png\r\n\r\nAfter\r\nhttps://raw.githubusercontent.com/dchuyko/hadoop/86bdc68a142e19c8eff083daa2de0d11257aae12/terasort-32M-bad.png\r\n\r\nIn the new gap between actual MR work all processes that are alive take <1% cpu, but then we sample stacks and it turns out that reaper and launcher logic at NodeManager take the time\r\nhttps://raw.githubusercontent.com/dchuyko/hadoop/86bdc68a142e19c8eff083daa2de0d11257aae12/terasort-32M-bad-gap-stacks.png\r\n\r\nAnd now the issue is gone.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dchuyko","name":"dchuyko","key":"dchuyko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dchuyko&avatarId=33801","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dchuyko&avatarId=33801","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dchuyko&avatarId=33801","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dchuyko&avatarId=33801"},"displayName":"Dmitry Chuyko","active":true,"timeZone":"Etc/UTC"},"created":"2018-06-29T15:45:23.571+0000","updated":"2018-06-29T15:45:23.571+0000"}],"maxResults":15,"total":15,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/YARN-8326/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3twnb:"}}