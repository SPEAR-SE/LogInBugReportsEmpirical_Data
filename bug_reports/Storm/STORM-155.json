{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=iwasakims&avatarId=18289",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=iwasakims&avatarId=18289",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=iwasakims&avatarId=18289",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=iwasakims&avatarId=18289"
            },
            "displayName": "Masatake Iwasaki",
            "key": "iwasakims",
            "name": "iwasakims",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=iwasakims",
            "timeZone": "Asia/Tokyo"
        },
        "components": [{
            "description": "Core storm daemons and APIs including trident",
            "id": "12327950",
            "name": "storm-core",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12327950"
        }],
        "created": "2013-12-15T06:52:13.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xumingming&avatarId=18354",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xumingming&avatarId=18354",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xumingming&avatarId=18354",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xumingming&avatarId=18354"
            },
            "displayName": "James Xu",
            "key": "xumingming",
            "name": "xumingming",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=xumingming",
            "timeZone": "Asia/Shanghai"
        },
        "customfield_10010": null,
        "customfield_12310220": "2014-07-16T06:58:07.061+0000",
        "customfield_12310222": "1_*:*_1_*:*_19233945763_*|*_5_*:*_1_*:*_0",
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310420": "363854",
        "customfield_12310920": "364160",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i1qpuv:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Jul 25 21:37:58 UTC 2014",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "https://github.com/nathanmarz/storm/issues/551\n\nWe're seeing an issue when rebalancing topologies on the clusters causes workers being assigned to multiple topologies which causes supervisors to fail. This can be easily reproduced locally by starting a single supervisor with 4 workers and nimbus, running several topologies and rebalancing all of them to use 1 worker.\n\nI tracked the issue to the mk-assignments function in nimbus.clj. In this function, the \"existing-assignments\" binding is assigned a list of all topologies with assignments except the one being rebalanced. The comment implies this is done to treat all the workers of the topology being rebalanced as unused, and that's what actually happens. However, the lack of the topology being rebalanced in the \"existing-assignments\" list causes this topology being ignore completely by scheduler and other code, so as the result all workers assigned to that topology will be taken over by other topologies, but\nno changes to the topology being rebalanced will be made, effective making all it's workers\nassigned to 2 topologies.\n\nI think that was not the intended behavior. I made a small change to treat all the workers of the topology being rebalanced dead instead, causing them to be reassigned fairly between all the topologies. This seems to work well and reliably for us.\n\nLet me know what do you think.\n\n--- a/src/clj/backtype/storm/daemon/nimbus.clj\n+++ b/src/clj/backtype/storm/daemon/nimbus.clj\n@@ -437,8 +437,11 @@\n   (into {} (for [[tid assignment] existing-assignments\n                  :let [topology-details (.getById topologies tid)\n                        all-executors (topology->executors tid)\n+                       ;; for the topology which wants rebalance (specified by the scratch-topology-id)\n+                       ;; we consider all its execturors to be dead so they will be treated\n+                       ;; as free slots in the scheduler code.\n                        alive-executors (if (and scratch-topology-id (= scratch-topology-id tid))\n-                                         all-executors\n+                                         (set nil)\n                                          (set (alive-executors nimbus topology-details all-executors assignment)))]]\n              {tid alive-executors})))\n\n@@ -638,11 +641,7 @@\n         ;; read all the assignments\n         assigned-topology-ids (.assignments storm-cluster-state nil)\n         existing-assignments (into {} (for [tid assigned-topology-ids]\n-                                        ;; for the topology which wants rebalance (specified by the scratch-topology-id)\n-                                        ;; we exclude its assignment, meaning that all the slots occupied by its assignment\n-                                        ;; will be treated as free slot in the scheduler code.\n-                                        (when (or (nil? scratch-topology-id) (not= tid scratch-topology-id))\n-                                          {tid (.assignment-info storm-cluster-state tid nil)})))\n+                                        {tid (.assignment-info storm-cluster-state tid nil)}))\n         ;; make the new assignments for topologies\n         topology->executor->node+port (compute-new-topology->executor->node+port\n                                        nimbus\n\n----------\nxumingming: The following code needs to be synchonized(mk-assignments):\n\n(defn do-rebalance [nimbus storm-id status]\n  (.update-storm! (:storm-cluster-state nimbus)\n                  storm-id\n                  (assoc-non-nil\n                    {:component->executors (:executor-overrides status)}\n                    :num-workers\n                    (:num-workers status)))\n  (mk-assignments nimbus :scratch-topology-id storm-id))\notherwise it will cause race condition here: https://github.com/nathanmarz/storm/blob/master/storm-core/src/jvm/backtype/storm/scheduler/Cluster.java#L264\n\nthen one port will be assigned to multiple topologies.\n\n----------\nstass: That's not the issue here. I initially thought it was a race condition as well, but my problem with rebalancing was in the sequential part of the algorithm (as described in analysis). Needless to say, it fixed the issue for us and I have not seen any multiple assignments in months when running with the patch I submitted.\n\n----------\nd2r: We are also hitting this issue and would welcome a fix.\n\n----------\nnathanmarz: The problem with the proposed patch is that it doesn't treat the slots of the topology that's rebalancing as free. It will certainly consider its executors as dead, but it won't make use of its slots during re-scheduling.\n\n----------\nstass: Hi, Nathan, thanks for comment.\n\nIf my reading of the code is right, the workers of the topology being rebalanced will be treated as free since they will be marked as dead (that's the purpose of the top hunk of the diff), and the scheduling code only looks at free workers when assigning workers to topologies. That is also what I observed in practice with this patch running.\n\nAm I missing something?\n\n----------\nvinceyang: @nathanmarz , We are also hitting this issue , I read the nimbus code,it seems when do rebalance , in mk-assignment function,the rebalancing topolgy's assignment info has out of date , but supervisor not kown this information. when other topolgy's assignment has the same port with the out of date assignment the problem occur. if we remove the out of date assignment in ZK this problem will not occur. if my Idea is OKï¼ŒI will work on it to fix this issue.\n\n----------\nrevans2: We have hit this issue too, and so I have been looking into it. It seems that it can happen in two different situations.\n\nFirst a topology is not assigned anything after it previously had slots assigned to it.\n\nThis happens most commonly when re-balancing because the scheduler is not aware the rebalanced topology had anything assigned to it previously, but I have been able to reproduce this with other hacked up schedulers.\n\nWhen this happens the supervisor in question will crash continuously until one of the topologies is killed.\n\nThe fix seems to be that we should include assigned-topology-ids in topology->executor->node+port when missing but with the topology pointing to nil.\n\nSecond the supervisor uses partially written scheduling data from ZK.\n\n(.set-assignment! storm-cluster-state topology-id assignment) is atomic for a single topology, but not for multiple topologies. This means that the supervisor can read data from ZK that has had some topologies updated, but not all of them.\n\nWhen this happens the supervisor will crash and then come back up and recover because the rest of the scheduling data was written to ZK.\n\nThe fix for this seems to be that we need to \"lock\" zookeeper with a watch file during the update. The supervisors would not read the data until nimbus is done updating. I don't think this is as critical to fix because the supervisor recovers fairly quickly.\n\nDoes my analysis seem correct? I don't understand all of the code perfectly, so I want to be sure I am not missing something.\n",
        "duedate": null,
        "environment": null,
        "fixVersions": [{
            "archived": false,
            "id": "12327112",
            "name": "0.9.3",
            "releaseDate": "2014-11-25",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12327112"
        }],
        "issuelinks": [{
            "id": "12406095",
            "inwardIssue": {
                "fields": {
                    "issuetype": {
                        "avatarId": 21133,
                        "description": "A problem which impairs or prevents the functions of the product.",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                        "id": "1",
                        "name": "Bug",
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                        "subtask": false
                    },
                    "priority": {
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                        "id": "4",
                        "name": "Minor",
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/4"
                    },
                    "status": {
                        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                        "id": "5",
                        "name": "Resolved",
                        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                        "statusCategory": {
                            "colorName": "green",
                            "id": 3,
                            "key": "done",
                            "name": "Done",
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
                        }
                    },
                    "summary": "Multitple topologies assigned to one port"
                },
                "id": "12684673",
                "key": "STORM-99",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/12684673"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12406095",
            "type": {
                "id": "12310000",
                "inward": "is duplicated by",
                "name": "Duplicate",
                "outward": "duplicates",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"
            }
        }],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12314820&avatarId=21667",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12314820&avatarId=21667",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12314820&avatarId=21667",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12314820&avatarId=21667"
            },
            "id": "12314820",
            "key": "STORM",
            "name": "Apache Storm",
            "projectCategory": {
                "description": "Apache Storm Related",
                "id": "13260",
                "name": "Storm",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13260"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12314820"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xumingming&avatarId=18354",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xumingming&avatarId=18354",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xumingming&avatarId=18354",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xumingming&avatarId=18354"
            },
            "displayName": "James Xu",
            "key": "xumingming",
            "name": "xumingming",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=xumingming",
            "timeZone": "Asia/Shanghai"
        },
        "resolution": {
            "description": "A fix for this issue is checked into the tree and tested.",
            "id": "1",
            "name": "Fixed",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1"
        },
        "resolutiondate": "2014-07-25T21:37:58.000+0000",
        "status": {
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "id": "5",
            "name": "Resolved",
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "Storm rebalancing code causes multiple topologies assigned to a single port",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2015-10-09T00:52:49.000+0000",
        "versions": [],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/STORM-155/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/STORM-155/watchers",
            "watchCount": 5
        },
        "workratio": -1
    },
    "id": "12684782",
    "key": "STORM-155",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12684782"
}