{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13040284","self":"https://issues.apache.org/jira/rest/api/2/issue/13040284","key":"STORM-2343","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":53400,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12314820","id":"12314820","key":"STORM","name":"Apache Storm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12314820&avatarId=21667","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12314820&avatarId=21667","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12314820&avatarId=21667","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12314820&avatarId=21667"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/13260","id":"13260","description":"Apache Storm Related","name":"Storm"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334657","id":"12334657","name":"2.0.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12339656","id":"12339656","name":"1.1.1","archived":false,"released":true,"releaseDate":"2017-08-01"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12341047","id":"12341047","name":"1.2.0","archived":false,"released":true,"releaseDate":"2018-02-15"}],"aggregatetimespent":53400,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-06-07T13:31:08.994+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sat Jun 10 18:00:30 UTC 2017","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_9194571281_*|*_5_*:*_1_*:*_0","customfield_12310420":"9223372036854775807","customfield_12312321":null,"resolutiondate":"2017-05-21T09:55:29.950+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-2343/watchers","watchCount":2,"isWatching":false},"created":"2017-02-03T23:52:38.725+0000","priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":0,"aggregatetimeoriginalestimate":null,"customfield_12311120":"STORM-1856","customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334657","id":"12334657","name":"2.0.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12335748","id":"12335748","name":"1.1.0","archived":false,"released":true,"releaseDate":"2017-03-29"}],"customfield_12312339":null,"issuelinks":[{"id":"12506190","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12506190","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13078879","key":"STORM-2549","self":"https://issues.apache.org/jira/rest/api/2/issue/13078879","fields":{"summary":"The fix for STORM-2343 is incomplete, and the spout can still get stuck on failed tuples","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-03T15:03:49.964+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12331080","id":"12331080","name":"storm-kafka-client"}],"timeoriginalestimate":null,"description":"It doesn't look like the spout is respecting maxUncommittedOffsets in all cases. If the underlying consumer returns more records in a call to poll() than maxUncommittedOffsets, they will all be added to waitingToEmit. Since poll may return up to 500 records by default (Kafka 0.10.1.1), this is pretty likely to happen with low maxUncommittedOffsets.\n\nThe spout only checks for tuples to retry if it decides to poll, and it only decides to poll if numUncommittedOffsets < maxUncommittedOffsets. Since maxUncommittedOffsets isn't being respected when retrieving or emitting records, numUncommittedOffsets can be much larger than maxUncommittedOffsets. If more than maxUncommittedOffsets messages fail, this can cause the spout to stop polling entirely.","customfield_10010":null,"timetracking":{"remainingEstimate":"0h","timeSpent":"14h 50m","remainingEstimateSeconds":0,"timeSpentSeconds":53400},"customfield_12312026":null,"customfield_12312023":null,"customfield_12312024":null,"attachment":[],"customfield_12312340":null,"aggregatetimeestimate":0,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"New Kafka spout can stop emitting tuples if more than maxUncommittedOffsets tuples fail at once","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"subtasks":[],"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"customfield_12310291":null,"customfield_12310290":null,"aggregateprogress":{"progress":53400,"total":53400,"percent":100},"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":53400,"total":53400,"percent":100},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/15852357","id":"15852357","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"These issues were pointed out here http://mail-archives.apache.org/mod_mbox/storm-dev/201702.mbox/%3C3E125946-CB8B-4FA9-8943-CB5AF367F92B%40coviam.com%3E","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-02-03T23:54:16.437+0000","updated":"2017-02-03T23:54:16.437+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16040880","id":"16040880","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"[~Srdo] For a tuple to be retried, it needs to be emitted again which can only happen if kafka consumer poll is called. Even with this fix I feel it is possible for the spout to get stuck if a specific tuple fails repeatedly and a lot of subsequent ones succeed, get acked but are blocked on commit inside OffsetManager.\n\nMy suggestion is that as long as there is a tuple ready to be retried (retryService.readyMessageCount() > 0), poll should proceed independent of the numUncommittedOffsets. This will allow for the possibility that that tuple is successfully processed and the topology is not stalled. \n\nWe can optionally add logic in the emitTupleIfNotEmitted(ConsumerRecord<K, V> record) method to restrict fresh tuples from being emitted if the numUncommittedOffsets threshold has been breached while still allowing kafka consumer poll to happen as long as retryService.readyMessageCount() is non-zero.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-07T13:31:08.994+0000","updated":"2017-06-07T13:32:14.786+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16041522","id":"16041522","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"[~ranganp] You are right. I missed some cases.\n\nJust to illustrate:\nSay maxUncommittedOffsets is 10, maxPollRecords is 5, and the committedOffset is 0.\nThe spout will initially emit up to offset 10, because it is allowed to poll until numNonRetriableTuples is >= maxUncommittedOffsets\nThe spout will be allowed to emit another 5 tuples if offset 10 fails, so if that happens, offsets 10-14 will get emitted. If offset 1 fails and 2-14 get acked, the spout gets stuck because it will count the \"extra tuples\" 11-14 in numNonRetriableTuples.\n\nAn similar case is the one where maxPollRecords doesn't divide maxUncommittedOffsets evenly. If it were 3 in the example above, the spout might just immediately emit offsets 1-12. If 2-12 get acked, offset 1 cannot be reemitted.\n\nYour suggestion would solve this, but also means that we can't meaningfully cap numUncommittedOffsets. If we allow a poll whenever there's a failed tuple, the spout might emit an arbitrary number of tuples if tuples fail in an unlucky order.\n\nFor example, take the case where maxUncommittedOffsets is 10, maxPollRecords is 5, and the committedOffset is 0, and 10 tuples have been emitted.\nIf offset 10 fails and is retried, 10-14 may be emitted. If 14 fails, 14-19 get emitted. This can be repeated as many times as needed. The issue is more likely to happen when multiple partitions are involved, because then you might have a failed tuple on partition 0 cause the spout to emit fresh tuples on partition 1, regardless of how far past the limit partition 1 already is.\n\nI'm torn on whether this kind of limit violation is really serious enough to matter. It's possible that we run a lot past maxUncommittedOffsets, but it doesn't seem likely that the bad tuple failure pattern keeps repeating. It's just a gut feeling though, which is why I made an attempt to put a real cap on maxUncommittedOffsets in the linked PRs.\n\nIf it is serious enough that we want to fix it I think there are two decent approaches. \n\nOne is to do what you mention and filter out the fresh tuples we happen to also receive when we poll for failed tuples. The downside is that in order to avoid losing tuples, we have to seek the consumer back to the first tuple we filtered out for each partition, or we have to be able to emit only the retriable tuples while keeping the rest in memory for later. \n\nThe other option would be to always allow retries (per partition) only for those tuples that are within maxUncommittedOffsets tuples of committedOffsets. For example, if the maxUncommittedOffsets is 10, maxPollRecords is 5, and the committedOffset is 0, and 14 tuples have been emitted, we only allow retries for the partition if the earliest retriable tuple is one of the first 10. The point would be that we never fail to retry tuples within maxUncommittedOffsets of the committed offset, but we don't retry tuples that are beyond that boundary until the committed offset moves, so we can cap numUncommittedOffsets. The downsides to this are we have to do this evaluation per partition, and we might have to pause nonretriable partitions when we're beyond maxUncommittedOffsets to ensure that we actually emit the retriable tuples and not just some new ones on unrelated partitions.\n\nI don't really have a particular preference here. What do you think?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-07T20:10:28.031+0000","updated":"2017-06-07T20:10:28.031+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16041545","id":"16041545","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"[~Srdo] I need some time to properly digest your input and reply. Will do so. Meanwhile, I was thinking about this issue and have the following to add to what I stated earlier:\n\nOne issue that I see currently is that the spout emits greater than maxUncommittedOffsets tuples into the topology. This happens because we check for numUncommittedOffsets in poll() but once it succeeds and a kafka broker call is made, we go ahead and emit ALL the tuples received from the broker. This effectively means that there can be an undefined number of tuples over and beyond the maxUncommittedOffsets being processed by the topology.\n\nNow, if we capped the total number of tuples under process (including those ready or waiting for retry) then the logic we have for poll() would always ensure that failed tuples are fetched from the brokers. Does this make sense? Or am I missing any edge case here?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-07T20:24:17.829+0000","updated":"2017-06-07T20:24:17.829+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16041563","id":"16041563","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"[~Srdo] Need a basic clarification on the following: Offsets are TopicPartition specific but numUncommittedOffsets is global across all the partitions that belong to the spout instance / executor in question, right?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-07T20:32:21.375+0000","updated":"2017-06-07T20:32:21.375+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16041573","id":"16041573","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"That isn't quite true. There is a limit to how far past maxUncommittedOffsets we can emit, because the KafkaConsumer has a limit to how many messages a call to poll will return (max.poll.records)\n\nI agree that the problem is that we either need to not emit all the tuples we receive in order to respect maxUncommittedOffsets, or we need to do something else to ensure that we won't poll if we're sufficiently far past the maxUncommittedOffsets limit. I don't think we're treating maxUncommittedOffsets as a strict cap (because it is hard to enforce if we want to emit all the tuples we receive in poll), but we'd like to at least be able to say we won't go farther than x tuples past it.\n\nI think you're missing the case where the poll returns messages from an unexpected partition. e.g. if you have retriable tuples on partition 1 and are at the cap of how many tuples you're willing to process, you would want to allow poll so you can retry the tuples. Unless you pause the partitions that don't have retriable tuples, the consumer might decide to poll from those instead. So you might be trying to get retries for partition 1, but the poll might get tuples for partition 2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-07T20:40:24.338+0000","updated":"2017-06-07T20:40:24.338+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16041575","id":"16041575","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"Yes, offsets are specific to the TopicPartition, and numUncommittedOffsets is a global limit currently. If we want to interpret it as a per partition limit, that's fine too IMO, as long as there's a limit.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-07T20:41:25.015+0000","updated":"2017-06-07T20:41:25.015+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16042382","id":"16042382","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"The illustrative example is very useful in thinking through the issue. Thanks for the same. Generally speaking, my preference is for a simple solution with minimal overhead in the Spout. \n\nThere is another critical issue that we need to consider though. Created the JIRA just now: STORM-2546\n\nI am trying to come up with a solution that would address both these issues comprehensively. We are essentially dealing with the challenge of ensuring failing tuples are properly accounted for and the at least once processing guarantee is enforced properly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-08T07:55:03.264+0000","updated":"2017-06-08T07:56:09.199+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16044785","id":"16044785","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"[~ranganp] I spent a while thinking about this, but it seems to me to be something where there's a lot of corner cases to consider. Here's my best effort.\nRegarding fixing STORM-2546:\nThe only way to know that a tuple has been deleted from Kafka is to try polling for it. We can know for sure that a failed tuple has been deleted if we seek to the failed tuple's offset (or earlier) on the relevant partition and poll, and we then encounter a tuple that has a higher offset than the failed tuple on that partition earlier in the result set.\n\nFor instance:\nOffset 0...5 have failed and also been compacted away. Offset 6 has failed and is present, offset 7 has failed and is not present.\nWe seek to offset 0 for the partition.\nIf we then see that the first message in the poll result is offset 6, we can be sure that offset 0...5 are deleted, because otherwise they would have been returned in the poll. Offset 7 cannot be removed from the spout because we can't be sure that it was deleted, the consumer may just have received too few messages.\n\nI believe we can also conclude that offsets have been removed if we seek to their offsets, poll and receive an empty result. I'm not entirely sure about this, but I don't think the consumer will return empty polls if there are more messages to consume.\n\nI think we can use this method to remove failed, deleted tuples from the offset manager. When we do a poll, we examine the retriable tuples for each partition. For each partition where we received tuples, we compare the earliest received tuple to the retriable tuples for that partition. If the offset of a given retriable tuple is lower than the offset of the earliest received tuple, then the retriable tuple must have been deleted. \n\nAbout this issue:\nThe fact that failed tuples can be removed from Kafka before they can be retried is something I overlooked in what I wrote earlier. I think either solution can deal with it though.\n\nOne correction to what I wrote earlier regarding emitTupleIfNotEmitted filtering btw: We'll should also pause partitions in this solution IMO. Otherwise it is possible (even likely if there are few retriable partitions) to allow poll due to retriable tuples, and get no retriable tuples from the poll, in which case we'll discard all the messages and try again later. I think it would make that solution unacceptably wasteful (we'd risk multiple useless polls for unrelated partitions every time we have to retry a tuple while at the maxUncommittedOffsets limit), so we should pause nonretriable partitions.\n\nThe solutions I see to this issue right now are:\n\n* Don't enforce maxUncommittedOffsets if there are retriable tuples at all. This is simple to implement, but I don't really have a good feeling for what the likelihood is that maxUncommittedOffsets will be exceeded by \"too much\".\n\nExample of this functionality:\nMaxUncommittedOffsets is 100\nMaxPollRecords is 10\nCommitted offset for partition 0 and 1 is 0.\nPartition 0 has emitted 0\nPartition 1 has emitted 0...95, 97, 99, 101, 103 (some offsets compacted away)\nPartition 1, message 97 is retriable\nThe spout seeks to message 97 and polls\nIt gets back offsets 99, 101, 103 and potentially 7 new tuples. Say the new tuples are in the range 104-110.\nIf any of 104-110 become retriable, the spout may emit another set of 9 (maxPollRecords - 1) tuples.\nThis can repeat for each newly emitted set. The likelihood of this happening in real life is unclear to me.\n----\n* Enforce maxUncommittedOffsets globally by always allowing poll if there are retriable tuples, pause any non-retriable partitions if the spout has passed the maxUncommittedOffsets limit, and filter out fresh tuples from the poll result. This should work to enforce maxUncommittedOffsets. In order to avoid dropping messages, the consumer has to seek back to the earliest offset on each partition that was filtered out by this new check. As far as I can tell we won't be increasing the number of discarded tuples by an unreasonable number as long as we pause non-retriable partitions. This is because the spout will currently discard any acked or already emitted offset it receives in a poll. This solution will additionally discard those that are entirely new, which means they have to have a higher offset than the newest currently emitted tuple on the retried partition. It seems (assuming tuple failures are evenly distributed in the emitte set) more likely to me that most retries will happen somewhere \"in the middle\" of the currently emitted tuples. \n\nExample of this functionality:\nMaxUncommittedOffsets is 100\nMaxPollRecords is 10\nCommitted offset for partition 0 and 1 is 0.\nPartition 0 has emitted 0\nPartition 1 has emitted 0...95, 97, 99, 101, 103 (some offsets compacted away)\nPartition 1, message 99 is retriable\nWe pause partition 0, seek to offset 99 on partition 1 and poll.\nWe get back offsets 99, 101, 103 and potentially 7 new tuples. Say the lowest of these is at offset 104.\nWe prefilter the offset list to remove acked, emitted and new tuples, leaving 99. The 7 new tuples are filtered out.\nThe consumer seeks to offset 104 to pick up there on next poll.\nThe spout emits offset 99.\n\nI'd like to highlight that the filtering solution only discards tuples when it gets new tuples back in a poll, so if the retriable tuple in the example had been e.g. 50, it would not have been unnecessarily discarding anything. \n----\n* Enforce maxUncommittedOffsets on a per partition basis (i.e. actual limit will be multiplied by the number of partitions) by always allowing poll for retriable tuples that are within maxUncommittedOffsets tuples of the committed offset. Pause any non-retriable partitions if the spout has passed the maxUncommittedOffsets limit. There is some additional bookkeeping in this solution, because we have to know for each partition whether the maxUncommittedOffsets limit has been reached, and if so what the offset of the tuple at the limit is (e.g. if the limit is 10, we want to know the offset of the 10th tuple emitted after the current committed offset). I believe we should be able to get that information out of the acked and emitted sets in OffsetManager. \n\nExample of this functionality:\nMaxUncommittedOffsets is 100\nMaxPollRecords is 10\nCommitted offset for partition 0 and 1 is 0.\nPartition 0 has emitted 0\nPartition 1 has emitted 0...95, 97, 99, 101, 103 (some offsets compacted away)\nPartition 1, message 99 is retriable\nWe check that message 99 is within 100 emitted tuples of offset 0 (it is the 97th tuple after offset 0, so it is)\nWe pause partition 0, seek to offset 99 on partition 1 and poll\nWe get back offset 99, 101, 103 and potentially 7 new tuples. Say the lowest of these is at offset 104.\nThe spout emits offset 99, filters out 101 and 103 because they were already emitted, and emits the 7 new tuples.\nIf offset 104 (or later) become retriable, they are not retried until the committed offset moves. This is because offset 104 is the 101st tuple emitted after offset 0, so it isn't allowed to retry until the committed offset moves.\n\nI think either solution of the last two solutions should work, and we should be able to implement the fix for STORM-2546 on top of either. I can't offhand say whether one solution is better than the other. I think it depends on the cost of discarding a few extra messages, vs. doing the extra bookkeeping for the other solution. \n\nI'd be happy if you would consider whether either of these solutions seem workable to you :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-09T18:09:44.550+0000","updated":"2017-06-09T18:09:44.550+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16044883","id":"16044883","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"Thanks a ton for the awesome writeup of the issue and potential solutions. My thoughts so far around potential solutions are mostly in-line with yours. I wanted but did not get around to confirming the behaviour of Kafka Broker / Group Coordinator when the client node that paused a partition crashes OR leaves group OR suffers network partition before calling resume() for that partition. We need to confirm the behaviour in this scenario and handle it accordingly in the spout.\n\nAbout Solution #3:\nI am assuming we need NOT pause partition 0 in solution #3 for the scenario described. This solution, to me, is basically extending the current logic around maxUncommittedOffsets to every partition in the spout. If a spout handles only one partition then we would never really pause it. We simply stop calling poll if a partition reaches maxUncommittedOffsets without any failed tuples. Otherwise the partition should continue to be polled. The logic should then simply take care of seeking to the appropriate offset depending on whether retriable tuples are present.\n\nAgree completely that the choice is between #2 and #3. Am leaning toward #3 for the following reasons:\n- Partition is a fundamental building block / concept in Kafka and this solution fits neatly into it and extends it\n- For Storm spout, Kafka Partitions enable scaling and isolation among other things. It is not acceptable for a 'healthy' partition to be blocked by an 'unhealthy' one\n- We do a fair bit of partition-specific bookkeeping in OffsetManager already. More bookkeeping is a fair price to pay given the reward on offer.. :-)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-09T19:27:36.487+0000","updated":"2017-06-09T19:27:36.487+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16044979","id":"16044979","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"{quote}\nWe need to confirm the behaviour in this scenario and handle it accordingly in the spout.\n{quote}\nAs far as I know pausing/resuming is a purely local operation for the KafkaConsumer. It just causes it to not fetch records for the paused partitions. The paused state is not preserved if the client crashes (because the local state is then lost), or if the consumers rebalance (see https://github.com/apache/kafka/blob/2af4dd8653dd6717cca1630a57b2835a2698a1bc/clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java#L49). I don't think we need to worry about this. Also I'm pushing for us dropping support for Kafka-managed subscriptions here https://github.com/apache/storm/pull/2151 so I'm hoping this ends up being irrelevant.\n\n{quote}\nI am assuming we need NOT pause partition 0 in solution #3 for the scenario described\n{quote}\nThe reason we want to pause is that when the spout is at (or past) the maxUncommittedOffsets limit, it should only emit retries or a very limited number of new tuples. In the example I gave above, if we don't pause partition 0, then the poll triggered to fetch offset 99 on partition 1 might just return a full batch of messages from partition 0. There is no guarantee that the poll will even contain the retriable tuple, so we might do this multiple times. If there were 10 additional partitions we might get full polls for any of those as well before we get the retriable tuple.\n\nIf we don't pause we can't really enforce maxUncommittedOffsets as far as I can tell. \n\nI agree that if there's only one partition it should never be paused. The rest of your outline seems right to me as well.\n\n{quote}\nFor Storm spout, Kafka Partitions enable scaling and isolation among other things. It is not acceptable for a 'healthy' partition to be blocked by an 'unhealthy' one\n{quote}\nI don't think the healthy partitions will be blocked for very long. Each poll where we pause will reemit (or discard pending the fix for STORM-2546) some retriable tuples. The only way the spout should be completely blocked due to retries is if the user hasn't configured a retry limit and the tuples fail consistently.\n\nI agree that it isn't ideal, but I don't see a way to have a limit like maxUncommittedOffsets be properly enforced without pausing (and thus blocking) the healthy partitions when we get in this state where maxUncommittedOffsets is violated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-09T20:30:47.287+0000","updated":"2017-06-09T20:30:47.287+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16045046","id":"16045046","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"Wait, I think this is fixable. Instead of pausing nonretriable partitions, we could instead keep track of numUncommittedOffsets per partition, so we can pause only those partitions that have no retriable tuples and are at the maxUncommittedOffsets limit. That way unhealthy partitions can't block healthy partitions, and we avoid the case described above where a failed tuple on one partition causes new (limit breaking) tuples to be emitted on a different partition.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-09T21:20:47.281+0000","updated":"2017-06-09T21:20:47.281+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16045388","id":"16045388","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"{quote}\nInstead of pausing nonretriable partitions, we could instead keep track of numUncommittedOffsets per partition, so we can pause only those partitions that have no retriable tuples and are at the maxUncommittedOffsets limit. That way unhealthy partitions can't block healthy partitions, and we avoid the case described above where a failed tuple on one partition causes new (limit breaking) tuples to be emitted on a different partition.\n{quote}\n\nYes, this is what we should do. Pause partitions only in the above scenario. In the special case of a spout handling only one partition, we can simply skip poll() instead of pausing even when this condition is met.\n\nNoted your update on kafka consumer pause being locally managed. Makes sense.\n\nSTORM-2542 is interesting. Will comment on that in that JIRA once I catch up on it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-10T05:03:38.531+0000","updated":"2017-06-10T05:03:38.531+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16045434","id":"16045434","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"I'll take a look at implementing this fix soon. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-10T07:14:18.707+0000","updated":"2017-06-10T07:14:18.707+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16045629","id":"16045629","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"[~ranganp] Put up a PR for the proposed fix here https://github.com/apache/storm/pull/2156","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-10T17:46:13.091+0000","updated":"2017-06-10T17:46:13.091+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/comment/16045636","id":"16045636","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"Awesome. Will take a look.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-10T18:00:30.806+0000","updated":"2017-06-10T18:00:30.806+0000"}],"maxResults":16,"total":16,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-2343/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":102,"worklogs":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36735","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"GitHub user srdo opened a pull request:\n\n    https://github.com/apache/storm/pull/1924\n\n    STORM-2343: New Kafka spout can stop emitting tuples if more than maxUncommittedOffsets tuples fail at once\n\n    This builds on https://github.com/apache/storm/pull/1832, sorry for the mixed diff\n    \n    This makes the spout emit failed tuples even when numUncommittedOffsets has reached maxUncommittedOffsets. Previously the spout would fail to progress in that case. I haven't really been able to enforce maxUncommittedOffsets strictly without having to request extra messages from Kafka and throwing them away. The consumer doesn't allow the spout to limit how many tuples it requests dynamically, and I'd prefer that the spout doesn't truncate the list of records it receives.\n    \n    Instead, maxUncommittedOffsets is now a soft cap on the number of tuples that can be emitted before some must be committed. In some cases (e.g. 10 partitions with 1 failed message emitted on each), the spout will exceed the limit. It should never be by more than 1 maxPollRecords size though.\n    \n    This also makes KafkaSpoutRetryExponentialBackoff use Storm Time, and it fixes a bug where messages could be lost if they were scheduled for retry at the same timestamp. It also fixes double counting failed tuples in numUncommittedOffsets when retrying, since the counter isn't decreased when the tuple is scheduled for retry.\n    \n    maxPollRecords is now capped by maxUncommittedOffsets, since if maxPollRecords is higher the spout will exceed the limit on any poll where Kafka can return maxPollRecords messages.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/srdo/storm STORM-2343\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/storm/pull/1924.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #1924\n    \n----\ncommit d7432f8e39c2dd91902bb281be6381d8ce4d53fe\nAuthor: Stig Rohde Døssing <sdo@it-minds.dk>\nDate:   2017-02-04T08:04:19Z\n\n    STORM-2250: Kafka Spout Refactoring to Increase Modularity and Testability\n\ncommit 1c3f9ab53995236cb1a92ba8a51cdfcf73a21bfc\nAuthor: Stig Rohde Døssing <sdo@it-minds.dk>\nDate:   2017-02-05T18:46:49Z\n\n    STORM-2343: Fix new kafka spout stopping processing if more than maxUncommittedOffsets tuples fail at once\n\n----\n","created":"2017-02-05T18:57:28.107+0000","updated":"2017-02-05T18:57:28.107+0000","started":"2017-02-05T18:57:28.106+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36735","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36737","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/1924\n  \n    I think it would be best if we merged maxPollOffsets and maxUncommittedOffsets, since having them be different has some undesirable side effects. @hmcl do you have an opinion.\n    \n    Also this drops support for Kafka 0.9 in KafkaSpoutConfig, since I believe https://github.com/apache/storm/pull/1556 already means that this release won't work with 0.9.\n","created":"2017-02-05T19:17:15.961+0000","updated":"2017-02-05T19:17:15.961+0000","started":"2017-02-05T19:17:15.961+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36737","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36746","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/1924\n  \n    This also changes the exponential backoff formula from currentTime + delayPeriod^(failCount-1) to currentTime + delayPeriod*2^(failCount-1). Multiplying the delay by itself causes the delay to grow extremely quickly, and probably wasn't intended.\n    \n    It might make sense to add jitter to the backoff as well.\n","created":"2017-02-06T07:24:02.602+0000","updated":"2017-02-06T07:24:02.602+0000","started":"2017-02-06T07:24:02.600+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36746","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36755","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99624959\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java ---\n    @@ -27,13 +27,15 @@\n     import java.util.Comparator;\n     import java.util.HashSet;\n     import java.util.Iterator;\n    +import java.util.Objects;\n     import java.util.Set;\n     import java.util.TreeSet;\n     import java.util.concurrent.TimeUnit;\n    +import org.apache.storm.utils.Time;\n     \n     /**\n      * Implementation of {@link KafkaSpoutRetryService} using the exponential backoff formula. The time of the nextRetry is set as follows:\n    - * nextRetry = failCount == 1 ? currentTime + initialDelay : currentTime + delayPeriod^(failCount-1)    where failCount = 1, 2, 3, ...\n    + * nextRetry = failCount == 1 ? currentTime + initialDelay : currentTime + delayPeriod*2^(failCount-1)    where failCount = 1, 2, 3, ...\n      * nextRetry = Min(nextRetry, currentTime + maxDelay)\n    --- End diff --\n    \n    The exponential backoff is a geometric progression with rate `delayPeriod`, hence has a very well known formula. Wy change the formula? Why the constant factor of 2 ?\n","created":"2017-02-06T16:31:01.751+0000","updated":"2017-02-06T16:31:01.751+0000","started":"2017-02-06T16:31:01.744+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36755","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36759","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99634206\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java ---\n    @@ -54,59 +56,92 @@\n         private static class RetryEntryTimeStampComparator implements Serializable, Comparator<RetrySchedule> {\n             @Override\n             public int compare(RetrySchedule entry1, RetrySchedule entry2) {\n    -            return Long.valueOf(entry1.nextRetryTimeNanos()).compareTo(entry2.nextRetryTimeNanos());\n    +            int result = Long.valueOf(entry1.nextRetryTimeMs()).compareTo(entry2.nextRetryTimeMs());\n    +            \n    +            if(result == 0) {\n    +                //TreeSet uses compareTo instead of equals() for the Set contract\n    +                //Ensure that we can save two retry schedules with the same timestamp\n    +                result = entry1.hashCode() - entry2.hashCode();\n    +            }\n    +            return result;\n             }\n         }\n     \n         private class RetrySchedule {\n             private final KafkaSpoutMessageId msgId;\n    -        private long nextRetryTimeNanos;\n    +        private long nextRetryTimeMs;\n    --- End diff --\n    \n    Any special reason to change this for ms ?\n","created":"2017-02-06T17:08:03.620+0000","updated":"2017-02-06T17:08:03.620+0000","started":"2017-02-06T17:08:03.620+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36759","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36760","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99634404\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java ---\n    @@ -54,59 +56,92 @@\n         private static class RetryEntryTimeStampComparator implements Serializable, Comparator<RetrySchedule> {\n             @Override\n             public int compare(RetrySchedule entry1, RetrySchedule entry2) {\n    -            return Long.valueOf(entry1.nextRetryTimeNanos()).compareTo(entry2.nextRetryTimeNanos());\n    +            int result = Long.valueOf(entry1.nextRetryTimeMs()).compareTo(entry2.nextRetryTimeMs());\n    +            \n    +            if(result == 0) {\n    +                //TreeSet uses compareTo instead of equals() for the Set contract\n    +                //Ensure that we can save two retry schedules with the same timestamp\n    +                result = entry1.hashCode() - entry2.hashCode();\n    +            }\n    +            return result;\n             }\n         }\n     \n         private class RetrySchedule {\n             private final KafkaSpoutMessageId msgId;\n    -        private long nextRetryTimeNanos;\n    +        private long nextRetryTimeMs;\n     \n             public RetrySchedule(KafkaSpoutMessageId msgId, long nextRetryTime) {\n                 this.msgId = msgId;\n    -            this.nextRetryTimeNanos = nextRetryTime;\n    +            this.nextRetryTimeMs = nextRetryTime;\n                 LOG.debug(\"Created {}\", this);\n             }\n     \n             public void setNextRetryTime() {\n    -            nextRetryTimeNanos = nextTime(msgId);\n    +            nextRetryTimeMs = nextTime(msgId);\n                 LOG.debug(\"Updated {}\", this);\n             }\n     \n    -        public boolean retry(long currentTimeNanos) {\n    -            return nextRetryTimeNanos <= currentTimeNanos;\n    +        public boolean retry(long currentTimeMs) {\n    +            return nextRetryTimeMs <= currentTimeMs;\n             }\n     \n             @Override\n             public String toString() {\n                 return \"RetrySchedule{\" +\n                         \"msgId=\" + msgId +\n    -                    \", nextRetryTime=\" + nextRetryTimeNanos +\n    +                    \", nextRetryTimeMs=\" + nextRetryTimeMs +\n                         '}';\n             }\n     \n    +        @Override\n    +        public int hashCode() {\n    --- End diff --\n    \n    why did you decide to implement  equals and hashcode ?\n","created":"2017-02-06T17:08:55.801+0000","updated":"2017-02-06T17:08:55.801+0000","started":"2017-02-06T17:08:55.801+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36760","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36761","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99634538\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java ---\n    @@ -54,59 +56,92 @@\n         private static class RetryEntryTimeStampComparator implements Serializable, Comparator<RetrySchedule> {\n             @Override\n             public int compare(RetrySchedule entry1, RetrySchedule entry2) {\n    -            return Long.valueOf(entry1.nextRetryTimeNanos()).compareTo(entry2.nextRetryTimeNanos());\n    +            int result = Long.valueOf(entry1.nextRetryTimeMs()).compareTo(entry2.nextRetryTimeMs());\n    +            \n    +            if(result == 0) {\n    +                //TreeSet uses compareTo instead of equals() for the Set contract\n    +                //Ensure that we can save two retry schedules with the same timestamp\n    +                result = entry1.hashCode() - entry2.hashCode();\n    +            }\n    +            return result;\n             }\n         }\n     \n         private class RetrySchedule {\n             private final KafkaSpoutMessageId msgId;\n    -        private long nextRetryTimeNanos;\n    +        private long nextRetryTimeMs;\n     \n             public RetrySchedule(KafkaSpoutMessageId msgId, long nextRetryTime) {\n                 this.msgId = msgId;\n    -            this.nextRetryTimeNanos = nextRetryTime;\n    +            this.nextRetryTimeMs = nextRetryTime;\n                 LOG.debug(\"Created {}\", this);\n             }\n     \n             public void setNextRetryTime() {\n    -            nextRetryTimeNanos = nextTime(msgId);\n    +            nextRetryTimeMs = nextTime(msgId);\n                 LOG.debug(\"Updated {}\", this);\n             }\n     \n    -        public boolean retry(long currentTimeNanos) {\n    -            return nextRetryTimeNanos <= currentTimeNanos;\n    +        public boolean retry(long currentTimeMs) {\n    +            return nextRetryTimeMs <= currentTimeMs;\n             }\n     \n             @Override\n             public String toString() {\n                 return \"RetrySchedule{\" +\n                         \"msgId=\" + msgId +\n    -                    \", nextRetryTime=\" + nextRetryTimeNanos +\n    +                    \", nextRetryTimeMs=\" + nextRetryTimeMs +\n                         '}';\n             }\n     \n    +        @Override\n    +        public int hashCode() {\n    +            int hash = 5;\n    +            hash = 29 * hash + Objects.hashCode(this.msgId);\n    +            return hash;\n    +        }\n    +\n    +        @Override\n    +        public boolean equals(Object obj) {\n    +            if (this == obj) {\n    +                return true;\n    +            }\n    +            if (obj == null) {\n    +                return false;\n    +            }\n    +            if (getClass() != obj.getClass()) {\n    +                return false;\n    +            }\n    +            final RetrySchedule other = (RetrySchedule) obj;\n    +            if (!Objects.equals(this.msgId, other.msgId)) {\n    +                return false;\n    +            }\n    +            return true;\n    +        }\n    +\n             public KafkaSpoutMessageId msgId() {\n                 return msgId;\n             }\n     \n    -        public long nextRetryTimeNanos() {\n    -            return nextRetryTimeNanos;\n    +        public long nextRetryTimeMs() {\n    +            return nextRetryTimeMs;\n             }\n         }\n     \n         public static class TimeInterval implements Serializable {\n    -        private final long lengthNanos;\n    -        private final long length;\n    -        private final TimeUnit timeUnit;\n    +        private final long lengthMs;\n     \n             /**\n              * @param length length of the time interval in the units specified by {@link TimeUnit}\n    -         * @param timeUnit unit used to specify a time interval on which to specify a time unit\n    +         * @param timeUnit unit used to specify a time interval on which to specify a time unit. Smallest supported unit is milliseconds\n              */\n             public TimeInterval(long length, TimeUnit timeUnit) {\n    -            this.length = length;\n    -            this.timeUnit = timeUnit;\n    -            this.lengthNanos = timeUnit.toNanos(length);\n    +            \n    +            if(timeUnit == TimeUnit.MICROSECONDS || timeUnit == TimeUnit.NANOSECONDS) {\n    +                throw new IllegalArgumentException(\"TimeInterval does not support time units smaller than milliseconds\");\n    +            }\n    --- End diff --\n    \n    Why?\n","created":"2017-02-06T17:09:29.835+0000","updated":"2017-02-06T17:09:29.835+0000","started":"2017-02-06T17:09:29.834+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36761","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36762","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99635118\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java ---\n    @@ -329,11 +328,13 @@ private Builder(Builder<?, ?> builder, SerializableDeserializer<K> keyDes, Class\n                 this.offsetCommitPeriodMs = offsetCommitPeriodMs;\n                 return this;\n             }\n    -        \n    +\n             /**\n              * Defines the max number of polled offsets (records) that can be pending commit, before another poll can take place.\n              * Once this limit is reached, no more offsets (records) can be polled until the next successful commit(s) sets the number\n    -         * of pending offsets bellow the threshold. The default is {@link #DEFAULT_MAX_UNCOMMITTED_OFFSETS}.\n    +         * of pending offsets below the threshold. The default is {@link #DEFAULT_MAX_UNCOMMITTED_OFFSETS}.\n    +         * Note that this limit can in some cases be exceeded, but no partition will exceed this limit by more than 2*maxPollRecords.\n    --- End diff --\n    \n    where is the number 2*maxPollRecords coming from?\n","created":"2017-02-06T17:11:42.073+0000","updated":"2017-02-06T17:11:42.073+0000","started":"2017-02-06T17:11:42.073+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36762","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36763","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99635403\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java ---\n    @@ -78,7 +78,7 @@\n         private transient Map<TopicPartition, OffsetManager> acked;           // Tuples that were successfully acked. These tuples will be committed periodically when the commit timer expires, after consumer rebalance, or on close/deactivate\n         private transient Set<KafkaSpoutMessageId> emitted;                 // Tuples that have been emitted but that are \"on the wire\", i.e. pending being acked or failed\n         private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;         // Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple()\n    -    private transient long numUncommittedOffsets;                       // Number of offsets that have been polled and emitted but not yet been committed\n    +    private transient int numUncommittedOffsets;                       // Number of offsets that have been polled and emitted but not yet been committed\n    --- End diff --\n    \n    Why impose a more restricted data type? What if the user desires a large number?\n","created":"2017-02-06T17:12:47.283+0000","updated":"2017-02-06T17:12:47.283+0000","started":"2017-02-06T17:12:47.282+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36763","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36765","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99637679\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java ---\n    @@ -27,13 +27,15 @@\n     import java.util.Comparator;\n     import java.util.HashSet;\n     import java.util.Iterator;\n    +import java.util.Objects;\n     import java.util.Set;\n     import java.util.TreeSet;\n     import java.util.concurrent.TimeUnit;\n    +import org.apache.storm.utils.Time;\n     \n     /**\n      * Implementation of {@link KafkaSpoutRetryService} using the exponential backoff formula. The time of the nextRetry is set as follows:\n    - * nextRetry = failCount == 1 ? currentTime + initialDelay : currentTime + delayPeriod^(failCount-1)    where failCount = 1, 2, 3, ...\n    + * nextRetry = failCount == 1 ? currentTime + initialDelay : currentTime + delayPeriod*2^(failCount-1)    where failCount = 1, 2, 3, ...\n      * nextRetry = Min(nextRetry, currentTime + maxDelay)\n    --- End diff --\n    \n    The current implementation will multiply the delay by delayPeriod for each retry after the first. Let's say I set initialDelay to 1000ms and retryPeriod to 1000ms. The first retry is then scheduled at 1 second after failure. The second at 1 second after that. The third retry is scheduled 1000 seconds after that. The usual exponential backoff function would multiply the backoff by 2 for each failure, not by the backoff itself. This is a geometric progression where you can only set ratio, not scale factor, which isn't very useful (for anything except tiny backoffs, it grows much too fast). If we want to have backoffs with multipliers other than 2, we could add another parameter for that? (in that case that parameter should be an int, not a time period IMO)\n","created":"2017-02-06T17:23:00.446+0000","updated":"2017-02-06T17:23:00.446+0000","started":"2017-02-06T17:23:00.446+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36765","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36766","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99638011\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java ---\n    @@ -54,59 +56,92 @@\n         private static class RetryEntryTimeStampComparator implements Serializable, Comparator<RetrySchedule> {\n             @Override\n             public int compare(RetrySchedule entry1, RetrySchedule entry2) {\n    -            return Long.valueOf(entry1.nextRetryTimeNanos()).compareTo(entry2.nextRetryTimeNanos());\n    +            int result = Long.valueOf(entry1.nextRetryTimeMs()).compareTo(entry2.nextRetryTimeMs());\n    +            \n    +            if(result == 0) {\n    +                //TreeSet uses compareTo instead of equals() for the Set contract\n    +                //Ensure that we can save two retry schedules with the same timestamp\n    +                result = entry1.hashCode() - entry2.hashCode();\n    +            }\n    +            return result;\n             }\n         }\n     \n         private class RetrySchedule {\n             private final KafkaSpoutMessageId msgId;\n    -        private long nextRetryTimeNanos;\n    +        private long nextRetryTimeMs;\n    --- End diff --\n    \n    Storm's time simulation doesn't support values smaller than ms. I'd be happy to implement nanosecond support in Time instead if anyone actually needs sub-millis support.\n","created":"2017-02-06T17:24:19.995+0000","updated":"2017-02-06T17:24:19.995+0000","started":"2017-02-06T17:24:19.995+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36766","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36767","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99638909\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java ---\n    @@ -54,59 +56,92 @@\n         private static class RetryEntryTimeStampComparator implements Serializable, Comparator<RetrySchedule> {\n             @Override\n             public int compare(RetrySchedule entry1, RetrySchedule entry2) {\n    -            return Long.valueOf(entry1.nextRetryTimeNanos()).compareTo(entry2.nextRetryTimeNanos());\n    +            int result = Long.valueOf(entry1.nextRetryTimeMs()).compareTo(entry2.nextRetryTimeMs());\n    +            \n    +            if(result == 0) {\n    +                //TreeSet uses compareTo instead of equals() for the Set contract\n    +                //Ensure that we can save two retry schedules with the same timestamp\n    +                result = entry1.hashCode() - entry2.hashCode();\n    +            }\n    +            return result;\n             }\n         }\n     \n         private class RetrySchedule {\n             private final KafkaSpoutMessageId msgId;\n    -        private long nextRetryTimeNanos;\n    +        private long nextRetryTimeMs;\n     \n             public RetrySchedule(KafkaSpoutMessageId msgId, long nextRetryTime) {\n                 this.msgId = msgId;\n    -            this.nextRetryTimeNanos = nextRetryTime;\n    +            this.nextRetryTimeMs = nextRetryTime;\n                 LOG.debug(\"Created {}\", this);\n             }\n     \n             public void setNextRetryTime() {\n    -            nextRetryTimeNanos = nextTime(msgId);\n    +            nextRetryTimeMs = nextTime(msgId);\n                 LOG.debug(\"Updated {}\", this);\n             }\n     \n    -        public boolean retry(long currentTimeNanos) {\n    -            return nextRetryTimeNanos <= currentTimeNanos;\n    +        public boolean retry(long currentTimeMs) {\n    +            return nextRetryTimeMs <= currentTimeMs;\n             }\n     \n             @Override\n             public String toString() {\n                 return \"RetrySchedule{\" +\n                         \"msgId=\" + msgId +\n    -                    \", nextRetryTime=\" + nextRetryTimeNanos +\n    +                    \", nextRetryTimeMs=\" + nextRetryTimeMs +\n                         '}';\n             }\n     \n    +        @Override\n    +        public int hashCode() {\n    +            int hash = 5;\n    +            hash = 29 * hash + Objects.hashCode(this.msgId);\n    +            return hash;\n    +        }\n    +\n    +        @Override\n    +        public boolean equals(Object obj) {\n    +            if (this == obj) {\n    +                return true;\n    +            }\n    +            if (obj == null) {\n    +                return false;\n    +            }\n    +            if (getClass() != obj.getClass()) {\n    +                return false;\n    +            }\n    +            final RetrySchedule other = (RetrySchedule) obj;\n    +            if (!Objects.equals(this.msgId, other.msgId)) {\n    +                return false;\n    +            }\n    +            return true;\n    +        }\n    +\n             public KafkaSpoutMessageId msgId() {\n                 return msgId;\n             }\n     \n    -        public long nextRetryTimeNanos() {\n    -            return nextRetryTimeNanos;\n    +        public long nextRetryTimeMs() {\n    +            return nextRetryTimeMs;\n             }\n         }\n     \n         public static class TimeInterval implements Serializable {\n    -        private final long lengthNanos;\n    -        private final long length;\n    -        private final TimeUnit timeUnit;\n    +        private final long lengthMs;\n     \n             /**\n              * @param length length of the time interval in the units specified by {@link TimeUnit}\n    -         * @param timeUnit unit used to specify a time interval on which to specify a time unit\n    +         * @param timeUnit unit used to specify a time interval on which to specify a time unit. Smallest supported unit is milliseconds\n              */\n             public TimeInterval(long length, TimeUnit timeUnit) {\n    -            this.length = length;\n    -            this.timeUnit = timeUnit;\n    -            this.lengthNanos = timeUnit.toNanos(length);\n    +            \n    +            if(timeUnit == TimeUnit.MICROSECONDS || timeUnit == TimeUnit.NANOSECONDS) {\n    +                throw new IllegalArgumentException(\"TimeInterval does not support time units smaller than milliseconds\");\n    +            }\n    --- End diff --\n    \n    The class doesn't support time units less than milliseconds. I'd prefer to warn about misconfiguration rather than just clamping to 0 or 1 millisecond.\n","created":"2017-02-06T17:28:43.600+0000","updated":"2017-02-06T17:28:43.600+0000","started":"2017-02-06T17:28:43.599+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36767","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36768","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99638933\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java ---\n    @@ -78,7 +78,7 @@\n         private transient Map<TopicPartition, OffsetManager> acked;           // Tuples that were successfully acked. These tuples will be committed periodically when the commit timer expires, after consumer rebalance, or on close/deactivate\n         private transient Set<KafkaSpoutMessageId> emitted;                 // Tuples that have been emitted but that are \"on the wire\", i.e. pending being acked or failed\n         private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;         // Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple()\n    -    private transient long numUncommittedOffsets;                       // Number of offsets that have been polled and emitted but not yet been committed\n    +    private transient int numUncommittedOffsets;                       // Number of offsets that have been polled and emitted but not yet been committed\n    --- End diff --\n    \n    maxUncommittedOffsets is an int. This just makes it consistent.\n","created":"2017-02-06T17:28:50.172+0000","updated":"2017-02-06T17:28:50.172+0000","started":"2017-02-06T17:28:50.171+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36768","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36769","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99639068\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java ---\n    @@ -78,7 +78,7 @@\n         private transient Map<TopicPartition, OffsetManager> acked;           // Tuples that were successfully acked. These tuples will be committed periodically when the commit timer expires, after consumer rebalance, or on close/deactivate\n         private transient Set<KafkaSpoutMessageId> emitted;                 // Tuples that have been emitted but that are \"on the wire\", i.e. pending being acked or failed\n         private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;         // Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple()\n    -    private transient long numUncommittedOffsets;                       // Number of offsets that have been polled and emitted but not yet been committed\n    +    private transient int numUncommittedOffsets;                       // Number of offsets that have been polled and emitted but not yet been committed\n    --- End diff --\n    \n    Actually you're right, since the spout may emit more than maxUncommittedOffsets this should be changed back.\n","created":"2017-02-06T17:29:26.237+0000","updated":"2017-02-06T17:29:26.237+0000","started":"2017-02-06T17:29:26.237+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36769","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36770","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99639454\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java ---\n    @@ -329,11 +328,13 @@ private Builder(Builder<?, ?> builder, SerializableDeserializer<K> keyDes, Class\n                 this.offsetCommitPeriodMs = offsetCommitPeriodMs;\n                 return this;\n             }\n    -        \n    +\n             /**\n              * Defines the max number of polled offsets (records) that can be pending commit, before another poll can take place.\n              * Once this limit is reached, no more offsets (records) can be polled until the next successful commit(s) sets the number\n    -         * of pending offsets bellow the threshold. The default is {@link #DEFAULT_MAX_UNCOMMITTED_OFFSETS}.\n    +         * of pending offsets below the threshold. The default is {@link #DEFAULT_MAX_UNCOMMITTED_OFFSETS}.\n    +         * Note that this limit can in some cases be exceeded, but no partition will exceed this limit by more than 2*maxPollRecords.\n    --- End diff --\n    \n    Sorry, this is a mistake. The worst case is one where maxUncommittedOffsets - 1 messages have been emitted. In that case, the spout may poll and emit another maxPollRecords messages. So the comment should say maxPollRecords, not 2*maxPollRecords.\n","created":"2017-02-06T17:31:34.576+0000","updated":"2017-02-06T17:31:34.576+0000","started":"2017-02-06T17:31:34.575+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36770","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36771","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99640360\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java ---\n    @@ -240,7 +240,9 @@ private boolean commit() {\n     \n         private boolean poll() {\n             final int maxUncommittedOffsets = kafkaSpoutConfig.getMaxUncommittedOffsets();\n    -        final boolean poll = !waitingToEmit() && numUncommittedOffsets < maxUncommittedOffsets;\n    +        final boolean poll = !waitingToEmit() &&\n    +            (numUncommittedOffsets < maxUncommittedOffsets ||\n    +            !retryService.retriableTopicPartitions().isEmpty());\n    --- End diff --\n    \n    `!retryService.retriableTopicPartitions().isEmpty()` can't this condition cause unbounded  polling (i.e. continue forever), if some specific tuples are always failing?\n    \n    The goal is to have a limit in how much can be polled (hence kept in memory) in case of recurring failures.\n","created":"2017-02-06T17:35:46.426+0000","updated":"2017-02-06T17:35:46.426+0000","started":"2017-02-06T17:35:46.425+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36771","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36772","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99641026\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java ---\n    @@ -289,6 +293,13 @@ private void doSeekRetriableTopicPartitions() {\n                     kafkaConsumer.seek(rtp, acked.get(rtp).getCommittedOffset() + 1);    // Seek to last committed offset\n                 }\n             }\n    +        \n    +        if(!retriableTopicPartitions.isEmpty()) {\n    +            //Pause other partitions for this poll so maxUncommittedPartitions isn't exceeded by too much\n    --- End diff --\n    \n    I am a bit confused about this. When can it be exceeded? By how much? This should be a deterministic number.\n    \n    I read your comment in the email thread about  maxSpoutPending. Is this related to that ?\n","created":"2017-02-06T17:39:22.745+0000","updated":"2017-02-06T17:39:22.745+0000","started":"2017-02-06T17:39:22.745+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36772","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36775","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99642616\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java ---\n    @@ -289,6 +293,13 @@ private void doSeekRetriableTopicPartitions() {\n                     kafkaConsumer.seek(rtp, acked.get(rtp).getCommittedOffset() + 1);    // Seek to last committed offset\n                 }\n             }\n    +        \n    +        if(!retriableTopicPartitions.isEmpty()) {\n    +            //Pause other partitions for this poll so maxUncommittedPartitions isn't exceeded by too much\n    --- End diff --\n    \n    No, I made a mistake there. maxSpoutPending is being enforced correctly. \n","created":"2017-02-06T17:47:08.182+0000","updated":"2017-02-06T17:47:08.182+0000","started":"2017-02-06T17:47:08.181+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36775","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36776","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99642608\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java ---\n    @@ -240,7 +240,9 @@ private boolean commit() {\n     \n         private boolean poll() {\n             final int maxUncommittedOffsets = kafkaSpoutConfig.getMaxUncommittedOffsets();\n    -        final boolean poll = !waitingToEmit() && numUncommittedOffsets < maxUncommittedOffsets;\n    +        final boolean poll = !waitingToEmit() &&\n    +            (numUncommittedOffsets < maxUncommittedOffsets ||\n    +            !retryService.retriableTopicPartitions().isEmpty());\n    --- End diff --\n    \n    No, I don't think so. The tuples need to be both failed and ready for retry for this to be true, and if it's true, the consumer will seek back to the last committed offset on the relevant topic partitions. The consumer pause in doSeekRetriableTopicPartitions ensures that we don't get messages for the partitions that don't have failed tuples. maxPollRecords then caps how far past the commit offset we can read on those partitions. If the same tuples are always failing, we'll never get more than maxPollRecords from the commit offset on those partitions as far as I can tell.\n","created":"2017-02-06T17:47:08.182+0000","updated":"2017-02-06T17:47:08.182+0000","started":"2017-02-06T17:47:08.182+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36776","issueId":"13040284"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13040284/worklog/36791","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/1924#discussion_r99658822\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java ---\n    @@ -54,59 +56,92 @@\n         private static class RetryEntryTimeStampComparator implements Serializable, Comparator<RetrySchedule> {\n             @Override\n             public int compare(RetrySchedule entry1, RetrySchedule entry2) {\n    -            return Long.valueOf(entry1.nextRetryTimeNanos()).compareTo(entry2.nextRetryTimeNanos());\n    +            int result = Long.valueOf(entry1.nextRetryTimeMs()).compareTo(entry2.nextRetryTimeMs());\n    +            \n    +            if(result == 0) {\n    +                //TreeSet uses compareTo instead of equals() for the Set contract\n    +                //Ensure that we can save two retry schedules with the same timestamp\n    +                result = entry1.hashCode() - entry2.hashCode();\n    +            }\n    +            return result;\n             }\n         }\n     \n         private class RetrySchedule {\n             private final KafkaSpoutMessageId msgId;\n    -        private long nextRetryTimeNanos;\n    +        private long nextRetryTimeMs;\n     \n             public RetrySchedule(KafkaSpoutMessageId msgId, long nextRetryTime) {\n                 this.msgId = msgId;\n    -            this.nextRetryTimeNanos = nextRetryTime;\n    +            this.nextRetryTimeMs = nextRetryTime;\n                 LOG.debug(\"Created {}\", this);\n             }\n     \n             public void setNextRetryTime() {\n    -            nextRetryTimeNanos = nextTime(msgId);\n    +            nextRetryTimeMs = nextTime(msgId);\n                 LOG.debug(\"Updated {}\", this);\n             }\n     \n    -        public boolean retry(long currentTimeNanos) {\n    -            return nextRetryTimeNanos <= currentTimeNanos;\n    +        public boolean retry(long currentTimeMs) {\n    +            return nextRetryTimeMs <= currentTimeMs;\n             }\n     \n             @Override\n             public String toString() {\n                 return \"RetrySchedule{\" +\n                         \"msgId=\" + msgId +\n    -                    \", nextRetryTime=\" + nextRetryTimeNanos +\n    +                    \", nextRetryTimeMs=\" + nextRetryTimeMs +\n                         '}';\n             }\n     \n    +        @Override\n    +        public int hashCode() {\n    --- End diff --\n    \n    I'll remove these two again, the Comparator change should work even with the default hashcode.\n","created":"2017-02-06T19:06:10.095+0000","updated":"2017-02-06T19:06:10.095+0000","started":"2017-02-06T19:06:10.094+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"36791","issueId":"13040284"}]},"customfield_12311820":"0|i39ltj:"}}