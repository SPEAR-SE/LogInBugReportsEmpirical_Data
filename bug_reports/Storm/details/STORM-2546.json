{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13078239","self":"https://issues.apache.org/jira/rest/api/2/issue/13078239","key":"STORM-2546","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":6600,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12314820","id":"12314820","key":"STORM","name":"Apache Storm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12314820&avatarId=21667","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12314820&avatarId=21667","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12314820&avatarId=21667","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12314820&avatarId=21667"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/13260","id":"13260","description":"Apache Storm Related","name":"Storm"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334657","id":"12334657","name":"2.0.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12341047","id":"12341047","name":"1.2.0","archived":false,"released":true,"releaseDate":"2018-02-15"}],"aggregatetimespent":6600,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-06-10T17:48:16.581+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Sep 04 18:31:51 UTC 2017","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_3480040037_*|*_3_*:*_1_*:*_10424576961_*|*_5_*:*_1_*:*_0","customfield_12310420":"9223372036854775807","customfield_12312321":null,"resolutiondate":"2017-11-16T06:12:05.819+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-2546/watchers","watchCount":3,"isWatching":false},"created":"2017-06-08T07:48:28.893+0000","priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["pull-request-available"],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":0,"aggregatetimeoriginalestimate":null,"customfield_12311120":"STORM-2710","customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334657","id":"12334657","name":"2.0.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12335748","id":"12335748","name":"1.1.0","archived":false,"released":true,"releaseDate":"2017-03-29"}],"customfield_12312339":null,"issuelinks":[{"id":"12520020","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12520020","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13058139","key":"STORM-2430","self":"https://issues.apache.org/jira/rest/api/2/issue/13058139","fields":{"summary":"Potential Race condition in Kafka Spout","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-11-16T06:13:54.290+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12331080","id":"12331080","name":"storm-kafka-client"}],"timeoriginalestimate":null,"description":"The mechanism for replaying a failed tuple involves seeking the kafka consumer to the failing offset and then re-emitting it into the topology. A tuple, when emitted the first time, will have an entry created in OffsetManager. This entry will be removed only after the tuple is successfully acknowledged and its offset successfully committed. Till then, commits for offsets beyond the failing offset for that TopicPartition will be blocked.\n\nIt is possible that when the spout seeks the consumer to the failing offset, the corresponding kafka message is not returned in the poll response. This can happen due to that offset being deleted or compacted away. In this scenario that partition will be blocked from committing and progressing.","customfield_10010":null,"timetracking":{"remainingEstimate":"0h","timeSpent":"1h 50m","remainingEstimateSeconds":0,"timeSpentSeconds":6600},"customfield_12312026":null,"customfield_12312023":null,"customfield_12312024":null,"attachment":[],"customfield_12312340":null,"aggregatetimeestimate":0,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Kafka spout can stall / get stuck due to edge case with failing tuples","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"customfield_12310291":null,"customfield_12310290":null,"aggregateprogress":{"progress":6600,"total":6600,"percent":100},"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":6600,"total":6600,"percent":100},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16045632","id":"16045632","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"Will look at fixing this once https://github.com/apache/storm/pull/2156 has been resolved.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-06-10T17:48:16.581+0000","updated":"2017-06-10T17:48:16.581+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16151172","id":"16151172","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"Here's my suggestion for a fix, which depends on the changes in https://issues.apache.org/jira/browse/STORM-2549:\nThe only way to know that a tuple has been deleted from Kafka is to try polling for it. We can know for sure that a failed tuple has been deleted if we seek to the failed tuple's offset (or earlier) on the relevant partition and poll, and we then encounter a tuple that has a higher offset than the failed tuple on that partition earlier in the result set.\n\nFor instance:\nOffset 0...5 have failed and also been compacted away. Offset 6 has failed and is present, offset 7 has failed and is not present.\nWe seek to offset 0 for the partition.\nIf we then see that the first message in the poll result is offset 6, we can be sure that offset 0...5 are deleted, because otherwise they would have been returned in the poll. Offset 7 cannot be removed from the spout because we can't be sure that it was deleted, the consumer may just have received too few messages.\n\nI think we can use this method to remove failed, deleted tuples from the offset manager. When we do a poll, we examine the retriable offsets for each partition. For each partition where we received messages, we compare the earliest received message's offset to the retriable offsets for that partition. If a given retriable offset is lower than the offset of the earliest received message, then the retriable offset must have been deleted. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-09-01T21:27:39.068+0000","updated":"2017-09-02T18:10:56.081+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16151676","id":"16151676","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"Consider the following example:\n - Kafka offsets initially emitted  [0..10]\n - Offsets 6 & 7 fail. Other offsets succeed\n - Compaction happens and Kafka broker now has offsets [4,6,10]\n - Let's assume we seek to Offset 0\n - If the _auto.offset.reset_ property is set to 'latest' then poll will return Offset 10\n - If the _auto.offset.reset_ property is set to 'earliest' then poll will return Offset 4\n\nWe need to factor in the effect of the _auto.offset.reset_ property in the solution. Will think through this and suggest updates to the solution.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-09-03T04:49:31.341+0000","updated":"2017-09-03T04:49:31.341+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16151698","id":"16151698","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"I think for compaction support we might be lucky enough to avoid this. I haven't used log compaction, so I may be misunderstanding the documentation, but it's my impression that offsets always stay valid even when compacted away.\n\nThe relevant passage from https://kafka.apache.org/documentation/#design_compactionbasics is \"The picture above shows a log with a compacted tail. Note that the messages in the tail of the log retain the original offset assigned when they were first written—that never changes. Note also that all offsets remain valid positions in the log, even if the message with that offset has been compacted away; in this case this position is indistinguishable from the next highest offset that does appear in the log. \"\n\nI'm reading this as we don't need to worry about auto offset reset because for your example 0 remains a valid offset that just translates to 4, so auto.offset.reset isn't triggered.\n\nIf the cleanup.policy is delete and not compact, I think your example is right. I don't have a good solution off hand either, so I'm hoping you come up with something :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-09-03T06:25:47.976+0000","updated":"2017-09-03T06:25:47.976+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16151811","id":"16151811","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"I thought about it a bit more, and think we can reduce to 6 different situations for cleanup.policy = delete. I don't think we need to support auto offset reset policy being none, since that'll just cause an exception and restart the spout:\n\n* Auto offset reset is latest\n** 0 or more acked offsets when reset, no pending retries when reset happens:\nExample: 0-3 are acked, 0-1 are deleted. 10 is the latest offset.\nConsumer will seek to latest offset (10). When new tuples are acked after the reset, the OffsetManager will skip past all the unemitted tuples (4-9). I don't think we need to do anything special for this case.\n** 0 or more acked offsets when reset, earliest pending retry is deleted:\nExample: 0-3 are acked, 0-4 are deleted. 4-5 are retriable and 10 is the latest offset.\nConsumer will seek to latest offset (10). The spout will handle acks as described in the previous case. When the retriable tuples become ready, the consumer will seek back to the earliest retriable offset (4). Since 4 is deleted, the consumer seeks to latest. Since the spout asked for retriable offset 4 and got 10 as the earliest, it will consider 4-9 deleted and mark them as acked. While offset 5 is technically still possible to retry, I don't think this is necessarily unexpected or bad behavior. I'd be okay with leaving this behavior as is.\n** 0 or more acked offsets when reset, earliest pending retry is not deleted:\nExample: 0-3 are acked, 0-3 are deleted. 4-5 are retriable and 10 is the latest offset.\nSame as above, except when the retriable tuples become ready, the consumer will seek back to the earliest retriable offset (4). The spout continues processing from offset 4. This is a little inconsistent with the case above, but I don't think it's bad behavior, so I'm okay with leaving it like this.\n\n* Auto offset reset is earliest, cleanup policy is delete\n** 0 or more acked offsets when reset, no pending retries when reset happens:\nExactly the same as the case for latest, except the consumer seeks to earliest instead.\n** 0 or more acked offsets when reset, earliest pending retry is deleted:\nBroadly the same as for latest. When the retriable tuples become ready, the consumer will seek back to the earliest retriable offset (4). Since 4 is deleted, the consumer seeks to earliest (5). 5 is skipped initially because it is not ready for retry, and processing picks up at 6. 5 gets retried whenever it becomes ready.\n** 0 or more acked offsets when reset, earliest pending retry is not deleted:\nShould be the same as the case for latest. The consumer will seek to earliest and poll, receiving offset 4-10. Since 4 and 5 are scheduled for retry and not ready, they will be skipped but left in the retry service until they're ready. Processing starts at 6 and works as normal. This seems fine to me.\n\nFor these cases I'd be okay with not doing any more work. The only one that is a little counter intuitive is number 2, but I think when auto offset reset is set to latest, the user should expect that the spout may skip some tuples.\n\nMy only worry is what happens if the spout commits offsets that were deleted. The javadoc for KafkaConsumer.commitSync states that it may throw an exception in that case: \"KafkaException - for any other unrecoverable errors (e.g. if offset metadata is too large or if the committed offset is invalid).\". I tested it on an 0.11 broker and consumer and it doesn't appear to actually throw this exception even if the offset doesn't exist, so I've asked about it on the Kafka users list.\n\nWhat do you think, is this a reasonable way to look at it or do we need to do more?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-09-03T13:10:44.148+0000","updated":"2017-09-03T13:10:44.148+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16151889","id":"16151889","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"Copy pasting from the Kafka mailing list http://mail-archives.apache.org/mod_mbox/kafka-users/201709.mbox/%3CCA%2BOCqnYvhDTQ_dWthg68aO0JKgAENuwDyn-LgdnEyg%2BwAAgGMw%40mail.gmail.com%3E:\n\n{quote}\nI believe the Javadoc is slightly incorrect/misleading.\nWhen it says \"offset metadata is too large\", it is about the metadata\nyou can commit along with the offset, not the offset. See\nOffsetAndMetadata:\nhttp://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html\n\nRegarding the offset value, we only check if it's negative and that's\nonly performed client side (presumably 3rd party clients could commit\na negative offset). Apart from that, no checks are made if the offset\nis \"in range\" or not.\nWe had a look a while back to check if the offset is \"in range\" when\ncommitting but it's complicated, see the comments on\nhttps://issues.apache.org/jira/browse/KAFKA-4081\n\nI opened a PR to update the Javadoc: https://github.com/apache/kafka/pull/3780\n{quote}\n\nSo we don't need to worry about committing offsets that are deleted.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-09-03T18:03:25.719+0000","updated":"2017-09-04T14:54:49.350+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16152393","id":"16152393","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"Regarding committing deleted offsets and compacted offsets:\n\nThe javadoc/documentation and reply on mailing list sounds conclusive enough. We should design a few tests to verify the behaviour such as\n     1. Commit deleted offsets\n     2. Seek / Commit compacted offsets\n\nI particularly want to understand/confirm the broker behaviour when we seek or commit a compacted offset - will it return an earlier valid offset or a later one?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-09-04T09:40:33.390+0000","updated":"2017-09-04T09:40:33.390+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16152400","id":"16152400","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"{quote}\nAuto offset reset is latest\n* 0 or more acked offsets when reset, no pending retries when reset happens:\n    Example: 0-3 are acked, 0-1 are deleted. 10 is the latest offset.\n   Consumer will seek to latest offset (10). When new tuples are acked after the reset, the OffsetManager will skip past all the unemitted tuples (4-9). I don't think we need to do anything special for this case.\n\n* 0 or more acked offsets when reset, earliest pending retry is deleted:\n   Example: 0-3 are acked, 0-4 are deleted. 4-5 are retriable and 10 is the latest offset.\n   Consumer will seek to latest offset (10). The spout will handle acks as described in the previous case. When the retriable tuples become ready, the consumer will seek back to the earliest retriable offset (4). Since 4 is deleted, the consumer seeks to latest. Since the spout asked for retriable offset 4 and got 10 as the earliest, it will consider 4-9 deleted and mark them as acked. While offset 5 is technically still possible to retry, I don't think this is necessarily unexpected or bad behavior. I'd be okay with leaving this behavior as is.\n\n*  0 or more acked offsets when reset, earliest pending retry is not deleted:\n   Example: 0-3 are acked, 0-3 are deleted. 4-5 are retriable and 10 is the latest offset.\n   Same as above, except when the retriable tuples become ready, the consumer will seek back to the earliest retriable offset (4). The spout continues processing from offset 4. This is a little inconsistent with the case above, but I don't think it's bad behavior, so I'm okay with leaving it like this.\n{quote}\n\nThis approach sounds reasonable for #1 & #3. It will be good if we can identify the situations where offsets are being skipped and add appropriate logging. Kafka client prints a log when the consumer offset seek is invalid.\n\nFor #2, I am wondering if we need to identify the smallest valid offset larger than the invalid offset that the spout was trying to seek. In other words, we would be 'bridging' the gap as with the fix built for STORM-2505. The reason is that there could be a situation where a single deleted offset can result in the spout skipping ahead a lot and potentially pass over many valid offsets without even a single processing attempt. Need to think this over a bit more but feels like we can handle this scenario in a cleaner manner if the Kafka Consumer had an api to the effect of _seekNextValidOffset(Offset currentOffset)_","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-09-04T09:46:45.040+0000","updated":"2017-09-04T10:04:16.527+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16152543","id":"16152543","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"{quote}\nAuto offset reset is earliest, cleanup policy is delete\n* 0 or more acked offsets when reset, no pending retries when reset happens:\nExactly the same as the case for latest, except the consumer seeks to earliest instead.\n\n* 0 or more acked offsets when reset, earliest pending retry is deleted:\nBroadly the same as for latest. When the retriable tuples become ready, the consumer will seek back to the earliest retriable offset (4). Since 4 is deleted, the consumer seeks to earliest (5). 5 is skipped initially because it is not ready for retry, and processing picks up at 6. 5 gets retried whenever it becomes ready.\n\n* 0 or more acked offsets when reset, earliest pending retry is not deleted:\nShould be the same as the case for latest. The consumer will seek to earliest and poll, receiving offset 4-10. Since 4 and 5 are scheduled for retry and not ready, they will be skipped but left in the retry service until they're ready. Processing starts at 6 and works as normal. This seems fine to me.\n{quote}\n\nThe approach detailed looks good to me. Thought through the possibility of offset reset resulting in tracking back to an extremely old offset. Then again, I don't think this can happen since in a 'delete' topic if we can go back to offset X then any offset greater than X has to be valid. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-09-04T12:12:11.020+0000","updated":"2017-09-04T12:12:11.020+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16152712","id":"16152712","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"{quote}\nRegarding committing deleted offsets and compacted offsets:\n\nThe javadoc/documentation and reply on mailing list sounds conclusive enough. We should design a few tests to verify the behaviour such as\n1. Commit deleted offsets\n2. Seek / Commit compacted offsets\n\nI particularly want to understand/confirm the broker behaviour when we seek or commit a compacted offset - will it return an earlier valid offset or a later one?\n{quote}\nI feel like this type of test belongs in Kafka and not here, since it is testing KafkaConsumer behavior. About deleted offsets, I manually verified that the consumer doesn't care about commits of offsets that are out of range. See http://mail-archives.apache.org/mod_mbox/kafka-users/201709.mbox/%3CCAG09ER3XdnZyerPvD_pKJicg2FjtHmPXtmD2c4xAv9O%3DnZ94Yg%40mail.gmail.com%3E. So I don't think we need to worry about that, since committing a deleted offset doesn't hurt anything, and if we seek to that offset we'll just trigger the auto offset reset policy.\n\nAbout compacted offsets, the documentation on log compaction specifies this behavior:\n{quote}\nNote also that all offsets remain valid positions in the log, even if the message with that offset has been compacted away; in this case this position is indistinguishable from the next highest offset that does appear in the log\n{quote}\nSo for your questions in 2. the answers should be that committing has no extraordinary effect (it will just set the committed offset to the compacted offset and nothing else will happen), and seeking to the compacted offset will produce the next highest offset that hasn't been compacted.\n\n{quote}\nThis approach sounds reasonable for #1 & #3. It will be good if we can identify the situations where offsets are being skipped and add appropriate logging. Kafka client prints a log when the consumer offset seek is invalid.\n{quote}\nIf the consumer already logs that the auto offset reset policy is triggered, and we log any time we skip tuples due to offset gaps here https://github.com/apache/storm/pull/2307/files#diff-7d7cbc8f5444fa7ada7962033fc31c5eR363 and here https://github.com/apache/storm/blob/master/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java#L99, I think we might be covered? If we skip an offset the consumer hasn't seen yet, the consumer log and the second link should cover it. If we skip an offset when it was emitted and acked, it wasn't skipped. If we skip an offset that was emitted and failed, we log it in the first link.\n\n{quote}\nFor #2, I am wondering if we need to identify the smallest valid offset larger than the invalid offset that the spout was trying to seek. In other words, we would be 'bridging' the gap as with the fix built for STORM-2505. The reason is that there could be a situation where a single deleted offset can result in the spout skipping ahead a lot and potentially pass over many valid offsets without even a single processing attempt. Need to think this over a bit more but feels like we can handle this scenario in a cleaner manner if the Kafka Consumer had an api to the effect of seekNextValidOffset(Offset currentOffset)\n{quote}\nI think we should consider not doing anything in this case. Recall that the auto offset reset policy is set to latest, so seeking to the latest offset is the correct behavior. If we want to help people avoid a potential newbie trap here we could try to warn in the log if the spout is configured for at-least-once and the auto offset reset policy isn't set to earliest. Setting the offset reset policy to earliest has exactly the effect that a seekNextValidOffset function would have.\n\n{quote}\nThe approach detailed looks good to me. Thought through the possibility of offset reset resulting in tracking back to an extremely old offset. Then again, I don't think this can happen since in a 'delete' topic if we can go back to offset X then any offset greater than X has to be valid. \n{quote}\nI agree. For delete cleanup it doesn't seem like a problem. For compacted topics we hopefully never activate the auto offset reset policy, since all old offsets are valid and we don't seek \"in front of\" the Kafka log head.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-09-04T15:19:29.551+0000","updated":"2017-09-04T15:19:29.551+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16152824","id":"16152824","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"body":"Agree on the tests belonging more in Kafka and not here. Goal is primarily to confirm our assumptions of the Kafka consumer behaviour and manual verification would suffice.\n\n{quote}\nIf the consumer already logs that the auto offset reset policy is triggered, and we log any time we skip tuples due to offset gaps here https://github.com/apache/storm/pull/2307/files#diff-7d7cbc8f5444fa7ada7962033fc31c5eR363 and here https://github.com/apache/storm/blob/master/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java#L99, I think we might be covered? If we skip an offset the consumer hasn't seen yet, the consumer log and the second link should cover it. If we skip an offset when it was emitted and acked, it wasn't skipped. If we skip an offset that was emitted and failed, we log it in the first link.\n{quote}\n\nThe currently available logs seem sufficient. We can tweak it later if necessary.\n\n{quote}\nI think we should consider not doing anything in this case. Recall that the auto offset reset policy is set to latest, so seeking to the latest offset is the correct behavior. If we want to help people avoid a potential newbie trap here we could try to warn in the log if the spout is configured for at-least-once and the auto offset reset policy isn't set to earliest. Setting the offset reset policy to earliest has exactly the effect that a seekNextValidOffset function would have.\n{quote}\n\n+1 to adding warning in the log. Further to it, I feel we should go ahead and recommend _auto.offset.reset=earliest_ option for topics with _cleanup.policy=delete_ in the documentation for reliable at-least-once spout processing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ranganp","name":"ranganp","key":"ranganp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prasanna Ranganathan","active":true,"timeZone":"Etc/UTC"},"created":"2017-09-04T17:39:17.230+0000","updated":"2017-09-04T17:39:17.230+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/comment/16152855","id":"16152855","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"body":"{quote}\n+1 to adding warning in the log. Further to it, I feel we should go ahead and recommend auto.offset.reset=earliest option for topics with cleanup.policy=delete in the documentation for reliable at-least-once spout processing.\n{quote}\nI agree. It seems easiest to base on the changes in https://github.com/apache/storm/pull/2249 IMO.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"created":"2017-09-04T18:31:51.502+0000","updated":"2017-09-04T18:31:51.502+0000"}],"maxResults":12,"total":12,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-2546/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":11,"worklogs":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/51584","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"GitHub user srdo opened a pull request:\n\n    https://github.com/apache/storm/pull/2307\n\n    STORM-2546: Fix storm-kafka-client spout getting stuck when retriable offsets were deleted from the Kafka log due to topic compaction\n\n    This requires the changes in https://github.com/apache/storm/pull/2156 for correctness, please ignore the first commit.\n    \n    Here's the idea behind these changes https://issues.apache.org/jira/browse/STORM-2546?focusedCommentId=16151172&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16151172\n    \n    KafkaSpoutCommitTest was renamed, so the first test in KafkaSpoutLogCompactionSupportTest was already there and can be skipped.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/srdo/storm STORM-2546\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/storm/pull/2307.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #2307\n    \n----\ncommit 5d13d3a1142d808107e630479514a922904f2187\nAuthor: Stig Rohde Døssing <stigdoessing@gmail.com>\nDate:   2017-06-10T17:38:22Z\n\n    STORM-2549: Fix broken enforcement mechanism for maxUncommittedOffsets in storm-kafka-client spout\n\ncommit 19a6c4e8824bbf1c98d36e4073e68631ece9047b\nAuthor: Stig Rohde Døssing <srdo@apache.org>\nDate:   2017-09-02T21:50:33Z\n\n    STORM-2546: Fix storm-kafka-client spout getting stuck when retriable tuples were deleted from the Kafka log due to topic compaction\n\n----\n","created":"2017-09-02T22:09:12.556+0000","updated":"2017-09-02T22:09:12.556+0000","started":"2017-09-02T22:09:12.552+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"51584","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/54499","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2307\n  \n    @askprasanna I've made the spout default to an auto offset reset policy of \"earliest\" when the user requests the at-least-once processing guarantee. I think this addresses your request here https://issues.apache.org/jira/browse/STORM-2546?focusedCommentId=16152824&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16152824.\n","created":"2017-10-14T09:48:27.442+0000","updated":"2017-10-14T09:48:27.442+0000","started":"2017-10-14T09:48:27.438+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"54499","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/55822","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user askprasanna commented on the issue:\n\n    https://github.com/apache/storm/pull/2307\n  \n    @srdo Awesome. Thanks for following up.\n","created":"2017-10-30T08:35:42.241+0000","updated":"2017-10-30T08:35:42.241+0000","started":"2017-10-30T08:35:42.240+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"55822","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/57108","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user HeartSaVioR commented on the issue:\n\n    https://github.com/apache/storm/pull/2307\n  \n    @srdo \r\n    I found same suggestion as my 2 cents is already discussed from JIRA issue. Is it already addressed?\n","created":"2017-11-13T03:00:35.064+0000","updated":"2017-11-13T03:00:35.064+0000","started":"2017-11-13T03:00:35.062+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"57108","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/57263","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2307\n  \n    @HeartSaVioR Yes, you are right. We set the auto offset reset policy to `earliest` by default when `AT_LEAST_ONCE` is picked if the user hasn't explicitly set anything else. \r\n    \r\n    The misconfiguration I'm most worried about is where users set `AT_LEAST_ONCE`, and then forget to set the auto offset reset policy which defaults to `latest`, because it's an easy mistake to make if you're not already very familiar with Kafka's options. The other weird configurations (`AT_LEAST_ONCE`+`latest`, `AT_MOST_ONCE`+`earliest`) have to be explicitly chosen by the user.\r\n    \r\n    I can't think of a reason why users would want to use those configurations, but I thought it might be better not to prevent the user from using those settings if they really want to, because there might be a use case I'm not seeing.\r\n    \r\n    I'm happy to add in checks and error messages (or maybe even throwing exceptions) when using those configurations, if you think it makes sense?\n","created":"2017-11-13T19:13:13.637+0000","updated":"2017-11-13T19:13:13.637+0000","started":"2017-11-13T19:13:13.636+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"57263","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/57299","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user HeartSaVioR commented on the issue:\n\n    https://github.com/apache/storm/pull/2307\n  \n    @srdo \r\n    Sure. Makes sense. Let's add a warning message.\n","created":"2017-11-13T22:23:36.853+0000","updated":"2017-11-13T22:23:36.853+0000","started":"2017-11-13T22:23:36.852+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"57299","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/57472","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2307\n  \n    @HeartSaVioR Added the warning. Will squash and merge if the warning looks good.\n","created":"2017-11-14T21:58:17.125+0000","updated":"2017-11-14T21:58:17.125+0000","started":"2017-11-14T21:58:17.124+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"57472","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/57505","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user asfgit closed the pull request at:\n\n    https://github.com/apache/storm/pull/2307\n","created":"2017-11-15T07:22:19.323+0000","updated":"2017-11-15T07:22:19.323+0000","started":"2017-11-15T07:22:19.322+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"57505","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/57506","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2307\n  \n    @HeartSaVioR Merged to master. Yes, I'll put up a 1.x version tonight.\n","created":"2017-11-15T07:22:57.153+0000","updated":"2017-11-15T07:22:57.153+0000","started":"2017-11-15T07:22:57.153+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"57506","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/57552","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"GitHub user srdo opened a pull request:\n\n    https://github.com/apache/storm/pull/2423\n\n    STORM-2546: Fix storm-kafka-client spout stuck due to topic compaction (1.x)\n\n    1.x version of https://github.com/apache/storm/pull/2307.\r\n    \r\n    All changes are in the last commit, will squash when reviewed.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/srdo/storm STORM-2546-1.x\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/storm/pull/2423.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #2423\n    \n----\ncommit 9a66732a4649ed8622410616b13a9584ed38e11e\nAuthor: Stig Rohde Døssing <srdo@apache.org>\nDate:   2017-09-02T21:50:33Z\n\n    STORM-2546: Fix storm-kafka-client spout getting stuck when retriable offsets were deleted from the Kafka log due to topic compaction\n\ncommit 85e32f946a17bdfdb8e975d9724492da59327f78\nAuthor: Stig Rohde Døssing <srdo@apache.org>\nDate:   2017-11-15T17:31:20Z\n\n    Backport\n\n----\n","created":"2017-11-15T17:54:06.182+0000","updated":"2017-11-15T17:54:06.182+0000","started":"2017-11-15T17:54:06.181+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"57552","issueId":"13078239"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078239/worklog/57585","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user asfgit closed the pull request at:\n\n    https://github.com/apache/storm/pull/2423\n","created":"2017-11-16T06:13:54.280+0000","updated":"2017-11-16T06:13:54.280+0000","started":"2017-11-16T06:13:54.279+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"57585","issueId":"13078239"}]},"customfield_12311820":"0|i3g0wv:"}}