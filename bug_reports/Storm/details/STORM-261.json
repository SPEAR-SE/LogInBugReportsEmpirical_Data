{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12702963","self":"https://issues.apache.org/jira/rest/api/2/issue/12702963","key":"STORM-261","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12314820","id":"12314820","key":"STORM","name":"Apache Storm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12314820&avatarId=21667","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12314820&avatarId=21667","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12314820&avatarId=21667","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12314820&avatarId=21667"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/13260","id":"13260","description":"Apache Storm Related","name":"Storm"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-03-22T22:18:27.834+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Mar 24 19:05:09 UTC 2014","customfield_12312320":null,"customfield_12310222":null,"customfield_12310420":"381300","customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-261/watchers","watchCount":5,"isWatching":false},"created":"2014-03-21T20:23:40.990+0000","priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326452","id":"12326452","name":"0.9.2-incubating","archived":false,"released":true,"releaseDate":"2014-06-25"}],"customfield_12312339":null,"issuelinks":[],"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-10-09T00:23:27.877+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12327950","id":"12327950","name":"storm-core","description":"Core storm daemons and APIs including trident"}],"timeoriginalestimate":null,"description":"I know this is a bit far fetched.\n\nIf for some reason a supervisor dies and does not come back up again, dead HDD for example, but the workers remain up, and the scheduler decides to move the worker to a new host, a rebalance for instance, the old workers will never go away.  Ideally the worker should know that it is not running in the correct place any more and die instead of waiting for the supervisor to kill it.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12312024":null,"attachment":[],"customfield_12312340":null,"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"381576","customfield_12312823":null,"summary":"Workers should commit suicide if not scheduled any more.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"subtasks":[],"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"customfield_12310291":null,"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13944258","id":"13944258","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jmlogan","name":"jmlogan","key":"jmlogan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jon logan","active":true,"timeZone":"America/New_York"},"body":"All storm services are designed to be fail fast and should be continuously managed by something like supervosord. The reason workers don't self destruct is because you would then depend on the supervisor being alive. Once scheduled, workers can survive a supervisor death and restart. \n\nThis would require heartbeats from the supervisors and closing if missing or require workers to communicate with zookeeper looking for reassignments. \n\nFurthermore a worker cannot easily kill itself. Currently workers are killed externally through a kill 9 from the supervisor. This absolves any risk of non jvm termination due to thread leaks or such. \n\n\nI believe when the supervisor is restarted it should try to kill any rescheduled tasks on that machine while it was offline. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jmlogan","name":"jmlogan","key":"jmlogan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jon logan","active":true,"timeZone":"America/New_York"},"created":"2014-03-22T22:18:27.834+0000","updated":"2014-03-22T22:18:27.834+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13945163","id":"13945163","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dagit","name":"dagit","key":"dagit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dagit&avatarId=25742","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dagit&avatarId=25742","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dagit&avatarId=25742","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dagit&avatarId=25742"},"displayName":"Derek Dagit","active":true,"timeZone":"America/Chicago"},"body":"This issue was created to consider the case when the supervisor is unable to be restarted, yet its workers are still running.  If there is nothing left to supervise the workers, then a worker will go on participating in the topology (communicating with other legitimate workers) without knowing it should stop.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dagit","name":"dagit","key":"dagit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dagit&avatarId=25742","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dagit&avatarId=25742","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dagit&avatarId=25742","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dagit&avatarId=25742"},"displayName":"Derek Dagit","active":true,"timeZone":"America/Chicago"},"created":"2014-03-24T14:35:23.202+0000","updated":"2014-03-24T14:35:23.202+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13945190","id":"13945190","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jmlogan","name":"jmlogan","key":"jmlogan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jon logan","active":true,"timeZone":"America/New_York"},"body":"I haven't actually verified this, but when it's scheduled out of the topology, shouldn't it stop receiving new data? In that case, the only data it outputs would be older buffered data, or if there was a thread (or similar)-type tick outputting tuples. Otherwise, it should just be sitting there, wasting resources, but not outputting data...I think?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jmlogan","name":"jmlogan","key":"jmlogan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jon logan","active":true,"timeZone":"America/New_York"},"created":"2014-03-24T15:01:26.783+0000","updated":"2014-03-24T15:01:26.783+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13945250","id":"13945250","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=marz","name":"marz","key":"marz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nathan Marz","active":true,"timeZone":"Etc/UTC"},"body":"I'm +1 on this. This kind of redundancy can only be beneficial. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=marz","name":"marz","key":"marz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nathan Marz","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-24T15:49:47.678+0000","updated":"2014-03-24T15:49:47.678+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13945260","id":"13945260","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jmlogan","name":"jmlogan","key":"jmlogan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jon logan","active":true,"timeZone":"America/New_York"},"body":"I'm just not sure how this can be accomplished without breaking the idea that workers can survive a supervisor death/restart (short of having every worker heartbeat to zookeeper).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jmlogan","name":"jmlogan","key":"jmlogan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jon logan","active":true,"timeZone":"America/New_York"},"created":"2014-03-24T15:53:32.182+0000","updated":"2014-03-24T15:53:32.182+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13945265","id":"13945265","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=marz","name":"marz","key":"marz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nathan Marz","active":true,"timeZone":"Etc/UTC"},"body":"It doesn't suicide if the supervisor is down, but if Nimbus reassigns that worker. In that case the supervisor would just kill the worker immediately on startup anyway.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=marz","name":"marz","key":"marz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nathan Marz","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-24T15:56:49.873+0000","updated":"2014-03-24T15:56:49.873+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13945272","id":"13945272","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jmlogan","name":"jmlogan","key":"jmlogan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jon logan","active":true,"timeZone":"America/New_York"},"body":"But isn't that the exact scenario this is supposed to address, when the supervisor is down, and cannot be started?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jmlogan","name":"jmlogan","key":"jmlogan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jon logan","active":true,"timeZone":"America/New_York"},"created":"2014-03-24T16:03:22.609+0000","updated":"2014-03-24T16:03:22.609+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13945321","id":"13945321","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=marz","name":"marz","key":"marz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nathan Marz","active":true,"timeZone":"Etc/UTC"},"body":"RIght now it doesn't do that. I'm saying that's the behavior resolving this issue would have.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=marz","name":"marz","key":"marz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nathan Marz","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-24T16:33:39.227+0000","updated":"2014-03-24T16:33:39.227+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12702963/comment/13945545","id":"13945545","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"body":"bq. I haven't actually verified this, but when it's scheduled out of the topology, shouldn't it stop receiving new data?\n\nWe ran into this issue while running some tests. Someone accidentally had brought up a supervisor on the same node as nimbus.  They took the supervisor back down, and then noticed that things were a bit out of whack.  There was a word count topology running completely on a node that wasn't a part of the cluster any more.  Out of curiosity I rebalanced the topology to see what would happen, and now there were two copies of the topology running.  Looking at the logs both appeared to be processing data. \n\nI marked this as minor because like [~jmlogan] stated before I didn't see much of a way this would cause problems in the real world.  Thinking about it further I can see some use cases where if a spout is left active it could be causing problems, like consuming data that is never fully processes, or by continuing to process data after the topology has been killed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"created":"2014-03-24T19:05:09.834+0000","updated":"2014-03-24T19:05:09.834+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-261/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1tp0v:"}}