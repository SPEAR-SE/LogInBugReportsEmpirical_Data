{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12720423","self":"https://issues.apache.org/jira/rest/api/2/issue/12720423","key":"STORM-346","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12314820","id":"12314820","key":"STORM","name":"Apache Storm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12314820&avatarId=21667","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12314820&avatarId=21667","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12314820&avatarId=21667","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12314820&avatarId=21667"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/13260","id":"13260","description":"Apache Storm Related","name":"Storm"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327123","id":"12327123","description":"security feature branch","name":"0.10.0","archived":false,"released":true,"releaseDate":"2015-11-05"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2014-06-24T17:02:16.787+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Aug 07 15:13:53 UTC 2014","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_4995898380_*|*_5_*:*_1_*:*_0","customfield_12310420":"398622","customfield_12312321":null,"resolutiondate":"2014-08-07T15:13:53.038+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-346/watchers","watchCount":8,"isWatching":false},"created":"2014-06-10T19:28:54.699+0000","priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["security"],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"customfield_12312339":null,"issuelinks":[],"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=parth.brahmbhatt","name":"parth.brahmbhatt","key":"parth.brahmbhatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Parth Brahmbhatt","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-10-09T00:49:29.738+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12327950","id":"12327950","name":"storm-core","description":"Core storm daemons and APIs including trident"}],"timeoriginalestimate":null,"description":"Oozie has the ability to fetch delegation tokens on behalf of other users by running as a super user that can become a proxy user for almost anyone else.\n\nWe should build one or more classes similar to AutoTGT that can fetch a delegation token for HDFS/HBase, renew the token if needed, and then once the token is about to permanently expire fetch a new one.\n\nAccording to some people I have talked with HBase may need to have a JIRA filed against it so that it can pick up a new delegation token without needing to restart the process.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12312024":null,"attachment":[],"customfield_12312340":null,"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"398747","customfield_12312823":null,"summary":"(Security) Oozie style delegation tokens for HDFS","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"subtasks":[],"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"customfield_12310291":null,"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14042369","id":"14042369","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=parth.brahmbhatt","name":"parth.brahmbhatt","key":"parth.brahmbhatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Parth Brahmbhatt","active":true,"timeZone":"America/Los_Angeles"},"body":"I have started to work on this but after looking through the security code I am not clear about the issue completely. Currently we have some code in AutoTGT that attempts auto login to hadoop if hadoop is in the classPath. Is this task asking for implementation to ensure we check if HDFS and HBASE is in class path and if yes, fetch delegation tokens and store it? Shouldn't this be part of HDFS and HBASE bolts, where the actual interactions would happen? Or this issue is just for creating a helper so clients that use HDFS and HBASE can get delegation token using this helper?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=parth.brahmbhatt","name":"parth.brahmbhatt","key":"parth.brahmbhatt","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Parth Brahmbhatt","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-06-24T17:02:16.787+0000","updated":"2014-06-24T17:02:16.787+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14042671","id":"14042671","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"body":"Sorry I did throw up a design or explain this very well.\n\nIAutoCredentials right now only has APIs to fetch credentials on the client side, and then use them on the topology side to populate a Subject.  For this we need to extend it so that it could also fetch the credentials on nimbus when the topology is submitted (if it is configured to do so).\n\nSo for an HDFS delegation token I would expect the code to do something like the following.\n\n# The client would submit a topology with a config set that says it wants hdfs delegation tokens. i.e. topology.autohdfs.namenodes=[\"hdfs://foo.com/\",\"hdfs://bar.com\"]\n# Nimbus before if finished submitting the topology it runs the configured IAutoCredentials instances (Probably need a new config for this, or extend ICredentialsRenewer to also take the topology config).\n## AutoHDFS would look at the config and become a proxy user for the different NameNodes to fetch the delegation token and put it into the credentials.\n# On the topology side AutoHDFS would take the delegation token and populate it into the UGI.\n\nPeriodically Nimbus would run AutoHDFS as an ICredentialsRenewer.  If the token needs to be renewed it would connect to the name node and renew it.  If the renewal period is about to expire it would fetch a new delegation token, and replace the old one in the credentials map.\n\nFor HBase it should be similar, but replace hdfs with hbase.  I have heard though that for this to work properly hbase may need a fix too.  Some people I have talked to have indicated that the RPC layer of HBase caches the delegation token, so even if it is updated in the UGI it will not be used to make new connections to HBase, but I don't know for sure.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"created":"2014-06-24T20:56:20.630+0000","updated":"2014-06-24T20:56:20.630+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14059194","id":"14059194","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"GitHub user Parth-Brahmbhatt opened a pull request:\n\n    https://github.com/apache/incubator-storm/pull/189\n\n    STORM-346: added AutoHDFS class that will get hdfs delegation tokens on behalf of users, push it to workers and renew the delegation tokens automatically.\n\n    \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/Parth-Brahmbhatt/incubator-storm STORM-346\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/incubator-storm/pull/189.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #189\n    \n----\ncommit fe5f41aa8332700d3f98422cb7d986fc47289bcd\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-05-21T16:03:11Z\n\n    STORM-216: Added Authentication and Authorization.\n    \n    This is an upmerged version of https://github.com/yahoo/incubator-storm/tree/security\n\ncommit ce6e5d26384d7f5d831b35e4eff126fe214981d1\nAuthor: Derek Dagit <derekd@yahoo-inc.com>\nDate:   2014-05-22T18:34:23Z\n\n    rename test for consistent capitalization\n\ncommit 698bb9c9788b82d4127d861fb3ecf06a06b683c2\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-05-28T13:45:22Z\n\n    Merge branch 'master' into security\n\ncommit 6592b8209c49a98db15b3d6d228f488aa6c2e623\nAuthor: Kishor Patil <patik523@yahoo.com>\nDate:   2014-06-09T15:42:39Z\n\n    Add missing ACLs to error znodes and remove auto vivification of error znodes\n\ncommit cf2e8b7ee06b455a90bd4b3bfd53facef1369612\nAuthor: Sriharsha Chintalapani <mail@harsha.io>\nDate:   2014-06-10T22:01:33Z\n\n    Storm 344. (Security) nimbus renew-credentials not calling ICredentialsRenewer.renew\n\ncommit 0a98bee214b46ed20b566a9b49c3eca2895f9fd5\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-11T16:07:59Z\n\n    Merge branch 'master' into security-upmerge\n    \n    Conflicts:\n    \t.gitignore\n    \tstorm-core/src/clj/backtype/storm/daemon/drpc.clj\n    \tstorm-core/src/clj/backtype/storm/daemon/executor.clj\n    \tstorm-core/src/clj/backtype/storm/daemon/logviewer.clj\n    \tstorm-core/src/clj/backtype/storm/daemon/worker.clj\n    \tstorm-core/src/clj/backtype/storm/timer.clj\n    \tstorm-core/src/clj/backtype/storm/ui/core.clj\n    \tstorm-core/src/clj/backtype/storm/ui/helpers.clj\n    \tstorm-core/src/clj/backtype/storm/util.clj\n    \tstorm-core/src/jvm/backtype/storm/Config.java\n    \tstorm-core/src/jvm/backtype/storm/utils/Utils.java\n\ncommit 118b9221b492ed8b91e6633c3cfb748bc1b82790\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-11T16:10:04Z\n\n    Merge branch 'master' into security\n\ncommit 2131a0aeb9074b2c83a09d7515ff8e8ae86f6eaf\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-12T15:58:13Z\n\n    Added back in the user to the web ui.\n\ncommit 41615b3c4b174077ac1c729af4aef32e5b79d3c5\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-12T15:59:46Z\n\n    Merge branch 'master' into security\n\ncommit bc91ed88d77e392f38c406d143e7ac37bc634564\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-12T16:01:35Z\n\n    Added license to UI template.\n\ncommit a762f1c5f99a7a9e77038399f0f14ae03b1414c7\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-12T17:48:51Z\n\n    Merge branch 'STORM-344' of https://github.com/harshach/incubator-storm into STORM-344\n    \n    STORM-344: (Security) nimbus renew-credentials not calling ICredentialsRenewer.renew\n\ncommit 92e3a5742374a3a7c3aae20cbeda32ce7b033526\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-12T21:09:57Z\n\n    Merge branch 'master' into security-upmerge\n    \n    Conflicts:\n    \tstorm-core/src/clj/backtype/storm/LocalCluster.clj\n    \tstorm-core/src/clj/backtype/storm/cluster.clj\n    \tstorm-core/src/clj/backtype/storm/config.clj\n    \tstorm-core/src/clj/backtype/storm/daemon/drpc.clj\n    \tstorm-core/src/clj/backtype/storm/testing.clj\n    \tstorm-core/src/clj/backtype/storm/testing4j.clj\n    \tstorm-core/src/clj/backtype/storm/thrift.clj\n    \tstorm-core/src/clj/backtype/storm/ui/core.clj\n    \tstorm-core/src/clj/backtype/storm/util.clj\n    \tstorm-core/src/clj/backtype/storm/zookeeper.clj\n\ncommit ab7784e49d251ca4da967c6ec6bc340cc7f940aa\nAuthor: Kishor Patil <patik523@yahoo.com>\nDate:   2014-06-17T15:19:00Z\n\n    Force free a slot in bad-state\n\ncommit d1ba4fc4acdadd5e5e138395bdc5892dfdb88bff\nAuthor: Derek Dagit <derekd@yahoo-inc.com>\nDate:   2014-06-17T15:56:51Z\n\n    Do not clean up user file when rmr is unsuccessful\n\ncommit 87cdbf5fdf5bfb49b983604542283f05123d0d51\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-17T18:32:18Z\n\n    Merge branch 'STORM-357' of https://github.com/d2r/incubator-storm into STORM-357\n    \n    STORM-357: Cleans workers-users file only when rmr is successful\n\ncommit ea946d04dcb6df8e65dbf16500a361eaaba13432\nAuthor: Kishor Patil <kpatil@yahoo-inc.com>\nDate:   2014-06-18T23:58:33Z\n\n    Show node details on errors for STORM-360 on security\n\ncommit 79089ad0da80e38eb36b7ea91be8b43795dc4efb\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-19T21:04:02Z\n\n    Merge tag 'v0.9.2-incubating' into security\n    \n    [maven-release-plugin]  copy for tag v0.9.2-incubating\n    \n    Conflicts:\n    \tstorm-core/pom.xml\n\ncommit f20df7d52d52abc9bc03a0cf45388241927cff5a\nAuthor: Kishor Patil <kpatil@yahoo-inc.com>\nDate:   2014-06-20T20:35:16Z\n\n    Fixing coding style and component template\n\ncommit c545b9d638067c0ae4528e16f14e67c56e0dd47e\nAuthor: Kishor Patil <kpatil@yahoo-inc.com>\nDate:   2014-06-20T23:01:00Z\n\n    Fix nimbus use of doto\n\ncommit d7c1d1d0a909079a370ed35aaac91668eef33a22\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-23T14:31:41Z\n\n    Merge branch 'node-on-error-security' of https://github.com/kishorvpatil/incubator-storm into STORM-360-security\n    \n    STORM-360: Add node details for Error Topology and Component pages on security\n\ncommit 65aee65af54dd29434af8f5ee403233b597561b6\nAuthor: Robert (Bobby) Evans <bobby@apache.org>\nDate:   2014-06-23T15:18:38Z\n\n    Merge branch 'master' into security\n    \n    Conflicts:\n    \tstorm-core/src/clj/backtype/storm/cluster.clj\n    \tstorm-core/src/clj/backtype/storm/ui/core.clj\n    \tstorm-core/test/clj/backtype/storm/cluster_test.clj\n\ncommit 28c168fd7d0272f88d586f6f572eab937b874f22\nAuthor: Kishor Patil <kpatil@yahoo-inc.com>\nDate:   2014-06-24T19:12:56Z\n\n    Add check for empty table before sorting on security\n\ncommit 3c6930dfe4447b6077916b9f9a07b062141b5305\nAuthor: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>\nDate:   2014-07-07T18:36:49Z\n\n    AutoHDFS for getting HDFS delegation token and auto renew.\n\ncommit 00e80e9a132764d4b73737d2f7a52282e5247856\nAuthor: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>\nDate:   2014-07-07T18:43:59Z\n\n    Merge remote-tracking branch 'upstream/security' into security\n\ncommit e04c37356c96d9851c00542c739d053e4bf36481\nAuthor: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>\nDate:   2014-07-07T18:49:42Z\n\n    Revert \"AutoHDFS for getting HDFS delegation token and auto renew.\"\n    \n    This reverts commit 3c6930dfe4447b6077916b9f9a07b062141b5305.\n\ncommit 1094762bf9c3ae339500a3a4500d742367c33e63\nAuthor: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>\nDate:   2014-07-11T18:50:07Z\n\n    STORM-346: added AutoHDFS class that will get hdfs delegation tokens on behalf of users, push it to workers and renew the delegation tokens automatically.\n\n----\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-11T18:51:43.946+0000","updated":"2014-07-11T18:51:43.946+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14059220","id":"14059220","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user ptgoetz commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/189#issuecomment-48771543\n  \n    @Parth-Brahmbhatt Is this pull request meant for the security branch? Currently it's pointing to master.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-11T19:17:39.275+0000","updated":"2014-07-11T19:17:39.275+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14059224","id":"14059224","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt closed the pull request at:\n\n    https://github.com/apache/incubator-storm/pull/189\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-11T19:19:48.032+0000","updated":"2014-07-11T19:19:48.032+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14059228","id":"14059228","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"GitHub user Parth-Brahmbhatt opened a pull request:\n\n    https://github.com/apache/incubator-storm/pull/190\n\n    STORM-346: added AutoHDFS class that will get hdfs delegation tokens on behalf of users, push it to workers and renew the delegation tokens automatically.\n\n    \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/Parth-Brahmbhatt/incubator-storm STORM-346\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/incubator-storm/pull/190.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #190\n    \n----\ncommit 3c6930dfe4447b6077916b9f9a07b062141b5305\nAuthor: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>\nDate:   2014-07-07T18:36:49Z\n\n    AutoHDFS for getting HDFS delegation token and auto renew.\n\ncommit 00e80e9a132764d4b73737d2f7a52282e5247856\nAuthor: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>\nDate:   2014-07-07T18:43:59Z\n\n    Merge remote-tracking branch 'upstream/security' into security\n\ncommit e04c37356c96d9851c00542c739d053e4bf36481\nAuthor: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>\nDate:   2014-07-07T18:49:42Z\n\n    Revert \"AutoHDFS for getting HDFS delegation token and auto renew.\"\n    \n    This reverts commit 3c6930dfe4447b6077916b9f9a07b062141b5305.\n\ncommit 1094762bf9c3ae339500a3a4500d742367c33e63\nAuthor: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>\nDate:   2014-07-11T18:50:07Z\n\n    STORM-346: added AutoHDFS class that will get hdfs delegation tokens on behalf of users, push it to workers and renew the delegation tokens automatically.\n\n----\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-11T19:21:17.609+0000","updated":"2014-07-11T19:21:17.609+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14065594","id":"14065594","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15085803\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,298 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +    private static final String CONF_KEYTAB_KEY = \"keytab\";\n    +    private static final String CONF_USER_KEY = \"user\";\n    +\n    +    private Map conf;\n    +\n    +    public void prepare(Map conf) {\n    +        this.conf = conf;\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private Object getConfiguration() {\n    --- End diff --\n    \n    I think I would rather have hdfs-site.xml be configured properly rather then trying to have storm know enough about Hadoop configuration to make it work.  Configuring Hadoop is a real pain, and I think this will just end up being a losing battle.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-17T21:36:23.399+0000","updated":"2014-07-17T21:36:23.399+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14065605","id":"14065605","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15086078\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,298 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +    private static final String CONF_KEYTAB_KEY = \"keytab\";\n    +    private static final String CONF_USER_KEY = \"user\";\n    +\n    +    private Map conf;\n    +\n    +    public void prepare(Map conf) {\n    +        this.conf = conf;\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private Object getConfiguration() {\n    +        try {\n    +            final String hdfsUser = (String) conf.get(Config.HDFS_USER);\n    +            final String hdfsUserKeyTab = (String) conf.get(Config.HDFS_USER_KEYTAB);\n    +\n    +            /**\n    +             *  Configuration configuration = new Configuration();\n    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +             *  configuration.set(CONF_USER_KEY, hdfsUser);\n    +             */\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            Object configuration = configurationClass.newInstance();\n    +\n    +            Method setMethod = configurationClass.getMethod(\"set\", String.class, String.class);\n    +            setMethod.invoke(configuration, CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +            setMethod.invoke(configuration, CONF_USER_KEY, hdfsUser);\n    +            /**\n    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml\n    +             * and core-site.xml in the class path which should set these configuration.\n    +             * setMethod.invoke(configuration, \"hadoop.security.authentication\", \"KERBEROS\");\n    +             * setMethod.invoke(configuration,\"dfs.namenode.kerberos.principal\",\n    +             *                                \"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +             * setMethod.invoke(configuration, \"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +             */\n    +\n    +            setMethod.invoke(configuration, \"hadoop.security.authentication\", \"KERBEROS\");\n    +            setMethod.invoke(configuration, \"dfs.namenode.kerberos.principal\",\"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +            setMethod.invoke(configuration, \"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +\n    +            //UserGroupInformation.setConfiguration(configuration);\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            Method setConfigurationMethod = ugiClass.getMethod(\"setConfiguration\", configurationClass);\n    +            setConfigurationMethod.invoke(null, configuration);\n    +            return configuration;\n    +        }  catch (Exception e) {\n    +            throw new RuntimeException(e);\n    +        }\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private void login(Object configuration) {\n    +        try {\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            final Class securityUtilClass = Class.forName(\"org.apache.hadoop.security.SecurityUtil\");\n    +            Method loginMethod = securityUtilClass.getMethod(\"login\", configurationClass, String.class, String.class);\n    +            loginMethod.invoke(null, configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);\n    +        } catch (Exception e) {\n    +           throw new RuntimeException(\"Failed to login to hdfs .\", e);\n    +        }\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken() throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    +             *  Configuration configuration = new Configuration();\n    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +             *  configuration.set(CONF_USER_KEY, hdfsUser);\n    +             *  UserGroupInformation.setConfiguration(configuration);\n    +             *  if(UserGroupInformation.isSecurityEnabled) {\n    +             *      SecurityUtil.login(configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);\n    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +             *      Credentials credential= proxyUser.getCredentials();\n    +             *      fs.addDelegationToken(hdfsUser, credential);\n    +             * }\n    +             * and then return the credential object as a bytearray.\n    +             */\n    +            Object configuration = getConfiguration();\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod(\"isSecurityEnabled\");\n    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);\n    +            if(isSecurityEnabled) {\n    +                login(configuration);\n    +\n    +                final URI nameNodeURI = URI.create((String) conf.get(Config.HDFS_NAMENODE_URL));\n    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);\n    +                final String hdfsUser = (String) conf.get(Config.HDFS_USER);\n    +\n    +                Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +\n    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +                Class fileSystemClass = Class.forName(\"org.apache.hadoop.fs.FileSystem\");\n    +                Method getMethod = fileSystemClass.getMethod(\"get\", URI.class, configurationClass, String.class);\n    +                Object fileSystem = getMethod.invoke(null, nameNodeURI, configuration, topologySubmitterUser);\n    +\n    +                //UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +                Method getCurrentUserMethod = ugiClass.getMethod(\"getCurrentUser\");\n    +                final Object ugi = getCurrentUserMethod.invoke(null);\n    +\n    +                //UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +                Method createProxyUserMethod = ugiClass.getMethod(\"createProxyUser\", String.class, ugiClass);\n    +                Object proxyUGI = createProxyUserMethod.invoke(null, topologySubmitterUser, ugi);\n    +\n    +                //Credentials credential= proxyUser.getCredentials();\n    +                Method getCredentialsMethod = ugiClass.getMethod(\"getCredentials\");\n    +                Object credentials = getCredentialsMethod.invoke(proxyUGI);\n    +\n    +                //fs.addDelegationToken(hdfsUser, credential);\n    +                Class credentialClass = Class.forName(\"org.apache.hadoop.security.Credentials\");\n    +                Method addDelegationTokensMethod = fileSystemClass.getMethod(\"addDelegationTokens\", String.class,\n    +                        credentialClass);\n    +                addDelegationTokensMethod.invoke(fileSystem, hdfsUser, credentials);\n    +\n    +\n    +                ByteArrayOutputStream bao = new ByteArrayOutputStream();\n    +                ObjectOutputStream out = new ObjectOutputStream(bao);\n    +                Method writeMethod = credentialClass.getMethod(\"write\", DataOutput.class);\n    +                writeMethod.invoke(credentials, out);\n    +                out.flush();\n    +                out.close();\n    +\n    +                LOG.info(bao.toString());\n    +                return bao.toByteArray();\n    +            } else {\n    +                throw new RuntimeException(\"Security is not enabled for HDFS\");\n    +            }\n    +        } catch (Exception ex) {\n    +            throw new RuntimeException(\"Failed to get delegation tokens.\" , ex);\n    +        }\n    +    }\n    +\n    +    @Override\n    +    public void populateCredentials(Map<String, String> credentials) {\n    --- End diff --\n    \n    Populate credentials is called on the gateway by the end user.  As such there is no reason to use a proxyUser get a delegation token, because I can just get it.  In fact the proxyUser should fail, because regular users are not authorized to act as proxy users.\n    \n    The idea was to change some of how nimbus used an ICredentialsRenewer, or have a different interface so that nimbus can ask AutoHDFS to populate the credentials at the very beginning. I'll try to explain better in the more general comments.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-17T21:41:04.422+0000","updated":"2014-07-17T21:41:04.422+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14065634","id":"14065634","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-49372008\n  \n    I really guess I didn't explain things very well in the JIRA.  So the current way IAutoCredentials and ICredentialsRenewer work with nimbus are.\n    \n    IAutoCredentials will run on the gateway as the topology user.  It gets handed a map that it can place any credentials in needs into.\n    Those credentials are uploaded to nimbus, which places them in zookeeper.\n    Periodically nimbus will download the serialized credentials and hand them to ICredentialsRenewer which will run as the nimbus user. If it modifies the the credentials they are pushed to zookeeper.\n    \n    At the same time the worker running as the topology user will download the credentials from zookeeper and pass them to IAutoCredentials to the passed in Subject with them. This repeats any time the credentials change in zookeeper.\n    \n    \n    What I would like to see is something where IAutoCredentials has an option of not running on the gateway, or it is a noop on the gateway.  Instead after the credentials are submitted to nimbus, something similar to, or an extended version of ICredentialsRenewer would run as the nimbus user and get the credentials on behalf of the topology user.\n    \n    The rest of the process would stay the same, except if a token is about to expire completely.  In that case it would fetch a brand new token.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-17T21:55:15.726+0000","updated":"2014-07-17T21:55:15.726+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14066672","id":"14066672","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15125867\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,298 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +    private static final String CONF_KEYTAB_KEY = \"keytab\";\n    +    private static final String CONF_USER_KEY = \"user\";\n    +\n    +    private Map conf;\n    +\n    +    public void prepare(Map conf) {\n    +        this.conf = conf;\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private Object getConfiguration() {\n    +        try {\n    +            final String hdfsUser = (String) conf.get(Config.HDFS_USER);\n    +            final String hdfsUserKeyTab = (String) conf.get(Config.HDFS_USER_KEYTAB);\n    +\n    +            /**\n    +             *  Configuration configuration = new Configuration();\n    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +             *  configuration.set(CONF_USER_KEY, hdfsUser);\n    +             */\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            Object configuration = configurationClass.newInstance();\n    +\n    +            Method setMethod = configurationClass.getMethod(\"set\", String.class, String.class);\n    +            setMethod.invoke(configuration, CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +            setMethod.invoke(configuration, CONF_USER_KEY, hdfsUser);\n    +            /**\n    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml\n    +             * and core-site.xml in the class path which should set these configuration.\n    +             * setMethod.invoke(configuration, \"hadoop.security.authentication\", \"KERBEROS\");\n    +             * setMethod.invoke(configuration,\"dfs.namenode.kerberos.principal\",\n    +             *                                \"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +             * setMethod.invoke(configuration, \"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +             */\n    +\n    +            setMethod.invoke(configuration, \"hadoop.security.authentication\", \"KERBEROS\");\n    +            setMethod.invoke(configuration, \"dfs.namenode.kerberos.principal\",\"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +            setMethod.invoke(configuration, \"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +\n    +            //UserGroupInformation.setConfiguration(configuration);\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            Method setConfigurationMethod = ugiClass.getMethod(\"setConfiguration\", configurationClass);\n    +            setConfigurationMethod.invoke(null, configuration);\n    +            return configuration;\n    +        }  catch (Exception e) {\n    +            throw new RuntimeException(e);\n    +        }\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private void login(Object configuration) {\n    +        try {\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            final Class securityUtilClass = Class.forName(\"org.apache.hadoop.security.SecurityUtil\");\n    +            Method loginMethod = securityUtilClass.getMethod(\"login\", configurationClass, String.class, String.class);\n    +            loginMethod.invoke(null, configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);\n    +        } catch (Exception e) {\n    +           throw new RuntimeException(\"Failed to login to hdfs .\", e);\n    +        }\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken() throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    +             *  Configuration configuration = new Configuration();\n    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +             *  configuration.set(CONF_USER_KEY, hdfsUser);\n    +             *  UserGroupInformation.setConfiguration(configuration);\n    +             *  if(UserGroupInformation.isSecurityEnabled) {\n    +             *      SecurityUtil.login(configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);\n    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +             *      Credentials credential= proxyUser.getCredentials();\n    +             *      fs.addDelegationToken(hdfsUser, credential);\n    +             * }\n    +             * and then return the credential object as a bytearray.\n    +             */\n    +            Object configuration = getConfiguration();\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod(\"isSecurityEnabled\");\n    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);\n    +            if(isSecurityEnabled) {\n    +                login(configuration);\n    +\n    +                final URI nameNodeURI = URI.create((String) conf.get(Config.HDFS_NAMENODE_URL));\n    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);\n    +                final String hdfsUser = (String) conf.get(Config.HDFS_USER);\n    +\n    +                Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +\n    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +                Class fileSystemClass = Class.forName(\"org.apache.hadoop.fs.FileSystem\");\n    +                Method getMethod = fileSystemClass.getMethod(\"get\", URI.class, configurationClass, String.class);\n    +                Object fileSystem = getMethod.invoke(null, nameNodeURI, configuration, topologySubmitterUser);\n    +\n    +                //UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +                Method getCurrentUserMethod = ugiClass.getMethod(\"getCurrentUser\");\n    +                final Object ugi = getCurrentUserMethod.invoke(null);\n    +\n    +                //UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +                Method createProxyUserMethod = ugiClass.getMethod(\"createProxyUser\", String.class, ugiClass);\n    +                Object proxyUGI = createProxyUserMethod.invoke(null, topologySubmitterUser, ugi);\n    +\n    +                //Credentials credential= proxyUser.getCredentials();\n    +                Method getCredentialsMethod = ugiClass.getMethod(\"getCredentials\");\n    +                Object credentials = getCredentialsMethod.invoke(proxyUGI);\n    +\n    +                //fs.addDelegationToken(hdfsUser, credential);\n    +                Class credentialClass = Class.forName(\"org.apache.hadoop.security.Credentials\");\n    +                Method addDelegationTokensMethod = fileSystemClass.getMethod(\"addDelegationTokens\", String.class,\n    +                        credentialClass);\n    +                addDelegationTokensMethod.invoke(fileSystem, hdfsUser, credentials);\n    +\n    +\n    +                ByteArrayOutputStream bao = new ByteArrayOutputStream();\n    +                ObjectOutputStream out = new ObjectOutputStream(bao);\n    +                Method writeMethod = credentialClass.getMethod(\"write\", DataOutput.class);\n    +                writeMethod.invoke(credentials, out);\n    +                out.flush();\n    +                out.close();\n    +\n    +                LOG.info(bao.toString());\n    +                return bao.toByteArray();\n    +            } else {\n    +                throw new RuntimeException(\"Security is not enabled for HDFS\");\n    +            }\n    +        } catch (Exception ex) {\n    +            throw new RuntimeException(\"Failed to get delegation tokens.\" , ex);\n    +        }\n    +    }\n    +\n    +    @Override\n    +    public void populateCredentials(Map<String, String> credentials) {\n    --- End diff --\n    \n    completely my bad, I did not intend to keep these config lines uncommented. I had to keep them uncommented for testing as my hdfs-site.xml settings were not being picked up by the code due to some class path issue. Removing them.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-18T18:19:06.498+0000","updated":"2014-07-18T18:19:06.498+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14066673","id":"14066673","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15125886\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,298 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +    private static final String CONF_KEYTAB_KEY = \"keytab\";\n    +    private static final String CONF_USER_KEY = \"user\";\n    +\n    +    private Map conf;\n    +\n    +    public void prepare(Map conf) {\n    +        this.conf = conf;\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private Object getConfiguration() {\n    --- End diff --\n    \n    completely my bad, I did not intend to keep these config lines uncommented. I had to keep them uncommented for testing as my hdfs-site.xml settings were not being picked up by the code due to some class path issue. Removing them.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-18T18:19:21.734+0000","updated":"2014-07-18T18:19:21.734+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14066678","id":"14066678","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15126024\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,298 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +    private static final String CONF_KEYTAB_KEY = \"keytab\";\n    +    private static final String CONF_USER_KEY = \"user\";\n    +\n    +    private Map conf;\n    +\n    +    public void prepare(Map conf) {\n    +        this.conf = conf;\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private Object getConfiguration() {\n    +        try {\n    +            final String hdfsUser = (String) conf.get(Config.HDFS_USER);\n    +            final String hdfsUserKeyTab = (String) conf.get(Config.HDFS_USER_KEYTAB);\n    +\n    +            /**\n    +             *  Configuration configuration = new Configuration();\n    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +             *  configuration.set(CONF_USER_KEY, hdfsUser);\n    +             */\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            Object configuration = configurationClass.newInstance();\n    +\n    +            Method setMethod = configurationClass.getMethod(\"set\", String.class, String.class);\n    +            setMethod.invoke(configuration, CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +            setMethod.invoke(configuration, CONF_USER_KEY, hdfsUser);\n    +            /**\n    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml\n    +             * and core-site.xml in the class path which should set these configuration.\n    +             * setMethod.invoke(configuration, \"hadoop.security.authentication\", \"KERBEROS\");\n    +             * setMethod.invoke(configuration,\"dfs.namenode.kerberos.principal\",\n    +             *                                \"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +             * setMethod.invoke(configuration, \"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +             */\n    +\n    +            setMethod.invoke(configuration, \"hadoop.security.authentication\", \"KERBEROS\");\n    +            setMethod.invoke(configuration, \"dfs.namenode.kerberos.principal\",\"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +            setMethod.invoke(configuration, \"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +\n    +            //UserGroupInformation.setConfiguration(configuration);\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            Method setConfigurationMethod = ugiClass.getMethod(\"setConfiguration\", configurationClass);\n    +            setConfigurationMethod.invoke(null, configuration);\n    +            return configuration;\n    +        }  catch (Exception e) {\n    +            throw new RuntimeException(e);\n    +        }\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private void login(Object configuration) {\n    +        try {\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            final Class securityUtilClass = Class.forName(\"org.apache.hadoop.security.SecurityUtil\");\n    +            Method loginMethod = securityUtilClass.getMethod(\"login\", configurationClass, String.class, String.class);\n    +            loginMethod.invoke(null, configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);\n    +        } catch (Exception e) {\n    +           throw new RuntimeException(\"Failed to login to hdfs .\", e);\n    +        }\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken() throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    +             *  Configuration configuration = new Configuration();\n    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);\n    +             *  configuration.set(CONF_USER_KEY, hdfsUser);\n    +             *  UserGroupInformation.setConfiguration(configuration);\n    +             *  if(UserGroupInformation.isSecurityEnabled) {\n    +             *      SecurityUtil.login(configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);\n    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +             *      Credentials credential= proxyUser.getCredentials();\n    +             *      fs.addDelegationToken(hdfsUser, credential);\n    +             * }\n    +             * and then return the credential object as a bytearray.\n    +             */\n    +            Object configuration = getConfiguration();\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod(\"isSecurityEnabled\");\n    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);\n    +            if(isSecurityEnabled) {\n    +                login(configuration);\n    +\n    +                final URI nameNodeURI = URI.create((String) conf.get(Config.HDFS_NAMENODE_URL));\n    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);\n    +                final String hdfsUser = (String) conf.get(Config.HDFS_USER);\n    +\n    +                Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +\n    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +                Class fileSystemClass = Class.forName(\"org.apache.hadoop.fs.FileSystem\");\n    +                Method getMethod = fileSystemClass.getMethod(\"get\", URI.class, configurationClass, String.class);\n    +                Object fileSystem = getMethod.invoke(null, nameNodeURI, configuration, topologySubmitterUser);\n    +\n    +                //UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +                Method getCurrentUserMethod = ugiClass.getMethod(\"getCurrentUser\");\n    +                final Object ugi = getCurrentUserMethod.invoke(null);\n    +\n    +                //UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +                Method createProxyUserMethod = ugiClass.getMethod(\"createProxyUser\", String.class, ugiClass);\n    +                Object proxyUGI = createProxyUserMethod.invoke(null, topologySubmitterUser, ugi);\n    +\n    +                //Credentials credential= proxyUser.getCredentials();\n    +                Method getCredentialsMethod = ugiClass.getMethod(\"getCredentials\");\n    +                Object credentials = getCredentialsMethod.invoke(proxyUGI);\n    +\n    +                //fs.addDelegationToken(hdfsUser, credential);\n    +                Class credentialClass = Class.forName(\"org.apache.hadoop.security.Credentials\");\n    +                Method addDelegationTokensMethod = fileSystemClass.getMethod(\"addDelegationTokens\", String.class,\n    +                        credentialClass);\n    +                addDelegationTokensMethod.invoke(fileSystem, hdfsUser, credentials);\n    +\n    +\n    +                ByteArrayOutputStream bao = new ByteArrayOutputStream();\n    +                ObjectOutputStream out = new ObjectOutputStream(bao);\n    +                Method writeMethod = credentialClass.getMethod(\"write\", DataOutput.class);\n    +                writeMethod.invoke(credentials, out);\n    +                out.flush();\n    +                out.close();\n    +\n    +                LOG.info(bao.toString());\n    +                return bao.toByteArray();\n    +            } else {\n    +                throw new RuntimeException(\"Security is not enabled for HDFS\");\n    +            }\n    +        } catch (Exception ex) {\n    +            throw new RuntimeException(\"Failed to get delegation tokens.\" , ex);\n    +        }\n    +    }\n    +\n    +    @Override\n    +    public void populateCredentials(Map<String, String> credentials) {\n    --- End diff --\n    \n    Sorry for missing this, I thought the server side implementation of submitTopology is invoking populatecreds so I assume that code will be executed by nimbus and thus ended up creating a proxyUser.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-18T18:21:59.118+0000","updated":"2014-07-18T18:21:59.118+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14066765","id":"14066765","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-49469937\n  \n    The simplest alternative seems to be no implementation for IAutoCredentials needed for AutoHDFS to work. In other words users will not  have to specify any class for \"topology.auto-credentials\" config for auto hdfs to work.\n    \n    User will specify AutoHDFS.java as \"nimbus.credential.renewers.classes\" and AutoHDFS will only implement ICredentialsRenewer. In the prepare phase of AutoHDFS.java, which should be called on nimbus startup, we can get the HDFS credentials.\n    \n    I have one clarifying question. The ICredentialsRenewer implementations seems to be loaded by reading \"nimbus.credential.renewers.classes\" config at startup by nimbus. If I understand correctly this means if we use ICredentialsRenewer the users who have a running nimbus and wants to use AutoHDFS will have to change the config and restart the nimbus. Is that acceptable? \n    \n    \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-18T19:24:17.664+0000","updated":"2014-07-18T19:24:17.664+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14066821","id":"14066821","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-49474731\n  \n    Here is what I have so far:\n    \n    User can specify AutoHDFS.java as \"nimbus.credential.renewers.classes\" and AutoHDFS will only implement ICredentialsRenewer. In the prepare phase of AutoHDFS.java, which should be called on nimbus startup, we can get the HDFS credentials. However, I don't think the topology submitter user will be available at that time so we will not be able to get the token on behalf of the user but only as nimbus which I feel is unacceptable. \n    \n    In order to actually get the credentials as topology submitter user, we either need a new Interface that will run on nimbus when a topology is submitted as part of submitTopologyWithOpts implementation or we can add getCredentialForUser(Map conf) method to ICredentialsRenewer interface and call that as part of submitTopologyWithOpts. I personally prefer not to pollute the ICredentialsRenewer interface. Let me know if you have better alternatives or prefer one over another.\n    \n    I have one last question. The ICredentialsRenewer implementations seems to be loaded by reading \"nimbus.credential.renewers.classes\" config at startup by nimbus. This means the users who have a running nimbus and wants to use AutoHDFS or any other implementation of ICredentialsRenewer will have to change the config and restart the nimbus. Is that acceptable? \n    \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-18T20:14:28.145+0000","updated":"2014-07-18T20:14:28.145+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14070491","id":"14070491","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-49766164\n  \n    I have added a new interface that will execute on nimbus during submitTopology operation so nimbus can get credentials on behalf of the user.  I also had to change the ICredentialsRenewer so we can pass the topology configuration to the renew methods in addition to the credentials map. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-22T16:47:47.983+0000","updated":"2014-07-22T16:47:47.983+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14073692","id":"14073692","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-50082444\n  \n    Added a unit test on nimbus to verify that the credentials will be populated as part of submit topology. I did not find any equivalent unit test for renewers so I added renewer test as well, I think I found a bug in nimbus's renew-credentials method. When old and new credentials do not match, the method calls \n    (.set-credentials! storm-cluster-state id new-creds) which seems to be missing a parameter topology-conf so I changed it to (.set-credentials! storm-cluster-state id new-creds topology-conf). \n    \n    The test passed after this change but please confirm that this is indeed a bug and let me know if you want to file a separate JIRA for it.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-24T21:42:55.532+0000","updated":"2014-07-24T21:42:55.532+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074642","id":"14074642","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-50180758\n  \n    Yes that does appear to be a bug. Good catch.  This is why we need more unit tests :)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:38:05.537+0000","updated":"2014-07-25T17:38:05.537+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074658","id":"14074658","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15414285\n  \n    --- Diff: storm-core/src/clj/backtype/storm/daemon/nimbus.clj ---\n    @@ -1046,7 +1047,10 @@\n                                     (dissoc storm-conf STORM-ZOOKEEPER-TOPOLOGY-AUTH-SCHEME STORM-ZOOKEEPER-TOPOLOGY-AUTH-PAYLOAD))\n                     total-storm-conf (merge conf storm-conf)\n                     topology (normalize-topology total-storm-conf topology)\n    +                nimbus-autocred-plugins (AuthUtils/getNimbusAutoCredPlugins total-storm-conf)\n    --- End diff --\n    \n    I would prefer to see this generated at the startup of nimbus, and stored in the nimbus data structure.  Here every time a topology is submitted we have to create new instances of the plugins and never clean them up.  Also by using the total-storm-conf, it allows the topology to override the list of plugins, potentially loading a class on the classpath that administrators had disabled on purpose.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:46:27.690+0000","updated":"2014-07-25T17:46:27.690+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074659","id":"14074659","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15414374\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/Config.java ---\n    @@ -1186,7 +1193,15 @@\n          */\n         public static final String TOPOLOGY_ISOLATED_MACHINES = \"topology.isolate.machines\";\n         public static final Object TOPOLOGY_ISOLATED_MACHINES_SCHEMA = Number.class;\n    -    \n    +\n    +    /**\n    +     * HDFS information, used to get the delegation token on behalf of the topology\n    +     * submitter user and renew the tokens. see {@link backtype.storm.security.auth.kerberos.AutoHDFS}\n    +     * kerberos principal name with realm should be provided.\n    +     */\n    +    public static final Object HDFS_PRINCIPAL = \"topology.hdfs.user\";\n    --- End diff --\n    \n    can we name this TOPOLOGY_HDFS_PRINCIPAL? just so it is clear it is for the topology, even in the clojure/java code.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:47:46.656+0000","updated":"2014-07-25T17:47:46.656+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074661","id":"14074661","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15414455\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/INimbusCredentialPlugin.java ---\n    @@ -0,0 +1,39 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package backtype.storm.security;\n    +\n    +import java.util.Map;\n    +\n    +/**\n    + * Nimbus auto credential plugin that will be called on nimbus host\n    + * during submit topology option. User can specify a list of implementation using config key\n    + * nimbus.autocredential.plugins.classes.\n    + */\n    +public interface INimbusCredentialPlugin {\n    --- End diff --\n    \n    Can we add in a prepare and make this extend Shutdownable?  I can see some plugins wanting to initialize things and cleanup at the end.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:49:02.548+0000","updated":"2014-07-25T17:49:02.548+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074667","id":"14074667","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15414517\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/AuthUtils.java ---\n    @@ -109,6 +111,27 @@ public static IPrincipalToLocal GetPrincipalToLocalPlugin(Map storm_conf) {\n         }\n     \n         /**\n    +     * Get all the Nimbus Auto cred plugins that users want to use.\n    +     * @param topologyConf topologyConfiguration to use.\n    +     * @return nimbus auto credential plugins.\n    +     */\n    +    public static Collection<INimbusCredentialPlugin> getNimbusAutoCredPlugins(Map topologyConf) {\n    --- End diff --\n    \n    Again I am not sure we want this to come from the topology conf.  I would rather have a list of plugins that the administrator has approved, and let those plugins look at the topology conf to decide if they are going to do anything or not.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:50:08.447+0000","updated":"2014-07-25T17:50:08.447+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074668","id":"14074668","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15414609\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    --- End diff --\n    \n    Perhaps this should be in a different package.  Even though we do use kerberos, perhaps backtype.storm.security.auth.hadoop would be more appropriate.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:51:24.264+0000","updated":"2014-07-25T17:51:24.264+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074670","id":"14074670","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15414654\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    --- End diff --\n    \n    A comment would be better then logging.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:52:01.721+0000","updated":"2014-07-25T17:52:01.721+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074672","id":"14074672","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15414687\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    --- End diff --\n    \n    I really prefer line comments to block comments in the middle of a function.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:52:34.080+0000","updated":"2014-07-25T17:52:34.080+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074675","id":"14074675","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15414806\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    +             *  if(UserGroupInformation.isSecurityEnabled) {\n    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +             *      Credentials credential= proxyUser.getCredentials();\n    +             *      fs.addDelegationToken(hdfsUser, credential);\n    +             * }\n    +             * and then return the credential object as a bytearray.\n    +             *\n    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml\n    +             * and core-site.xml in the class path which should set these configuration.\n    +             * configuration.set(\"hadoop.security.authentication\", \"KERBEROS\");\n    --- End diff --\n    \n    A lot of this information should probably be in a new section of the SECURITY.md to describe how to use this correctly, and the changes that are needed on the NameNode to enable nimbus to user proxy users.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T17:54:56.632+0000","updated":"2014-07-25T17:54:56.632+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074747","id":"14074747","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15417146\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    +             *  if(UserGroupInformation.isSecurityEnabled) {\n    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +             *      Credentials credential= proxyUser.getCredentials();\n    +             *      fs.addDelegationToken(hdfsUser, credential);\n    +             * }\n    +             * and then return the credential object as a bytearray.\n    +             *\n    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml\n    +             * and core-site.xml in the class path which should set these configuration.\n    +             * configuration.set(\"hadoop.security.authentication\", \"KERBEROS\");\n    +             * configuration.set(\"dfs.namenode.kerberos.principal\",\n    +             *                                \"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +             * configuration.set(\"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +             * anf the ticket cache must have the hdfs user's creds.\n    +             */\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            Object configuration = configurationClass.newInstance();\n    +\n    +            //UserGroupInformation.isSecurityEnabled\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod(\"isSecurityEnabled\");\n    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);\n    +\n    +            if(isSecurityEnabled) {\n    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);\n    +                final String hdfsUser = (String) conf.get(Config.HDFS_PRINCIPAL);\n    +\n    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +                Class fileSystemClass = Class.forName(\"org.apache.hadoop.fs.FileSystem\");\n    +                Object defaultNameNodeURI = fileSystemClass.getMethod(\"getDefaultUri\", configurationClass).invoke(null, configuration);\n    --- End diff --\n    \n    It might be nice in the future to allow for more then just the default URI, but we could easily do that on a different JIRA. no reason to hold up this pull request.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T18:36:31.646+0000","updated":"2014-07-25T18:36:31.646+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074748","id":"14074748","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15417222\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    +             *  if(UserGroupInformation.isSecurityEnabled) {\n    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +             *      Credentials credential= proxyUser.getCredentials();\n    +             *      fs.addDelegationToken(hdfsUser, credential);\n    +             * }\n    +             * and then return the credential object as a bytearray.\n    +             *\n    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml\n    +             * and core-site.xml in the class path which should set these configuration.\n    +             * configuration.set(\"hadoop.security.authentication\", \"KERBEROS\");\n    +             * configuration.set(\"dfs.namenode.kerberos.principal\",\n    +             *                                \"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +             * configuration.set(\"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +             * anf the ticket cache must have the hdfs user's creds.\n    +             */\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            Object configuration = configurationClass.newInstance();\n    +\n    +            //UserGroupInformation.isSecurityEnabled\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod(\"isSecurityEnabled\");\n    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);\n    +\n    +            if(isSecurityEnabled) {\n    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);\n    +                final String hdfsUser = (String) conf.get(Config.HDFS_PRINCIPAL);\n    +\n    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +                Class fileSystemClass = Class.forName(\"org.apache.hadoop.fs.FileSystem\");\n    +                Object defaultNameNodeURI = fileSystemClass.getMethod(\"getDefaultUri\", configurationClass).invoke(null, configuration);\n    --- End diff --\n    \n    Actually could we just pull that URI out from the topology conf.  That way they have a flag to turn this on or off?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T18:37:54.077+0000","updated":"2014-07-25T18:37:54.077+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074750","id":"14074750","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-50187860\n  \n    It is looking really good.  Juts a few comments about the lifecycle of the plugins, and some about documentation.  I think this is really close.  Great work.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T18:39:07.067+0000","updated":"2014-07-25T18:39:07.067+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074762","id":"14074762","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15417963\n  \n    --- Diff: storm-core/src/clj/backtype/storm/daemon/nimbus.clj ---\n    @@ -1046,7 +1047,10 @@\n                                     (dissoc storm-conf STORM-ZOOKEEPER-TOPOLOGY-AUTH-SCHEME STORM-ZOOKEEPER-TOPOLOGY-AUTH-PAYLOAD))\n                     total-storm-conf (merge conf storm-conf)\n                     topology (normalize-topology total-storm-conf topology)\n    +                nimbus-autocred-plugins (AuthUtils/getNimbusAutoCredPlugins total-storm-conf)\n    --- End diff --\n    \n    I have a few questions:\n    * Shouldn't the plugin be cleared up as soon as the scope of submit topology is over?\n    * If we generate these instances at startup then it means anyone wanting to use it will have to change the nimbus config and restart nimbus. Right now the restart is required because renewers are designed that way. I could follow that pattern however I think this approach is better because no restart is required as long as all the hdfs configuration files are already in the class path. \n    \n    If you like to stick to this approach I can change the code so the list of plugins are loaded at startup and stored in nimbus data structure and their populateCred method is invoked as part of submit topology. \n    \n    We need some way of identifying who the topology submitter is. Without that information we don't know for whom are we getting the creds. I can pass just the topology config  and I think any future implementations of the interface would also need that. I could make the config immutable thus ensuring the user code can not modify it, does that work ?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T18:50:14.588+0000","updated":"2014-07-25T18:50:14.588+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074780","id":"14074780","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15419109\n  \n    --- Diff: storm-core/src/clj/backtype/storm/daemon/nimbus.clj ---\n    @@ -1046,7 +1047,10 @@\n                                     (dissoc storm-conf STORM-ZOOKEEPER-TOPOLOGY-AUTH-SCHEME STORM-ZOOKEEPER-TOPOLOGY-AUTH-PAYLOAD))\n                     total-storm-conf (merge conf storm-conf)\n                     topology (normalize-topology total-storm-conf topology)\n    +                nimbus-autocred-plugins (AuthUtils/getNimbusAutoCredPlugins total-storm-conf)\n    --- End diff --\n    \n    Why does the plugin need to be cleaned up as soon as the submission is over?  Creating an object each time a method is called feels like a lot of overhead to me.  It also makes it so that the plugin does not have the option to cache things and amortize the overhead across multiple calls.  Not that the code currently needs to worry about it, as the current code is more or less just a static function.\n    \n    > If we generate these instances at startup then it means anyone wanting to use it will have to change the nimbus config and restart nimbus.\n    \n    Yes, I can see that.  But that is kind of what I want.  It becomes a balancing  act between convenience/insecurity and security/inconvenience.  I just feel nervous that anything on the classpath the user gets to decide to load it into memory. It feels like it is too open ended, but honestly if you feel strongly about it I will not push it.  I would just want the config changed so that it is prefixed with topology instead of nimbus so that it is obvious that it is a topology specific config.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T19:12:03.506+0000","updated":"2014-07-25T19:12:03.506+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14074800","id":"14074800","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15419864\n  \n    --- Diff: storm-core/src/clj/backtype/storm/daemon/nimbus.clj ---\n    @@ -1046,7 +1047,10 @@\n                                     (dissoc storm-conf STORM-ZOOKEEPER-TOPOLOGY-AUTH-SCHEME STORM-ZOOKEEPER-TOPOLOGY-AUTH-PAYLOAD))\n                     total-storm-conf (merge conf storm-conf)\n                     topology (normalize-topology total-storm-conf topology)\n    +                nimbus-autocred-plugins (AuthUtils/getNimbusAutoCredPlugins total-storm-conf)\n    --- End diff --\n    \n    I can see how this can be a security hole when the user passes his own implementation and act as nimbus to get delegation tokens as some other user. Good catch. I will go with your approach. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T19:29:44.701+0000","updated":"2014-07-25T19:29:44.701+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14075108","id":"14075108","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15428650\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/Config.java ---\n    @@ -1186,7 +1193,15 @@\n          */\n         public static final String TOPOLOGY_ISOLATED_MACHINES = \"topology.isolate.machines\";\n         public static final Object TOPOLOGY_ISOLATED_MACHINES_SCHEMA = Number.class;\n    -    \n    +\n    +    /**\n    +     * HDFS information, used to get the delegation token on behalf of the topology\n    +     * submitter user and renew the tokens. see {@link backtype.storm.security.auth.kerberos.AutoHDFS}\n    +     * kerberos principal name with realm should be provided.\n    +     */\n    +    public static final Object HDFS_PRINCIPAL = \"topology.hdfs.user\";\n    --- End diff --\n    \n    done.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T23:04:36.841+0000","updated":"2014-07-25T23:04:36.841+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14075109","id":"14075109","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15428652\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/INimbusCredentialPlugin.java ---\n    @@ -0,0 +1,39 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package backtype.storm.security;\n    +\n    +import java.util.Map;\n    +\n    +/**\n    + * Nimbus auto credential plugin that will be called on nimbus host\n    + * during submit topology option. User can specify a list of implementation using config key\n    + * nimbus.autocredential.plugins.classes.\n    + */\n    +public interface INimbusCredentialPlugin {\n    --- End diff --\n    \n    done.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T23:04:45.317+0000","updated":"2014-07-25T23:04:45.317+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14075110","id":"14075110","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15428704\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/AuthUtils.java ---\n    @@ -109,6 +111,27 @@ public static IPrincipalToLocal GetPrincipalToLocalPlugin(Map storm_conf) {\n         }\n     \n         /**\n    +     * Get all the Nimbus Auto cred plugins that users want to use.\n    +     * @param topologyConf topologyConfiguration to use.\n    +     * @return nimbus auto credential plugins.\n    +     */\n    +    public static Collection<INimbusCredentialPlugin> getNimbusAutoCredPlugins(Map topologyConf) {\n    --- End diff --\n    \n    this is now a nimbus data configuration just like renewers.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T23:06:02.557+0000","updated":"2014-07-25T23:06:02.557+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14075111","id":"14075111","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15428705\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    --- End diff --\n    \n    done.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T23:06:12.522+0000","updated":"2014-07-25T23:06:12.522+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14075112","id":"14075112","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15428709\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    --- End diff --\n    \n    done.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T23:06:19.681+0000","updated":"2014-07-25T23:06:19.681+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14075113","id":"14075113","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15428712\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    --- End diff --\n    \n    done.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T23:06:23.507+0000","updated":"2014-07-25T23:06:23.507+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14075115","id":"14075115","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15428727\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    +             *  if(UserGroupInformation.isSecurityEnabled) {\n    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +             *      Credentials credential= proxyUser.getCredentials();\n    +             *      fs.addDelegationToken(hdfsUser, credential);\n    +             * }\n    +             * and then return the credential object as a bytearray.\n    +             *\n    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml\n    +             * and core-site.xml in the class path which should set these configuration.\n    +             * configuration.set(\"hadoop.security.authentication\", \"KERBEROS\");\n    --- End diff --\n    \n    added the configuration information to SECURITY.MD file.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T23:06:56.868+0000","updated":"2014-07-25T23:06:56.868+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14075116","id":"14075116","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on a diff in the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#discussion_r15428733\n  \n    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---\n    @@ -0,0 +1,254 @@\n    +/**\n    + * Licensed to the Apache Software Foundation (ASF) under one\n    + * or more contributor license agreements.  See the NOTICE file\n    + * distributed with this work for additional information\n    + * regarding copyright ownership.  The ASF licenses this file\n    + * to you under the Apache License, Version 2.0 (the\n    + * \"License\"); you may not use this file except in compliance\n    + * with the License.  You may obtain a copy of the License at\n    + *\n    + * http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package backtype.storm.security.auth.kerberos;\n    +\n    +import backtype.storm.Config;\n    +import backtype.storm.security.INimbusCredentialPlugin;\n    +import backtype.storm.security.auth.IAutoCredentials;\n    +import backtype.storm.security.auth.ICredentialsRenewer;\n    +import org.slf4j.Logger;\n    +import org.slf4j.LoggerFactory;\n    +\n    +import javax.security.auth.Subject;\n    +import javax.xml.bind.DatatypeConverter;\n    +import java.io.*;\n    +import java.lang.reflect.Method;\n    +import java.net.URI;\n    +import java.util.Collection;\n    +import java.util.HashMap;\n    +import java.util.Map;\n    +\n    +/**\n    + * Automatically get HDFS delegation tokens and push it to user's topology. The class\n    + * assumes that HDFS configuration files are in your class path.\n    + */\n    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {\n    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);\n    +    public static final String HDFS_CREDENTIALS = \"HDFS_CREDENTIALS\";\n    +\n    +    public void prepare(Map conf) {\n    +       LOG.debug(\"no op.\");\n    +    }\n    +\n    +    @SuppressWarnings(\"unchecked\")\n    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {\n    +\n    +        try {\n    +            /**\n    +             * What we want to do is following:\n    +             *  if(UserGroupInformation.isSecurityEnabled) {\n    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);\n    +             *      Credentials credential= proxyUser.getCredentials();\n    +             *      fs.addDelegationToken(hdfsUser, credential);\n    +             * }\n    +             * and then return the credential object as a bytearray.\n    +             *\n    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml\n    +             * and core-site.xml in the class path which should set these configuration.\n    +             * configuration.set(\"hadoop.security.authentication\", \"KERBEROS\");\n    +             * configuration.set(\"dfs.namenode.kerberos.principal\",\n    +             *                                \"hdfs/zookeeper.witzend.com@WITZEND.COM\");\n    +             * configuration.set(\"hadoop.security.kerberos.ticket.cache.path\", \"/tmp/krb5cc_1002\");\n    +             * anf the ticket cache must have the hdfs user's creds.\n    +             */\n    +            Class configurationClass = Class.forName(\"org.apache.hadoop.conf.Configuration\");\n    +            Object configuration = configurationClass.newInstance();\n    +\n    +            //UserGroupInformation.isSecurityEnabled\n    +            final Class ugiClass = Class.forName(\"org.apache.hadoop.security.UserGroupInformation\");\n    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod(\"isSecurityEnabled\");\n    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);\n    +\n    +            if(isSecurityEnabled) {\n    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);\n    +                final String hdfsUser = (String) conf.get(Config.HDFS_PRINCIPAL);\n    +\n    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);\n    +                Class fileSystemClass = Class.forName(\"org.apache.hadoop.fs.FileSystem\");\n    +                Object defaultNameNodeURI = fileSystemClass.getMethod(\"getDefaultUri\", configurationClass).invoke(null, configuration);\n    --- End diff --\n    \n    added.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-07-25T23:07:02.437+0000","updated":"2014-07-25T23:07:02.437+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14087922","id":"14087922","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user Parth-Brahmbhatt commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-51368333\n  \n    2 week ping.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-08-06T17:30:51.646+0000","updated":"2014-08-06T17:30:51.646+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14089296","id":"14089296","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-51483071\n  \n    Sorry, looking at the changes now. I need to make more time for reviews.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-08-07T14:56:32.637+0000","updated":"2014-08-07T14:56:32.637+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14089323","id":"14089323","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user revans2 commented on the pull request:\n\n    https://github.com/apache/incubator-storm/pull/190#issuecomment-51485317\n  \n    Looks good tests pass, I'll merge this in. +1\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-08-07T15:11:25.299+0000","updated":"2014-08-07T15:11:25.299+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14089325","id":"14089325","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"body":"Github user asfgit closed the pull request at:\n\n    https://github.com/apache/incubator-storm/pull/190\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"created":"2014-08-07T15:11:35.789+0000","updated":"2014-08-07T15:11:35.789+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12720423/comment/14089328","id":"14089328","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"body":"Thanks Parth,\n\nI merged this into branch security.  Sorry I took so long to review your changes.  They look great.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"created":"2014-08-07T15:13:53.069+0000","updated":"2014-08-07T15:13:53.069+0000"}],"maxResults":44,"total":44,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-346/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1wmlr:"}}