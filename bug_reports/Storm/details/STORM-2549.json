{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13078879","self":"https://issues.apache.org/jira/rest/api/2/issue/13078879","key":"STORM-2549","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":16200,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12314820","id":"12314820","key":"STORM","name":"Apache Storm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12314820&avatarId=21667","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12314820&avatarId=21667","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12314820&avatarId=21667","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12314820&avatarId=21667"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/13260","id":"13260","description":"Apache Storm Related","name":"Storm"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334657","id":"12334657","name":"2.0.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12341047","id":"12341047","name":"1.2.0","archived":false,"released":true,"releaseDate":"2018-02-15"}],"aggregatetimespent":16200,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-07-18T14:26:27.523+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jul 18 14:26:27 UTC 2017","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_3271833669_*|*_3_*:*_1_*:*_10034821470_*|*_5_*:*_1_*:*_0","customfield_12310420":"9223372036854775807","customfield_12312321":null,"resolutiondate":"2017-11-11T17:54:43.157+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-2549/watchers","watchCount":4,"isWatching":false},"created":"2017-06-10T17:37:08.086+0000","priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["pull-request-available"],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":0,"aggregatetimeoriginalestimate":null,"customfield_12311120":"STORM-2710","customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334657","id":"12334657","name":"2.0.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12335748","id":"12335748","name":"1.1.0","archived":false,"released":true,"releaseDate":"2017-03-29"}],"customfield_12312339":null,"issuelinks":[{"id":"12520019","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12520019","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13058139","key":"STORM-2430","self":"https://issues.apache.org/jira/rest/api/2/issue/13058139","fields":{"summary":"Potential Race condition in Kafka Spout","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12506190","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12506190","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13040284","key":"STORM-2343","self":"https://issues.apache.org/jira/rest/api/2/issue/13040284","fields":{"summary":"New Kafka spout can stop emitting tuples if more than maxUncommittedOffsets tuples fail at once","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12520021","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12520021","type":{"id":"12310051","name":"Supercedes","inward":"is superceded by","outward":"supercedes","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310051"},"outwardIssue":{"id":"13086812","key":"STORM-2625","self":"https://issues.apache.org/jira/rest/api/2/issue/13086812","fields":{"summary":"KafkaSpout is not calculating uncommitted correctly","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde DÃ¸ssing","active":true,"timeZone":"Europe/Copenhagen"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-11-14T06:16:46.448+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12331080","id":"12331080","name":"storm-kafka-client"}],"timeoriginalestimate":null,"description":"Example:\nSay maxUncommittedOffsets is 10, maxPollRecords is 5, and the committedOffset is 0.\nThe spout will initially emit up to offset 10, because it is allowed to poll until numNonRetriableTuples is >= maxUncommittedOffsets\nThe spout will be allowed to emit another 5 tuples if offset 10 fails, so if that happens, offsets 10-14 will get emitted. If offset 1 fails and 2-14 get acked, the spout gets stuck because it will count the \"extra tuples\" 11-14 in numNonRetriableTuples.\n\nAn similar case is the one where maxPollRecords doesn't divide maxUncommittedOffsets evenly. If it were 3 in the example above, the spout might just immediately emit offsets 1-12. If 2-12 get acked, offset 1 cannot be reemitted.\n\nThe proposed solution is the following:\n* Enforce maxUncommittedOffsets on a per partition basis (i.e. actual limit will be multiplied by the number of partitions) by always allowing poll for retriable tuples that are within maxUncommittedOffsets tuples of the committed offset. Pause any non-retriable partitions if the partition has passed the maxUncommittedOffsets limit, and some other partition is polling for retries while also at the maxUncommittedOffsets limit. \n\nExample of this functionality:\nMaxUncommittedOffsets is 100\nMaxPollRecords is 10\nCommitted offset for partition 0 and 1 is 0.\nPartition 0 has emitted 0\nPartition 1 has emitted 0...95, 97, 99, 101, 103 (some offsets compacted away)\nPartition 1, message 99 is retriable\nWe check that message 99 is within 100 emitted tuples of offset 0 (it is the 97th tuple after offset 0, so it is)\nWe do not pause partition 0 because that partition isn't at the maxUncommittedOffsets limit.\nSeek to offset 99 on partition 1 and poll\nWe get back offset 99, 101, 103 and potentially 7 new tuples. Say the lowest of these is at offset 104.\nThe spout emits offset 99, filters out 101 and 103 because they were already emitted, and emits the 7 new tuples.\nIf offset 104 (or later) become retriable, they are not retried until the committed offset moves. This is because offset 104 is the 101st tuple emitted after offset 0, so it isn't allowed to retry until the committed offset moves.","customfield_10010":null,"timetracking":{"remainingEstimate":"0h","timeSpent":"4.5h","remainingEstimateSeconds":0,"timeSpentSeconds":16200},"customfield_12312026":null,"customfield_12312023":null,"customfield_12312024":null,"attachment":[],"customfield_12312340":null,"aggregatetimeestimate":0,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"The fix for STORM-2343 is incomplete, and the spout can still get stuck on failed tuples","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde DÃ¸ssing","active":true,"timeZone":"Europe/Copenhagen"},"subtasks":[],"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde DÃ¸ssing","active":true,"timeZone":"Europe/Copenhagen"},"customfield_12310291":null,"customfield_12310290":null,"aggregateprogress":{"progress":16200,"total":16200,"percent":100},"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":16200,"total":16200,"percent":100},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/comment/16091618","id":"16091618","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hmclouro","name":"hmclouro","key":"hmclouro","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hugo Louro","active":true,"timeZone":"Etc/UTC"},"body":"[~Srdo] can you please marked this ticket as in progress. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hmclouro","name":"hmclouro","key":"hmclouro","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hugo Louro","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-18T14:26:27.523+0000","updated":"2017-07-18T14:26:27.523+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-2549/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":27,"worklogs":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/44685","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"GitHub user srdo opened a pull request:\n\n    https://github.com/apache/storm/pull/2156\n\n    STORM-2549: Fix broken enforcement mechanism for maxUncommittedOffsetâ¦\n\n    â¦s in storm-kafka-client spout\n    \n    Please see https://issues.apache.org/jira/browse/STORM-2549 and maybe also the comments on https://issues.apache.org/jira/browse/STORM-2343\n    \n    This moves enforcement of maxUncommittedOffsets to the partition level. I don't believe there's a way to enforce the single limit globally without hitting a bunch of issues with it either being too strict regarding retries, or not being strict enough and allowing an unbounded number of new tuples along with the retries.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/srdo/storm STORM-2549\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/storm/pull/2156.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #2156\n    \n----\ncommit 3837a6be6d9533e27058c32ed427bac7e4c684f4\nAuthor: Stig Rohde DÃ¸ssing <stigdoessing@gmail.com>\nDate:   2017-06-10T17:38:22Z\n\n    STORM-2549: Fix broken enforcement mechanism for maxUncommittedOffsets in storm-kafka-client spout\n\n----\n","created":"2017-06-10T17:43:50.249+0000","updated":"2017-06-10T17:43:50.249+0000","started":"2017-06-10T17:43:50.248+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"44685","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/44738","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user askprasanna commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2156#discussion_r121341716\n  \n    --- Diff: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutEmitTest.java ---\n    @@ -134,110 +130,86 @@ public void testNextTupleEmitsFailedMessagesEvenWhenMaxUncommittedOffsetsIsExcee\n     \n                 Time.advanceTime(50);\n                 //No backoff for test retry service, just check that messages will retry immediately\n    -            for (int i = 0; i < recordsForPartition.size(); i++) {\n    +            for (int i = 0; i < numRecords; i++) {\n                     spout.nextTuple();\n                 }\n     \n                 ArgumentCaptor<KafkaSpoutMessageId> retryMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collectorMock, times(recordsForPartition.size())).emit(anyObject(), anyObject(), retryMessageIds.capture());\n    +            verify(collectorMock, times(numRecords)).emit(anyObject(), anyObject(), retryMessageIds.capture());\n     \n                 //Verify that the poll started at the earliest retriable tuple offset\n                 List<Long> failedOffsets = new ArrayList<>();\n    -            for(KafkaSpoutMessageId msgId : messageIds.getAllValues()) {\n    +            for (KafkaSpoutMessageId msgId : messageIds.getAllValues()) {\n                     failedOffsets.add(msgId.offset());\n                 }\n                 InOrder inOrder = inOrder(consumerMock);\n                 inOrder.verify(consumerMock).seek(partition, failedOffsets.get(0));\n                 inOrder.verify(consumerMock).poll(anyLong());\n             }\n         }\n    -    \n    +\n    +    private List<ConsumerRecord<String, String>> createRecords(TopicPartition topic, long startingOffset, int numRecords) {\n    +        List<ConsumerRecord<String, String>> recordsForPartition = new ArrayList<>();\n    +        for (int i = 0; i < numRecords; i++) {\n    +            recordsForPartition.add(new ConsumerRecord(topic.topic(), topic.partition(), startingOffset + i, \"key\", \"value\"));\n    +        }\n    +        return recordsForPartition;\n    +    }\n    +\n         @Test\n    -    public void testNextTupleEmitsAtMostMaxUncommittedOffsetsPlusMaxPollRecordsWhenRetryingTuples() {\n    -        /*\n    -        The spout must reemit failed messages waiting for retry even if it is not allowed to poll for new messages due to maxUncommittedOffsets being exceeded.\n    -        numUncommittedOffsets is equal to numNonRetriableEmittedTuples + numRetriableTuples.\n    -        The spout will only emit if numUncommittedOffsets - numRetriableTuples < maxUncommittedOffsets (i.e. numNonRetriableEmittedTuples < maxUncommittedOffsets)\n    -        This means that the latest offset a poll can start at for a retriable partition,\n    -        counting from the last committed offset, is maxUncommittedOffsets,\n    -        where there are maxUncommittedOffsets - 1 uncommitted tuples \"to the left\".\n    -        If the retry poll starts at that offset, it at most emits the retried tuple plus maxPollRecords - 1 new tuples.\n    -        The limit on uncommitted offsets for one partition is therefore maxUncommittedOffsets + maxPollRecords - 1.\n    -        \n    -        It is only necessary to test this for a single partition, because partitions can't contribute negatively to numNonRetriableEmittedTuples,\n    -        so if the limit holds for one partition, it will also hold for each individual partition when multiple are involved.\n    -        \n    -        This makes the actual limit numPartitions * (maxUncommittedOffsets + maxPollRecords - 1)\n    -         */\n    -        \n    -        //Emit maxUncommittedOffsets messages, and fail only the last. Then ensure that the spout will allow no more than maxUncommittedOffsets + maxPollRecords - 1 uncommitted offsets when retrying\n    +    public void testSpoutWillSkipPartitionsAtTheMaxUncommittedOffsetsLimit() {\n    +        //This verifies that partitions can't prevent each other from retrying tuples due to the maxUncommittedOffsets limit.\n             try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            setupSpout(Collections.singleton(partition));\n    -            \n    -            Map<TopicPartition, List<ConsumerRecord<String, String>>> firstPollRecords = new HashMap<>();\n    -            List<ConsumerRecord<String, String>> firstPollRecordsForPartition = new ArrayList<>();\n    -            for (int i = 0; i < spoutConfig.getMaxUncommittedOffsets(); i++) {\n    -                //This is cheating a bit since maxPollRecords would normally spread this across multiple polls\n    -                firstPollRecordsForPartition.add(new ConsumerRecord(partition.topic(), partition.partition(), i, \"key\", \"value\"));\n    -            }\n    -            firstPollRecords.put(partition, firstPollRecordsForPartition);\n    -            \n    -            int maxPollRecords = 5;\n    -            Map<TopicPartition, List<ConsumerRecord<String, String>>> secondPollRecords = new HashMap<>();\n    -            List<ConsumerRecord<String, String>> secondPollRecordsForPartition = new ArrayList<>();\n    -            for(int i = 0; i < maxPollRecords; i++) {\n    -                secondPollRecordsForPartition.add(new ConsumerRecord(partition.topic(), partition.partition(), spoutConfig.getMaxUncommittedOffsets() + i, \"key\", \"value\"));\n    -            }\n    -            secondPollRecords.put(partition, secondPollRecordsForPartition);\n    +            TopicPartition partitionTwo = new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 2);\n    +            Set<TopicPartition> partitions = new HashSet<>();\n    +            partitions.add(partition);\n    +            partitions.add(partitionTwo);\n    +            setupSpout(partitions);\n    +            Map<TopicPartition, List<ConsumerRecord<String, String>>> records = new HashMap<>();\n    +            //This is cheating a bit since maxPollRecords would normally spread this across multiple polls\n    +            records.put(partition, createRecords(partition, 0, spoutConfig.getMaxUncommittedOffsets()));\n    +            records.put(partitionTwo, createRecords(partitionTwo, 0, spoutConfig.getMaxUncommittedOffsets()));\n     \n                 when(consumerMock.poll(anyLong()))\n    -                .thenReturn(new ConsumerRecords(firstPollRecords))\n    -                .thenReturn(new ConsumerRecords(secondPollRecords));\n    +                .thenReturn(new ConsumerRecords(records));\n    --- End diff --\n    \n    Nit: return generic ConsumerRecords to avoid warning around raw type\n","created":"2017-06-12T09:30:11.631+0000","updated":"2017-06-12T09:30:11.631+0000","started":"2017-06-12T09:30:11.631+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"44738","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/44739","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user askprasanna commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2156#discussion_r121341911\n  \n    --- Diff: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutEmitTest.java ---\n    @@ -134,110 +130,86 @@ public void testNextTupleEmitsFailedMessagesEvenWhenMaxUncommittedOffsetsIsExcee\n     \n                 Time.advanceTime(50);\n                 //No backoff for test retry service, just check that messages will retry immediately\n    -            for (int i = 0; i < recordsForPartition.size(); i++) {\n    +            for (int i = 0; i < numRecords; i++) {\n                     spout.nextTuple();\n                 }\n     \n                 ArgumentCaptor<KafkaSpoutMessageId> retryMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collectorMock, times(recordsForPartition.size())).emit(anyObject(), anyObject(), retryMessageIds.capture());\n    +            verify(collectorMock, times(numRecords)).emit(anyObject(), anyObject(), retryMessageIds.capture());\n     \n                 //Verify that the poll started at the earliest retriable tuple offset\n                 List<Long> failedOffsets = new ArrayList<>();\n    -            for(KafkaSpoutMessageId msgId : messageIds.getAllValues()) {\n    +            for (KafkaSpoutMessageId msgId : messageIds.getAllValues()) {\n                     failedOffsets.add(msgId.offset());\n                 }\n                 InOrder inOrder = inOrder(consumerMock);\n                 inOrder.verify(consumerMock).seek(partition, failedOffsets.get(0));\n                 inOrder.verify(consumerMock).poll(anyLong());\n             }\n         }\n    -    \n    +\n    +    private List<ConsumerRecord<String, String>> createRecords(TopicPartition topic, long startingOffset, int numRecords) {\n    +        List<ConsumerRecord<String, String>> recordsForPartition = new ArrayList<>();\n    +        for (int i = 0; i < numRecords; i++) {\n    +            recordsForPartition.add(new ConsumerRecord(topic.topic(), topic.partition(), startingOffset + i, \"key\", \"value\"));\n    --- End diff --\n    \n    Nit: return generic ConsumerRecords to avoid warning around raw type\n\n","created":"2017-06-12T09:30:11.784+0000","updated":"2017-06-12T09:30:11.784+0000","started":"2017-06-12T09:30:11.784+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"44739","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/44740","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user askprasanna commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2156#discussion_r121342954\n  \n    --- Diff: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutEmitTest.java ---\n    @@ -134,110 +130,86 @@ public void testNextTupleEmitsFailedMessagesEvenWhenMaxUncommittedOffsetsIsExcee\n     \n                 Time.advanceTime(50);\n                 //No backoff for test retry service, just check that messages will retry immediately\n    -            for (int i = 0; i < recordsForPartition.size(); i++) {\n    +            for (int i = 0; i < numRecords; i++) {\n                     spout.nextTuple();\n                 }\n     \n                 ArgumentCaptor<KafkaSpoutMessageId> retryMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collectorMock, times(recordsForPartition.size())).emit(anyObject(), anyObject(), retryMessageIds.capture());\n    +            verify(collectorMock, times(numRecords)).emit(anyObject(), anyObject(), retryMessageIds.capture());\n     \n                 //Verify that the poll started at the earliest retriable tuple offset\n                 List<Long> failedOffsets = new ArrayList<>();\n    -            for(KafkaSpoutMessageId msgId : messageIds.getAllValues()) {\n    +            for (KafkaSpoutMessageId msgId : messageIds.getAllValues()) {\n                     failedOffsets.add(msgId.offset());\n                 }\n                 InOrder inOrder = inOrder(consumerMock);\n                 inOrder.verify(consumerMock).seek(partition, failedOffsets.get(0));\n                 inOrder.verify(consumerMock).poll(anyLong());\n             }\n         }\n    -    \n    +\n    +    private List<ConsumerRecord<String, String>> createRecords(TopicPartition topic, long startingOffset, int numRecords) {\n    +        List<ConsumerRecord<String, String>> recordsForPartition = new ArrayList<>();\n    +        for (int i = 0; i < numRecords; i++) {\n    +            recordsForPartition.add(new ConsumerRecord(topic.topic(), topic.partition(), startingOffset + i, \"key\", \"value\"));\n    +        }\n    +        return recordsForPartition;\n    +    }\n    +\n         @Test\n    -    public void testNextTupleEmitsAtMostMaxUncommittedOffsetsPlusMaxPollRecordsWhenRetryingTuples() {\n    -        /*\n    -        The spout must reemit failed messages waiting for retry even if it is not allowed to poll for new messages due to maxUncommittedOffsets being exceeded.\n    -        numUncommittedOffsets is equal to numNonRetriableEmittedTuples + numRetriableTuples.\n    -        The spout will only emit if numUncommittedOffsets - numRetriableTuples < maxUncommittedOffsets (i.e. numNonRetriableEmittedTuples < maxUncommittedOffsets)\n    -        This means that the latest offset a poll can start at for a retriable partition,\n    -        counting from the last committed offset, is maxUncommittedOffsets,\n    -        where there are maxUncommittedOffsets - 1 uncommitted tuples \"to the left\".\n    -        If the retry poll starts at that offset, it at most emits the retried tuple plus maxPollRecords - 1 new tuples.\n    -        The limit on uncommitted offsets for one partition is therefore maxUncommittedOffsets + maxPollRecords - 1.\n    -        \n    -        It is only necessary to test this for a single partition, because partitions can't contribute negatively to numNonRetriableEmittedTuples,\n    -        so if the limit holds for one partition, it will also hold for each individual partition when multiple are involved.\n    -        \n    -        This makes the actual limit numPartitions * (maxUncommittedOffsets + maxPollRecords - 1)\n    -         */\n    -        \n    -        //Emit maxUncommittedOffsets messages, and fail only the last. Then ensure that the spout will allow no more than maxUncommittedOffsets + maxPollRecords - 1 uncommitted offsets when retrying\n    +    public void testSpoutWillSkipPartitionsAtTheMaxUncommittedOffsetsLimit() {\n    +        //This verifies that partitions can't prevent each other from retrying tuples due to the maxUncommittedOffsets limit.\n             try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            setupSpout(Collections.singleton(partition));\n    -            \n    -            Map<TopicPartition, List<ConsumerRecord<String, String>>> firstPollRecords = new HashMap<>();\n    -            List<ConsumerRecord<String, String>> firstPollRecordsForPartition = new ArrayList<>();\n    -            for (int i = 0; i < spoutConfig.getMaxUncommittedOffsets(); i++) {\n    -                //This is cheating a bit since maxPollRecords would normally spread this across multiple polls\n    -                firstPollRecordsForPartition.add(new ConsumerRecord(partition.topic(), partition.partition(), i, \"key\", \"value\"));\n    -            }\n    -            firstPollRecords.put(partition, firstPollRecordsForPartition);\n    -            \n    -            int maxPollRecords = 5;\n    -            Map<TopicPartition, List<ConsumerRecord<String, String>>> secondPollRecords = new HashMap<>();\n    -            List<ConsumerRecord<String, String>> secondPollRecordsForPartition = new ArrayList<>();\n    -            for(int i = 0; i < maxPollRecords; i++) {\n    -                secondPollRecordsForPartition.add(new ConsumerRecord(partition.topic(), partition.partition(), spoutConfig.getMaxUncommittedOffsets() + i, \"key\", \"value\"));\n    -            }\n    -            secondPollRecords.put(partition, secondPollRecordsForPartition);\n    +            TopicPartition partitionTwo = new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 2);\n    +            Set<TopicPartition> partitions = new HashSet<>();\n    +            partitions.add(partition);\n    +            partitions.add(partitionTwo);\n    +            setupSpout(partitions);\n    +            Map<TopicPartition, List<ConsumerRecord<String, String>>> records = new HashMap<>();\n    +            //This is cheating a bit since maxPollRecords would normally spread this across multiple polls\n    +            records.put(partition, createRecords(partition, 0, spoutConfig.getMaxUncommittedOffsets()));\n    +            records.put(partitionTwo, createRecords(partitionTwo, 0, spoutConfig.getMaxUncommittedOffsets()));\n     \n                 when(consumerMock.poll(anyLong()))\n    -                .thenReturn(new ConsumerRecords(firstPollRecords))\n    -                .thenReturn(new ConsumerRecords(secondPollRecords));\n    +                .thenReturn(new ConsumerRecords(records));\n     \n    -            for (int i = 0; i < spoutConfig.getMaxUncommittedOffsets() + maxPollRecords; i++) {\n    +            for (int i = 0; i < spoutConfig.getMaxUncommittedOffsets()*2; i++) {\n                     spout.nextTuple();\n                 }\n     \n                 ArgumentCaptor<KafkaSpoutMessageId> messageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collectorMock, times(firstPollRecordsForPartition.size())).emit(anyObject(), anyObject(), messageIds.capture());\n    -\n    -            KafkaSpoutMessageId failedMessageId = messageIds.getAllValues().get(messageIds.getAllValues().size() - 1);\n    -            spout.fail(failedMessageId);\n    -\n    +            verify(collectorMock, times(spoutConfig.getMaxUncommittedOffsets()*2)).emit(anyObject(), anyObject(), messageIds.capture());\n    +            \n    +            //Now fail a tuple on partition 0 and verify that it is allowed to retry\n    +            //Partition 1 should be paused, since it is at the uncommitted offsets limit\n    +            Optional<KafkaSpoutMessageId> failedMessageId = messageIds.getAllValues().stream()\n    +                .filter(messageId -> messageId.partition() == partition.partition())\n    +                .findAny();\n    +            \n    +            spout.fail(failedMessageId.get());\n    +            \n                 reset(collectorMock);\n    -\n    -            //Now make the single failed tuple retriable\n    +            \n                 Time.advanceTime(50);\n    -            //The spout should allow another poll since there are now only maxUncommittedOffsets - 1 nonretriable tuples\n    -            for (int i = 0; i < firstPollRecordsForPartition.size() + maxPollRecords; i++) {\n    -                spout.nextTuple();\n    -            }\n    -\n    -            ArgumentCaptor<KafkaSpoutMessageId> retryBatchMessageIdsCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collectorMock, times(maxPollRecords)).emit(anyObject(), anyObject(), retryBatchMessageIdsCaptor.capture());\n    -            reset(collectorMock);\n                 \n    -            //Check that the consumer started polling at the failed tuple offset\n    +            when(consumerMock.poll(anyLong()))\n    +                .thenReturn(new ConsumerRecords(Collections.singletonMap(partition, createRecords(partition, failedMessageId.get().offset(), 1))));\n    --- End diff --\n    \n    return generic ConsumerRecords to avoid warning around raw type\n    \n    In this case I am getting an incompatible bounds error on the V param to singletonMap()\n","created":"2017-06-12T09:30:12.034+0000","updated":"2017-06-12T09:30:12.034+0000","started":"2017-06-12T09:30:12.033+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"44740","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/44741","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user askprasanna commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2156#discussion_r121340307\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java ---\n    @@ -64,6 +65,27 @@ public void addToAckMsgs(KafkaSpoutMessageId msgId) {          // O(Log N)\n         public void addToEmitMsgs(long offset) {\n             this.emittedOffsets.add(offset);                  // O(Log N)\n         }\n    +    \n    +    public int getNumUncommittedOffsets() {\n    +        return this.emittedOffsets.size();\n    +    }\n    +    \n    +    /**\n    +     * Gets the offset of the nth emitted message after the committed offset. \n    +     * Example: If the committed offset is 0 and offsets 1, 2, 8, 10 have been emitted,\n    +     * getNthUncommittedOffsetAfterCommittedOffset(3) returns 8.\n    +     * \n    +     * @param index The index of the message to get the offset for\n    +     * @return The offset\n    +     * @throws NoSuchElementException if the index is out of range\n    +     */\n    +    public long getNthUncommittedOffsetAfterCommittedOffset(int index) {\n    +        Iterator<Long> offsetIter = emittedOffsets.iterator();\n    +        for (int i = 0; i < index - 1; i++) {\n    +            offsetIter.next();\n    +        }\n    --- End diff --\n    \n    how about calling toArray() on the set and then fetching the Nth offset directly using the array index? Trade-off speed for some memory/garbage. We can possibly even pre-allocate an array of size maxUncommitted + batch size for this purpose.\n","created":"2017-06-12T09:30:12.401+0000","updated":"2017-06-12T09:30:12.401+0000","started":"2017-06-12T09:30:12.400+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"44741","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/44808","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2156#discussion_r121458991\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java ---\n    @@ -64,6 +65,27 @@ public void addToAckMsgs(KafkaSpoutMessageId msgId) {          // O(Log N)\n         public void addToEmitMsgs(long offset) {\n             this.emittedOffsets.add(offset);                  // O(Log N)\n         }\n    +    \n    +    public int getNumUncommittedOffsets() {\n    +        return this.emittedOffsets.size();\n    +    }\n    +    \n    +    /**\n    +     * Gets the offset of the nth emitted message after the committed offset. \n    +     * Example: If the committed offset is 0 and offsets 1, 2, 8, 10 have been emitted,\n    +     * getNthUncommittedOffsetAfterCommittedOffset(3) returns 8.\n    +     * \n    +     * @param index The index of the message to get the offset for\n    +     * @return The offset\n    +     * @throws NoSuchElementException if the index is out of range\n    +     */\n    +    public long getNthUncommittedOffsetAfterCommittedOffset(int index) {\n    +        Iterator<Long> offsetIter = emittedOffsets.iterator();\n    +        for (int i = 0; i < index - 1; i++) {\n    +            offsetIter.next();\n    +        }\n    --- End diff --\n    \n    Is copying the entire set to an array faster than iterating the set?\n","created":"2017-06-12T16:19:18.659+0000","updated":"2017-06-12T16:19:18.659+0000","started":"2017-06-12T16:19:18.657+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"44808","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/44810","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user askprasanna commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2156#discussion_r121463093\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java ---\n    @@ -64,6 +65,27 @@ public void addToAckMsgs(KafkaSpoutMessageId msgId) {          // O(Log N)\n         public void addToEmitMsgs(long offset) {\n             this.emittedOffsets.add(offset);                  // O(Log N)\n         }\n    +    \n    +    public int getNumUncommittedOffsets() {\n    +        return this.emittedOffsets.size();\n    +    }\n    +    \n    +    /**\n    +     * Gets the offset of the nth emitted message after the committed offset. \n    +     * Example: If the committed offset is 0 and offsets 1, 2, 8, 10 have been emitted,\n    +     * getNthUncommittedOffsetAfterCommittedOffset(3) returns 8.\n    +     * \n    +     * @param index The index of the message to get the offset for\n    +     * @return The offset\n    +     * @throws NoSuchElementException if the index is out of range\n    +     */\n    +    public long getNthUncommittedOffsetAfterCommittedOffset(int index) {\n    +        Iterator<Long> offsetIter = emittedOffsets.iterator();\n    +        for (int i = 0; i < index - 1; i++) {\n    +            offsetIter.next();\n    +        }\n    --- End diff --\n    \n    Guess we ll need to check the implementation to confirm how the array is created from the data structures behind the Set interface. Agree that it is possible for the iteration to be as fast or faster than the array creation. There may not be a clear cut winner here and it might depend on 'N'.\n","created":"2017-06-12T16:37:13.234+0000","updated":"2017-06-12T16:37:13.234+0000","started":"2017-06-12T16:37:13.233+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"44810","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/44811","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user askprasanna commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2156#discussion_r121465722\n  \n    --- Diff: external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java ---\n    @@ -64,6 +65,27 @@ public void addToAckMsgs(KafkaSpoutMessageId msgId) {          // O(Log N)\n         public void addToEmitMsgs(long offset) {\n             this.emittedOffsets.add(offset);                  // O(Log N)\n         }\n    +    \n    +    public int getNumUncommittedOffsets() {\n    +        return this.emittedOffsets.size();\n    +    }\n    +    \n    +    /**\n    +     * Gets the offset of the nth emitted message after the committed offset. \n    +     * Example: If the committed offset is 0 and offsets 1, 2, 8, 10 have been emitted,\n    +     * getNthUncommittedOffsetAfterCommittedOffset(3) returns 8.\n    +     * \n    +     * @param index The index of the message to get the offset for\n    +     * @return The offset\n    +     * @throws NoSuchElementException if the index is out of range\n    +     */\n    +    public long getNthUncommittedOffsetAfterCommittedOffset(int index) {\n    +        Iterator<Long> offsetIter = emittedOffsets.iterator();\n    +        for (int i = 0; i < index - 1; i++) {\n    +            offsetIter.next();\n    +        }\n    --- End diff --\n    \n    Looks like an iterator is used under the covers to create the array (which explains the ordering guarantee in the array). No change required.. :-)\n","created":"2017-06-12T16:48:25.459+0000","updated":"2017-06-12T16:48:25.459+0000","started":"2017-06-12T16:48:25.459+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"44811","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/45106","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    @srdo I am reviewing this. Thanks.\n","created":"2017-06-14T15:55:30.931+0000","updated":"2017-06-14T15:55:30.931+0000","started":"2017-06-14T15:55:30.931+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"45106","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/48083","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user hmcl commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    @srdo I am doing one last review of this patch.\n","created":"2017-07-18T14:27:33.849+0000","updated":"2017-07-18T14:27:33.849+0000","started":"2017-07-18T14:27:33.849+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"48083","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/48172","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    Sorry, had to rebase to fix conflicts.\n","created":"2017-07-19T20:42:59.771+0000","updated":"2017-07-19T20:42:59.771+0000","started":"2017-07-19T20:42:59.770+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"48172","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/50244","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    Rebased again to fix conflicts\n","created":"2017-08-08T10:35:48.646+0000","updated":"2017-08-08T10:35:48.646+0000","started":"2017-08-08T10:35:48.645+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"50244","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/51583","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    Squashed\n","created":"2017-09-02T17:42:27.361+0000","updated":"2017-09-02T17:42:27.361+0000","started":"2017-09-02T17:42:27.359+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"51583","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/52034","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user WolfeeTJ commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    Hi @srdo , I happened to see errors like this in my production after applied your PR:\n    \n    ```\n    2017-09-06 13:12:01.707 o.a.s.util Thread-15-spout-executor[103 103] [ERROR] Async loop died!\n    java.lang.IllegalStateException: No current assignment for partition kafka_bd_trigger_action-22\n            at org.apache.kafka.clients.consumer.internals.SubscriptionState.assignedState(SubscriptionState.java:231) ~[stormjar.jar:?]\n            at org.apache.kafka.clients.consumer.internals.SubscriptionState.resume(SubscriptionState.java:350) ~[stormjar.jar:?]\n            at org.apache.kafka.clients.consumer.KafkaConsumer.resume(KafkaConsumer.java:1332) ~[stormjar.jar:?]\n            at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:323) ~[stormjar.jar:?]\n            at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:234) ~[stormjar.jar:?]\n            at org.apache.storm.daemon.executor$fn__9708$fn__9723$fn__9754.invoke(executor.clj:646) ~[storm-core-1.1.1-SNAPSHOT.jar:1.1.1-SNAPSHOT]\n            at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:484) [storm-core-1.1.1-SNAPSHOT.jar:1.1.1-SNAPSHOT]\n            at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]\n            at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\n    2017-09-06 13:12:01.709 o.a.s.d.executor Thread-15-spout-executor[103 103] [ERROR]\n    java.lang.IllegalStateException: No current assignment for partition kafka_bd_trigger_action-22\n            at org.apache.kafka.clients.consumer.internals.SubscriptionState.assignedState(SubscriptionState.java:231) ~[stormjar.jar:?]\n            at org.apache.kafka.clients.consumer.internals.SubscriptionState.resume(SubscriptionState.java:350) ~[stormjar.jar:?]\n            at org.apache.kafka.clients.consumer.KafkaConsumer.resume(KafkaConsumer.java:1332) ~[stormjar.jar:?]\n            at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:323) ~[stormjar.jar:?]\n    \n    ```\n    \n    I think the issue might happen because it could be possible partition reassignment takes effect at kafkaConsumer.poll(), and as a result the pausedPartitions is different from the actual assignment. How do you think?\n","created":"2017-09-11T05:52:25.478+0000","updated":"2017-09-11T05:52:25.478+0000","started":"2017-09-11T05:52:25.474+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"52034","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/52437","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    @WolfeeTJ Thanks for trying this out. I think you are right that it's because partition reassignment is happening at a bad time. I think we should move reassignment to be the first thing in nextTuple instead. Give me a little bit to fix this and maybe add a test.\n    \n    I've been thinking a bit about partition revocation and shuffling, and we might want to add some warnings to the manual partition assignment API as well. There are a few cases I can think of where shuffling partitions can cause bugs. The Trident spout doesn't support partition shuffling because Trident doesn't expect partitions to move from task to task, as far as I can tell. When we implement at-most-once support for this spout there's also a requirement that partitions don't move between tasks, since otherwise it is possible that tuples are emitted more than once.\n","created":"2017-09-13T18:22:31.038+0000","updated":"2017-09-13T18:22:31.038+0000","started":"2017-09-13T18:22:31.037+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"52437","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/52444","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    @WolfeeTJ I think it's fixed now, please take a look at the new commit.\n","created":"2017-09-13T20:49:14.355+0000","updated":"2017-09-13T20:49:14.355+0000","started":"2017-09-13T20:49:14.347+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"52444","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/52491","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user WolfeeTJ commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    Hi @srdo , I'm not sure, so please correct me if I were wrong:\n    \n    So far as I could see, I'm using the default `NamedSubscription`. And I checked the code, it's doing NOOP in `org.apache.storm.kafka.spout.Subscription#refreshAssignment()`.\n    My understanding is, actual reassignment happens when using \n    `final ConsumerRecords<K, V> consumerRecords = kafkaConsumer.poll(kafkaSpoutConfig.getPollTimeoutMs());`\n    in `org.apache.storm.kafka.spout.KafkaSpout#pollKafkaBroker()`.\n    \n    So we might need to get the fresh partition assignment for `kafkaConsumer.pause(pausedPartitions);` somehow after a `kafkaConsumer.poll()`, because the previous `pollablePartitions` list might has been changed after `kafkaConsumer.poll()` ?\n","created":"2017-09-14T06:25:27.563+0000","updated":"2017-09-14T06:25:27.563+0000","started":"2017-09-14T06:25:27.562+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"52491","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/52506","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    @WolfeeTJ We are dropping support for NamedSubscription and PatternSubscription in 2.0.0 and deprecating them in 1.2.0 because Kafka's mechanism for automatically assigning partitions to consumers is a bad fit for Storm. See https://issues.apache.org/jira/browse/STORM-2542 for details.  \n    \n    From 1.2.0 (1.x-branch) on, the spout will assign partitions using the https://github.com/apache/storm/blob/master/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/ManualPartitionSubscription.java class. The default subscription in KafkaSpoutConfig will also change to use this subscription.\n    \n    Partition assignment happens in refreshPartitions for the ManualPartitionSubscription, and poll won't trigger rebalance anymore, so we shouldn't need to worry about partitions changing when we call poll.\n","created":"2017-09-14T07:17:18.407+0000","updated":"2017-09-14T07:17:18.407+0000","started":"2017-09-14T07:17:18.407+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"52506","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/52507","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user WolfeeTJ commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    @srdo great, thank you a lot for your info. This helps a lot. In this case I think it should be ok. I'll apply the commit and try it out.\n    Thanks a lot!\n","created":"2017-09-14T07:28:12.362+0000","updated":"2017-09-14T07:28:12.362+0000","started":"2017-09-14T07:28:12.361+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"52507","issueId":"13078879"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13078879/worklog/52509","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2156\n  \n    @WolfeeTJ No problem :)\n    \n    Note that manual partition subscription doesn't work before version 1.1.2 https://issues.apache.org/jira/browse/STORM-2541, so you may want to check out 1.x-branch and apply this PR to it to get it working.\n","created":"2017-09-14T07:34:01.899+0000","updated":"2017-09-14T07:34:01.899+0000","started":"2017-09-14T07:34:01.897+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"52509","issueId":"13078879"}]},"customfield_12311820":"0|i3g4v3:"}}