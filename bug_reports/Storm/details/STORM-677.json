{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12775904","self":"https://issues.apache.org/jira/rest/api/2/issue/12775904","key":"STORM-677","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12314820","id":"12314820","key":"STORM","name":"Apache Storm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12314820&avatarId=21667","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12314820&avatarId=21667","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12314820&avatarId=21667","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12314820&avatarId=21667"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/13260","id":"13260","description":"Apache Storm Related","name":"Storm"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-02-18T18:52:35.874+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Feb 20 15:14:52 UTC 2015","customfield_12312320":null,"customfield_12310222":null,"customfield_12310420":"9223372036854775807","customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-677/watchers","watchCount":8,"isWatching":false},"created":"2015-02-18T16:24:18.704+0000","priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":["Netty"],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327112","id":"12327112","name":"0.9.3","archived":false,"released":true,"releaseDate":"2014-11-25"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12327123","id":"12327123","description":"security feature branch","name":"0.10.0","archived":false,"released":true,"releaseDate":"2015-11-05"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12333021","id":"12333021","name":"0.9.6","archived":false,"released":true,"releaseDate":"2015-11-05"}],"customfield_12312339":null,"issuelinks":[{"id":"12408420","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12408420","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12715654","key":"STORM-329","self":"https://issues.apache.org/jira/rest/api/2/issue/12715654","fields":{"summary":"Fix cascading Storm failure by improving reconnection strategy and buffering messages","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-12-08T16:53:41.753+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12327950","id":"12327950","name":"storm-core","description":"Core storm daemons and APIs including trident"}],"timeoriginalestimate":null,"description":"h3. Background\n\nStorm currently supports the configuration setting storm.messaging.netty.max_retries.  This setting is supposed to limit the number of reconnection attempts a Netty client will perform in case of a connection loss.\n\nUnfortunately users have run into situations where this behavior will result in data loss:\n\n{quote}\nhttps://github.com/apache/storm/pull/429/files#r24681006\n\nThis could be a separate JIRA, but we ran into a situation where we hit the maximum number of reconnection attempts, and the exception was eaten because it was thrown from a background thread and it just killed the background thread. This code appears to do the same thing.\n{quote}\n\nThe problem can be summarized by the following example:  Once a Netty client hits the maximum number of connection retries, it will stop trying to reconnect (as intended) but will also continue to run forever without being able to send any messages to its designated remote targets.  At this point data will be lost because any messages that the Netty client is supposed to send will be dropped (by design).  And since the Netty client is still alive and thus considered \"functional\", Storm is not able to do something about this data loss situation.\n\nFor a more detailed description please take a look at the discussion in https://github.com/apache/storm/pull/429/files#r24742354.\n\nh3. Possible solutions\n\n(Most of this section is copy-pasted from an [earlier discussion on this problem|https://github.com/apache/storm/pull/429/files#r24742354].)\n\nThere are at least three approaches we may consider:\n\n# Let the Netty client die if max retries is reached, so that the Storm task has the chance to re-create a client and thus break out of the client's discard-messages-forever state.\n# Let the \"parent\" Storm task die if (one of its possibly many) Netty clients dies, so that by restarting the task we'll also get a new Netty client.\n# Remove the max retries semantics as well as the corresponding setting from Storm's configuration. Here, a Netty client will continue to reconnect to a remote destination forever. The possible negative impact of these reconnects (e.g. number of TCP connection attempts in a cluster) are kept in check by our exponential backoff policy for such connection retries.\n\nMy personal opinion on these three approaches:\n\n- I do not like (1) because I feel it introduces potentially confusing semantics: We keep having a max retries setting, but it is not really a hard limit anymore. It rather becomes a \"max retries until we recreate a Netty client\", and would also reset any exponential backoff strategy of the \"previous\" Netty client instance (cf. StormBoundedExponentialBackoffRetry). If we do want such resets (but I don't think we do at this point), then a cleaner approach would be to implement such resetting inside the retry policy (again, cf. StormBoundedExponentialBackoffRetry).\n- I do not like (2) because a single \"bad\" Netty client would be able to take down a Storm task, which among other things would also impact any other, working Netty clients of the Storm task.\n- Option (3) seems a reasonable approach, although it breaks backwards compatibility with regard to Storm's configuration (because we'd now ignore storm.messaging.netty.max_retries).\n\n\nHere's initial feedback from other developers:\n\n{quote}\nhttps://github.com/apache/storm/pull/429/files#r24824540\n\nrevans2: I personally prefer option 3, no maximum number of reconnection attempts. Having the client decide that it is done, before nimbus does feels like it is asking for trouble.\n{quote}\n\n{quote}\nhttps://github.com/ptgoetz\n\nptgoetz: I'm in favor of option 3 as well. I'm not that concerned about storm.messaging.netty.max_retries being ignored. We could probably just log a warning that that configuration option is deprecated and will be ignored if the value is set.\n{quote}\n\n{quote}\nhttps://github.com/apache/storm/pull/429#issuecomment-74914806\n\nnathanmarz: Nimbus only knows a worker is having trouble when it stops sending heartbeats. If a worker gets into a bad state, the worst thing to do is have it continue trying to limp along in that bad state. It should instead suicide as quickly as possible. It seems counterintuitive, but this aggressive suiciding behavior actually makes things more robust as it prevents processes from getting into weird, potentially undefined states. This has been a crucial design principle in Storm from the beginning. One consequence of it is that any crucial system thread that receives an unrecoverable exception must suicide the process rather than die quietly.\n\nFor the connection retry problem, it's a tricky situation since it may not be able to connect because the other worker is still getting set up. So the retry policy should be somehow related to the launch timeouts for worker processes specified in the configuration. Not being able to connect after the launch timeout + a certain number of attempts + a buffer period would certainly qualify as a weird state, so the process should suicide in that case. Suiciding and restarting gets the worker back to a known state.\n\nSo in this case, I am heavily in favor of Option 2. I don't care about killing the other tasks in the worker because this is a rare situation. It is infinitely more important to get the worker back to a known, robust state than risk leaving it in a weird state permanently.\n{quote}\n\nIf we decide to go with option 3, then the essence of the fix is the following modification of Client.java:\n\n{code}\n    private boolean reconnectingAllowed() {\n        // BEFORE:\n        // return !closing && connectionAttempts.get() <= (maxReconnectionAttempts + 1);\n        return !closing;\n    }\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12312024":null,"attachment":[],"customfield_12312340":null,"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Maximum retries strategy may cause data loss","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miguno","name":"miguno","key":"miguno","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miguno&avatarId=26959","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miguno&avatarId=26959","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miguno&avatarId=26959","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miguno&avatarId=26959"},"displayName":"Michael Noll","active":true,"timeZone":"Europe/Amsterdam"},"subtasks":[],"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miguno","name":"miguno","key":"miguno","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miguno&avatarId=26959","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miguno&avatarId=26959","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miguno&avatarId=26959","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miguno&avatarId=26959"},"displayName":"Michael Noll","active":true,"timeZone":"Europe/Amsterdam"},"customfield_12310291":null,"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12775904/comment/14326374","id":"14326374","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"body":"on https://github.com/apache/storm/pull/429 [~marz] said\n\nbq. Nimbus only knows a worker is having trouble when it stops sending heartbeats. If a worker gets into a bad state, the worst thing to do is have it continue trying to limp along in that bad state. It should instead suicide as quickly as possible. It seems counterintuitive, but this aggressive suiciding behavior actually makes things more robust as it prevents processes from getting into weird, potentially undefined states. This has been a crucial design principle in Storm from the beginning. One consequence of it is that any crucial system thread that receives an unrecoverable exception must suicide the process rather than die quietly.\n\nbq. For the connection retry problem, it's a tricky situation since it may not be able to connect because the other worker is still getting set up. So the retry policy should be somehow related to the launch timeouts for worker processes specified in the configuration. Not being able to connect after the launch timeout + a certain number of attempts + a buffer period would certainly qualify as a weird state, so the process should suicide in that case. Suiciding and restarting gets the worker back to a known state.\n\nbq. So in this case, I am heavily in favor of Option 2. I don't care about killing the other tasks in the worker because this is a rare situation. It is infinitely more important to get the worker back to a known, robust state than risk leaving it in a weird state permanently.\n\nbq. I would like to see these issues addressed as part of this patch.\n\nI see your point and think option 2 is preferable long term as it sends a signal to nimbus that something is potentially wrong so it can take appropriate steps.  Ultimately all the options end up looking very similar. The connection is not being established so we do some things in between and try to establish the connection again.  The issue for me is the things that we do in between giving up on making a connection and trying to establish the connection again.  If that itself can cause other workers to give up on a connection it could result in the topology never reaching a stable state.  I don't see this happening in practice without the system being over loaded, which at least for us is more common then I like.     ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"created":"2015-02-18T18:52:35.874+0000","updated":"2015-02-18T18:52:35.874+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12775904/comment/14326412","id":"14326412","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miguno","name":"miguno","key":"miguno","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miguno&avatarId=26959","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miguno&avatarId=26959","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miguno&avatarId=26959","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miguno&avatarId=26959"},"displayName":"Michael Noll","active":true,"timeZone":"Europe/Amsterdam"},"body":"[~marz] FYI: I copied your feedback on the max retries issue to this ticket (STORM-677).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miguno","name":"miguno","key":"miguno","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miguno&avatarId=26959","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miguno&avatarId=26959","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miguno&avatarId=26959","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miguno&avatarId=26959"},"displayName":"Michael Noll","active":true,"timeZone":"Europe/Amsterdam"},"created":"2015-02-18T19:17:45.009+0000","updated":"2015-02-18T19:17:45.009+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12775904/comment/14327259","id":"14327259","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miguno","name":"miguno","key":"miguno","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miguno&avatarId=26959","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miguno&avatarId=26959","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miguno&avatarId=26959","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miguno&avatarId=26959"},"displayName":"Michael Noll","active":true,"timeZone":"Europe/Amsterdam"},"body":"I will reply to the matter at hand in a separate comment.  For now I'd like to ask a question that may or may not be related to this issue:  How does a worker (or a Netty client) determine that it is actually connected to correct remote worker (Netty server)?  While debugging STORM-329 I noticed that under certain conditions (think: connection/restart race condition) a worker may end up connecting to the wrong remote worker.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miguno","name":"miguno","key":"miguno","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miguno&avatarId=26959","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miguno&avatarId=26959","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miguno&avatarId=26959","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miguno&avatarId=26959"},"displayName":"Michael Noll","active":true,"timeZone":"Europe/Amsterdam"},"created":"2015-02-19T11:06:56.569+0000","updated":"2015-02-19T11:06:56.569+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12775904/comment/14327642","id":"14327642","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=marz","name":"marz","key":"marz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nathan Marz","active":true,"timeZone":"Etc/UTC"},"body":"Option 2 doesn't have to be long term as it should be easy to implement. I do not view the options as looking very similar as I think Option 2 will be significantly more robust – getting out of a weird state as fast as possible is really important.\n\n\"If that itself can cause other workers to give up on a connection it could result in the topology never reaching a stable state.\" –> This is exactly why the amount of time attempting to make a connection must be related to the start timeout for a worker. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=marz","name":"marz","key":"marz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nathan Marz","active":true,"timeZone":"Etc/UTC"},"created":"2015-02-19T15:42:56.813+0000","updated":"2015-02-19T15:42:56.813+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12775904/comment/14327661","id":"14327661","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"body":"Periodically, every 10 seconds at least, the current compiled topology is downloaded, if it changed, by all workers from zookeeper and the routing/connections are updated.  Connections not needed any longer are closed, and new connections are created.  If a worker receives any tuples that are not for it, they are discarded and an error message is logged.  If a connection is to the wrong worker, in my experience, it has always been because workers were reassigned and the remote supervisor launched new workers faster then the current worker could update the routing.  This is also why it is important that the send method in the client not bock, because of the way locking is handled in storm if it does block, it can block the routing/connections from being updated and incorrect connections can persist longer than the typical 10 second maximum.\n\nAlso secure versions of storm optionally use SASL when creating connections, so any connection to a worker of a different topology will never be established in the first place.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"created":"2015-02-19T15:53:07.189+0000","updated":"2015-02-19T15:53:07.189+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12775904/comment/14329040","id":"14329040","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miguno","name":"miguno","key":"miguno","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miguno&avatarId=26959","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miguno&avatarId=26959","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miguno&avatarId=26959","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miguno&avatarId=26959"},"displayName":"Michael Noll","active":true,"timeZone":"Europe/Amsterdam"},"body":"Thanks for the explanation, Bobby!\n\n> If a connection is to the wrong worker, in my experience,\n> it has always been because workers were reassigned and the\n> remote supervisor launched new workers faster then the\n> current worker could update the routing.\n\nYes, this has been my experience, too.\n\n> If a worker receives any tuples that are not for it,\n> they are discarded and an error message is logged.\n\nDo you have a pointer to the relevant code to see how exactly this check/discarding is being performed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=miguno","name":"miguno","key":"miguno","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=miguno&avatarId=26959","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=miguno&avatarId=26959","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=miguno&avatarId=26959","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=miguno&avatarId=26959"},"displayName":"Michael Noll","active":true,"timeZone":"Europe/Amsterdam"},"created":"2015-02-20T15:14:52.518+0000","updated":"2015-02-20T15:14:52.518+0000"}],"maxResults":6,"total":6,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-677/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i25rpj:"}}