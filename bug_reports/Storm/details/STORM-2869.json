{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13127266","self":"https://issues.apache.org/jira/rest/api/2/issue/13127266","key":"STORM-2869","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":4800,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12314820","id":"12314820","key":"STORM","name":"Apache Storm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12314820&avatarId=21667","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12314820&avatarId=21667","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12314820&avatarId=21667","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12314820&avatarId=21667"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/13260","id":"13260","description":"Apache Storm Related","name":"Storm"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334657","id":"12334657","name":"2.0.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12341047","id":"12341047","name":"1.2.0","archived":false,"released":true,"releaseDate":"2018-02-15"}],"aggregatetimespent":4800,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-12-29T02:19:33.369+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Dec 29 02:19:33 UTC 2017","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_86367_*|*_3_*:*_1_*:*_187294605_*|*_5_*:*_1_*:*_0","customfield_12310420":"9223372036854775807","customfield_12312321":null,"resolutiondate":"2017-12-29T02:19:33.316+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-2869/watchers","watchCount":2,"isWatching":false},"created":"2017-12-26T22:16:32.417+0000","priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["pull-request-available"],"customfield_12312333":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":0,"aggregatetimeoriginalestimate":null,"customfield_12311120":"STORM-2710","customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334657","id":"12334657","name":"2.0.0","archived":false,"released":false},{"self":"https://issues.apache.org/jira/rest/api/2/version/12341047","id":"12341047","name":"1.2.0","archived":false,"released":true,"releaseDate":"2018-02-15"}],"customfield_12312339":null,"issuelinks":[],"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-12-29T02:19:33.376+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12331080","id":"12331080","name":"storm-kafka-client"}],"timeoriginalestimate":null,"description":"As part of the STORM-2666 fix the spout clears out waitingToEmit when the consumer position falls behind the committed offset during a commit. We only need to do it for the affected partition, and then only for the records that are behind the committed offset.\r\n\r\nAlso the validation check in emitOrRetryTuple is slightly too permissive, it should check whether the current record is behind the committed offset, not whether the consumer position is behind the committed offset.","customfield_10010":null,"timetracking":{"remainingEstimate":"0h","timeSpent":"1h 20m","remainingEstimateSeconds":0,"timeSpentSeconds":4800},"customfield_12312026":null,"customfield_12312023":null,"customfield_12312024":null,"attachment":[],"customfield_12312340":null,"aggregatetimeestimate":0,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"KafkaSpout discards all pending records when adjusting the consumer position after a commit","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"subtasks":[],"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Srdo","name":"Srdo","key":"srdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stig Rohde Døssing","active":true,"timeZone":"Europe/Copenhagen"},"customfield_12310291":null,"customfield_12310290":null,"aggregateprogress":{"progress":4800,"total":4800,"percent":100},"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":4800,"total":4800,"percent":100},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/comment/16305909","id":"16305909","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kabhwan","name":"kabhwan","key":"kabhwan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jungtaek Lim","active":true,"timeZone":"Asia/Seoul"},"body":"Thanks [~Srdo], I merged into master and 1.x-branch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kabhwan","name":"kabhwan","key":"kabhwan","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jungtaek Lim","active":true,"timeZone":"Asia/Seoul"},"created":"2017-12-29T02:19:33.369+0000","updated":"2017-12-29T02:19:33.369+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/STORM-2869/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":8,"worklogs":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/worklog/63521","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"GitHub user srdo opened a pull request:\n\n    https://github.com/apache/storm/pull/2481\n\n    STORM-2869: Only discard outdated records when adjusting KafkaConsume…\n\n    …r position during commit\r\n    \r\n    See https://issues.apache.org/jira/browse/STORM-2869\r\n    \r\n    I also deleted SingleTopicKafkaSpoutTest, because it is a duplicate of KafkaSpoutSingleTopicTest.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/srdo/storm STORM-2869\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/storm/pull/2481.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #2481\n    \n----\ncommit df5187e803d38f2cbb35ac9f1853ed7f2410ad0e\nAuthor: Stig Rohde Døssing <srdo@...>\nDate:   2017-12-26T19:55:30Z\n\n    STORM-2869: Only discard outdated records when adjusting KafkaConsumer position during commit\n\n----\n","created":"2017-12-26T22:19:01.816+0000","updated":"2017-12-26T22:19:01.816+0000","started":"2017-12-26T22:19:01.815+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"63521","issueId":"13127266"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/worklog/63593","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user danny0405 commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2481#discussion_r158784650\n  \n    --- Diff: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/SingleTopicKafkaSpoutTest.java ---\n    @@ -1,379 +0,0 @@\n    -/*\n    - * Licensed to the Apache Software Foundation (ASF) under one\n    - *   or more contributor license agreements.  See the NOTICE file\n    - *   distributed with this work for additional information\n    - *   regarding copyright ownership.  The ASF licenses this file\n    - *   to you under the Apache License, Version 2.0 (the\n    - *   \"License\"); you may not use this file except in compliance\n    - *   with the License.  You may obtain a copy of the License at\n    - *\n    - *   http://www.apache.org/licenses/LICENSE-2.0\n    - *\n    - *   Unless required by applicable law or agreed to in writing, software\n    - *   distributed under the License is distributed on an \"AS IS\" BASIS,\n    - *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    - *   See the License for the specific language governing permissions and\n    - *   limitations under the License.\n    - */\n    -package org.apache.storm.kafka.spout;\n    -\n    -import static org.hamcrest.CoreMatchers.is;\n    -import static org.hamcrest.MatcherAssert.assertThat;\n    -import static org.mockito.Mockito.mock;\n    -import static org.mockito.Mockito.never;\n    -import static org.mockito.Mockito.reset;\n    -import static org.mockito.Mockito.spy;\n    -import static org.mockito.Mockito.times;\n    -import static org.mockito.Mockito.verify;\n    -\n    -import java.util.HashMap;\n    -import java.util.HashSet;\n    -import java.util.List;\n    -import java.util.Map;\n    -import java.util.Set;\n    -import java.util.stream.IntStream;\n    -import org.apache.kafka.clients.consumer.KafkaConsumer;\n    -import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n    -import org.apache.kafka.common.TopicPartition;\n    -import org.apache.storm.kafka.KafkaUnitRule;\n    -import org.apache.storm.kafka.spout.config.builder.SingleTopicKafkaSpoutConfiguration;\n    -import org.apache.storm.kafka.spout.internal.KafkaConsumerFactoryDefault;\n    -import org.apache.storm.spout.SpoutOutputCollector;\n    -import org.apache.storm.task.TopologyContext;\n    -import org.apache.storm.tuple.Values;\n    -import org.apache.storm.utils.Time;\n    -import org.apache.storm.utils.Time.SimulatedTime;\n    -import org.junit.Before;\n    -import org.junit.Rule;\n    -import org.junit.Test;\n    -import org.mockito.ArgumentCaptor;\n    -import org.mockito.Captor;\n    -import org.mockito.MockitoAnnotations;\n    -\n    -import java.util.regex.Pattern;\n    -\n    -import static org.mockito.ArgumentMatchers.any;\n    -import static org.mockito.ArgumentMatchers.anyList;\n    -import static org.mockito.ArgumentMatchers.anyString;\n    -import static org.mockito.ArgumentMatchers.eq;\n    -\n    -import org.apache.kafka.clients.consumer.ConsumerConfig;\n    -import org.hamcrest.Matchers;\n    -\n    -public class SingleTopicKafkaSpoutTest {\n    -\n    -    @Rule\n    -    public KafkaUnitRule kafkaUnitRule = new KafkaUnitRule();\n    -\n    -    @Captor\n    -    private ArgumentCaptor<Map<TopicPartition, OffsetAndMetadata>> commitCapture;\n    -\n    -    private final TopologyContext topologyContext = mock(TopologyContext.class);\n    -    private final Map<String, Object> conf = new HashMap<>();\n    -    private final SpoutOutputCollector collector = mock(SpoutOutputCollector.class);\n    -    private final long commitOffsetPeriodMs = 2_000;\n    -    private final int maxRetries = 3;\n    -    private KafkaConsumer<String, String> consumerSpy;\n    -    private KafkaSpout<String, String> spout;\n    -    private final int maxPollRecords = 10;\n    -\n    -    @Before\n    -    public void setUp() {\n    -        MockitoAnnotations.initMocks(this);\n    -        KafkaSpoutConfig<String, String> spoutConfig =\n    -            SingleTopicKafkaSpoutConfiguration.setCommonSpoutConfig(\n    -                KafkaSpoutConfig.builder(\"127.0.0.1:\" + kafkaUnitRule.getKafkaUnit().getKafkaPort(),\n    -                    Pattern.compile(SingleTopicKafkaSpoutConfiguration.TOPIC)))\n    -                .setOffsetCommitPeriodMs(commitOffsetPeriodMs)\n    -                .setRetry(new KafkaSpoutRetryExponentialBackoff(KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(0), KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(0),\n    -                    maxRetries, KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(0)))\n    -                .setProp(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords)\n    -                .build();\n    -        this.consumerSpy = spy(new KafkaConsumerFactoryDefault<String, String>().createConsumer(spoutConfig));\n    -        this.spout = new KafkaSpout<>(spoutConfig, (ignored) -> consumerSpy);\n    -    }\n    -\n    -    private void prepareSpout(int messageCount) throws Exception {\n    -        SingleTopicKafkaUnitSetupHelper.populateTopicData(kafkaUnitRule.getKafkaUnit(), SingleTopicKafkaSpoutConfiguration.TOPIC, messageCount);\n    -        SingleTopicKafkaUnitSetupHelper.initializeSpout(spout, conf, topologyContext, collector);\n    -    }\n    -\n    -    @Test\n    -    public void testSeekToCommittedOffsetIfConsumerPositionIsBehindWhenCommitting() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = maxPollRecords * 2;\n    -            prepareSpout(messageCount);\n    -\n    -            //Emit all messages and fail the first one while acking the rest\n    -            for (int i = 0; i < messageCount; i++) {\n    -                spout.nextTuple();\n    -            }\n    -            ArgumentCaptor<KafkaSpoutMessageId> messageIdCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collector, times(messageCount)).emit(anyString(), anyList(), messageIdCaptor.capture());\n    -            List<KafkaSpoutMessageId> messageIds = messageIdCaptor.getAllValues();\n    -            for (int i = 1; i < messageIds.size(); i++) {\n    -                spout.ack(messageIds.get(i));\n    -            }\n    -            KafkaSpoutMessageId failedTuple = messageIds.get(0);\n    -            spout.fail(failedTuple);\n    -\n    -            //Advance the time and replay the failed tuple. \n    -            reset(collector);\n    -            spout.nextTuple();\n    -            ArgumentCaptor<KafkaSpoutMessageId> failedIdReplayCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collector).emit(anyString(), anyList(), failedIdReplayCaptor.capture());\n    -\n    -            assertThat(\"Expected replay of failed tuple\", failedIdReplayCaptor.getValue(), is(failedTuple));\n    -\n    -            /* Ack the tuple, and commit.\n    -             * Since the tuple is more than max poll records behind the most recent emitted tuple, the consumer won't catch up in this poll.\n    -             */\n    -            Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + commitOffsetPeriodMs);\n    -            spout.ack(failedIdReplayCaptor.getValue());\n    -            spout.nextTuple();\n    -            verify(consumerSpy).commitSync(commitCapture.capture());\n    -            \n    -            Map<TopicPartition, OffsetAndMetadata> capturedCommit = commitCapture.getValue();\n    -            TopicPartition expectedTp = new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 0);\n    -            assertThat(\"Should have committed to the right topic\", capturedCommit, Matchers.hasKey(expectedTp));\n    -            assertThat(\"Should have committed all the acked messages\", capturedCommit.get(expectedTp).offset(), is((long)messageCount));\n    -\n    -            /* Verify that the following acked (now committed) tuples are not emitted again\n    -             * Since the consumer position was somewhere in the middle of the acked tuples when the commit happened,\n    -             * this verifies that the spout keeps the consumer position ahead of the committed offset when committing\n    -             */\n    -            reset(collector);\n    -            //Just do a few polls to check that nothing more is emitted\n    -            for(int i = 0; i < 3; i++) {\n    -                spout.nextTuple();\n    -            }\n    -            verify(collector, never()).emit(any(), any(), any());\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldContinueWithSlowDoubleAcks() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = 20;\n    -            prepareSpout(messageCount);\n    -\n    -            //play 1st tuple\n    -            ArgumentCaptor<Object> messageIdToDoubleAck = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdToDoubleAck.capture());\n    -            spout.ack(messageIdToDoubleAck.getValue());\n    -\n    -            //Emit some more messages\n    -            IntStream.range(0, messageCount / 2).forEach(value -> {\n    -                spout.nextTuple();\n    -            });\n    -\n    -            spout.ack(messageIdToDoubleAck.getValue());\n    -\n    -            //Emit any remaining messages\n    -            IntStream.range(0, messageCount).forEach(value -> {\n    -                spout.nextTuple();\n    -            });\n    -\n    -            //Verify that all messages are emitted, ack all the messages\n    -            ArgumentCaptor<Object> messageIds = ArgumentCaptor.forClass(Object.class);\n    -            verify(collector, times(messageCount)).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM),\n    -                anyList(),\n    -                messageIds.capture());\n    -            messageIds.getAllValues().iterator().forEachRemaining(spout::ack);\n    -\n    -            Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\n    -            //Commit offsets\n    -            spout.nextTuple();\n    -\n    -            SingleTopicKafkaUnitSetupHelper.verifyAllMessagesCommitted(consumerSpy, commitCapture, messageCount);\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldEmitAllMessages() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = 10;\n    -            prepareSpout(messageCount);\n    -\n    -            //Emit all messages and check that they are emitted. Ack the messages too\n    -            IntStream.range(0, messageCount).forEach(value -> {\n    -                spout.nextTuple();\n    -                ArgumentCaptor<Object> messageId = ArgumentCaptor.forClass(Object.class);\n    -                verify(collector).emit(\n    -                    eq(SingleTopicKafkaSpoutConfiguration.STREAM),\n    -                    eq(new Values(SingleTopicKafkaSpoutConfiguration.TOPIC,\n    -                        Integer.toString(value),\n    -                        Integer.toString(value))),\n    -                    messageId.capture());\n    -                spout.ack(messageId.getValue());\n    -                reset(collector);\n    -            });\n    -\n    -            Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\n    -            //Commit offsets\n    -            spout.nextTuple();\n    -\n    -            SingleTopicKafkaUnitSetupHelper.verifyAllMessagesCommitted(consumerSpy, commitCapture, messageCount);\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldReplayInOrderFailedMessages() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = 10;\n    -            prepareSpout(messageCount);\n    -\n    -            //play and ack 1 tuple\n    -            ArgumentCaptor<Object> messageIdAcked = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdAcked.capture());\n    -            spout.ack(messageIdAcked.getValue());\n    -            reset(collector);\n    -\n    -            //play and fail 1 tuple\n    -            ArgumentCaptor<Object> messageIdFailed = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdFailed.capture());\n    -            spout.fail(messageIdFailed.getValue());\n    -            reset(collector);\n    -\n    -            //Emit all remaining messages. Failed tuples retry immediately with current configuration, so no need to wait.\n    -            IntStream.range(0, messageCount).forEach(value -> {\n    -                spout.nextTuple();\n    -            });\n    -\n    -            ArgumentCaptor<Object> remainingMessageIds = ArgumentCaptor.forClass(Object.class);\n    -            //All messages except the first acked message should have been emitted\n    -            verify(collector, times(messageCount - 1)).emit(\n    -                eq(SingleTopicKafkaSpoutConfiguration.STREAM),\n    -                anyList(),\n    -                remainingMessageIds.capture());\n    -            remainingMessageIds.getAllValues().iterator().forEachRemaining(spout::ack);\n    -\n    -            Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\n    -            //Commit offsets\n    -            spout.nextTuple();\n    -\n    -            SingleTopicKafkaUnitSetupHelper.verifyAllMessagesCommitted(consumerSpy, commitCapture, messageCount);\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldReplayFirstTupleFailedOutOfOrder() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = 10;\n    -            prepareSpout(messageCount);\n    -\n    -            //play 1st tuple\n    -            ArgumentCaptor<Object> messageIdToFail = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdToFail.capture());\n    -            reset(collector);\n    -\n    -            //play 2nd tuple\n    -            ArgumentCaptor<Object> messageIdToAck = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdToAck.capture());\n    -            reset(collector);\n    -\n    -            //ack 2nd tuple\n    -            spout.ack(messageIdToAck.getValue());\n    -            //fail 1st tuple\n    -            spout.fail(messageIdToFail.getValue());\n    -\n    -            //Emit all remaining messages. Failed tuples retry immediately with current configuration, so no need to wait.\n    -            IntStream.range(0, messageCount).forEach(value -> {\n    -                spout.nextTuple();\n    -            });\n    -\n    -            ArgumentCaptor<Object> remainingIds = ArgumentCaptor.forClass(Object.class);\n    -            //All messages except the first acked message should have been emitted\n    -            verify(collector, times(messageCount - 1)).emit(\n    -                eq(SingleTopicKafkaSpoutConfiguration.STREAM),\n    -                anyList(),\n    -                remainingIds.capture());\n    -            remainingIds.getAllValues().iterator().forEachRemaining(spout::ack);\n    -\n    -            Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\n    -            //Commit offsets\n    -            spout.nextTuple();\n    -\n    -            SingleTopicKafkaUnitSetupHelper.verifyAllMessagesCommitted(consumerSpy, commitCapture, messageCount);\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldReplayAllFailedTuplesWhenFailedOutOfOrder() throws Exception {\n    -        //The spout must reemit retriable tuples, even if they fail out of order.\n    -        //The spout should be able to skip tuples it has already emitted when retrying messages, even if those tuples are also retries.\n    -        int messageCount = 10;\n    -        prepareSpout(messageCount);\n    -\n    -        //play all tuples\n    -        for (int i = 0; i < messageCount; i++) {\n    -            spout.nextTuple();\n    -        }\n    -        ArgumentCaptor<KafkaSpoutMessageId> messageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -        verify(collector, times(messageCount)).emit(anyString(), anyList(), messageIds.capture());\n    -        reset(collector);\n    -        //Fail tuple 5 and 3, call nextTuple, then fail tuple 2\n    -        List<KafkaSpoutMessageId> capturedMessageIds = messageIds.getAllValues();\n    -        spout.fail(capturedMessageIds.get(5));\n    -        spout.fail(capturedMessageIds.get(3));\n    -        spout.nextTuple();\n    -        spout.fail(capturedMessageIds.get(2));\n    -\n    -        //Check that the spout will reemit all 3 failed tuples and no other tuples\n    -        ArgumentCaptor<KafkaSpoutMessageId> reemittedMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -        for (int i = 0; i < messageCount; i++) {\n    -            spout.nextTuple();\n    -        }\n    -        verify(collector, times(3)).emit(anyString(), anyList(), reemittedMessageIds.capture());\n    -        Set<KafkaSpoutMessageId> expectedReemitIds = new HashSet<>();\n    -        expectedReemitIds.add(capturedMessageIds.get(5));\n    -        expectedReemitIds.add(capturedMessageIds.get(3));\n    -        expectedReemitIds.add(capturedMessageIds.get(2));\n    -        assertThat(\"Expected reemits to be the 3 failed tuples\", new HashSet<>(reemittedMessageIds.getAllValues()), is(expectedReemitIds));\n    -    }\n    -\n    -    @Test\n    -    public void testShouldDropMessagesAfterMaxRetriesAreReached() throws Exception {\n    -        //Check that if one message fails repeatedly, the retry cap limits how many times the message can be reemitted\n    -        int messageCount = 1;\n    -        prepareSpout(messageCount);\n    -\n    -        //Emit and fail the same tuple until we've reached retry limit\n    -        for (int i = 0; i <= maxRetries; i++) {\n    -            ArgumentCaptor<KafkaSpoutMessageId> messageIdFailed = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdFailed.capture());\n    -            KafkaSpoutMessageId msgId = messageIdFailed.getValue();\n    -            spout.fail(msgId);\n    -            assertThat(\"Expected message id number of failures to match the number of times the message has failed\", msgId.numFails(), is(i + 1));\n    -            reset(collector);\n    -        }\n    -\n    -        //Verify that the tuple is not emitted again\n    -        spout.nextTuple();\n    -        verify(collector, never()).emit(any(), any(), any());\n    -    }\n    -\n    -    @Test\n    -    public void testSpoutMustRefreshPartitionsEvenIfNotPolling() throws Exception {\n    -        try (SimulatedTime time = new SimulatedTime()) {\n    -            SingleTopicKafkaUnitSetupHelper.initializeSpout(spout, conf, topologyContext, collector);\n    -\n    -            //Nothing is assigned yet, should emit nothing\n    -            spout.nextTuple();\n    -            verify(collector, never()).emit(anyString(), anyList(), any(KafkaSpoutMessageId.class));\n    -\n    -            SingleTopicKafkaUnitSetupHelper.populateTopicData(kafkaUnitRule.getKafkaUnit(), SingleTopicKafkaSpoutConfiguration.TOPIC, 1);\n    -            Time.advanceTime(KafkaSpoutConfig.DEFAULT_PARTITION_REFRESH_PERIOD_MS + KafkaSpout.TIMER_DELAY_MS);\n    -\n    -            //The new partition should be discovered and the message should be emitted\n    -            spout.nextTuple();\n    --- End diff --\n    \n    Why delete the UT for single topic test?\n","created":"2017-12-27T09:14:07.530+0000","updated":"2017-12-27T09:14:07.530+0000","started":"2017-12-27T09:14:07.529+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"63593","issueId":"13127266"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/worklog/63597","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on a diff in the pull request:\n\n    https://github.com/apache/storm/pull/2481#discussion_r158788258\n  \n    --- Diff: external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/SingleTopicKafkaSpoutTest.java ---\n    @@ -1,379 +0,0 @@\n    -/*\n    - * Licensed to the Apache Software Foundation (ASF) under one\n    - *   or more contributor license agreements.  See the NOTICE file\n    - *   distributed with this work for additional information\n    - *   regarding copyright ownership.  The ASF licenses this file\n    - *   to you under the Apache License, Version 2.0 (the\n    - *   \"License\"); you may not use this file except in compliance\n    - *   with the License.  You may obtain a copy of the License at\n    - *\n    - *   http://www.apache.org/licenses/LICENSE-2.0\n    - *\n    - *   Unless required by applicable law or agreed to in writing, software\n    - *   distributed under the License is distributed on an \"AS IS\" BASIS,\n    - *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    - *   See the License for the specific language governing permissions and\n    - *   limitations under the License.\n    - */\n    -package org.apache.storm.kafka.spout;\n    -\n    -import static org.hamcrest.CoreMatchers.is;\n    -import static org.hamcrest.MatcherAssert.assertThat;\n    -import static org.mockito.Mockito.mock;\n    -import static org.mockito.Mockito.never;\n    -import static org.mockito.Mockito.reset;\n    -import static org.mockito.Mockito.spy;\n    -import static org.mockito.Mockito.times;\n    -import static org.mockito.Mockito.verify;\n    -\n    -import java.util.HashMap;\n    -import java.util.HashSet;\n    -import java.util.List;\n    -import java.util.Map;\n    -import java.util.Set;\n    -import java.util.stream.IntStream;\n    -import org.apache.kafka.clients.consumer.KafkaConsumer;\n    -import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n    -import org.apache.kafka.common.TopicPartition;\n    -import org.apache.storm.kafka.KafkaUnitRule;\n    -import org.apache.storm.kafka.spout.config.builder.SingleTopicKafkaSpoutConfiguration;\n    -import org.apache.storm.kafka.spout.internal.KafkaConsumerFactoryDefault;\n    -import org.apache.storm.spout.SpoutOutputCollector;\n    -import org.apache.storm.task.TopologyContext;\n    -import org.apache.storm.tuple.Values;\n    -import org.apache.storm.utils.Time;\n    -import org.apache.storm.utils.Time.SimulatedTime;\n    -import org.junit.Before;\n    -import org.junit.Rule;\n    -import org.junit.Test;\n    -import org.mockito.ArgumentCaptor;\n    -import org.mockito.Captor;\n    -import org.mockito.MockitoAnnotations;\n    -\n    -import java.util.regex.Pattern;\n    -\n    -import static org.mockito.ArgumentMatchers.any;\n    -import static org.mockito.ArgumentMatchers.anyList;\n    -import static org.mockito.ArgumentMatchers.anyString;\n    -import static org.mockito.ArgumentMatchers.eq;\n    -\n    -import org.apache.kafka.clients.consumer.ConsumerConfig;\n    -import org.hamcrest.Matchers;\n    -\n    -public class SingleTopicKafkaSpoutTest {\n    -\n    -    @Rule\n    -    public KafkaUnitRule kafkaUnitRule = new KafkaUnitRule();\n    -\n    -    @Captor\n    -    private ArgumentCaptor<Map<TopicPartition, OffsetAndMetadata>> commitCapture;\n    -\n    -    private final TopologyContext topologyContext = mock(TopologyContext.class);\n    -    private final Map<String, Object> conf = new HashMap<>();\n    -    private final SpoutOutputCollector collector = mock(SpoutOutputCollector.class);\n    -    private final long commitOffsetPeriodMs = 2_000;\n    -    private final int maxRetries = 3;\n    -    private KafkaConsumer<String, String> consumerSpy;\n    -    private KafkaSpout<String, String> spout;\n    -    private final int maxPollRecords = 10;\n    -\n    -    @Before\n    -    public void setUp() {\n    -        MockitoAnnotations.initMocks(this);\n    -        KafkaSpoutConfig<String, String> spoutConfig =\n    -            SingleTopicKafkaSpoutConfiguration.setCommonSpoutConfig(\n    -                KafkaSpoutConfig.builder(\"127.0.0.1:\" + kafkaUnitRule.getKafkaUnit().getKafkaPort(),\n    -                    Pattern.compile(SingleTopicKafkaSpoutConfiguration.TOPIC)))\n    -                .setOffsetCommitPeriodMs(commitOffsetPeriodMs)\n    -                .setRetry(new KafkaSpoutRetryExponentialBackoff(KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(0), KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(0),\n    -                    maxRetries, KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(0)))\n    -                .setProp(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords)\n    -                .build();\n    -        this.consumerSpy = spy(new KafkaConsumerFactoryDefault<String, String>().createConsumer(spoutConfig));\n    -        this.spout = new KafkaSpout<>(spoutConfig, (ignored) -> consumerSpy);\n    -    }\n    -\n    -    private void prepareSpout(int messageCount) throws Exception {\n    -        SingleTopicKafkaUnitSetupHelper.populateTopicData(kafkaUnitRule.getKafkaUnit(), SingleTopicKafkaSpoutConfiguration.TOPIC, messageCount);\n    -        SingleTopicKafkaUnitSetupHelper.initializeSpout(spout, conf, topologyContext, collector);\n    -    }\n    -\n    -    @Test\n    -    public void testSeekToCommittedOffsetIfConsumerPositionIsBehindWhenCommitting() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = maxPollRecords * 2;\n    -            prepareSpout(messageCount);\n    -\n    -            //Emit all messages and fail the first one while acking the rest\n    -            for (int i = 0; i < messageCount; i++) {\n    -                spout.nextTuple();\n    -            }\n    -            ArgumentCaptor<KafkaSpoutMessageId> messageIdCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collector, times(messageCount)).emit(anyString(), anyList(), messageIdCaptor.capture());\n    -            List<KafkaSpoutMessageId> messageIds = messageIdCaptor.getAllValues();\n    -            for (int i = 1; i < messageIds.size(); i++) {\n    -                spout.ack(messageIds.get(i));\n    -            }\n    -            KafkaSpoutMessageId failedTuple = messageIds.get(0);\n    -            spout.fail(failedTuple);\n    -\n    -            //Advance the time and replay the failed tuple. \n    -            reset(collector);\n    -            spout.nextTuple();\n    -            ArgumentCaptor<KafkaSpoutMessageId> failedIdReplayCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            verify(collector).emit(anyString(), anyList(), failedIdReplayCaptor.capture());\n    -\n    -            assertThat(\"Expected replay of failed tuple\", failedIdReplayCaptor.getValue(), is(failedTuple));\n    -\n    -            /* Ack the tuple, and commit.\n    -             * Since the tuple is more than max poll records behind the most recent emitted tuple, the consumer won't catch up in this poll.\n    -             */\n    -            Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + commitOffsetPeriodMs);\n    -            spout.ack(failedIdReplayCaptor.getValue());\n    -            spout.nextTuple();\n    -            verify(consumerSpy).commitSync(commitCapture.capture());\n    -            \n    -            Map<TopicPartition, OffsetAndMetadata> capturedCommit = commitCapture.getValue();\n    -            TopicPartition expectedTp = new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 0);\n    -            assertThat(\"Should have committed to the right topic\", capturedCommit, Matchers.hasKey(expectedTp));\n    -            assertThat(\"Should have committed all the acked messages\", capturedCommit.get(expectedTp).offset(), is((long)messageCount));\n    -\n    -            /* Verify that the following acked (now committed) tuples are not emitted again\n    -             * Since the consumer position was somewhere in the middle of the acked tuples when the commit happened,\n    -             * this verifies that the spout keeps the consumer position ahead of the committed offset when committing\n    -             */\n    -            reset(collector);\n    -            //Just do a few polls to check that nothing more is emitted\n    -            for(int i = 0; i < 3; i++) {\n    -                spout.nextTuple();\n    -            }\n    -            verify(collector, never()).emit(any(), any(), any());\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldContinueWithSlowDoubleAcks() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = 20;\n    -            prepareSpout(messageCount);\n    -\n    -            //play 1st tuple\n    -            ArgumentCaptor<Object> messageIdToDoubleAck = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdToDoubleAck.capture());\n    -            spout.ack(messageIdToDoubleAck.getValue());\n    -\n    -            //Emit some more messages\n    -            IntStream.range(0, messageCount / 2).forEach(value -> {\n    -                spout.nextTuple();\n    -            });\n    -\n    -            spout.ack(messageIdToDoubleAck.getValue());\n    -\n    -            //Emit any remaining messages\n    -            IntStream.range(0, messageCount).forEach(value -> {\n    -                spout.nextTuple();\n    -            });\n    -\n    -            //Verify that all messages are emitted, ack all the messages\n    -            ArgumentCaptor<Object> messageIds = ArgumentCaptor.forClass(Object.class);\n    -            verify(collector, times(messageCount)).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM),\n    -                anyList(),\n    -                messageIds.capture());\n    -            messageIds.getAllValues().iterator().forEachRemaining(spout::ack);\n    -\n    -            Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\n    -            //Commit offsets\n    -            spout.nextTuple();\n    -\n    -            SingleTopicKafkaUnitSetupHelper.verifyAllMessagesCommitted(consumerSpy, commitCapture, messageCount);\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldEmitAllMessages() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = 10;\n    -            prepareSpout(messageCount);\n    -\n    -            //Emit all messages and check that they are emitted. Ack the messages too\n    -            IntStream.range(0, messageCount).forEach(value -> {\n    -                spout.nextTuple();\n    -                ArgumentCaptor<Object> messageId = ArgumentCaptor.forClass(Object.class);\n    -                verify(collector).emit(\n    -                    eq(SingleTopicKafkaSpoutConfiguration.STREAM),\n    -                    eq(new Values(SingleTopicKafkaSpoutConfiguration.TOPIC,\n    -                        Integer.toString(value),\n    -                        Integer.toString(value))),\n    -                    messageId.capture());\n    -                spout.ack(messageId.getValue());\n    -                reset(collector);\n    -            });\n    -\n    -            Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\n    -            //Commit offsets\n    -            spout.nextTuple();\n    -\n    -            SingleTopicKafkaUnitSetupHelper.verifyAllMessagesCommitted(consumerSpy, commitCapture, messageCount);\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldReplayInOrderFailedMessages() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = 10;\n    -            prepareSpout(messageCount);\n    -\n    -            //play and ack 1 tuple\n    -            ArgumentCaptor<Object> messageIdAcked = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdAcked.capture());\n    -            spout.ack(messageIdAcked.getValue());\n    -            reset(collector);\n    -\n    -            //play and fail 1 tuple\n    -            ArgumentCaptor<Object> messageIdFailed = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdFailed.capture());\n    -            spout.fail(messageIdFailed.getValue());\n    -            reset(collector);\n    -\n    -            //Emit all remaining messages. Failed tuples retry immediately with current configuration, so no need to wait.\n    -            IntStream.range(0, messageCount).forEach(value -> {\n    -                spout.nextTuple();\n    -            });\n    -\n    -            ArgumentCaptor<Object> remainingMessageIds = ArgumentCaptor.forClass(Object.class);\n    -            //All messages except the first acked message should have been emitted\n    -            verify(collector, times(messageCount - 1)).emit(\n    -                eq(SingleTopicKafkaSpoutConfiguration.STREAM),\n    -                anyList(),\n    -                remainingMessageIds.capture());\n    -            remainingMessageIds.getAllValues().iterator().forEachRemaining(spout::ack);\n    -\n    -            Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\n    -            //Commit offsets\n    -            spout.nextTuple();\n    -\n    -            SingleTopicKafkaUnitSetupHelper.verifyAllMessagesCommitted(consumerSpy, commitCapture, messageCount);\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldReplayFirstTupleFailedOutOfOrder() throws Exception {\n    -        try (SimulatedTime simulatedTime = new SimulatedTime()) {\n    -            int messageCount = 10;\n    -            prepareSpout(messageCount);\n    -\n    -            //play 1st tuple\n    -            ArgumentCaptor<Object> messageIdToFail = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdToFail.capture());\n    -            reset(collector);\n    -\n    -            //play 2nd tuple\n    -            ArgumentCaptor<Object> messageIdToAck = ArgumentCaptor.forClass(Object.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdToAck.capture());\n    -            reset(collector);\n    -\n    -            //ack 2nd tuple\n    -            spout.ack(messageIdToAck.getValue());\n    -            //fail 1st tuple\n    -            spout.fail(messageIdToFail.getValue());\n    -\n    -            //Emit all remaining messages. Failed tuples retry immediately with current configuration, so no need to wait.\n    -            IntStream.range(0, messageCount).forEach(value -> {\n    -                spout.nextTuple();\n    -            });\n    -\n    -            ArgumentCaptor<Object> remainingIds = ArgumentCaptor.forClass(Object.class);\n    -            //All messages except the first acked message should have been emitted\n    -            verify(collector, times(messageCount - 1)).emit(\n    -                eq(SingleTopicKafkaSpoutConfiguration.STREAM),\n    -                anyList(),\n    -                remainingIds.capture());\n    -            remainingIds.getAllValues().iterator().forEachRemaining(spout::ack);\n    -\n    -            Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\n    -            //Commit offsets\n    -            spout.nextTuple();\n    -\n    -            SingleTopicKafkaUnitSetupHelper.verifyAllMessagesCommitted(consumerSpy, commitCapture, messageCount);\n    -        }\n    -    }\n    -\n    -    @Test\n    -    public void testShouldReplayAllFailedTuplesWhenFailedOutOfOrder() throws Exception {\n    -        //The spout must reemit retriable tuples, even if they fail out of order.\n    -        //The spout should be able to skip tuples it has already emitted when retrying messages, even if those tuples are also retries.\n    -        int messageCount = 10;\n    -        prepareSpout(messageCount);\n    -\n    -        //play all tuples\n    -        for (int i = 0; i < messageCount; i++) {\n    -            spout.nextTuple();\n    -        }\n    -        ArgumentCaptor<KafkaSpoutMessageId> messageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -        verify(collector, times(messageCount)).emit(anyString(), anyList(), messageIds.capture());\n    -        reset(collector);\n    -        //Fail tuple 5 and 3, call nextTuple, then fail tuple 2\n    -        List<KafkaSpoutMessageId> capturedMessageIds = messageIds.getAllValues();\n    -        spout.fail(capturedMessageIds.get(5));\n    -        spout.fail(capturedMessageIds.get(3));\n    -        spout.nextTuple();\n    -        spout.fail(capturedMessageIds.get(2));\n    -\n    -        //Check that the spout will reemit all 3 failed tuples and no other tuples\n    -        ArgumentCaptor<KafkaSpoutMessageId> reemittedMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -        for (int i = 0; i < messageCount; i++) {\n    -            spout.nextTuple();\n    -        }\n    -        verify(collector, times(3)).emit(anyString(), anyList(), reemittedMessageIds.capture());\n    -        Set<KafkaSpoutMessageId> expectedReemitIds = new HashSet<>();\n    -        expectedReemitIds.add(capturedMessageIds.get(5));\n    -        expectedReemitIds.add(capturedMessageIds.get(3));\n    -        expectedReemitIds.add(capturedMessageIds.get(2));\n    -        assertThat(\"Expected reemits to be the 3 failed tuples\", new HashSet<>(reemittedMessageIds.getAllValues()), is(expectedReemitIds));\n    -    }\n    -\n    -    @Test\n    -    public void testShouldDropMessagesAfterMaxRetriesAreReached() throws Exception {\n    -        //Check that if one message fails repeatedly, the retry cap limits how many times the message can be reemitted\n    -        int messageCount = 1;\n    -        prepareSpout(messageCount);\n    -\n    -        //Emit and fail the same tuple until we've reached retry limit\n    -        for (int i = 0; i <= maxRetries; i++) {\n    -            ArgumentCaptor<KafkaSpoutMessageId> messageIdFailed = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n    -            spout.nextTuple();\n    -            verify(collector).emit(anyString(), anyList(), messageIdFailed.capture());\n    -            KafkaSpoutMessageId msgId = messageIdFailed.getValue();\n    -            spout.fail(msgId);\n    -            assertThat(\"Expected message id number of failures to match the number of times the message has failed\", msgId.numFails(), is(i + 1));\n    -            reset(collector);\n    -        }\n    -\n    -        //Verify that the tuple is not emitted again\n    -        spout.nextTuple();\n    -        verify(collector, never()).emit(any(), any(), any());\n    -    }\n    -\n    -    @Test\n    -    public void testSpoutMustRefreshPartitionsEvenIfNotPolling() throws Exception {\n    -        try (SimulatedTime time = new SimulatedTime()) {\n    -            SingleTopicKafkaUnitSetupHelper.initializeSpout(spout, conf, topologyContext, collector);\n    -\n    -            //Nothing is assigned yet, should emit nothing\n    -            spout.nextTuple();\n    -            verify(collector, never()).emit(anyString(), anyList(), any(KafkaSpoutMessageId.class));\n    -\n    -            SingleTopicKafkaUnitSetupHelper.populateTopicData(kafkaUnitRule.getKafkaUnit(), SingleTopicKafkaSpoutConfiguration.TOPIC, 1);\n    -            Time.advanceTime(KafkaSpoutConfig.DEFAULT_PARTITION_REFRESH_PERIOD_MS + KafkaSpout.TIMER_DELAY_MS);\n    -\n    -            //The new partition should be discovered and the message should be emitted\n    -            spout.nextTuple();\n    --- End diff --\n    \n    It was renamed here, but the old file wasn't deleted https://github.com/apache/storm/pull/2466/files#diff-b827d459a46f189dced7382d2d551439\n","created":"2017-12-27T09:42:30.299+0000","updated":"2017-12-27T09:42:30.299+0000","started":"2017-12-27T09:42:30.297+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"63597","issueId":"13127266"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/worklog/63827","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"GitHub user srdo opened a pull request:\n\n    https://github.com/apache/storm/pull/2484\n\n    STORM-2869 1.x\n\n    https://github.com/apache/storm/pull/2481 for 1.x\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/srdo/storm STORM-2869-1.x\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/storm/pull/2484.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #2484\n    \n----\ncommit 8129368771341d33fb7d36754c29351a86bc49a0\nAuthor: Stig Rohde Døssing <srdo@...>\nDate:   2017-12-26T19:55:30Z\n\n    STORM-2869: Only discard outdated records when adjusting KafkaConsumer position during commit\n\ncommit bcec474ae27a9524f69f5d68a88e9ab1e0eb4bf9\nAuthor: Stig Rohde Døssing <srdo@...>\nDate:   2017-12-28T09:32:45Z\n\n    Backport\n\n----\n","created":"2017-12-28T09:33:39.261+0000","updated":"2017-12-28T09:33:39.261+0000","started":"2017-12-28T09:33:39.260+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"63827","issueId":"13127266"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/worklog/63828","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user srdo commented on the issue:\n\n    https://github.com/apache/storm/pull/2481\n  \n    Thanks for the reviews. 1.x version here https://github.com/apache/storm/pull/2484\n","created":"2017-12-28T09:33:52.552+0000","updated":"2017-12-28T09:33:52.552+0000","started":"2017-12-28T09:33:52.551+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"63828","issueId":"13127266"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/worklog/63840","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user HeartSaVioR commented on the issue:\n\n    https://github.com/apache/storm/pull/2484\n  \n    +1\r\n    @srdo Please squash commits into one. We could merge it without 24hrs for backport case.\n","created":"2017-12-28T10:48:01.822+0000","updated":"2017-12-28T10:48:01.822+0000","started":"2017-12-28T10:48:01.821+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"63840","issueId":"13127266"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/worklog/64002","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user asfgit closed the pull request at:\n\n    https://github.com/apache/storm/pull/2484\n","created":"2017-12-29T02:18:08.955+0000","updated":"2017-12-29T02:18:08.955+0000","started":"2017-12-29T02:18:08.955+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"64002","issueId":"13127266"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13127266/worklog/64003","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=githubbot","name":"githubbot","key":"githubbot","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"ASF GitHub Bot","active":true,"timeZone":"Etc/UTC"},"comment":"Github user asfgit closed the pull request at:\n\n    https://github.com/apache/storm/pull/2481\n","created":"2017-12-29T02:18:24.449+0000","updated":"2017-12-29T02:18:24.449+0000","started":"2017-12-29T02:18:24.449+0000","timeSpent":"10m","timeSpentSeconds":600,"id":"64003","issueId":"13127266"}]},"customfield_12311820":"0|i3oarz:"}}