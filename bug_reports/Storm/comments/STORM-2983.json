[[~ethanli] 

I notice two issues with this setting:
{code:java}
storm.scheduler: "org.apache.storm.scheduler.resource.ResourceAwareScheduler"{code}
*1) Wrong Number of workers:*

With this setting applied, storm is spinning up the wrong number of workers for this command line:
{code:java}
bin/storm jar storm-loadgen-2.0.0-SNAPSHOT.jar  org.apache.storm.loadgen.ThroughputVsLatency  --name tvl  --spouts 1 --splitters 2 --counters 1   -c topology.workers=1{code}
It actually spins up 2 workers instead of 1. The acker is isolated into its own separate worker process while the remaining spouts and bolts sit in a single worker. 

In 2306, if worker count is 1, the inter-worker transfer thread will not be spun up.  [Reference|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java#L219-L222].

Thats why the problem was not noticed previously.

 

*2) That setting is internally unkown.* 

Strangely, In the code I dont see any setting with the name `storm.scheduler` .  I do see something called `topology.scheduler.strategy`. Its a bit puzzling why an unknown setting makes any impact., [~roshan_naik] 

Thanks very much for looking into it.

1)

With ResourceAwareScheduler, the number of worker is actually calculated based on resources. `topology.workers` has no impact here.  As you pointed out, I believe the calculated `topology.workers` is not populated to Worker.

2)

We do have the config `storm.scheduler`:  [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/DaemonConfig.java#L116]

 , Maybe a good idea to add an a info level log message somewhere when not honoring the worker count., Yes you are right. It can be confusing., [~roshan_naik] Are you still working on fixing this issue? Or do you want me to put up a fix for it? Thanks., Go  ahead. I didn’t take it up since the changes  are in the RAS code path. 
, [~roshan_naik] Do you think it's OK to just get rid of the special case, i.e. no matter how many workers there are, we launch the inter-worker transfer thread?  i think this is the easiest way to do. , [~ethanli]

I also think that's OK to get rid of optimization for such case. Most of use cases for Storm should be multi-workers., [~kabhwan] Thanks. Will file a pull request quickly, [~ethanli]

We have a problem in Storm with an excessive number of threads. See details in STORM-2647 for how bad the issue is. In general, best to avoid having useless threads lingering around as they are not free in terms of energy consumption in in the long run and keep bumping off useful threads from their cores periodically.

IMO, the right approach is to fix this by addressing the fundamental issue of worker count reflecting the true state of the topology. Having the settings reflect the right values is also useful for topologies (as they can query them to find out the state of the topology). This would have to be fixed sooner or later.

 , While I agree that reflecting correct worker count is better, I'm OK for that once UI show the information correctly, with proper notice for RAS.

I don't see specific reason to optimize single worker case. What Storm basically provides is "distributed realtime computation system", not a single JVM process to run faster. Such optimization was effective only for single worker case which I think is minor and looks like only for benchmarking. I guess we concerned about inter-worker latency (as well as acker) which looks like the thing to focus., [~kabhwan] 

IMO this should not be viewed as a sidestepping an minor optimization. In which case it does look like a minor issue. The optimization only exposed a bug. So it is only the "messenger".

The key issue here is supporting (internal or user code) to check the state correctly .. to do whatever it needs to do .... optimization or behavior change. 

By not fixing the core bug, we wlll sidestep the core issue. IMO this a critical bug and needs to fixed. Or we need to provide a diff mechanism for code to make such checks correctly.

 , [~roshan_naik] [topologyConf|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java#L143] in the code is read from supervisor which is the  stormconf.ser from user's submission.  Some of the configs in user's submission serves as a hint. topologyConf here won't reflect any dynamically calculated values. We might need to check the usage of topologyConf and make sure it's not used for some dynamically calculated values like this. As for this particular issue, I agree with [~kabhwan] that the optimization for singe worker case is not really an issue., Removing / preserving the optimization will not address the core problem ... which is .. topology.workers is not usable and therefore any code (present/future) would be considered broken with no workaround.

There are other use cases in code for this setting plus more that I can think of, but to limit the scope I will stay with the topic of some settings in topoConf being unreliable. I dont think its a good idea to say any such code should not allowed in storm.

 

Since topoConf is part of the API  ...  it is surprising to know that any setting stored in it cannot be relied upon. The  [documentation|http://storm.apache.org/releases/1.2.1/Configuration.html] specifically states that the system supports overriding settings.  *The actual bug here is that we are not dynamically overriding the setting to what it should be.*

I think we need to have a good justification for making any setting unreliable and not open the door for more of the same.

Would like to know:
 - Is there good reason why this setting cannot be made reliable ?

 - Are you aware of other such settings that are not reflecting the correct values ?

 , [~roshan_naik] [~ethanli]

Looks like this is about how we provide users to specify resources which are needed for running topology.

Other than RAS, we assume there’re homogeneous JVM processes (fixed memory, no restriction on CPU) so overall available memory for topology is configurable via how much memory end-users assigns to single worker and how many workers end-users assigns to topology.

This is no longer true in RAS, and maybe container environment, too. If my understanding is correct, the major point on RAS / container scheduler is fully utilizing resources in cluster, so user mainly configures how much resources each component/worker need, and physical plan and placement is left to scheduler.

I also agree topology configuration should be considered to public API. So if such configuration is not effective to RAS, RAS document should mention such behavior. Even better if we explain why it ignores such configuration.

I’m cc.ing [~revans2] since I think he is the best folk to answer about RAS, and also cc.ing [~erikdw] to see how storm-mesos works for scheduling., Happy to add my 2 cents on this.

 

The point of RAS is for multiple reasons. Yes one of them is to allow the cluster to be more utilized by having finer grained resource scheduling.

 

When we added in RAS we did stop honoring {{topology.workers}}.  There are only a handfull of places where it is used outside of the scheduler, so it didn't turn out to be that big of a deal.  The biggest issue we ran into was around the default number of ackers.  In really old versions of storm it was 1, but then was updated to be the number of workers.  We worked around this by having RAS "guess" an approximate number of workers as an alternative default.

[https://github.com/apache/storm/blob/402a371ccdb39ccd7146fe9743e91ca36fee6d15/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L2911-L2924]

 

For me there are two different things that need to be addressed.

 

1) we need to update the documentation for {{topology.workers}} to better reflect how it is actually used.

https://github.com/apache/storm/blob/402a371ccdb39ccd7146fe9743e91ca36fee6d15/storm-client/src/jvm/org/apache/storm/Config.java#L194-L199

 

2) We need to fix the current issue. 

We can do that by either not relying on the config and instead looking to see if all of the executors for the topology are in the current worker, by turning it off like the current patch does, or by having RAS do a similar "guess" about the actual number of workers, but making sure that we always set it to at least 2, because we don't know how many there really will be., If we really want to address this via not removing micro optimization but fixing it, it can be done in the right way. It would be better to check whether there're outgoing tasks/connections in each worker, not just checking topology worker count., I think it's not worth it to add complexity for this trivial optimization. How about filing a separate Jira for this RAS bug and use [https://github.com/apache/storm/pull/2614] for this particular bug?, As stated before, the core issue is not the specific optimization. Along with removing this optimization we would have to remove all other code that checks the same. It is impt to get RAS working but needs to be done correctly. 

My concern is that (independent of the existence/absence of this optimization)...  the mechanism to check the worker count by storm internal code or end user code is broken. Fixing that will address RAS as well as does not need to remove similar code.

So would like to ask my prev question again:

 
 - Is there good reason why topology.workers cannot be dynamically updated to reflect the actual worker count. 

 , We definitely need to read through the code and see what RAS breaks and fix them. That's why I suggested to file a separate Jira to track it and have somebody to do it.  I want to get this in because it blocks my current work for quite a long time and I want to get that done sooner than later, if this is OK with you., My Q was not about finding and fixing all things that RAS breaks. It is limited to fixing this issue with the worker count that is causing the breakage. Any code we try to delete now to unblock you would be useful to revive once the worker count issue is fixed. 

- Instead of the proposed fix, can you update the worker count to the right value ?

- else, could you consider unblocking your work by commenting out the optimization in your local build ?, I'm still +1 to the proposed fix. Even better if we can avoid removing the optimization code with determining there's outgoing connection, but it's OK to skip if it is not trivial.

Topology configuration is what users set up the values, and users will get confused if some of them are dynamically updated. And we already know that not all the configurations may be valid according to the scheduler. "topology.workers" is popular one so it doesn't look trivial to ignore such value, but I think we can explain why it is ignored in RAS document.

If we really want to maintain runtime status, better to have separated one instead of modifying topology configuration to retain users' input., [~kabhwan] i think you are again missing what I am stressing. 

We need a way to in code to check the worker count (for internal and user code). Not be removing code that does such checks. I am not concerned about retaining this one optimization. 

But there is no point removing reasonable code and then put it back again.

I would like to see why we cannot either fix the topology.workers or  provide something else as substitute.

So i ask again... why cant we fix this setting. , If you're not concerned about retaining the optimization then let's remove it. As explained enough (single worker should be minor one in production use cases), I think we don't need to put it back again. Other concerns look like the things to discuss, since we don't look like having consensus. That's why [~ethanli] would like to file new issues regarding them, and IMHO I think it may be better to initiate them in dev. mailing list so that we can share the concerns and discuss them.

As you seem to think I'm missing, I may need to explain my thought below:

1. Even we want to retain the optimization, I think I proposed "the right way" to do it in my comment. Checking topology worker count is "simplest" one for activating/deactivating transfer thread, but "right one" is checking whether outgoing stream exists in that worker. Multiple workers can be run independently (not communicating with each other) depending on how scheduler plans. Isn't it? We never utilized topology worker count in specific worker, and I think worker still has enough information to deal with such optimization without knowing topology worker count.

2. Regarding modifying topology configuration, I think I already put my 2 cent on last comment. Whenever I need to check which value I set in configuration, I'm checking the value in UI. If the value is different than what user put before, we still need to explain why it is happening to avoid confusion. That's not ideal. I think the topology configuration should be immutable one, though we don't guarantee it from the code. Maybe we would need to have context object like StormContext?

3. We already have StormTopology as well as TopologyContext which provide necessary information. Dynamic informations like assignment are handled by worker. The number of worker in topology is not exposed directly, but can be calculated from Assignment. (Please refer [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java#L357] and [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/generated/Assignment.java]) Based on 2, I would try to calculate it if it really needs.

4. We construct the configuration from combination of command line, code, conf. file, etc. We assume they're not changing during runtime, which makes us safe to read the configuration multiple times in different places. There's a chance for the assumption to be broken if we modify the value in runtime.

5. I admit that it is too ideal for the state of current Storm, but the count of worker in topology can be increased or decreased in runtime if we address elasticity. If we deal with this via relaunching all the workers then the value can be considered as static, but if not, the value will be dynamic which topology configuration is not doable to contain it.

 , Mutability of topology configuration is explicitly allowed in the documentation (previously referenced).

 

Usefulness of single worker use case should not be under-estimated. For perf critical topos it is a reasonable option to consider deploying multiple single worker instances of the topo as opposed to a single multiworker instance to avoid interworker communication. I have recommended users on mailing list to consider it (for things like video processing) and I have seen others recommend users to consider 1worker mode as well.

 

Given the fact that RA creates the divergence in specification v/s reality, confusion is bound to exist no matter which we resolve this value of this setting. Although IMO it is not a strong enough case to not reflect actual worker count ... if it is contentious ... I am ok to resolve it either way ....  as long we provide an API for so that code can check.  Currently I am aware of a few additional use cases for knowing worker count. 
 * enable/disable interworker BP subsystem.(already in place)
 * auto disabling Locality Awareness for 1 worker (i have measured this impact to be a bit under 10% or something for 1 worker)
 * user bolts inspecting this to make decisions..  like potentially allocate a certain number of external resources based on worker count... or who knows what.

 

My concern is limited to the fact that we must have official support to be able to detect the worker count. It is simply too heavy handed to simply say NO to all current and potential use cases., I think [~kabhwan] has a very good point. So I am proposing:

1) a new Jira for RAS issues.

    We all agree that RAS issues are important and need to be addressed. I believe it's better to have a separate Jira to discuss and fix them in the right way.

2) checking whether outgoing stream/tasks/connections exists in the worker instead of checking topology.worker. I agree that this is the right way to do it and it's more flexible. It also keeps this minor optimization. (Made updates to the PR)., [~kabhwan] and [~roshan_naik]

"Is there good reason why topology.workers cannot be dynamically updated to reflect the actual worker count. "

Yes.

It would require restarting all workers every time there was a scheduling change in the number of workers.  We can support it, and if someone shows a good use case as to why we would need it we can try and do it, but it is far from ideal.

"The  [documentation|http://storm.apache.org/releases/1.2.1/Configuration.html] specifically states that the system supports overriding settings. "

Yes but that is not a dynamic override.  This is because the config is treated as immutable within the worker.  Additionally there is a convention where configs that start with "topology." are meant for users to set, and for the system to read, not the other way around.

"I think the topology configuration should be immutable one, though we don't guarantee it from the code."

Actually all currently released versions of storm are based off of clojure.  The Map that we pass around is an immutable clojure map.  I would be 100% on board with making the map we pass around in the worker immutable for 2.x as well.  Hadoop has so many horrible issues because they do not enforce this in their config.  We really don't want to get into the business of allowing this within a worker.  The Config object must be mutable because users are creating it to launch their topology.  Once it is created, except for a few configs we set at topology submission time, and through the rebalance API which will enforce the workers restarting, we do not change the config.

 

"We need a way to in code to check the worker count (for internal and user code)."

For internal system code I can see a use case for this and I think we can provide it.  However, I don't see any value in providing this for user code.  

"as they can query them to find out the state of the topology"

Finding the current state of a topology by a bolt/spout is something we have always been bad at.  I would love to see more of this exposed to topology users, but I don't think config is the right place to expose it.  Also I would like to have a real solid use case for everything we expose.  The issue is the more we expose to end users the harder it is to change the internals of the system.  If we had exposed something about the outbound queues to end users it would have been much harder to make the changes that got rid of the outbound queues.

I understand letting the grouping know how overloaded downstream bolts are, which we added with load aware groupings.

I understand letting the grouping know where downstream bolts are schedules, which we added with locality aware groupings.

I understand letting a bolt know how full its inbound queue is so it can possibly play games with batching data to an external system (something we don't currently do but would be good to).

What I don't understand is what would a bolt do differently for a single worker setup vs a multi-worker setup. Would they try to communicate to one another differently?  Would they send different types or sizes of messages?  I just don't see any use case where a bolt or spout would care, but if you have a use case we can totally provide it.

 

I agree that the single worker use case is critical for performance.  Hopefully in the future we can get the scheduler and the routers to be smart enough that it is less of an issue, but as for now it is needed.

 , Thanks [~ethanli], I merged into master.

[~roshan_naik] [~revans2]

Please note that [https://github.com/apache/storm/pull/2614] fixed the issue via proposed way (even better) : it can also cover multiple all-in-one workers use case as well.

We can continue discussion, but maybe better to move to dev. list or file issue(s) since the topic(s) are out of topic for this issue., Thanks [~kabhwan] and [~ethanli] for getting that in.

 

[~roshan_naik] I am happy to continue any discussion on the mailing list or on follow on JIRAs., Thanks [~revans2] for those clarifications.

Thanks [~ethanli] for the revised fix.

Similar fixes are needed elsewhere as well, since they are all probably broken:
 - [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java#L276]

 - [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/StormCommon.java#L363-L364] 
 - [https://github.com/apache/storm/blob/master/examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/CaptureLoad.java#L109-L112]

 - [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java#L306]  

 

 , [~revans2]  [~kabhwan] [~roshan_naik] 

I filed a separate Jira STORM-3021 to track this RAS issue.]