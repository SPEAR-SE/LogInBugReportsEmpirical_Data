[[~erikdw], [~Srdo], [~revans2], [~harshach] kindly provide your insight on this issue. , Hi. Yes, the numbers in the brackets for the executor thread are the tasks that executor is running, they should be set here https://github.com/apache/storm/blob/a4afacd9617d620f50cf026fc599821f7ac25c79/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L589.

I think executor1 should correspond to tasks 3,4,5,6,7,8,9,10, and executor2 should correspond to 11,12,13,14,15,16,17,18. How did you get the task lists you posted?

I don't think it's exactly right that all the tasks in executor1 (3 10) will be reading from topics 4,6,5,7. Each task has its own spout instance (and corresponding KafkaConsumer), so only task 10 is actually assigned those partitions at the start of the log. I'm not sure how task 3 is even emitting anything? Is the start of the log the first occurence of partition assignments in the log?

I also noticed that many tasks are emitting what appears to be the same message? e.g. 

{code}
2017-05-03 13:50:47.655 o.a.s.d.task Thread-8-kafkaspout-executor[11 18] [INFO] Emitting: kafkaspout 18 default [8topic, 2, 0, null, 1]
2017-05-03 13:50:47.658 o.a.s.d.task Thread-8-kafkaspout-executor[11 18] [INFO] Emitting: kafkaspout 11 default [8topic, 2, 0, null, 1]
2017-05-03 13:50:47.660 o.a.s.d.task Thread-12-kafkaspout-executor[3 10] [INFO] Emitting: kafkaspout 7 default [8topic, 2, 0, null, 1]
2017-05-03 13:50:47.670 o.a.s.d.task Thread-12-kafkaspout-executor[3 10] [INFO] Emitting: kafkaspout 8 default [8topic, 2, 0, null, 1]
{code}

Are you using automatic or manual subscription to Kafka?

Caveat: I haven't really looked too much at the executor/task code before, so everything I posted above may be wrong :), Out of curiosity are you seeing weird logs if you have one task per executor?, Thanks for the clarification. Please find answers to your questions inline:

*How did you get the task lists you posted?*
{noformat}
Task IDs ------- 3, 4, 7, 8, 9, 11, 15, 18 ------------ Partitions 0, 1, 2, 3
Task IDs ------- 5, 6, 10, 12, 13, 14, 16, 17 --------- Partition 4, 5, 6, 7
{noformat}
The above table has been derived from the worker logs. For instance, if you search for {{kafkaspout 3}}, which means Task ID 3, you will find results for messages which are being read from partitions 0, 1, 2 and 3. This table seems to convey that {noformat}  5, 6, 10, 12, 13, 14, 16, 17 correspond to executor1 and 3, 4, 7, 8, 9, 11, 15, 18 correspond to executor2 {noformat}
If I assume that Executor1 should only be running tasks 3, 4, 5, 6, 7, 8, 9, 10, then only these tasks should be reading from partitions 4, 5, 6 or 7.

*I don't think it's exactly right that all the tasks in executor1 (3 10) will be reading from topics 4,6,5,7.*
Sorry for the confusion, I meant only tasks in Executor1 should read from the partitions 4, 5, 6 or 7. I have re-worded the same in the description above. I agree that there is a possibility that not all the tasks will end up reading from partitions 4, 5, 6 and 7.

*Each task has its own spout instance (and corresponding KafkaConsumer), so only task 10 is actually assigned those partitions at the start of the log. I'm not sure how task 3 is even emitting anything?*
Shouldn’t 3 more tasks (from Executor1) apart from task ID 10 be assigned to read from partitions 4, 5, 6 and 7. If there are more tasks than the number of partitions there should be one to one mapping between tasks and partitions is what I remember reading in the docs. Hence it is not surprising for me that task 3 is reading from a Kafka partition, although it does not seem to read from the assigned Kafka partition (according to the {{Setting newly assigned}} log line ).

*I also noticed that many tasks are emitting what appears to be the same message?*
{noformat}
e.g.
2017-05-03 13:50:47.655 o.a.s.d.task Thread-8-kafkaspout-executor[11 18] [INFO] Emitting: kafkaspout 18 default [8topic, 2, 0, null, 1]
2017-05-03 13:50:47.658 o.a.s.d.task Thread-8-kafkaspout-executor[11 18] [INFO] Emitting: kafkaspout 11 default [8topic, 2, 0, null, 1]
2017-05-03 13:50:47.660 o.a.s.d.task Thread-12-kafkaspout-executor[3 10] [INFO] Emitting: kafkaspout 7 default [8topic, 2, 0, null, 1]
2017-05-03 13:50:47.670 o.a.s.d.task Thread-12-kafkaspout-executor[3 10] [INFO] Emitting: kafkaspout 8 default [8topic, 2, 0, null, 1]
{noformat}
Are you deriving this information from the fact that all of the above messages have the same message ID (0) ? 

*Is the start of the log the first occurrence of partition assignments in the log?*
No, I have randomly taken a segment from the worker logs. I am attaching the full worker.log file in this ticket.

*Are you using automatic or manual subscription to Kafka?*
I did not understand the question. I am running Kafka on my local machine and using a KafkaSpout to read a topic from this local instance of Kafka. , Attaching the original worker.log file., [~Srdo] the logs are as expected if the number of tasks are equivalent to number of executors., Uploaded test code demonstrating how KafkaConsumer acts when multiple consumers in a thread use the subscribe() API, Sorry, I should have explained. The KafkaConsumer supports subscriptions via two APIs: subscribe() and assign(). subscribe() lets Kafka deal with assigning partitions to each consumer, assign() lets the calling code assign them manually. When I asked if you were using automatic assignment, I meant which of these APIs your spout was using. The default is subscribe().  

I think the subscribe() API doesn't really work when there are multiple consumers in a thread. As far as I can tell, the KafkaConsumer will initiate a partition reassignment whenever a new consumer joins the consumer group. The rebalance seems to require all active consumers to call poll(), and poll() doesn't return until the rebalance is complete. 

The problem seems to be that when we run multiple spout tasks in one executor, we end up triggering a rebalance when each consumer joins the group. Since the other consumers in the executor can't call poll until the rebalance is complete, because the newly joined consumer is blocking the thread in poll(), they are booted from the list of active consumers. For the version of Kafka you're running, this will happen once the session.timeout.ms expires (default 30 seconds), at which point the blocking poll() returns. At this point Kafka considers only the newly joined consumer to be alive. When the other tasks in the executor next call poll, they try to rejoin the group, which again causes a rebalance and and reassignment.

The log you posted shows partition reassignment to a new task id every 30 seconds for each executor (look for "Partitions reassignment."). I don't think the subscribe() API was designed with the expectation that one thread might be running multiple consumers.

I attached some test code that starts two KafkaConsumers and repeatedly polls with them. It seems to demonstrate this behavior where the subscribe() API works poorly when there are multiple consumers in a thread. The assignments keep flip flopping between the consumers. With one consumer per thread, or manual assignment through the assign() API, it works fine.

Here's a sample print
{code}
176 [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.0
176 [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : b8642491e78c5a13
176 [main] INFO com.mycompany.scratch.NewClass - Polling from c1
301 [main] INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator DESKTOP-AGC8TKM:9092 (id: 2147483647 rack: null) for group test.
302 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Revoking previously assigned partitions [] for group test
302 [main] INFO com.mycompany.scratch.NewClass - Partitions [] revoked for c1
302 [main] INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator - (Re-)joining group test
309 [main] INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Successfully joined group test with generation 42
309 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Setting newly assigned partitions [topic-0, topic-1] for group test
309 [main] INFO com.mycompany.scratch.NewClass - Partitions [topic-0, topic-1] assigned to c1
2185 [main] INFO com.mycompany.scratch.NewClass - Polling from c2
2290 [main] INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator DESKTOP-AGC8TKM:9092 (id: 2147483647 rack: null) for group test.
2292 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Revoking previously assigned partitions [] for group test
2292 [main] INFO com.mycompany.scratch.NewClass - Partitions [] revoked for c2
2292 [main] INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator - (Re-)joining group test
30321 [main] INFO org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Successfully joined group test with generation 43
30321 [main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Setting newly assigned partitions [topic-0, topic-1] for group test
30321 [main] INFO com.mycompany.scratch.NewClass - Partitions [topic-0, topic-1] assigned to c2
35340 [main] INFO com.mycompany.scratch.NewClass - Polling from c1
{code}

During the hang in poll, the consumer group tool shows the group as rebalancing
{code}
$ ./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group test
Consumer group `test` is rebalancing.
{code}

I think we can work around this by switching to manual partition assignment (as the default?), since rebalances don't happen then. We already have support for this with https://github.com/apache/storm/blob/master/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitionNamedSubscription.java. I'm not sure I see a good reason we'd need Kafka to manage assignments in any case.

[~srishtyagrawal9@gmail.com] can you try to set the Subscription to the class I mentioned above in the KafkaSpoutConfig, and see if it helps?, *Shouldn’t 3 more tasks (from Executor1) apart from task ID 10 be assigned to read from partitions 4, 5, 6 and 7*
Just to clarify
{code}
o.a.s.k.s.KafkaSpout Thread-12-kafkaspout-executor[3 10] [INFO] Partitions reassignment. [taskID=10, consumer-group=kafkaSpoutTestGroup, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@108e79ce, topic-partitions=[8topic-4, 8topic-6, 8topic-5, 8topic-7]]
{code}
This log line ought to mean that only task 10 is reading from partitions 4,5,6,7 until next reassignment. The partitions are being assigned to the KafkaConsumer in task 10, not to all the tasks in executor 1.

In general I'd expect the partitions to be reasonably evenly distributed across all tasks. Assigning a lot of them to a single task is suspicious, and is hopefully due to a problem with multiple consumers in a thread breaking the rebalance mechanism.

*Are you deriving this information from the fact that all of the above messages have the same message ID (0) ?*
Yes. Shouldn't partition 2 offset 0 refer to the same message every time?, Thanks for the explanation [~Srdo]. Yes, I agree that any combination of partition and offset number should refer to the same message.

I am still struggling to get the example working with the ManualPartitioner(using RoundRobinManualPartitioner). I will be posting the results soon., [~srishtyagrawal9@gmail.com] Have you had any luck with the ManualPartitioner?, Sorry [~Srdo] I completely missed your comment. No, I have not been able to dedicate enough time to test the ManualPartitioner, will update the ticket with results when I am able to get it working., [~srishtyagrawal9@gmail.com] Don't worry about it :). I think the manual partitioning code is slightly broken at the moment, see https://github.com/apache/storm/pull/2150, [~srishtyagrawal9@gmail.com] Did you have any luck with this?, [~Srdo] Sorry, I had forgotten about this ticket altogether. I see that the PR that you had linked earlier has been fixed. I can keep this in my `To Do list` and will update it when I start working on it again. ]