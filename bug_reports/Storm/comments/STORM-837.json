[Is it accomplished with implementing IBackingMap<T>, as storm-redis did?
Seems like we can't make multiGet() with HDFS itself, but we can make it with Zookeeper from Hadoop Cluster., HdfsState is not a MapState.  It is just a State, there are no get operations supported on it, it is a sink that writes all input to HDFS.  The issue is with others reading the data.  The readers in this case are likely to be a batch job using a Hadoop input format to read the data.  For regular storm it provides at most once or at least once semantics, and the HdfsBolt, which is also a sink, provides the exact same semantics, so if there is duplicate data or lost data it could be for more reasons then just the Bolt not syncing things correctly.  In that case however, I would like to see the spout not ack a tuple until it has been synced to disk, that way we truly can be sure no data is lost, but that is another issue.

for trident we expect exactly once semantics, especially form something that comes as an official part of storm.  The File formats that the data is written out in are just a log with no ability to overwrite out of data data like a MapState can.  They also have no knowledge of zookeeper and the batch ids that have been or not been fully committed.  And even if they did have that knowledge the entries have to commit ID in them to let the reader know which ones it should ignore while reading., [~revans2] Can we add an option to log the txid as the first field separated by the field separator along with each tuple.  For sequence files can we log the txid in the value along with the actual data ?

Makes sense to move the flush and rotation logic into commit. 

Also can these methods (preCommit, update & commit) get invoked by multiple threads simultaneously or is it safe to assume that the pre-commit for batch 2 will not happen until commit for batch 1 returns ?., So for each instance of the state beginCommit, update and commit will be called from a single thread, but if there is parallelism for the state, each of them may be called in parallel but for different instances.  We have to store the batch ID somewhere to make this work, but we need a way to dedupe the data, either on the read side or as part of the commit  If we store the batch ID with each tuple and do nothing on the commit side the consumer of the data has to dedup duplicate batches.  We can do that on the read side but we essentially have to dedupe everything in the data set, even previously processed data, and hope that the storm code produced the exact same result each time or else we could end up with a problem.  Because of the complexity of doign this right, and the fact that we would put this burden on the end user I personally would rather see two files employed and we dedupe on the commit side.  One file is for writing out data, and the second file is batch ID index into the first file.

In the simplest case the second file is just an edit log and can be thrown away when the commit is complete.

beginCommit would write out to the edit log that batch X is starting at file Y offset Z and flush to be sure it is on disk.
update would write out the data similar to what it does now.
commit would at least flush the data file and possibly close and rotate it to the final location, and would write out to the edit log that batch X is finished at file Y offset whatever.

The interesting code would all be in the error handling.

If the state comes up and it sees existing files that have not been rotated it needs to recover any batches that were committed but not rotated to the final location.
If beginCommit is called without the previous batch ID being committed {
    if the commit ID is the same as the current batch ID, we need to recover the batches that succeeded but have not been rotated, then continue on as normal
    else if the commit ID is not the same as the current batch ID we disable writing any data out until the current batch ID fully commits. 
}
if beginCommit is called for a batchID that we already completed we disable output until we move on to the next batch ID.

There may be other cases that we need to think about too, I just threw this out as a starting point., Agree with the approach of keeping data and txd id index in separate files. 

Based on my understanding of transactional and opaque transactional spouts, 

1. Transactional spout - (if same batch is re-emitted, it would contain the same tuples as before)
 - If we have seen the txn earlier and committed we can ignore this batch
 - If we have seen the txn earlier and not committed, we go to the txn offset and overwrite (to keep it simple).
2. Opaque transactional spout - (same batch could be re-emitted with tuples having different values, which should override)
 - If we have seen the txn earlier, we need to overwrite from the txn start offset whether we have committed or not since the tuple values may have changed.

Hence if we get a txn where we have seen “preCommit”, we go to the start offset in the corresponding data file and start writing from that offset (which would handle both 1 & 2).

I think the third case where we get a txn id and the prev txn id is not committed ideally should not happen since trident guarantees that state updates would be ordered. (https://storm.apache.org/documentation/Trident-state).
, I agree that we need to overwrite, but the issue with overwrite is that HDFS is write once.  Even append does not really work all that well in any version.  So to overwrite we need to recover the successful data and delete the old file., In HdfsState we support a TimedRotation policy which can make state saving complicated. For e.g. if the rotation happens after we record the offset for txnid in index file but before any data is written we can lose track of the data file.

A few options are :-

1. Support only size based rotation policy to keep it simple.
2. Automatically turn off exactly once in HdfsState (with warnings in logs) if TimedRotation policy is chosen.
3. Add another implementation of HdfsState where exactly-once semantics is supported and can only take sized based rotation policy.
, GitHub user arunmahadevan opened a pull request:

    https://github.com/apache/storm/pull/644

    [STORM-837] Support for exactly once semantics in HdfsState

    Changes to support exactly once semantics in HdfsState.
    
    1. Moved the file rotation and sync to commit()
    2. In pre-commit, if we have previously seen the txnid, recover the data up to that point by copying to a new file and discard the current data file.
    3. In pre-commit atomically update [current txid, the datafile path and the current offset] in a (per partition) index file.
    4. To keep it simple, automatically turn off exactly once semantics if TimedRotation policy is in use.
    
    Have tested with the normal flow and simulating the recovery scenario, with both regular Hdfs file and sequence files. Appears to work fine.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/arunmahadevan/storm master

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/644.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #644
    
----
commit 5631b9d4746f34127a1bc89cb4488d2b2d8ec9d7
Author: Arun Mahadevan <aiyer@hortonworks.com>
Date:   2015-07-21T19:04:09Z

    Support for exactly once semantics in HdfsState

----
, Github user knusbaum commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r35155252
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -17,14 +17,14 @@
      */
     package org.apache.storm.hdfs.trident;
     
    +import backtype.storm.Config;
     import backtype.storm.task.IMetricsContext;
     import backtype.storm.topology.FailedException;
     import org.apache.hadoop.conf.Configuration;
    -import org.apache.hadoop.fs.FSDataOutputStream;
    -import org.apache.hadoop.fs.FileSystem;
    -import org.apache.hadoop.fs.Path;
    +import org.apache.hadoop.fs.*;
    --- End diff --
    
    Let's not import *
, Github user knusbaum commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r35155299
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -40,8 +40,7 @@
     import storm.trident.state.State;
     import storm.trident.tuple.TridentTuple;
     
    -import java.io.IOException;
    -import java.io.Serializable;
    +import java.io.*;
    --- End diff --
    
    Again here.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-123562150
  
    @knusbaum makes sense, have removed wildcard imports.
, Github user harshach commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r35240425
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -136,33 +193,40 @@ public void run() {
             private transient FSDataOutputStream out;
             protected RecordFormat format;
             private long offset = 0;
    +        private int bufferSize =  131072; // default 128 K
     
    -        public HdfsFileOptions withFsUrl(String fsUrl){
    +        public HdfsFileOptions withFsUrl(String fsUrl) {
                 this.fsUrl = fsUrl;
                 return this;
             }
     
    -        public HdfsFileOptions withConfigKey(String configKey){
    +        public HdfsFileOptions withConfigKey(String configKey) {
                 this.configKey = configKey;
                 return this;
             }
     
    -        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat){
    +        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat) {
                 this.fileNameFormat = fileNameFormat;
                 return this;
             }
     
    -        public HdfsFileOptions withRecordFormat(RecordFormat format){
    +        public HdfsFileOptions withRecordFormat(RecordFormat format) {
                 this.format = format;
                 return this;
             }
     
    -        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy){
    +        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy) {
                 this.rotationPolicy = rotationPolicy;
                 return this;
             }
     
    -        public HdfsFileOptions addRotationAction(RotationAction action){
    +        public HdfsFileOptions withBufferSize(int size) {
    +            this.bufferSize = Math.max(4096, size); // at least 4K
    --- End diff --
    
    4096 should be substituted with buffersize? or do we need to take max it looks like we are saying the minimum should be default bufferSize. if this is needed we need to document it.
, Github user arunmahadevan commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r35243876
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -136,33 +193,40 @@ public void run() {
             private transient FSDataOutputStream out;
             protected RecordFormat format;
             private long offset = 0;
    +        private int bufferSize =  131072; // default 128 K
     
    -        public HdfsFileOptions withFsUrl(String fsUrl){
    +        public HdfsFileOptions withFsUrl(String fsUrl) {
                 this.fsUrl = fsUrl;
                 return this;
             }
     
    -        public HdfsFileOptions withConfigKey(String configKey){
    +        public HdfsFileOptions withConfigKey(String configKey) {
                 this.configKey = configKey;
                 return this;
             }
     
    -        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat){
    +        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat) {
                 this.fileNameFormat = fileNameFormat;
                 return this;
             }
     
    -        public HdfsFileOptions withRecordFormat(RecordFormat format){
    +        public HdfsFileOptions withRecordFormat(RecordFormat format) {
                 this.format = format;
                 return this;
             }
     
    -        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy){
    +        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy) {
                 this.rotationPolicy = rotationPolicy;
                 return this;
             }
     
    -        public HdfsFileOptions addRotationAction(RotationAction action){
    +        public HdfsFileOptions withBufferSize(int size) {
    +            this.bufferSize = Math.max(4096, size); // at least 4K
    --- End diff --
    
    The default is 131072 (128 K). Here just ensuring that, if at all user tries to change the default, it does not go below 4 K.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-124537309
  
    @revans2 @ptgoetz can you take a look at this patch. Thanks.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-124560610
  
    @arunmahadevan Quick question (I haven't done a full review yet): Is there a way to make this work for Timed rotation policies? That one of the most widely used rotation policies.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-124612279
  
    It took me some time to follow what's going on with this patch, so I'll document it here for the benefit other reviewers.
    
    It operates by using the concept of a temporary "transaction file" that contains the last successfully written/committed file write offset and the transaction ID. That file is initially read in the `prepare()` method call, and is initialized if it does not already exist. The POJO representing that file is stored in a `lastTxn` instance variable, which represents that last committed transaction.
    
    In the `beginCommit()` method, it checks the transaction ID against the one stored in `lastTxn`. If it is <= (indicating a replay of a batch), it triggers recovery (more on that later), otherwise it continues normally (writing data in the `execute()` method).
    
    In the `commit()` method, it essentially re-writes the transaction file with the new transaction ID and the current file offset after the latest batch data was written. This offset serves as the starting point for recovery.
    
    **Recovery**
    When recovery is triggered, it is assumed that there was a failure during the `execute()` method, and the data file now contains data from tuples that will be replayed. If it were to simply append to that file, there would be duplicate records. So it effectively truncates the data file by creating new file, and copying everything from the old data file *up to the last successfully committed offset,* and discarding the remaining data.
    
    I think this is an interesting approach and is certainly more resilient than what we currently have, but I think there's a scaling problem with the recovery functionality. Recovery works by essentially copying the old/orphaned data file into a new file and skipping any extra, uncommitted data it contains. This is fine for small files, but what about the case where the data file is really big and the recovery time would take longer than `topology.message.timeout.secs`?
    
    Another problem I see is the lack of support for time-based file rotation. I understand why it's not supported by this patch, but as I mentioned earlier, that's one of the most common use cases.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-124635613
  
    @ptgoetz to address the concern of recovery times for large files, can we recommend (or auto enforce) the max size in "size based rotation" to a reasonable threshold (say 1GB). As long as the files get rotated at a reasonable threshold which can be copied over quickly there wont be issues.
    
    Regarding time based rotation: It may be possible to support by guarding all the operations with locks and then have the new file path updated in the index along with rotation. (there might be other corner cases which needs to be thought through). 
    
    However the issue I see with time based rotation is we don't have a control of the file size and if users configure a daily rotation policy and the data rate is high, we wont be able to recover such huge files in case of failure.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-124856128
  
    @ptgoetz @arunmahadevan  can we not make this as a default behaviour instead make this as option. We can document that exactly-once behaviour  comes with limitations around file rotation.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-125086304
  
    @harshach Are you saying that we don't support time based rotation at all? The current approach automatically turns off exactly once if TimedRotation is in use, so that who want to use time based rotation would still be able to use it with the limitation that they will not get exactly once semantics.
    
    I also think we can do the same with max file size in case of size based rotation policy.
    

, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-126570170
  
    @revans2 @ptgoetz @harshach can we go ahead by automatically disabling exactly once for time based rotation and for file sizes > threshold, support exactly once for the other cases and document this behavior ?
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-126706239
  
    I am +1 on the last approach. @arunmahadevan  please add to the README under storm-hdfs.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-127146640
  
    Hi @harshach, added a note to storm hdfs README.md and modified code to disable exactly once based on file size as well.

, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-128140442
  
    I did a quick pass through the code and it looks OK, but I have not looked at it in great detail.  I am not very happy with the limitations on which rotation policy you can use, nor on the size limit.  I would rather be correct but slow by default in all cases, even if they set bad configs (> 1GB), and give them the power to make it fast but wrong if they know the risks and can deal with it.  Also a hard coded 1GB limit seems a little strange.  What if we have a 10GigE connection or even infiniband for HDFS and all of the data happens to be in memory.  We could in theory have processed the 1GB in less then a second, still painful but not the end of the world.
    
    Why don't we want to support a time based rotation, that rotates at the end of a batch after the time has passed instead of in the middle of the batch?
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-128266209
  
    I was trying to put a safety net  so that the recovery would always work. The limitation on the timed rotation policy is also on similar lines that we don't know how much data would be written to the files before the timed rotation kicks in.
    
    So,
    
    - For file size rotation, remove the restrictions on the file size and just log warnings.
    - For time based rotation, set a flag and do the actual rotation in `doCommit` as you suggested.
    - Add a note in the README about the risk that the recovery would fail if files cannot be recovered within timeout and hence should be kept to reasonable sizes (and time interval) or the message timeout should be increased.
    
    Does it sound reasonable ? If we agree, will make the changes.
    
    

, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-128368560
  
    I am fine with those changes in principal.  Although I do want to spend some time reading through the code to convince myself that there are no corner cases that we are missing.
    
    Also I would love to see some unit tests to show we can recover.  I believe that hsync works on the local file system so you would not need to bring up an HDFS mini cluster, just create a file system using ```file:///...``` as the URL and write there. 
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-129346179
  
    Removed restrictions on size and time based rotation policy, updated the README and also added unit tests.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-130162824
  
    @revans2 Made changes as you suggested. can you take a look?
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-130315702
  
    For me internally the tests pass and everything builds.  Although the rest of storm seems a bit unstable with tests.
    
    I think the changes look fine +1 for merging this in.
    
    I don't think we have any split brain problems because of how we route messages, so the begin command and commit messages should prevent this.
    
    There are some things that I think we can do better, but are not wrong. 
    1) I am scared that the transaction file is going to put more of a load on the Name Node than I like.  I am mostly concerned that if we have thousands of instances of hdfs state creating/deleting transaction files multiple times a second the load is going to noticeable on a large cluster.  But this is a premature optimization so I would say we don't address it unless we see it as an actual problem.
    2) The RotationActions don't have a recovery interface, and things like the MoveFile action could potentially fail after successfully rotating the file which might leave the file in the wrong directory.
    3) I don't really like the special case for the TimedRotationAction.start.  I would rather see start a part of the RotationAction interface, that can be ignored by those that don't want/need it.
    
    The last two could perhaps be a follow on JIRA.  I am fine with the code as is, but I would like another pair of eyes looking at this too before merging it in.
, Github user knusbaum commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-130776850
  
    +1 
    I looked through it and didn't see any obvious flaws. I'm not generally in favor of the whitespace-only changes, but that's not a huge deal.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r37006689
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -136,44 +174,98 @@ public void run() {
             private transient FSDataOutputStream out;
             protected RecordFormat format;
             private long offset = 0;
    +        private int bufferSize =  131072; // default 128 K
     
    -        public HdfsFileOptions withFsUrl(String fsUrl){
    +        public HdfsFileOptions withFsUrl(String fsUrl) {
                 this.fsUrl = fsUrl;
                 return this;
             }
     
    -        public HdfsFileOptions withConfigKey(String configKey){
    +        public HdfsFileOptions withConfigKey(String configKey) {
                 this.configKey = configKey;
                 return this;
             }
     
    -        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat){
    +        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat) {
                 this.fileNameFormat = fileNameFormat;
                 return this;
             }
     
    -        public HdfsFileOptions withRecordFormat(RecordFormat format){
    +        public HdfsFileOptions withRecordFormat(RecordFormat format) {
                 this.format = format;
                 return this;
             }
     
    -        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy){
    +        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy) {
                 this.rotationPolicy = rotationPolicy;
                 return this;
             }
     
    -        public HdfsFileOptions addRotationAction(RotationAction action){
    +        /**
    +         * <p>Set the size of the buffer used for hdfs file copy in case of recovery. The default
    +         * value is 131072.</p>
    +         *
    +         * <p> Note: The lower limit for the parameter is 4096, below which the
    +         * option is ignored. </p>
    +         *
    +         * @param sizeInBytes the buffer size in bytes
    +         * @return {@link HdfsFileOptions}
    +         */
    +        public HdfsFileOptions withBufferSize(int sizeInBytes) {
    +            this.bufferSize = Math.max(4096, sizeInBytes); // at least 4K
    +            return this;
    +        }
    +
    +        @Deprecated
    --- End diff --
    
    Why do we deprecate this?
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r37024551
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -69,63 +78,92 @@
     
             abstract void doPrepare(Map conf, int partitionIndex, int numPartitions) throws IOException;
     
    -        protected void rotateOutputFile() throws IOException {
    +        abstract long getCurrentOffset() throws  IOException;
    +
    +        abstract void doCommit(Long txId) throws IOException;
    +
    +        abstract void doRecover(Path srcPath, long nBytes) throws Exception;
    +
    +        protected void rotateOutputFile(boolean doRotateAction) throws IOException {
                 LOG.info("Rotating output file...");
                 long start = System.currentTimeMillis();
    -            synchronized (this.writeLock) {
    -                closeOutputFile();
    -                this.rotation++;
    -
    -                Path newFile = createOutputFile();
    +            closeOutputFile();
    +            this.rotation++;
    +            Path newFile = createOutputFile();
    +            if (doRotateAction) {
                     LOG.info("Performing {} file rotation actions.", this.rotationActions.size());
                     for (RotationAction action : this.rotationActions) {
                         action.execute(this.fs, this.currentFile);
                     }
    -                this.currentFile = newFile;
                 }
    +            this.currentFile = newFile;
                 long time = System.currentTimeMillis() - start;
                 LOG.info("File rotation took {} ms.", time);
    +        }
     
    -
    +        protected void rotateOutputFile() throws IOException {
    +            rotateOutputFile(true);
             }
     
    -        void prepare(Map conf, int partitionIndex, int numPartitions){
    -            this.writeLock = new Object();
    -            if (this.rotationPolicy == null) throw new IllegalStateException("RotationPolicy must be specified.");
    +
    +        void prepare(Map conf, int partitionIndex, int numPartitions) {
    +            if (this.rotationPolicy == null) {
    +                throw new IllegalStateException("RotationPolicy must be specified.");
    +            } else if (this.rotationPolicy instanceof FileSizeRotationPolicy) {
    +                long rotationBytes = ((FileSizeRotationPolicy) rotationPolicy).getMaxBytes();
    +                LOG.warn("FileSizeRotationPolicy specified with {} bytes.", rotationBytes);
    +                LOG.warn("Recovery will fail if data files cannot be copied within topology.message.timeout.secs.");
    +                LOG.warn("Ensure that the data files does not grow too big with the FileSizeRotationPolicy.");
    +            } else if (this.rotationPolicy instanceof TimedRotationPolicy) {
    +                LOG.warn("TimedRotationPolicy specified with interval {} ms.", ((TimedRotationPolicy) rotationPolicy).getInterval());
    +                LOG.warn("Recovery will fail if data files cannot be copied within topology.message.timeout.secs.");
    +                LOG.warn("Ensure that the data files does not grow too big with the TimedRotationPolicy.");
    +            }
    --- End diff --
    
    It might be nicer to move this code into the subclasses own methods: FileSizeRotationPolicy#prepare and TimedRotationPolicy#prepare.  These prepare methods can each call `super.prepare(conf, partitionIndex, numPartitions)`.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r37025284
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -69,63 +78,92 @@
     
             abstract void doPrepare(Map conf, int partitionIndex, int numPartitions) throws IOException;
     
    -        protected void rotateOutputFile() throws IOException {
    +        abstract long getCurrentOffset() throws  IOException;
    +
    +        abstract void doCommit(Long txId) throws IOException;
    +
    +        abstract void doRecover(Path srcPath, long nBytes) throws Exception;
    +
    +        protected void rotateOutputFile(boolean doRotateAction) throws IOException {
                 LOG.info("Rotating output file...");
                 long start = System.currentTimeMillis();
    -            synchronized (this.writeLock) {
    -                closeOutputFile();
    -                this.rotation++;
    -
    -                Path newFile = createOutputFile();
    +            closeOutputFile();
    +            this.rotation++;
    +            Path newFile = createOutputFile();
    +            if (doRotateAction) {
                     LOG.info("Performing {} file rotation actions.", this.rotationActions.size());
                     for (RotationAction action : this.rotationActions) {
                         action.execute(this.fs, this.currentFile);
                     }
    -                this.currentFile = newFile;
                 }
    +            this.currentFile = newFile;
                 long time = System.currentTimeMillis() - start;
                 LOG.info("File rotation took {} ms.", time);
    +        }
     
    -
    +        protected void rotateOutputFile() throws IOException {
    +            rotateOutputFile(true);
             }
     
    -        void prepare(Map conf, int partitionIndex, int numPartitions){
    -            this.writeLock = new Object();
    -            if (this.rotationPolicy == null) throw new IllegalStateException("RotationPolicy must be specified.");
    +
    +        void prepare(Map conf, int partitionIndex, int numPartitions) {
    +            if (this.rotationPolicy == null) {
    +                throw new IllegalStateException("RotationPolicy must be specified.");
    +            } else if (this.rotationPolicy instanceof FileSizeRotationPolicy) {
    +                long rotationBytes = ((FileSizeRotationPolicy) rotationPolicy).getMaxBytes();
    +                LOG.warn("FileSizeRotationPolicy specified with {} bytes.", rotationBytes);
    +                LOG.warn("Recovery will fail if data files cannot be copied within topology.message.timeout.secs.");
    +                LOG.warn("Ensure that the data files does not grow too big with the FileSizeRotationPolicy.");
    +            } else if (this.rotationPolicy instanceof TimedRotationPolicy) {
    +                LOG.warn("TimedRotationPolicy specified with interval {} ms.", ((TimedRotationPolicy) rotationPolicy).getInterval());
    +                LOG.warn("Recovery will fail if data files cannot be copied within topology.message.timeout.secs.");
    +                LOG.warn("Ensure that the data files does not grow too big with the TimedRotationPolicy.");
    +            }
                 if (this.fsUrl == null) {
                     throw new IllegalStateException("File system URL must be specified.");
                 }
                 this.fileNameFormat.prepare(conf, partitionIndex, numPartitions);
                 this.hdfsConfig = new Configuration();
    -            Map<String, Object> map = (Map<String, Object>)conf.get(this.configKey);
    -            if(map != null){
    -                for(String key : map.keySet()){
    +            Map<String, Object> map = (Map<String, Object>) conf.get(this.configKey);
    +            if (map != null) {
    +                for (String key : map.keySet()) {
                         this.hdfsConfig.set(key, String.valueOf(map.get(key)));
                     }
                 }
    -            try{
    +            try {
                     HdfsSecurityUtil.login(conf, hdfsConfig);
                     doPrepare(conf, partitionIndex, numPartitions);
                     this.currentFile = createOutputFile();
     
    -            } catch (Exception e){
    +            } catch (Exception e) {
                     throw new RuntimeException("Error preparing HdfsState: " + e.getMessage(), e);
                 }
     
    -            if(this.rotationPolicy instanceof TimedRotationPolicy){
    -                long interval = ((TimedRotationPolicy)this.rotationPolicy).getInterval();
    -                this.rotationTimer = new Timer(true);
    -                TimerTask task = new TimerTask() {
    -                    @Override
    -                    public void run() {
    -                        try {
    -                            rotateOutputFile();
    -                        } catch(IOException e){
    -                            LOG.warn("IOException during scheduled file rotation.", e);
    -                        }
    -                    }
    -                };
    -                this.rotationTimer.scheduleAtFixedRate(task, interval, interval);
    +            if (this.rotationPolicy instanceof TimedRotationPolicy) {
    +                ((TimedRotationPolicy) this.rotationPolicy).start();
    +            }
    --- End diff --
    
    Referencing the [other comment above](https://github.com/apache/storm/pull/644/files#r37024551), TimedRotationPolicy#prepare could look like:
    ```Java
    LOG.warn("TimedRotationPolicy specified with interval {} ms.", ((TimedRotationPolicy) rotationPolicy).getInterval());
    LOG.warn("Recovery will fail if data files cannot be copied within topology.message.timeout.secs.");
    LOG.warn("Ensure that the data files does not grow too big with the TimedRotationPolicy.");
    
    super.prepare();
    
    this.rotationPolicy.start();
    ```

, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r37043067
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -288,47 +391,168 @@ void closeOutputFile() throws IOException {
     
             @Override
             public void execute(List<TridentTuple> tuples) throws IOException {
    -            long offset;
    -            for(TridentTuple tuple : tuples) {
    -                synchronized (this.writeLock) {
    -                    this.writer.append(this.format.key(tuple), this.format.value(tuple));
    -                    offset = this.writer.getLength();
    -                }
    -
    -                if (this.rotationPolicy.mark(tuple, offset)) {
    -                    rotateOutputFile();
    -                    this.rotationPolicy.reset();
    -                }
    +            for (TridentTuple tuple : tuples) {
    +                this.writer.append(this.format.key(tuple), this.format.value(tuple));
                 }
             }
     
         }
     
    +    /**
    +     * TxnRecord [txnid, data_file_path, data_file_offset]
    +     * <p>
    +     * This is written to the index file during beginCommit() and used for recovery.
    +     * </p>
    +     */
    +    private static class TxnRecord {
    +        private long txnid;
    +        private String dataFilePath;
    +        private long offset;
    +
    +        private TxnRecord(long txnId, String dataFilePath, long offset) {
    +            this.txnid = txnId;
    +            this.dataFilePath = dataFilePath;
    +            this.offset = offset;
    +        }
    +
    +        @Override
    +        public String toString() {
    +            return Long.toString(txnid) + "," + dataFilePath + "," + Long.toString(offset);
    +        }
    +    }
    +
    +
         public static final Logger LOG = LoggerFactory.getLogger(HdfsState.class);
         private Options options;
    +    private volatile TxnRecord lastSeenTxn;
    +    private Path indexFilePath;
     
    -    HdfsState(Options options){
    +    HdfsState(Options options) {
             this.options = options;
         }
     
    -    void prepare(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions){
    +    void prepare(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             this.options.prepare(conf, partitionIndex, numPartitions);
    +        initLastTxn(conf, partitionIndex);
    +    }
    +
    +    private TxnRecord readTxnRecord(Path path) throws IOException {
    +        FSDataInputStream inputStream = null;
    +        try {
    +            inputStream = this.options.fs.open(path);
    +            BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream));
    +            String line;
    +            if ((line = reader.readLine()) != null) {
    +                String[] fields = line.split(",");
    +                return new TxnRecord(Long.valueOf(fields[0]), fields[1], Long.valueOf(fields[2]));
    +            }
    +        } finally {
    +            if (inputStream != null) {
    +                inputStream.close();
    +            }
    +        }
    +        return new TxnRecord(0, options.currentFile.toString(), 0);
    +    }
    +
    +    /**
    +     * Reads the last txn record from index file if it exists, if not
    +     * from .tmp file if exists.
    +     *
    +     * @param indexFilePath the index file path
    +     * @return the txn record from the index file or a default initial record.
    +     * @throws IOException
    +     */
    +    private TxnRecord getTxnRecord(Path indexFilePath) throws IOException {
    +        Path tmpPath = new Path(indexFilePath.toString() + ".tmp");
    --- End diff --
    
    It would be good to have this pulled into its own method, just so that we do not have ".tmp" hard-coded in two spots.
, Github user arunmahadevan commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r37049715
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -136,44 +174,98 @@ public void run() {
             private transient FSDataOutputStream out;
             protected RecordFormat format;
             private long offset = 0;
    +        private int bufferSize =  131072; // default 128 K
     
    -        public HdfsFileOptions withFsUrl(String fsUrl){
    +        public HdfsFileOptions withFsUrl(String fsUrl) {
                 this.fsUrl = fsUrl;
                 return this;
             }
     
    -        public HdfsFileOptions withConfigKey(String configKey){
    +        public HdfsFileOptions withConfigKey(String configKey) {
                 this.configKey = configKey;
                 return this;
             }
     
    -        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat){
    +        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat) {
                 this.fileNameFormat = fileNameFormat;
                 return this;
             }
     
    -        public HdfsFileOptions withRecordFormat(RecordFormat format){
    +        public HdfsFileOptions withRecordFormat(RecordFormat format) {
                 this.format = format;
                 return this;
             }
     
    -        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy){
    +        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy) {
                 this.rotationPolicy = rotationPolicy;
                 return this;
             }
     
    -        public HdfsFileOptions addRotationAction(RotationAction action){
    +        /**
    +         * <p>Set the size of the buffer used for hdfs file copy in case of recovery. The default
    +         * value is 131072.</p>
    +         *
    +         * <p> Note: The lower limit for the parameter is 4096, below which the
    +         * option is ignored. </p>
    +         *
    +         * @param sizeInBytes the buffer size in bytes
    +         * @return {@link HdfsFileOptions}
    +         */
    +        public HdfsFileOptions withBufferSize(int sizeInBytes) {
    +            this.bufferSize = Math.max(4096, sizeInBytes); // at least 4K
    +            return this;
    +        }
    +
    +        @Deprecated
    --- End diff --
    
    As mentioned in the previous comments and also in the jira (STORM-837), rotation actions are too generic and make the recovery difficult if the process crash in the middle of the action. Hence its best to avoid rotation actions in case exactly once semantics is expected. Will add a note in the README.
, Github user arunmahadevan commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r37049831
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -69,63 +78,92 @@
     
             abstract void doPrepare(Map conf, int partitionIndex, int numPartitions) throws IOException;
     
    -        protected void rotateOutputFile() throws IOException {
    +        abstract long getCurrentOffset() throws  IOException;
    +
    +        abstract void doCommit(Long txId) throws IOException;
    +
    +        abstract void doRecover(Path srcPath, long nBytes) throws Exception;
    +
    +        protected void rotateOutputFile(boolean doRotateAction) throws IOException {
                 LOG.info("Rotating output file...");
                 long start = System.currentTimeMillis();
    -            synchronized (this.writeLock) {
    -                closeOutputFile();
    -                this.rotation++;
    -
    -                Path newFile = createOutputFile();
    +            closeOutputFile();
    +            this.rotation++;
    +            Path newFile = createOutputFile();
    +            if (doRotateAction) {
                     LOG.info("Performing {} file rotation actions.", this.rotationActions.size());
                     for (RotationAction action : this.rotationActions) {
                         action.execute(this.fs, this.currentFile);
                     }
    -                this.currentFile = newFile;
                 }
    +            this.currentFile = newFile;
                 long time = System.currentTimeMillis() - start;
                 LOG.info("File rotation took {} ms.", time);
    +        }
     
    -
    +        protected void rotateOutputFile() throws IOException {
    +            rotateOutputFile(true);
             }
     
    -        void prepare(Map conf, int partitionIndex, int numPartitions){
    -            this.writeLock = new Object();
    -            if (this.rotationPolicy == null) throw new IllegalStateException("RotationPolicy must be specified.");
    +
    +        void prepare(Map conf, int partitionIndex, int numPartitions) {
    +            if (this.rotationPolicy == null) {
    +                throw new IllegalStateException("RotationPolicy must be specified.");
    +            } else if (this.rotationPolicy instanceof FileSizeRotationPolicy) {
    +                long rotationBytes = ((FileSizeRotationPolicy) rotationPolicy).getMaxBytes();
    +                LOG.warn("FileSizeRotationPolicy specified with {} bytes.", rotationBytes);
    +                LOG.warn("Recovery will fail if data files cannot be copied within topology.message.timeout.secs.");
    +                LOG.warn("Ensure that the data files does not grow too big with the FileSizeRotationPolicy.");
    +            } else if (this.rotationPolicy instanceof TimedRotationPolicy) {
    +                LOG.warn("TimedRotationPolicy specified with interval {} ms.", ((TimedRotationPolicy) rotationPolicy).getInterval());
    +                LOG.warn("Recovery will fail if data files cannot be copied within topology.message.timeout.secs.");
    +                LOG.warn("Ensure that the data files does not grow too big with the TimedRotationPolicy.");
    +            }
                 if (this.fsUrl == null) {
                     throw new IllegalStateException("File system URL must be specified.");
                 }
                 this.fileNameFormat.prepare(conf, partitionIndex, numPartitions);
                 this.hdfsConfig = new Configuration();
    -            Map<String, Object> map = (Map<String, Object>)conf.get(this.configKey);
    -            if(map != null){
    -                for(String key : map.keySet()){
    +            Map<String, Object> map = (Map<String, Object>) conf.get(this.configKey);
    +            if (map != null) {
    +                for (String key : map.keySet()) {
                         this.hdfsConfig.set(key, String.valueOf(map.get(key)));
                     }
                 }
    -            try{
    +            try {
                     HdfsSecurityUtil.login(conf, hdfsConfig);
                     doPrepare(conf, partitionIndex, numPartitions);
                     this.currentFile = createOutputFile();
     
    -            } catch (Exception e){
    +            } catch (Exception e) {
                     throw new RuntimeException("Error preparing HdfsState: " + e.getMessage(), e);
                 }
     
    -            if(this.rotationPolicy instanceof TimedRotationPolicy){
    -                long interval = ((TimedRotationPolicy)this.rotationPolicy).getInterval();
    -                this.rotationTimer = new Timer(true);
    -                TimerTask task = new TimerTask() {
    -                    @Override
    -                    public void run() {
    -                        try {
    -                            rotateOutputFile();
    -                        } catch(IOException e){
    -                            LOG.warn("IOException during scheduled file rotation.", e);
    -                        }
    -                    }
    -                };
    -                this.rotationTimer.scheduleAtFixedRate(task, interval, interval);
    +            if (this.rotationPolicy instanceof TimedRotationPolicy) {
    +                ((TimedRotationPolicy) this.rotationPolicy).start();
    +            }
    --- End diff --
    
    Actually the warnings are specific to HDFS state and by moving it to the rotation policy we make the rotation policy aware of the recovery which might not be ideal if the rotation policy class needs to be reused in some other context.
    
    Moving the start logic to policy specific prepare methods makes sense. I will make this change.
, Github user arunmahadevan commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r37049859
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -288,47 +391,168 @@ void closeOutputFile() throws IOException {
     
             @Override
             public void execute(List<TridentTuple> tuples) throws IOException {
    -            long offset;
    -            for(TridentTuple tuple : tuples) {
    -                synchronized (this.writeLock) {
    -                    this.writer.append(this.format.key(tuple), this.format.value(tuple));
    -                    offset = this.writer.getLength();
    -                }
    -
    -                if (this.rotationPolicy.mark(tuple, offset)) {
    -                    rotateOutputFile();
    -                    this.rotationPolicy.reset();
    -                }
    +            for (TridentTuple tuple : tuples) {
    +                this.writer.append(this.format.key(tuple), this.format.value(tuple));
                 }
             }
     
         }
     
    +    /**
    +     * TxnRecord [txnid, data_file_path, data_file_offset]
    +     * <p>
    +     * This is written to the index file during beginCommit() and used for recovery.
    +     * </p>
    +     */
    +    private static class TxnRecord {
    +        private long txnid;
    +        private String dataFilePath;
    +        private long offset;
    +
    +        private TxnRecord(long txnId, String dataFilePath, long offset) {
    +            this.txnid = txnId;
    +            this.dataFilePath = dataFilePath;
    +            this.offset = offset;
    +        }
    +
    +        @Override
    +        public String toString() {
    +            return Long.toString(txnid) + "," + dataFilePath + "," + Long.toString(offset);
    +        }
    +    }
    +
    +
         public static final Logger LOG = LoggerFactory.getLogger(HdfsState.class);
         private Options options;
    +    private volatile TxnRecord lastSeenTxn;
    +    private Path indexFilePath;
     
    -    HdfsState(Options options){
    +    HdfsState(Options options) {
             this.options = options;
         }
     
    -    void prepare(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions){
    +    void prepare(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
             this.options.prepare(conf, partitionIndex, numPartitions);
    +        initLastTxn(conf, partitionIndex);
    +    }
    +
    +    private TxnRecord readTxnRecord(Path path) throws IOException {
    +        FSDataInputStream inputStream = null;
    +        try {
    +            inputStream = this.options.fs.open(path);
    +            BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream));
    +            String line;
    +            if ((line = reader.readLine()) != null) {
    +                String[] fields = line.split(",");
    +                return new TxnRecord(Long.valueOf(fields[0]), fields[1], Long.valueOf(fields[2]));
    +            }
    +        } finally {
    +            if (inputStream != null) {
    +                inputStream.close();
    +            }
    +        }
    +        return new TxnRecord(0, options.currentFile.toString(), 0);
    +    }
    +
    +    /**
    +     * Reads the last txn record from index file if it exists, if not
    +     * from .tmp file if exists.
    +     *
    +     * @param indexFilePath the index file path
    +     * @return the txn record from the index file or a default initial record.
    +     * @throws IOException
    +     */
    +    private TxnRecord getTxnRecord(Path indexFilePath) throws IOException {
    +        Path tmpPath = new Path(indexFilePath.toString() + ".tmp");
    --- End diff --
    
    will do this.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-130991133
  
    @d2r Thanks for the review, incorporated your feedback.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/644#discussion_r37093803
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/trident/HdfsState.java ---
    @@ -136,44 +174,98 @@ public void run() {
             private transient FSDataOutputStream out;
             protected RecordFormat format;
             private long offset = 0;
    +        private int bufferSize =  131072; // default 128 K
     
    -        public HdfsFileOptions withFsUrl(String fsUrl){
    +        public HdfsFileOptions withFsUrl(String fsUrl) {
                 this.fsUrl = fsUrl;
                 return this;
             }
     
    -        public HdfsFileOptions withConfigKey(String configKey){
    +        public HdfsFileOptions withConfigKey(String configKey) {
                 this.configKey = configKey;
                 return this;
             }
     
    -        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat){
    +        public HdfsFileOptions withFileNameFormat(FileNameFormat fileNameFormat) {
                 this.fileNameFormat = fileNameFormat;
                 return this;
             }
     
    -        public HdfsFileOptions withRecordFormat(RecordFormat format){
    +        public HdfsFileOptions withRecordFormat(RecordFormat format) {
                 this.format = format;
                 return this;
             }
     
    -        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy){
    +        public HdfsFileOptions withRotationPolicy(FileRotationPolicy rotationPolicy) {
                 this.rotationPolicy = rotationPolicy;
                 return this;
             }
     
    -        public HdfsFileOptions addRotationAction(RotationAction action){
    +        /**
    +         * <p>Set the size of the buffer used for hdfs file copy in case of recovery. The default
    +         * value is 131072.</p>
    +         *
    +         * <p> Note: The lower limit for the parameter is 4096, below which the
    +         * option is ignored. </p>
    +         *
    +         * @param sizeInBytes the buffer size in bytes
    +         * @return {@link HdfsFileOptions}
    +         */
    +        public HdfsFileOptions withBufferSize(int sizeInBytes) {
    +            this.bufferSize = Math.max(4096, sizeInBytes); // at least 4K
    +            return this;
    +        }
    +
    +        @Deprecated
    --- End diff --
    
    Ah, I hadn't noticed that part of the JIRA description.  That's a good answer; just wanted to make sure the change was intentional.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-131175103
  
    Thanks @arunmahadevan 
    
    +1 It seems fine to me, and the new tests pass for me.  The remaining comment on refactoring is minor, so I am ok merging it.
    
    Regarding Bobby's comments: 1) HDFS name node load, I agree we can open an Issue if we see it is really a problem; 2) no recovery from actions, this could be a problem, but we are marking it as deprecated.  If we want to keep support for actions, we should do follow-on work that provides for some recovery of those operations too.
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-131188419
  
    Still +1 looks good.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/644#issuecomment-131192881
  
    +1
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/644
, Thanks [~arunmahadevan], I merged this to master.]