[This can be reproducible with following steps
1) Run a storm cluster  with atleast 2 supervisors with 4 slots each
2) Deploy a topology that uses 4 workers, topology will be distributed with each supervisor having two workers each
3) kill one of the supervisor lets say supervisor1 
4) wait till topology re-balances to occupy 4 workers on supervisor2
5) now bring up supervisor1, It goes through the cycle of cleaning up old topology code
6) nimbus re-balances topology which triggers supervisor.sync-process method
7) sync-process tries to launch a worker for the topology whose code data is delete when the supervisor started causing it throw up following exception

2015-01-28T17:15:01.512-0800 b.s.d.supervisor [INFO] Removing code for storm id WordCount-1-1422493587
2015-01-28T17:15:01.521-0800 b.s.d.supervisor [INFO] Shutting down and clearing state for id 6ef95e67-10ab-4dca-8d8f-a43a172012d2. 
Current supervisor time: 1422494101. State: :timed-out, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1422493
975, :storm-id "WordCount-1-1422493587", :executors #{[2 2] [22 22] [18 18] [6 6] [-1 -1] [26 26] [10 10] [14 14]}, :port 6706}
2015-01-28T17:15:01.521-0800 b.s.d.supervisor [INFO] Shutting down 289c048e-1848-42aa-92a2-7e1d69dbe6fe:6ef95e67-10ab-4dca-8d8f-a43
a172012d2
2015-01-28T17:15:01.522-0800 b.s.config [INFO] GET worker-user 6ef95e67-10ab-4dca-8d8f-a43a172012d2
2015-01-28T17:15:01.551-0800 b.s.util [INFO] Error when trying to kill 76150. Process is probably already dead.
2015-01-28T17:15:01.559-0800 b.s.util [INFO] Error when trying to kill 76171. Process is probably already dead.
2015-01-28T17:15:01.567-0800 b.s.util [INFO] Error when trying to kill 76172. Process is probably already dead.
2015-01-28T17:15:02.579-0800 b.s.util [INFO] Error when trying to kill 76150. Process is probably already dead.
2015-01-28T17:15:02.588-0800 b.s.util [INFO] Error when trying to kill 76171. Process is probably already dead.
2015-01-28T17:15:02.595-0800 b.s.util [INFO] Error when trying to kill 76172. Process is probably already dead.
2015-01-28T17:15:02.612-0800 b.s.config [INFO] REMOVE worker-user 6ef95e67-10ab-4dca-8d8f-a43a172012d2
2015-01-28T17:15:02.613-0800 b.s.d.supervisor [INFO] Shut down 289c048e-1848-42aa-92a2-7e1d69dbe6fe:6ef95e67-10ab-4dca-8d8f-a43a172
012d2
2015-01-28T17:15:02.615-0800 b.s.d.supervisor [INFO] Shutting down and clearing state for id e27c5dad-2d2e-46f3-9597-4236f6eaa682. 
Current supervisor time: 1422494101. State: :timed-out, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1422493
976, :storm-id "WordCount-1-1422493587", :executors #{[8 8] [12 12] [24 24] [28 28] [20 20] [-1 -1] [16 16] [4 4]}, :port 6707}
2015-01-28T17:15:02.615-0800 b.s.d.supervisor [INFO] Shutting down 289c048e-1848-42aa-92a2-7e1d69dbe6fe:e27c5dad-2d2e-46f3-9597-423
6f6eaa682
2015-01-28T17:15:02.615-0800 b.s.config [INFO] GET worker-user e27c5dad-2d2e-46f3-9597-4236f6eaa682
2015-01-28T17:15:02.624-0800 b.s.util [INFO] Error when trying to kill 76151. Process is probably already dead.
2015-01-28T17:15:02.644-0800 b.s.util [INFO] Error when trying to kill 76166. Process is probably already dead.
2015-01-28T17:15:02.656-0800 b.s.util [INFO] Error when trying to kill 76168. Process is probably already dead.
2015-01-28T17:15:03.665-0800 b.s.util [INFO] Error when trying to kill 76151. Process is probably already dead.
2015-01-28T17:15:03.675-0800 b.s.util [INFO] Error when trying to kill 76166. Process is probably already dead.
2015-01-28T17:15:03.685-0800 b.s.util [INFO] Error when trying to kill 76168. Process is probably already dead.
2015-01-28T17:15:03.733-0800 b.s.config [INFO] REMOVE worker-user e27c5dad-2d2e-46f3-9597-4236f6eaa682
2015-01-28T17:15:03.734-0800 b.s.d.supervisor [INFO] Shut down 289c048e-1848-42aa-92a2-7e1d69dbe6fe:e27c5dad-2d2e-46f3-9597-4236f6e
aa682
2015-01-28T17:15:03.799-0800 b.s.d.supervisor [INFO] Launching worker with assignment #backtype.storm.daemon.supervisor.LocalAssign
ment{:storm-id "WordCount-1-1422493587", :executors ([2 2] [22 22] [18 18] [6 6] [26 26] [10 10] [14 14])} for this supervisor 289c
048e-1848-42aa-92a2-7e1d69dbe6fe on port 6706 with id 82392b07-e3e7-4ec9-915b-5c1e79da5aae
2015-01-28T17:15:03.933-0800 b.s.event [ERROR] Error when processing event
java.io.FileNotFoundException: File 'storm-local/supervisor/stormdist/WordCount-1-1422493587/stormconf.ser' does not exist
        at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
        at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
        at backtype.storm.config$read_supervisor_storm_conf.invoke(config.clj:214) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
        at backtype.storm.daemon.supervisor$fn__5499.invoke(supervisor.clj:627) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
        at clojure.lang.MultiFn.invoke(MultiFn.java:241) ~[clojure-1.6.0.jar:na]
        at backtype.storm.daemon.supervisor$sync_processes$iter__5355__5359$fn__5360.invoke(supervisor.clj:366) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
, GitHub user harshach opened a pull request:

    https://github.com/apache/storm/pull/400

    STORM-130. Supervisor getting killed due to java.io.FileNotFoundException: File '../stormconf.ser' does not exist.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/harshach/incubator-storm STORM-130

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/400.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #400
    
----
commit a16a95c5e8d889685389607f3a67514b73688e14
Author: Sriharsha Chintalapani <mail@harsha.io>
Date:   2015-01-29T02:07:42Z

    STORM-130. Supervisor getting killed due to java.io.FileNotFoundException: File '../stormconf.ser' does not exist.

----
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/400#issuecomment-71956562
  
    @revans2 @ptgoetz @d2r @Parth-Brahmbhatt  please take a look at this patch. Thanks.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/400#issuecomment-71970386
  
    @harshach One nit: white space changes make the diff harder to read. (Though appending '?w=1' to the URL will force github to ignore white space.)
    
    Otherwise, given a cursory review of the changes, I'm +1 initially. I'd like to verify the patch myself and/or give others a chance to do so as well.
    
    Regardless, nice work on tracking down the probable root cause and formulating a fix. This has been an oft reported issue that's difficult to reproduce.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/400#issuecomment-72070868
  
    Closing this pr will re-open a new one. Thanks.
, Github user harshach closed the pull request at:

    https://github.com/apache/storm/pull/400
, Github user Parth-Brahmbhatt commented on the pull request:

    https://github.com/apache/storm/pull/400#issuecomment-72071447
  
    +1, lgtm.
, GitHub user harshach opened a pull request:

    https://github.com/apache/storm/pull/401

    STORM-130: Supervisor getting killed due to java.io.FileNotFoundException: File '../stormconf.ser' does not exist.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/harshach/incubator-storm STORM-130-V2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/401.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #401
    
----
commit 6b061aaab5a39e4b95670e3f0590bb48de4375fd
Author: Sriharsha Chintalapani <mail@harsha.io>
Date:   2015-01-29T18:22:20Z

    STORM-130: Supervisor getting killed due to java.io.FileNotFoundException: File '../stormconf.ser' does not exist.

----
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-72078838
  
    @ptgoetz  @d2r @revans2 @Parth-Brahmbhatt  Please take a look at the new PR. I apologize for the whitespace issue. There are still few left in this patch. As Taylor suggested please ?w=1 to the URL.

, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/storm/pull/401#discussion_r23801424
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/supervisor.clj ---
    @@ -349,6 +350,20 @@
                             (keys keepers))
                (zipmap (vals new-worker-ids) (keys new-worker-ids))
                ))
    +
    +    ;; check storm topology code dir exists before launching workers
    +    (doseq [[port assignment] reassign-executors]
    +      (let [storm-cluster-state (:storm-cluster-state supervisor)
    +            downloaded-storm-ids (set (read-downloaded-storm-ids conf))
    +            storm-id (:storm-id assignment)
    +            assignment-info (.assignment-info-with-version storm-cluster-state storm-id nil)
    --- End diff --
    
    This is going to cause a large load on zookeeper.  There is a reason we have the cache for the assignment info, and we only download it when it has changed.  We really should use the cache here if we can.
, Github user harshach commented on a diff in the pull request:

    https://github.com/apache/storm/pull/401#discussion_r23801588
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/supervisor.clj ---
    @@ -349,6 +350,20 @@
                             (keys keepers))
                (zipmap (vals new-worker-ids) (keys new-worker-ids))
                ))
    +
    +    ;; check storm topology code dir exists before launching workers
    +    (doseq [[port assignment] reassign-executors]
    +      (let [storm-cluster-state (:storm-cluster-state supervisor)
    +            downloaded-storm-ids (set (read-downloaded-storm-ids conf))
    +            storm-id (:storm-id assignment)
    +            assignment-info (.assignment-info-with-version storm-cluster-state storm-id nil)
    --- End diff --
    
    @revans2 are you talking about .assignment-info-with-version?
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-72102590
  
    For the most part it looks good, I would like to see us use the cached assignment instead of going to zookeeper again to get it.  I would also like to understand better how we avoid issues with downloading the same thing in two different threads.  I'm not sure I understand the exact cause of what puts us in this situation to begin with, so I don't feel comfortable that we will not try to download the same thing twice and cause some issues when we do.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-72103305
  
    @revans2 thanks for the review. I am looking for cached assignment code will send updated patch. 
    To address downloading the same code twice. It can happen in download-storm-code method I am checking if the topology code dir exists in the supervisor it won't overwrite and deletes the tmp dir.  More details on this comment on how this can happen https://issues.apache.org/jira/browse/STORM-130?focusedCommentId=14296241&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14296241
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/storm/pull/401#discussion_r23803166
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/supervisor.clj ---
    @@ -349,6 +350,20 @@
                             (keys keepers))
                (zipmap (vals new-worker-ids) (keys new-worker-ids))
                ))
    +
    +    ;; check storm topology code dir exists before launching workers
    +    (doseq [[port assignment] reassign-executors]
    +      (let [storm-cluster-state (:storm-cluster-state supervisor)
    +            downloaded-storm-ids (set (read-downloaded-storm-ids conf))
    +            storm-id (:storm-id assignment)
    +            assignment-info (.assignment-info-with-version storm-cluster-state storm-id nil)
    --- End diff --
    
    Yes.  We could replace it with ```@(:assignment-versions supervisor)``` and presumably get the same result without hitting zookeeper.
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-72105884
  
    Ok So this is for an existing topology that moved and then moved back.  I understand now about the race.  I am +1 for the change, once we avoid hitting ZK quite so frequently.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-72131353
  
    @revans2 thanks for pointer on using cached assignment. I also spoke to @Parth-Brahmbhatt  about download-storm-code, added the lock to make sure there won't be multiple threads downloading the same code . Please take a look.
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-72235020
  
    +1 looks good to me.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-72725245
  
    +1. I'd also like to see this back-ported to the 0.9.3 branch, but that shouldn't block this from getting merged to master.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-72726626
  
    @ptgoetz  will send a new PR against 0.9.3 branch. Thanks.
, GitHub user harshach opened a pull request:

    https://github.com/apache/storm/pull/418

    STORM-130: Supervisor getting killed due to java.io.FileNotFoundException: File '../stormconf.ser' does not exist.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/harshach/incubator-storm STORM-130-0.9.3

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/418.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #418
    
----
commit ccd28f8a356f468e66865fa9d9901b0a2628ec74
Author: Sriharsha Chintalapani <mail@harsha.io>
Date:   2015-02-05T18:08:05Z

    STORM-130: Supervisor getting killed due to java.io.FileNotFoundException: File '../stormconf.ser' does not exist.

----
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/401#issuecomment-73105790
  
    @revans2 @ptgoetz  opened a PR against 0.9.3-branch https://github.com/apache/storm/pull/418 . Also updated the current PR to handle another case.
    This issue can happen if supervisors are down and user kills a topology , restart supervisors. These supervisors can crash with same issue as sync-process can start after cleaning up local deleted topology files . In general having try catch around launch-worker is good idea as there is no need to kill the supervisor itself if the launch-worker fails.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-74099551
  
    +1
    
    I'll presume +1 approvals carry over from #401, but allow others time to comment.
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-75577186
  
    yes +1
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/401
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-75580704
  
    @ptgoetz  I merged the corresponding pull into master, but I'm not totally sure what version number you want for a 0.9.3 update,  will it be 0.9.4?  0.9.3.1?  If you just want to merge it in and update the JIRA with the new number that will be great, otherwise I'll just go with 0.9.4 and call it good. 
, Thanks [~sriharsha], I merged this into master.  I wasn't sure about the new version number for the 0.9.3 pull request so I'll leave it up to [~ptgoetz] for now to decide on a version number and pull it in., Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-75842588
  
    merged to 0.9.3-branch. The next release of that branch will be 0.9.4, I'll update JIRA.
    
    @harshach Can you close this PR?
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-75844084
  
    Thanks @ptgoetz 
, Github user harshach closed the pull request at:

    https://github.com/apache/storm/pull/418
, Github user sweetest commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-76107560
  
    hi guys, I want to know about when you will release 0.9.4 including this feature.
, I'm also facing this critical issue. It'll be appreciated that 0.9.4 can be released in the near future., I have had this issue occur with 0.9.4. , [~halorgium] Did it caused supervisor to crash. Are you upgrading from 0.9.3 or is this a new cluster and how big of a cluster is this., The supervisor did not crash, but the worker started up and could not find the stormconf.ser. 
Logs at https://gist.github.com/halorgium/6f558ad0e91d142e4560, The cluster is a single worker node. I had upgraded from 0.9.3 a week or so ago. , [~halorgium]  It will be great if you can provide more details on when did you see this issue. After deploying topology ? and did it caused the supervisor to crash or did you notice the error in the logs. We are catching these exceptions now and it shouldn't crash supervisor and also when you upgraded is there any chance that there is an older storm-core*.jar present in the under STORM_HOME/lib. Are you able to reproduce this issue and if so can you provide any steps with 0.9.4.
 Any help with the details greatly appreciated., The issue occurs when we have a crash in our worker process. The supervisor (or nimbus) eventually notices that the topology is no longer running and restarts the topology worker process. 
I noticed this issue because I have been doing maintenance on our DB tier. 
This exception is in the worker log not the supervisor log. The supervisor did not crash. 

There are no old storm jars in the lib directory. 

The steps to reproduce are: 
* Deploy a topology
* Cause the topology to crash for an unexpected reason
* Watch the supervisor start the worker
* Watch the supervisor remove the code for the topology
* The worker then cannot find the stormconf.ser file
* The supervisor eventually downloads the topology to the filesystem, [~halorgium] Thanks for the details. Thats exactly the behavior we are looking for . Before this patch we may get into the same scenario as you noted where a worker might die causing supervisor to clear the state and restarting it would result in the topology files not found causing the supervisor to crash. If supervisor is not crashing and downloading the topology bit later that is fine. The reason supervisor might be removing code is if the worker is not sending any heartbeats the assignments for that supervisor might be taken away by nimbus 
and in that case supervisor will remove the topology jar code.
 After the last step are you seeing worker to start and topology to run?
, No, after the last step the supervisor/nimbus has to re-notice that the worker has crashed. 
Very often, the same issue occurs many times in a row. 

We generally have to kill and deploy the topology to recover. , [~halorgium] it will definitely happen in a row as your topology keep exiting with an error no? . If the topology has unrecoverable error causing worker to crash these steps will happen repeatedly. , The topology never restarts after the crash. The worker is unable to startup because of the missing file. , The error is recoverable, but has caused the worker process to crash. , If you can upload the topology to the JIRA I can retry reproducing this issue. Thanks., [~halorgium] I made changes to wordcount topology to do a System.exit(-1) if the word count reached to 100 and here is the supervisor.log

 ea96eb72-4b14-4342-a60e-d4c8d590814f still hasn't started
2015-03-30T20:05:00.577-0700 b.s.d.supervisor [INFO] Worker Process f144d64e-6a60-465e-a838-9f9c2621e240 exited with code: 20
2015-03-30T20:05:02.947-0700 b.s.d.supervisor [INFO] Worker Process f144d64e-6a60-465e-a838-9f9c2621e240 has died!
2015-03-30T20:05:02.971-0700 b.s.d.supervisor [INFO] Shutting down and clearing state for id f144d64e-6a60-465e-a838-9f9c2621e240
. Current supervisor time: 1427771102. State: :timed-out, Heartbeat: {:time-secs 1427771100, :storm-id "wordcount-1-1427771089", 
:executors #{[8 8] [2 2] [20 20] [23 23] [11 11] [-1 -1] [5 5] [26 26] [14 14] [17 17]}, :port 6702}
2015-03-30T20:05:02.971-0700 b.s.d.supervisor [INFO] Shutting down 93d81c4b-c87c-491c-a808-b8e8d7f259a6:f144d64e-6a60-465e-a838-9
f9c2621e240
2015-03-30T20:05:02.972-0700 b.s.config [INFO] GET worker-user f144d64e-6a60-465e-a838-9f9c2621e240
2015-03-30T20:05:02.996-0700 b.s.util [INFO] Error when trying to kill 56327. Process is probably already dead.
2015-03-30T20:05:03.001-0700 b.s.util [INFO] Error when trying to kill 56332. Process is probably already dead.
2015-03-30T20:05:03.020-0700 b.s.util [INFO] Error when trying to kill 56333. Process is probably already dead.
2015-03-30T20:05:03.029-0700 b.s.util [INFO] Error when trying to kill 56334. Process is probably already dead.
2015-03-30T20:05:03.029-0700 b.s.d.supervisor [INFO] Sleep 1 seconds for execution of cleanup threads on worker.
2015-03-30T20:05:04.040-0700 b.s.util [INFO] Error when trying to kill 56327. Process is probably already dead.
2015-03-30T20:05:04.046-0700 b.s.util [INFO] Error when trying to kill 56332. Process is probably already dead.
2015-03-30T20:05:04.050-0700 b.s.util [INFO] Error when trying to kill 56333. Process is probably already dead.
2015-03-30T20:05:04.055-0700 b.s.util [INFO] Error when trying to kill 56334. Process is probably already dead.
2015-03-30T20:05:04.057-0700 b.s.config [INFO] REMOVE worker-user f144d64e-6a60-465e-a838-9f9c2621e240
2015-03-30T20:05:04.057-0700 b.s.d.supervisor [INFO] Shut down 93d81c4b-c87c-491c-a808-b8e8d7f259a6:f144d64e-6a60-465e-a838-9f9c2
621e240
2015-03-30T20:05:04.063-0700 b.s.d.supervisor [INFO] Launching worker with assignment #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id "wordcount-1-1427771089", :executors ([8 8] [2 2] [20 20] [23 23] [11 11] [5 5] [26 26] [14 14] [17 17])} for th
is supervisor 93d81c4b-c87c-491c-a808-b8e8d7f259a6 on port 6702 with id 80cbcdc2-9c5a-49b5-b658-e4f478a382ef


When the word count system.exit(-1) happens it kills the worker processes and supervisor notices this and restarts the worker with newly downloaded topology. 
I don't see this as an issue and this is an expected behavior. Storm worker processes is fail-fast(infact all of storm dameons) i.e if topology has a coding issue like not catching exceptions and doing system.exit(-1) workers will be killed and than restarted by the topology. Its up to the user to identify these issues in the topologies.

If incase you are noticing different pattern of an error and if its possible for you to upload your topology where you are seeing this issue that will be helpful., My issue might be related to the fact that our topology is 30MB. 
As you can see from my logs, the supervisor removes the code after starting the worker and then eventually downloads the code. 

Is there an ordering problem in the supervisor code where it gets confused about which topologies are assigned to the worker and it removes and redownloads the code instead of keeping it around?, [~halorgium] Filed STORM-739 to fix deletion of topology jar from supervisor incase if a worker dies., [~halorgium] went through supervisor code again to see if its re-downloading if the worker dies. Actually this is not happening it only clears the topology jars if the assignments are not there anymore. So the current code looks fine to me. If you can attach the supervisor log where you seeing the deletion happens that will be helpful. I can't reproduce this case with a modified wordcount topology.  In your cluster are you running single supervisor or multiple supervisors., [~halorgium] also can you try increasing nimbus.monitor.freq.secs to 120 or higher value. Its by default set to 10 which causes the topology rebalance to happen quickly., file not found, stormconf.ser, occurrences in 0.9.4

Notes:
* these logs are from a brand new storm and zookeeper cluster built from 0.9.4
* logs are in debug mode
* the worker logs are split into worker.id's
* fields have been added to the logs

Observations:
* This behavior "seems" the same to me as what I was seeing in 0.9.3
* Our app logs show it best - the workers are restarting about every 14 minutes.  Their heartbeats just "stop".  I don't know if / how this may be related to the tmp files being pulled out from underneath them ( stormconf.ser file not found ).  I figure it is related in that the stormconf.ser file not found error seems to be related to crashing workers.

worker_logs.tar.gz
------------------------worker logs not dealing with zookeeper/kafka
workers_with_stormconf.ser.gz
------------------------a list of worker instances/logs where the stormconf.ser file not found error occurred
worker_logs_of_zookeeper_traffic_*
------------------------worker logs filtered by logger
worker_logs_of_kafka_traffic.tar.gz
------------------------worker logs filtered by logger
supervisor_logs.tar.gz
nimbus.log.gz
6703_all_storm_topologies.log
------------------------application logging from our bolts, Github user HQebupt commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-92704866
  
    It works. Thanks.
, [~maassql] Thanks for the details and logs. Did you try increasing the nimbus.monitor.freq.secs , did that help or you are still seeing the same issues., Github user ssudhaiyer commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-107232634
  
    I'm using 0.9.4 and seeing this issue. This seems to happen for me after an exception in my topology. I deactivate the topology and try deploying the topology again and I run into this issue.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-107257398
  
    @ssudhaiyer can you give us some steps on reproducing this.
, Github user ssudhaiyer commented on the pull request:

    https://github.com/apache/storm/pull/418#issuecomment-110348566
  
    I'm seeing this error again. I'm able to reproduce this when there in an exception in my spout ( in this case it was a gson parse failure), I attempted to deactivate and activate my topology. I see this issue. Though I find the .ser file is present in the directory.
    
    java.io.FileNotFoundException: File '/mnt/storm/supervisor/stormdist/my-topology-3-1433613276/stormconf.ser' does not exist
    	at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
    	at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
    	at backtype.storm.config$read_supervisor_storm_conf.invoke(config.clj:212) ~[storm-core-0.9.4.jar:0.9.4]
    	at backtype.storm.daemon.worker$worker_data.invoke(worker.clj:182) ~[storm-core-0.9.4.jar:0.9.4]
    	at backtype.storm.daemon.worker$fn__5033$exec_fn__1754__auto____5034.invoke(worker.clj:398) ~[storm-core-0.9.4.jar:0.9.4]
    	at clojure.lang.AFn.applyToHelper(AFn.java:185) [clojure-1.5.1.jar:na]
    	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
    	at clojure.core$apply.invoke(core.clj:617) ~[clojure-1.5.1.jar:na]
    	at backtype.storm.daemon.worker$fn__5033$mk_worker__5089.doInvoke(worker.clj:389) [storm-core-0.9.4.jar:0.9.4]
    	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
    	at backtype.storm.daemon.worker$_main.invoke(worker.clj:500) [storm-core-0.9.4.jar:0.9.4]
    	at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
    	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
    	at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.4.jar:0.9.4]
    2015-06-09 12:48:58 b.s.util [ERROR] Halting process: ("Error on initialization")
    java.lang.RuntimeException: ("Error on initialization")
    	at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.4.jar:0.9.4]
    	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
    	at backtype.storm.daemon.worker$fn__5033$mk_worker__5089.doInvoke(worker.clj:389) [storm-core-0.9.4.jar:0.9.4]
    	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
    	at backtype.storm.daemon.worker$_main.invoke(worker.clj:500) [storm-core-0.9.4.jar:0.9.4]
    	at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
    	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
    	at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.4.jar:0.9.4]
, Hello,

looks like this issue was actually solved in 0.9.5?
Could anybody confirm?

If so, could the jira be updated accordingly?

Thanks a lot for the help :-)

Kind regards,
Frantz, [~fmazoyer] Yes, this is fixed in 0.9.5. It was partially fixed in 0.9.4, but some commits were missed. It is also fixed in 0.10.0.

I will update the JIRA., I am seeing same issue with 0.9.5.   Does anyone else see it?
2015-08-21 02:02:50,677 - storm - ERROR: [main-clojure.tools.logging$eval1$fn__7] Error on initialization of server mk-worker
E         java.io.FileNotFoundException: File 'var/storm/supervisor/stormdist/naas-default-1-1440147731/stormcode.ser' does not exist
E         	at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
E         	at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
E         	at backtype.storm.config$read_supervisor_topology.invoke(config.clj:219) ~[storm-core-0.9.5.jar:0.9.5]
E         	at backtype.storm.daemon.worker$worker_data.invoke(worker.clj:194) ~[storm-core-0.9.5.jar:0.9.5]
E         	at backtype.storm.daemon.worker$fn__7033$exec_fn__2594__auto____7034.invoke(worker.clj:400) ~[storm-core-0.9.5.jar:0.9.5]
E         	at clojure.lang.AFn.applyToHelper(AFn.java:185) [clojure-1.5.1.jar:na]
E         	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
E         	at clojure.core$apply.invoke(core.clj:617) ~[clojure-1.5.1.jar:na]
E         	at backtype.storm.daemon.worker$fn__7033$mk_worker__7089.doInvoke(worker.clj:391) [storm-core-0.9.5.jar:0.9.5]
E         	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
E         	at backtype.storm.daemon.worker$_main.invoke(worker.clj:502) [storm-core-0.9.5.jar:0.9.5]
E         	at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
E         	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
E         	at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.5.jar:0.9.5], Yes, I also saw same exception with 0.9.5 

2015-08-21T10:45:52.190-0700 b.s.d.worker [ERROR] Error on initialization of server mk-worker
java.io.FileNotFoundException: File '/lm/storm_home/data/supervisor/stormdist/lm-hbme-topology-2-1440178401/stormconf.ser' does not exist
        at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
        at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
        at backtype.storm.config$read_supervisor_storm_conf.invoke(config.clj:212) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.worker$worker_data.invoke(worker.clj:184) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.worker$fn__6959$exec_fn__1103__auto____6960.invoke(worker.clj:400) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.applyToHelper(AFn.java:185) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:617) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$fn__6959$mk_worker__7015.doInvoke(worker.clj:391) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$_main.invoke(worker.clj:502) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.5.jar:0.9.5]
2015-08-21T10:45:52.193-0700 b.s.util [ERROR] Halting process: ("Error on initialization")
java.lang.RuntimeException: ("Error on initialization")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$fn__6959$mk_worker__7015.doInvoke(worker.clj:391) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$_main.invoke(worker.clj:502) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.5.jar:0.9.5]
, [~rashshah] [~darsh221]  Are you noticing supervisors going down when this issue occurs? . Its common to see this in the log files after the fix as it can happen due to async nature of download and re-balancing being triggered. The fix here prevents supervisor crashing incase of such failures but it shouldn't be affecting the topology as the nimbus reassigns it to another supervisor or the supervisor tries to download the jar again., We just upgraded to 0.9.5 (because we also ran into this issue often in prev version), and we also ran into this.

the supervisors did go down in our case. 

1 caution in our upgrade is we started a new nimbus, without any supervisors attached. Then we deployed topologies (from CICD). Next we build new supervisors and the supervisors will start on startup. However, in between the network service is restarted (due to hostname changed during the build <- chef). Just wanna throw this out in case this makes a difference. 

we also see this: 
{code}
SEVERE: RuntimeException while executing runnable org.apache.storm.guava.util.concurrent.Futures$4@445058b with executor org.apache.storm.guava.util.concurrent.MoreExecutors$SameThreadExecutorService@691bc565
java.lang.RuntimeException: Failed to connect to Netty-Client-usw2b-grunt-drone32-prod.amz.relateiq.com/10.30.103.202:6700
at backtype.storm.messaging.netty.Client.connect(Client.java:308)
at backtype.storm.messaging.netty.Client.access$1100(Client.java:78)
at backtype.storm.messaging.netty.Client$2.reconnectAgain(Client.java:297)
at backtype.storm.messaging.netty.Client$2.onSuccess(Client.java:283)
at backtype.storm.messaging.netty.Client$2.onSuccess(Client.java:275)
at org.apache.storm.guava.util.concurrent.Futures$4.run(Futures.java:1181)
at org.apache.storm.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
at org.apache.storm.guava.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
at org.apache.storm.guava.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
at org.apache.storm.guava.util.concurrent.ListenableFutureTask.done(ListenableFutureTask.java:91)
at java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:384)
at java.util.concurrent.FutureTask.set(FutureTask.java:233)
at java.util.concurrent.FutureTask.run(FutureTask.java:274)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Giving up to connect to Netty-Client-usw2b-grunt-drone32-prod.amz.relateiq.com/10.30.103.202:6700 after 102 failed attempts
at backtype.storm.messaging.netty.Client.connect(Client.java:303)
{code}, I too have encountered the problem in 0.9.5 release., [~longtimer] Did you see supervisors go down or just the error appear in log?. Can you reliably reproduce this issue. If so can you try increasing nimbus.monitor.freq.secs 120 secs or higher?.

[~cywjackson] the error you pasted is not related to this JIRA., [~sriharsha] fair enough. Just wanna throw it out there in case its related. We did see the ../stormconf.ser , just didn't wanna repeat the same stacktrace. But just to confirm this still happens in 0.9.5:

{code}2015-08-26 21:08:04,135 -0700 ERROR           backtype.storm.daemon.worker:0 - Error on initialization of server mk-worker
java.io.FileNotFoundException: File '/mnt/apps/storm/data/supervisor/stormdist/EwsSubscription-14-1440646299/stormconf.ser' does not exist
        at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
        at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
        at backtype.storm.config$read_supervisor_storm_conf.invoke(config.clj:212) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.worker$worker_data.invoke(worker.clj:184) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.worker$fn__6959$exec_fn__1103__auto____6960.invoke(worker.clj:400) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.applyToHelper(AFn.java:185) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:617) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$fn__6959$mk_worker__7015.doInvoke(worker.clj:391) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$_main.invoke(worker.clj:502) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.5.jar:0.9.5]
2015-08-26 21:08:04,144 -0700 ERROR                    backtype.storm.util:0 - Halting process: ("Error on initialization")
java.lang.RuntimeException: ("Error on initialization")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$fn__6959$mk_worker__7015.doInvoke(worker.clj:391) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$_main.invoke(worker.clj:502) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.5.jar:0.9.5]
{code}, [~cywjackson] Thanks. Can you please file a jira if you seeing the other issue.  Can you confirm if the above error killed the supervisor?, [~sriharsha] , created https://issues.apache.org/jira/browse/STORM-1022 . 

our supervisor did go down. But I am not sure if it goes down because of this issue or the other issue., [~cywjackson] Thanks for the JIRA. If you have the full supervisor.log where this error occurred "/mnt/apps/storm/data/supervisor/stormdist/EwsSubscription-14-1440646299/stormconf.ser' does not exist"  I can verify., [~sriharsha] here u go. you could see there is a halt/restart of supervisor at 2015-08-26 21:04:09 . It is after downloading the EwsSubscription-14-1440646299 at 2015-08-26 21:03:44,908 -0700.  What I don't know is if this shutdown of supervisor was due to network being restart. , [~cywjackson] Thanks for the log  but this looks like some other issue not because of topology download issue though., Hi folks. I experience the same bug in 0.10.0. In my case the supervisor survives but the topology dies and it's getting rebalanced to another supervisor.

This is what I see in the supervisor log:
{code}
2015-12-16 12:56:27.746 b.s.u.Utils [INFO] Using defaults.yaml from resources
2015-12-16 12:56:27.784 b.s.u.Utils [INFO] Using storm.yaml from resources
2015-12-16 12:56:27.841 b.s.d.supervisor [INFO] Launching worker with command: 'java' '-cp' '/home/fogetti/downloads/apache-storm-0.10.0/lib/kryo-2.21.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-over-slf4j-1.6.6.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-slf4j-impl-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/minlog-1.2.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/asm-4.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-core-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/reflectasm-1.07-shaded.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-api-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/hadoop-auth-2.4.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/slf4j-api-1.7.7.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/clojure-1.6.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/servlet-api-2.5.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/disruptor-2.10.4.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/storm-core-0.10.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/conf:/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/stormjar.jar' '-Dlogfile.name=phish-storm-topology-2-1450237054-worker-6700.log' '-Dstorm.home=/home/fogetti/downloads/apache-storm-0.10.0' '-Dstorm.id=phish-storm-topology-2-1450237054' '-Dworker.id=d3819964-7671-447d-8763-827ab5cd6140' '-Dworker.port=6700' '-Dstorm.log.dir=/home/fogetti/downloads/apache-storm-0.10.0/logs' '-Dlog4j.configurationFile=/home/fogetti/downloads/apache-storm-0.10.0/log4j2/worker.xml' 'backtype.storm.LogWriter' 'java' '-server' '-Xmx768m' '-Djava.library.path=/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/resources/Linux-amd64:/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/resources:/usr' '-Dlogfile.name=phish-storm-topology-2-1450237054-worker-6700.log' '-Dstorm.home=/home/fogetti/downloads/apache-storm-0.10.0' '-Dstorm.conf.file=' '-Dstorm.options=' '-Dstorm.log.dir=/home/fogetti/downloads/apache-storm-0.10.0/logs' '-Dlogging.sensitivity=S3' '-Dlog4j.configurationFile=/home/fogetti/downloads/apache-storm-0.10.0/log4j2/worker.xml' '-Dstorm.id=phish-storm-topology-2-1450237054' '-Dworker.id=d3819964-7671-447d-8763-827ab5cd6140' '-Dworker.port=6700' '-cp' '/home/fogetti/downloads/apache-storm-0.10.0/lib/kryo-2.21.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-over-slf4j-1.6.6.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-slf4j-impl-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/minlog-1.2.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/asm-4.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-core-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/reflectasm-1.07-shaded.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-api-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/hadoop-auth-2.4.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/slf4j-api-1.7.7.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/clojure-1.6.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/servlet-api-2.5.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/disruptor-2.10.4.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/storm-core-0.10.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/conf:/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/stormjar.jar' 'backtype.storm.daemon.worker' 'phish-storm-topology-2-1450237054' '6c528751-1a10-4c33-bd54-a1ec9cb26d86' '6700' 'd3819964-7671-447d-8763-827ab5cd6140'
2015-12-16 12:56:27.888 b.s.config [INFO] SET worker-user d3819964-7671-447d-8763-827ab5cd6140 
2015-12-16 12:56:27.948 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:28.169 b.s.d.supervisor [INFO] Removing code for storm id phish-storm-topology-2-1450237054
2015-12-16 12:56:28.454 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:28.956 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:29.457 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:29.957 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:30.458 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:30.960 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:31.463 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:31.964 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:32.465 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:32.966 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:33.467 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:33.968 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:34.469 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:34.970 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:35.471 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:35.972 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:36.473 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:36.974 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:37.076 b.s.d.supervisor [INFO] Worker Process d3819964-7671-447d-8763-827ab5cd6140 exited with code: 13
2015-12-16 12:56:37.475 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:56:37.976 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
...
2015-12-16 12:58:23.196 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:58:23.697 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:58:24.198 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:58:24.699 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:58:25.200 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:58:25.701 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:58:26.202 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:58:26.702 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started
2015-12-16 12:58:27.203 b.s.d.supervisor [INFO] Worker d3819964-7671-447d-8763-827ab5cd6140 failed to start
2015-12-16 12:58:27.206 b.s.d.supervisor [INFO] Shutting down and clearing state for id d3819964-7671-447d-8763-827ab5cd6140. Current supervisor time: 1450238307. State: :not-started, Heartbeat: nil
2015-12-16 12:58:27.207 b.s.d.supervisor [INFO] Shutting down 6c528751-1a10-4c33-bd54-a1ec9cb26d86:d3819964-7671-447d-8763-827ab5cd6140
2015-12-16 12:58:27.208 b.s.config [INFO] GET worker-user d3819964-7671-447d-8763-827ab5cd6140
2015-12-16 12:58:27.259 b.s.util [INFO] Error when trying to kill 5076. Process is probably already dead.
2015-12-16 12:58:27.260 b.s.d.supervisor [INFO] Sleep 1 seconds for execution of cleanup threads on worker.
2015-12-16 12:58:28.266 b.s.util [INFO] Error when trying to kill 5076. Process is probably already dead.
2015-12-16 12:58:28.275 b.s.config [INFO] REMOVE worker-user d3819964-7671-447d-8763-827ab5cd6140
2015-12-16 12:58:28.276 b.s.d.supervisor [INFO] Shut down 6c528751-1a10-4c33-bd54-a1ec9cb26d86:d3819964-7671-447d-8763-827ab5cd6140
{code}

And this is what I see in the worker log:
{code}
2015-12-16 12:56:36.702 b.s.u.Utils [INFO] Using defaults.yaml from resources
2015-12-16 12:56:36.769 b.s.u.Utils [INFO] Using storm.yaml from resources
2015-12-16 12:56:36.921 b.s.d.worker [INFO] Launching worker for phish-storm-topology-2-1450237054 on 6c528751-1a10-4c33-bd54-a1ec9cb26d86:6700 with id d3819964-7671-447d-8763-827ab5cd6140 and conf {"topology.builtin.metrics.bucket.size.secs" 60, "nimbus.childopts" "-Xmx1024m", "ui.filter.params" nil, "storm.cluster.mode" "distributed", "storm.messaging.netty.client_worker_threads" 1, "supervisor.run.worker.as.user" false, "topology.max.task.parallelism" nil, "zmq.threads" 1, "storm.group.mapping.service" "backtype.storm.security.auth.ShellBasedGroupsMapping", "transactional.zookeeper.root" "/transactional", "topology.sleep.spout.wait.strategy.time.ms" 1, "drpc.invocations.port" 3773, "topology.multilang.serializer" "backtype.storm.multilang.JsonSerializer", "storm.messaging.netty.server_worker_threads" 1, "topology.max.error.report.per.interval" 5, "storm.thrift.transport" "backtype.storm.security.auth.SimpleTransportPlugin", "zmq.hwm" 0, "storm.principal.tolocal" "backtype.storm.security.auth.DefaultPrincipalToLocal", "supervisor.worker.shutdown.sleep.secs" 1, "storm.zookeeper.retry.times" 5, "ui.actions.enabled" true, "zmq.linger.millis" 5000, "supervisor.enable" true, "topology.stats.sample.rate" 0.05, "storm.messaging.netty.min_wait_ms" 100, "storm.zookeeper.port" 2181, "supervisor.heartbeat.frequency.secs" 5, "topology.enable.message.timeouts" true, "drpc.worker.threads" 64, "drpc.queue.size" 128, "drpc.https.keystore.password" "", "logviewer.port" 8000, "nimbus.reassign" true, "topology.executor.send.buffer.size" 1024, "topology.spout.wait.strategy" "backtype.storm.spout.SleepSpoutWaitStrategy", "ui.host" "0.0.0.0", "storm.nimbus.retry.interval.millis" 2000, "nimbus.inbox.jar.expiration.secs" 3600, "dev.zookeeper.path" "/tmp/dev-storm-zookeeper", "topology.acker.executors" nil, "topology.fall.back.on.java.serialization" true, "storm.zookeeper.servers" ["dimebag" "petrucci" "hetfield"], "nimbus.thrift.threads" 64, "logviewer.cleanup.age.mins" 10080, "topology.worker.childopts" nil, "topology.classpath" nil, "supervisor.monitor.frequency.secs" 3, "nimbus.credential.renewers.freq.secs" 600, "topology.skip.missing.kryo.registrations" false, "drpc.authorizer.acl.filename" "drpc-auth-acl.yaml", "storm.group.mapping.service.cache.duration.secs" 120, "topology.testing.always.try.serialize" false, "nimbus.monitor.freq.secs" 10, "supervisor.supervisors" [], "topology.tasks" nil, "topology.bolts.outgoing.overflow.buffer.enable" false, "storm.messaging.netty.socket.backlog" 500, "topology.workers" 1, "storm.local.dir" "/home/fogetti/downloads/apache-storm-0.10.0/storm-local", "worker.childopts" "-Xmx768m", "storm.auth.simple-white-list.users" [], "topology.message.timeout.secs" 30, "topology.state.synchronization.timeout.secs" 60, "topology.tuple.serializer" "backtype.storm.serialization.types.ListDelegateSerializer", "supervisor.supervisors.commands" [], "logviewer.childopts" "-Xmx128m", "topology.environment" nil, "topology.debug" false, "storm.messaging.netty.max_retries" 300, "ui.childopts" "-Xmx768m", "storm.zookeeper.session.timeout" 20000, "drpc.childopts" "-Xmx768m", "drpc.http.creds.plugin" "backtype.storm.security.auth.DefaultHttpCredentialsPlugin", "storm.zookeeper.connection.timeout" 15000, "storm.zookeeper.auth.user" nil, "storm.meta.serialization.delegate" "backtype.storm.serialization.GzipThriftSerializationDelegate", "topology.max.spout.pending" nil, "nimbus.supervisor.timeout.secs" 60, "nimbus.task.timeout.secs" 30, "drpc.port" 3772, "storm.zookeeper.retry.intervalceiling.millis" 30000, "nimbus.thrift.port" 6627, "storm.auth.simple-acl.admins" [], "storm.nimbus.retry.times" 5, "supervisor.worker.start.timeout.secs" 120, "storm.zookeeper.retry.interval" 1000, "logs.users" nil, "transactional.zookeeper.port" nil, "drpc.max_buffer_size" 1048576, "task.credentials.poll.secs" 30, "drpc.https.keystore.type" "JKS", "topology.worker.receiver.thread.count" 1, "supervisor.slots.ports" [6700 6701], "topology.transfer.buffer.size" 1024, "topology.worker.shared.thread.pool.size" 4, "drpc.authorizer.acl.strict" false, "nimbus.file.copy.expiration.secs" 600, "topology.executor.receive.buffer.size" 1024, "nimbus.task.launch.secs" 120, "storm.local.mode.zmq" false, "storm.messaging.netty.buffer_size" 5242880, "worker.heartbeat.frequency.secs" 1, "ui.http.creds.plugin" "backtype.storm.security.auth.DefaultHttpCredentialsPlugin", "storm.zookeeper.root" "/storm", "topology.tick.tuple.freq.secs" nil, "drpc.https.port" -1, "task.refresh.poll.secs" 10, "task.heartbeat.frequency.secs" 3, "storm.messaging.netty.max_wait_ms" 1000, "nimbus.impersonation.authorizer" "backtype.storm.security.auth.authorizer.ImpersonationAuthorizer", "drpc.http.port" 3774, "topology.error.throttle.interval.secs" 10, "storm.messaging.transport" "backtype.storm.messaging.netty.Context", "storm.messaging.netty.authentication" false, "topology.kryo.factory" "backtype.storm.serialization.DefaultKryoFactory", "worker.gc.childopts" "", "nimbus.topology.validator" "backtype.storm.nimbus.DefaultTopologyValidator", "nimbus.cleanup.inbox.freq.secs" 600, "ui.users" nil, "transactional.zookeeper.servers" nil, "supervisor.worker.timeout.secs" 30, "storm.zookeeper.auth.password" nil, "supervisor.childopts" "-Xmx256m", "ui.filter" nil, "ui.header.buffer.bytes" 4096, "topology.disruptor.wait.timeout.millis" 1000, "storm.nimbus.retry.intervalceiling.millis" 60000, "topology.trident.batch.emit.interval.millis" 500, "topology.disruptor.wait.strategy" "com.lmax.disruptor.BlockingWaitStrategy", "storm.auth.simple-acl.users" [], "drpc.invocations.threads" 64, "java.library.path" "/usr", "ui.port" 8080, "storm.messaging.netty.transfer.batch.size" 262144, "logviewer.appender.name" "A1", "nimbus.thrift.max_buffer_size" 1048576, "nimbus.host" "dimebag", "storm.auth.simple-acl.users.commands" [], "drpc.request.timeout.secs" 600}
2015-12-16 12:56:36.947 b.s.util [DEBUG] Touching file at /home/fogetti/downloads/apache-storm-0.10.0/storm-local/workers/d3819964-7671-447d-8763-827ab5cd6140/pids/5076
2015-12-16 12:56:36.962 b.s.d.worker [ERROR] Error on initialization of server mk-worker
java.io.FileNotFoundException: File '/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/stormconf.ser' does not exist
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[storm-core-0.10.0.jar:0.10.0]
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.config$read_supervisor_storm_conf.invoke(config.clj:222) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.daemon.worker$fn__7098$exec_fn__1236__auto____7099.invoke(worker.clj:418) ~[storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?]
        at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:?]
        at backtype.storm.daemon.worker$fn__7098$mk_worker__7175.doInvoke(worker.clj:409) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:?]
        at backtype.storm.daemon.worker$_main.invoke(worker.clj:542) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?]
        at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.0.jar:0.10.0]
2015-12-16 12:56:36.984 b.s.util [ERROR] Halting process: ("Error on initialization")
java.lang.RuntimeException: ("Error on initialization")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:336) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.6.0.jar:?]
        at backtype.storm.daemon.worker$fn__7098$mk_worker__7175.doInvoke(worker.clj:409) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:?]
        at backtype.storm.daemon.worker$_main.invoke(worker.clj:542) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?]
        at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.0.jar:0.10.0]
{code}

And this is what I see in the nimbus log:
{code}
2015-12-16 12:52:26.929 b.s.d.nimbus [INFO] Delaying event :do-rebalance for 10 secs for phish-storm-topology-2-1450237054
2015-12-16 12:52:27.257 b.s.d.nimbus [INFO] [req 29] Access from:  principal: op:getTopologyConf
2015-12-16 12:52:27.290 b.s.d.nimbus [INFO] [req 28] Access from:  principal: op:getTopologyInfo
2015-12-16 12:52:27.376 b.s.d.nimbus [INFO] [req 34] Access from:  principal: op:getTopology
2015-12-16 12:52:27.378 b.s.d.nimbus [INFO] [req 30] Access from:  principal: op:getTopologyConf
2015-12-16 12:52:28.505 b.s.d.nimbus [INFO] [req 32] Access from:  principal: op:getClusterInfo
2015-12-16 12:52:28.510 b.s.d.nimbus [INFO] [req 31] Access from:  principal: op:getClusterInfo
2015-12-16 12:52:28.524 b.s.d.nimbus [INFO] [req 38] Access from:  principal: op:getClusterInfo
2015-12-16 12:52:28.529 b.s.d.nimbus [INFO] [req 33] Access from:  principal: op:getNimbusConf
2015-12-16 12:52:37.055 b.s.s.EvenScheduler [INFO] Available slots: (["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6701] ["6c528751-1a10-4c33-bd54-a1ec9cb26d86" 6700] ["6c528751-1a10-4c33-bd54-a1
ec9cb26d86" 6701])
2015-12-16 12:52:37.057 b.s.d.nimbus [INFO] Setting new assignment for topology id phish-storm-topology-2-1450237054: #backtype.storm.daemon.common.Assignment{:master-code-dir "/home/fogetti/downloads/apache-storm-0.10.0/storm-local/ni
mbus/stormdist/phish-storm-topology-2-1450237054", :node->host {"6c528751-1a10-4c33-bd54-a1ec9cb26d86" "petrucci", "2c80eadd-a7bd-4470-9ba5-d17be81c94ae" "hetfield"}, :executor->node+port {[8 8] ["6c528751-1a10-4c33-bd54-a1ec9cb26d86" 
6700], [7 7] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700], [5 6] ["6c528751-1a10-4c33-bd54-a1ec9cb26d86" 6700], [3 4] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700], [9 10] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700], [2 2] ["6c528751-1a10-4c33-bd54-a1ec9cb26d86" 6700], [1 1] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700]}, :executor->start-time-secs {[9 10] 1450237957, [7 7] 1450237957, [3 4] 1450237957, [1 1] 1450237957, [8 8] 1450237957, [5 6] 1450237957, [2 2] 1450237957}}
...
2015-12-16 12:54:41.453 b.s.d.nimbus [INFO] Executor phish-storm-topology-2-1450237054:[8 8] not alive
2015-12-16 12:54:41.453 b.s.d.nimbus [INFO] Executor phish-storm-topology-2-1450237054:[2 2] not alive
2015-12-16 12:54:41.453 b.s.d.nimbus [INFO] Executor phish-storm-topology-2-1450237054:[5 6] not alive
2015-12-16 12:54:41.457 b.s.s.EvenScheduler [INFO] Available slots: (["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6701] ["6c528751-1a10-4c33-bd54-a1ec9cb26d86" 6701])
2015-12-16 12:54:41.457 b.s.d.nimbus [INFO] Reassigning phish-storm-topology-2-1450237054 to 2 slots
2015-12-16 12:54:41.457 b.s.d.nimbus [INFO] Reassign executors: [[8 8] [5 6] [2 2]]
2015-12-16 12:54:41.462 b.s.d.nimbus [INFO] Setting new assignment for topology id phish-storm-topology-2-1450237054: #backtype.storm.daemon.common.Assignment{:master-code-dir "/home/fogetti/downloads/apache-storm-0.10.0/storm-local/nimbus/stormdist/phish-storm-topology-2-1450237054", :node->host {"2c80eadd-a7bd-4470-9ba5-d17be81c94ae" "hetfield"}, :executor->node+port {[8 8] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6701], [7 7] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700], [5 6] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6701], [3 4] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700], [9 10] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700], [2 2] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6701], [1 1] ["2c80eadd-a7bd-4470-9ba5-d17be81c94ae" 6700]}, :executor->start-time-secs {[7 7] 1450237957, [1 1] 1450237957, [8 8] 1450238081, [2 2] 1450238081, [9 10] 1450237957, [3 4] 1450237957, [5 6] 1450238081}}
{code}

Hope it helps.]