[We are also affected by this, seemingly every time we increase the number of tasks of an intensive bolt above the number of executors. It's not clear to me why there would be a threadsafety issue hereâ€”aren't executors single-threaded by definition? And wouldn't there be one {{shuffle-grouper}} per executor per downstream component?

I would appreciate an increase of the priority of this ticket, as we seem to be wholly unable to increase our task/executor-level parallelism.

Here's an example of the traceback we see:
{code}
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:107)
	at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:78)
	at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:77)
	at backtype.storm.daemon.executor$eval3918$fn__3919$fn__3931$fn__3978.invoke(executor.clj:745)
	at backtype.storm.util$async_loop$fn__384.invoke(util.clj:433)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
	at backtype.storm.task.ShellBolt.execute(ShellBolt.java:164)
	at backtype.storm.daemon.executor$eval3918$fn__3919$tuple_action_fn__3921.invoke(executor.clj:630)
	at backtype.storm.daemon.executor$mk_task_receiver$fn__3839.invoke(executor.clj:398)
	at backtype.storm.disruptor$clojure_handler$reify__1560.onEvent(disruptor.clj:58)
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:104)
	... 6 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at backtype.storm.util$acquire_random_range_id.invoke(util.clj:641)
	at backtype.storm.daemon.executor$mk_shuffle_grouper$fn__3601.invoke(executor.clj:44)
	at backtype.storm.daemon.task$mk_tasks_fn$fn__3372.invoke(task.clj:158)
	at backtype.storm.daemon.executor$eval3918$fn__3919$fn__3931$bolt_emit__3958.invoke(executor.clj:660)
	at backtype.storm.daemon.executor$eval3918$fn__3919$fn$reify__3964.emit(executor.clj:695)
	at backtype.storm.task.OutputCollector.emit(OutputCollector.java:203)
	at backtype.storm.task.ShellBolt.handleEmit(ShellBolt.java:232)
	at backtype.storm.task.ShellBolt.access$500(ShellBolt.java:66)
	at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:129)
	... 1 more
{code}, It seems like it should be simple enough to change the acquire-random-rande-id
{code}
(defn acquire-random-range-id [[^MutableInt curr ^List state ^Random rand]]
  (locking curr
    (when (>= (.increment curr) (.size state))
      (.set curr 0)
      (Collections/shuffle state rand))
    (.get state (.get curr))))
{code}

But I agree that I want to understand better how multiple instances of this could be called in parallel on the same curr without the bolt or spout specifically doing it from multiple threads.  , GitHub user patricklucas opened a pull request:

    https://github.com/apache/incubator-storm/pull/49

    Add locking in acquire-random-range-id

    This fixes a threadsafety issue documented in STORM-120. This is an
    acceptable temporary fix, but the cause of concurrent access to this
    variable should still be identified.
    
    @revans2 suggested this fix.
    
    Refs STORM-120

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/patricklucas/incubator-storm STORM-120_fix_executor_threadsafety

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-storm/pull/49.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #49
    
----
commit ac993b43db57e8971a9a662c208f8ac07571e44d
Author: Patrick Lucas <plucas@yelp.com>
Date:   2014-03-21T20:33:56Z

    Add locking in acquire-random-range-id
    
    This fixes a threadsafety issue documented in STORM-120. This is an
    acceptable temporary fix, but the cause of concurrent access to this
    variable should still be identified.
    
    Refs STORM-120

----
, We have been running topologies similar to yours for a long time now and have never seen this happen, but it sounds like this is very simple for you to reproduce this, which is great for us to try and fix it.  The trick is that we need to understand why multiple threads are calling that code at the same time for the same counter, and just reading through the code I don't see anything obvious about how that is happening.

If you could create a small topology that reproduces this issue reliably that you are willing to share with the community then we can dig into it and see what is happening.  If would rather I can try and walk you through some debugging of your existing topology, but that is going to be more difficult.  My biggest question is if the stack traces are all the same when this happens?  If you can capture as many of these stack traces as possible, it would be very helpful., Sure I'll try to come up with a minimal reproducing case. In the mean time I'll collect some tracebacks. I believe they are almost always the same: an IndexOutOfBoundsError on the list of downstream tasks in a shufflegrouping, and the bad index is always the same as the size of the list. (off-by-one), What I am interested in is the full stack trace simply because it can give an indication of the thread that execution is happening on.  In this case we see that the shuffle code was called form backtype.storm.task.ShellBolt$1.run(ShellBolt.java:129) which is the base thread insize teh ShellBolt calling handleEmit.  If there is any other type of stack trace that shows up it could indicate how/where we are having the count object shared with other threads., Github user patricklucas closed the pull request at:

    https://github.com/apache/incubator-storm/pull/49
, [~revans2]: I haven't had a chance yet to create a minimal reproducing topology, but here are examples of the two different tracebacks we regularly see in such a topology:

{code}
java.lang.RuntimeException: java.lang.NullPointerException
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:107)
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:78)
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:77)
    at backtype.storm.disruptor$consume_loop_STAR_$fn__1577.invoke(disruptor.clj:89)
    at backtype.storm.util$async_loop$fn__384.invoke(util.clj:433)
    at clojure.lang.AFn.run(AFn.java:24)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
    at backtype.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:41)
    at backtype.storm.daemon.worker$mk_transfer_fn$fn__4217$fn__4221.invoke(worker.clj:123)
    at backtype.storm.util$fast_list_map.invoke(util.clj:832)
    at backtype.storm.daemon.worker$mk_transfer_fn$fn__4217.invoke(worker.clj:123)
    at backtype.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__3746.invoke(executor.clj:255)
    at backtype.storm.disruptor$clojure_handler$reify__1560.onEvent(disruptor.clj:58)
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:104)
    ... 6 more
{code}

and

{code}
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:107)
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:78)
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:77)
    at backtype.storm.daemon.executor$eval3918$fn__3919$fn__3931$fn__3978.invoke(executor.clj:745)
    at backtype.storm.util$async_loop$fn__384.invoke(util.clj:433)
    at clojure.lang.AFn.run(AFn.java:24)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
    at backtype.storm.task.ShellBolt.execute(ShellBolt.java:164)
    at backtype.storm.daemon.executor$eval3918$fn__3919$tuple_action_fn__3921.invoke(executor.clj:630)
    at backtype.storm.daemon.executor$mk_task_receiver$fn__3839.invoke(executor.clj:398)
    at backtype.storm.disruptor$clojure_handler$reify__1560.onEvent(disruptor.clj:58)
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:104)
    ... 6 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
    at java.util.ArrayList.RangeCheck(ArrayList.java:547)
    at java.util.ArrayList.get(ArrayList.java:322)
    at backtype.storm.util$acquire_random_range_id.invoke(util.clj:641)
    at backtype.storm.daemon.executor$mk_shuffle_grouper$fn__3601.invoke(executor.clj:44)
    at backtype.storm.daemon.task$mk_tasks_fn$fn__3372.invoke(task.clj:158)
    at backtype.storm.daemon.executor$eval3918$fn__3919$fn__3931$bolt_emit__3958.invoke(executor.clj:660)
    at backtype.storm.daemon.executor$eval3918$fn__3919$fn$reify__3964.emit(executor.clj:695)
    at backtype.storm.task.OutputCollector.emit(OutputCollector.java:203)
    at backtype.storm.task.ShellBolt.handleEmit(ShellBolt.java:232)
    at backtype.storm.task.ShellBolt.access$500(ShellBolt.java:66)
    at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:129)
    ... 1 more
{code}

This is happening to us in the following situation:
 * The upstream bolt has m executors and 2*m tasks
 * The downstream bolt has m executors and 2*m tasks
 * The downstream bolt uses a shuffle grouping from a non-default stream of the upstream bolt
 * When the upstream bolt has the same number of tasks as executors, this does not happen, even if the downstream bolt is m/2*m.

The traceback occurs where the upstream bolt chooses the downstream task to which to send a tuple., I tried adding locking as a temporary fix. This eliminated the IndexOutOfBoundsException, but the NullPointerException remains (albeit with slightly different line numbers due to running v0.9.2-incubating-SNAPSHOT):

{code}
java.lang.RuntimeException: java.lang.NullPointerException
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:107)
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:78)
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:77)
    at backtype.storm.disruptor$consume_loop_STAR_$fn__1577.invoke(disruptor.clj:89)
    at backtype.storm.util$async_loop$fn__384.invoke(util.clj:433)
    at clojure.lang.AFn.run(AFn.java:24)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
    at backtype.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:41)
    at backtype.storm.daemon.worker$mk_transfer_fn$fn__4217$fn__4221.invoke(worker.clj:123)
    at backtype.storm.util$fast_list_map.invoke(util.clj:832)
    at backtype.storm.daemon.worker$mk_transfer_fn$fn__4217.invoke(worker.clj:123)
    at backtype.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__3746.invoke(executor.clj:255)
    at backtype.storm.disruptor$clojure_handler$reify__1560.onEvent(disruptor.clj:58)
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:104)
    ... 6 more
{code}, Regarding the NullPointerException, it seems that a null tuple is being enqueued for transmission by Disruptor in this function, but I'm not familiar with the Storm code going up to have an intuition of where it might be coming from:

{code:clojure}
(defn mk-transfer-fn [worker]
  (let [local-tasks (-> worker :task-ids set)
        local-transfer (:transfer-local-fn worker)
        ^DisruptorQueue transfer-queue (:transfer-queue worker)]
    (fn [^KryoTupleSerializer serializer tuple-batch]
      (let [local (ArrayList.)
            remote (ArrayList.)]
        (fast-list-iter [[task tuple :as pair] tuple-batch]
          (if (local-tasks task)
            (.add local pair)
            (.add remote pair)
            ))
        (local-transfer local)
        ;; not using map because the lazy seq shows up in perf profiles
        (let [serialized-pairs (fast-list-for [[task ^TupleImpl tuple] remote] [task (.serialize serializer tuple)])]
          (disruptor/publish transfer-queue serialized-pairs)
          )))))
{code}, I believe I'm closing in on a cause: I don't think ShellBolts (and presumably spouts) are threadsafe when there are multiple running in a single executor.

In ShellBolt.prepare, two threads are created: [one|https://github.com/apache/incubator-storm/blob/1a0b46e95ab4ac467525314a75819a75dec92c40/storm-core/src/jvm/backtype/storm/task/ShellBolt.java#L109] for reading from a synchronous queue of events populated by ShellBolt.execute and writing to stdin of the subprocess, and [one|https://github.com/apache/incubator-storm/blob/1a0b46e95ab4ac467525314a75819a75dec92c40/storm-core/src/jvm/backtype/storm/task/ShellBolt.java#L141] for reading from stdout of the subprocess and calling emit/ack/fail/etc. on the OutputCollector. I believe these calls are the source of the thread safety problems.

If there are two ShellBolt tasks in the same executor, there will be two "readerThread"s emitting in parallel. These calls need to be synchronized on some aspect of the OutputCollector: To test this theory, I [patched|https://github.com/patricklucas/incubator-storm/blob/caeb80d3402fa4eeae7e03871379c18f8f4e8838/storm-core/src/clj/backtype/storm/daemon/executor.clj#L689] Storm to lock on the executor's grouper, and have not seen any of these errors having turned up the tasks-per-executor on my bolts. (I don't intend to offer this patch to merge; just as a proof-of-concept), Good catch, you are absolutely correct about that being the cause. I'm not sure on the best fix, the two approaches would be to either lock like your proposed, or to insert a queue in front of the OutputCollector and make sure only one thread ever writes to it., Any progress on this bug? With parallel streams for Collections in Java 8, this issue seems like it should bump up in priority.]