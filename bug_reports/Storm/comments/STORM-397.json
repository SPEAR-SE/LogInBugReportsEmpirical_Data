[We have not seen this issue so far.  How reproducible is the issue?  Does this happen every time the supervisor dies?  Are you using the default even scheduler or is it the isolation scheduler? If it is the isolation scheduler are either of the topologies isolated?  I assume not, because you indicated that multiple topologies were running on the same supervisor.

Is it possible for you to share the logs, including the logs for nimbus, the supervisors, and the workers.  Or to share a stripped down version of your topologies that can use to reproduce the issue?

In general storm does not reassign workers when a supervisor goes down, because the supervisor is supposed to be under supervision and be restarted quickly.  Nimbus assumes that any failure there is transient.  However, if a worker fails to heartbeat into nimbus, through zookeeper, for a default timeout of 30 seconds, it will assume that the worker has died and will mark the worker to be rescheduled.  If the supervisor is down at the time this rescheduling happens then nimbus will reschedule the worker on a different node that has a slot free.  If the supervisor is still up when the rescheduling happens or manages to restart in time for nimbus to see it up there is a possibility that the scheduler will place the worker in the same slot it was in before.  There are also lots of corner cases when you have a cluster that is nearly full and enough nodes go down that there are no longer enough slots to run everything.

Just looking at the log lines provided, the only thing that seems to be changing between the assignments is the start time aka when they were reassigned, and two assignments switch places in their order.  Looking at the code the assignment ordering looks like it might be enough to trigger the logs being printed out, but I don't really know for sure.  In either case, both schedulers really only reschedules workers when they think that they are not up and functioning., looking at the JIRA you referenced STORM-341 looks like it may be the reason you are getting reassignments, but it does not address the root cause of why one topology was rescheduled and not the other one., This is completely reproducible - every time on cluster startup we need to manually rebalance nimbus to get it running the topologies somewhere, and every time we kill the machine running the topologies it stops working.

Here's the nimbus logfile - sc-alpha-r (5abe7707-763e-461a-9327-ae2ad2979e4d) is running two topologies, called 'Sync' and 'Async', sc-beta-r is running nimbus and another spare supervisor (b252b5bd-c0c3-442b-b464-9936f628fd01), and a third supervisor is on sc-gamma-r (9466061c-dd13-4de6-927c-ac6a5c60adeb). The zookeeper cluster is running across those three machines as well.

I pull the network cable out of sc-alpha-r between 17:15:40 and 17:16:00. Notice that, beforehand, nimbus is still trying to reassign Sync to sc-alpha-r, even though it's running there already (that log file was originally 90MB, filled with those messages)

After 30s, nimbus notices that Async is not running, and reassigns it to sc-beta-r. It still tries to reassign Sync to sc-alpha-r.

Interestingly, in the UI, it now shows the two supervisors still running, but only one slot is being used (two should be used, one for each topology). I've attached the screenshot.

At 17:37:04, I manually rebalance Sync (notice that nimbus thinks that Sync is still up), and it reassigns it to sc-beta-r. But then it continues to reassign to sc-beta-r, and it will continue to do so forever.


My guess is that nimbus is getting stuck in a loop, continually trying to reassign the Sync topology, and is so busy doing that that it doesn't notice the worker running Sync has disappeared., This is really odd I have not been able to reproduce this.  My only guess is that when you pull the network cable Async gets an error and dies quickly, where as Sync does not and tries to reconnect to zookeeper.  When you plug the cable back in, it finally succeeds in reconnecting and Nimbus continues to see it up and running.  Do you have your supervisors running under supervision so they are restarted when they exit?

The 90MB of logs is almost definitely STORM-341.  I'll try to take the proposed fix for STORM-341 and come up with a patch you can try and see if that at least clear up the logs for you.  Could you please look at the logs for your workers on sc-alpha-r and try to determine when the workers crashed because of the network cut, or if they continued running., [~thecoop1984]
I made the change based off of 0.9.2, I want to be sure the fix is working before I make the pull request official.  Could you try out

https://github.com/revans2/incubator-storm/compare/STORM-341.diff

or

https://github.com/revans2/incubator-storm/tree/STORM-341

and let me know if it fixes anything, even if it just makes the logs smaller., Looks like that patch has fixed both issues - the log spam and nimbus not reassigning the topology properly. Thanks very much for looking at the problem, all looks ok now!, For your information, the worker and supervisor on sc-alpha-r all crashed when the network cable was pulled because they couldn't connect to ZK (which is also running across those 3 machines), [~thecoop1984] Sorry it took me so long to see your comment about the fix working.  I turned the diff into a formal pull request against STORM-341.  Because it fixed both issues I will dupe this one to STORM-341 and increase the priority of it to critical to match the priority here. If you feel this is in error please let me know.]