[Similar problem exists with Bolts too .. for example a Hive Bolt may not be able to write to Hive due to some intermittent network issues.  Bolts will not have 'no incoming data' case though., Have you considered implementing `backtype.storm.spout.ISpoutSleepStrategy`?

{code}
/**
 * The strategy a spout needs to use when its waiting. Waiting is
 * triggered in one of two conditions:
 * 
 * 1. `nextTuple()` emits no tuples
 * 2. The spout has hit maxSpoutPending and can't emit any more tuples
 * 
 * The default strategy sleeps for one millisecond.
 */
public interface ISpoutWaitStrategy {
    void prepare(Map conf);
    void emptyEmit(long streak);
}
{code}

If the call to a spout's `nextTuple()` method does not emit anything, Storm will call the `emptyEmit()` method with the number of times `nextTuple()` has consecutively failed to emit anything (`streak` will be reset to 0 if when the spout emits something).

The default implementation (`backtype.storm.spout.SleepSpoutWaitStrategy`) will sleep for configurable (`Config.TOPOLOGY_SLEEP_SPOUT_WAIT_STRATEGY_TIME_MS`) number of milliseconds., What do you mean by topology runner?  If you want different backoff logic ISpoutWaitStrategy would be the consumer of that data as [~ptgoetz] has pointed out.  But, I don't see much value in backing off for a different amount of time when there is no data vs. there is an error.  The wait strategy is trying to pick up the next tuple to process as quickly as possible without overwhelming the downstream system.  A single 1 ms wait works really well for this.  Adding more of a wait in most cases is to avoid doing a DDOS on the system you are consuming data from.  That does not seem to be the case here.

If you want to communicate error conditions or similar problems to an end user or a monitoring/alerting system a metric would be preferable.  Spouts can already register their own metrics for reporting errors, and the latest version of storm includes a system metric that includes counts as to why nextTuple was not called., I dont think there is a need for a different backoff logic for errors v/s no data. only metric update need to be different.

Taylor's suggestion seems promising.

Exponential backoff is typically the preferred way in such systems as opposed to 1ms fixed back off., Implementing ISpoutWaitStrategy and sleep longer seems not a great idea cause Spout should also handle messages (ack / fail) from Acker in same loop (mean same thread).
If we really want this, we should "skip" calling nextTuple for some moment without sleep, or sleep a little bit., Hi [~roshan_naik]
I think 1ms waiting time as [~revans2] recommended is simple and effective for this scenario. We had a testing for this before. 1ms waiting can make the cpu usage reduced from hundreds to around 1% while we can ensure the response of spout is still quick., Since we are going to do the java core migration and merging JStorm, I'd like to share a solution in JStorm. Let's have a look if it is helpful. There are two modes of spout, "single-thread" and "multi-thread" in JStorm. The "single-thread" mode is simliar to Storm while the "multi-thread" mode separates the processing of ack/fail and nextTuple to two threads. It means we can stay in nextTuple for a long time without any impact on ack/fail. For this scenario, user can define different backoff logic for 'no data', 'experiencing error' or 'all good' status in nextTuple. After each backoff logic, user don't need to return from nextTuple, but just start to process next event to ensure the response of spout and throughput. But, for this mode, the max pending configuration of spout might be not useful as expectation, while backpressure is a good choice for dynamic flow control., Implementing ISpoutWaitStrategy is enough to solve the problem for errors v/s no data. Of course,  Two modes of spout, "single-thread" and "multi-theads", maybe be a good choice., Multi-thread spout support is interesting, and is something we could support.  [~basti.lj] I don't think we have a JIRA for porting that functionality over yet.  If we don't could you file one? We can move discussion about that to the corresponding JIRA.

Exponential backoff makes since if you want to avoid wasting a resource and are willing to sacrifice latency to do that.  I think a 1ms fixed sleep makes the most since for the default, but if someone wants to donate a bounded exponential backoff ISpoutWaitStrategy that would be fine with me., Thanks [~basti.lj]  for the insight on CPU usage drop with a simple 1ms timeout.  I assume thats for a single spout/bolt ?
Seems like that might be adequate for many cases.

Given the prevalence of exponential back-off...there will surely be users who would prefer to employ it.. and we could give that as an option... as [~revans2] suggests.

, OK, I will create one for porting that., Yes, that is for a single spout. But I don't think we need to do this in bolt. Because, after returning from execute(), the thread of bolt will wait untill there is available tuples in receiving queue. The returning immediately from execute() in bolt will not cause any cpu spike., The bolt will need backoff in case of certain error conditions.. for example if the Hive Bolt is unable to deliver data due to I/O exceptions then it should wait before retrying...  in those cases the exponential back off might be more valuable., [~roshan_naik],

The bolts don't have any pause built in.  Spouts read from the disruptor queue in a non-blocking way, so they can handle the ack/fail messages that are sent to them.  They essentially are polling from two different sources, the user given spout code and the disruptor queue, so the wait strategy is the way to not do it in a busy loop.  Bolts are only reading from a single source, the disruptor queue.  In the common case a bolt is blocking so if a client, like HBase, is getting errors and the bolt thinks it needs to slow down its processing, it can just sleep.  This will cause the input queue to backup and if you have load aware shuffle enabled it will start to try and route tuples around the slow bolt giving them to bolts that are not getting errors.  If all of the bolts are getting errors automatic back-pressure, if it is enabled, will stop the spouts from sending any more messages until the blockage clears up.  There are potentially lots of things we can do to improve on this system, as the current back-pressure is a fairly big hammer, and it would be nice to only stop spouts that can impact the blockage, or simply throttle them so they are going at a better pace.

If you have an asynchronous bolt you can still implement a similar throttling yourself in your bolt code by limiting the maximum number of tuples that can be in flight at any point in time.  None if this really needs to be a part of the system.  If you would like to work on a solution that make it more transparent to bolts and provides system level metrics about throttling that seems OK., My observation here is ... it is preferable to externalize the fixed sleep/other sleep strategy rather than each bolt implement it.
The back pressure buildup mechanism you describe would still trigger.

In tune with the rest of your comments.]