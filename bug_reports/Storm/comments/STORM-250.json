[When this is happening have you checked to see where all of the file handles are being used?  you can run sudo lsof -p PID and look to see what is taking all of the file handles.  There might be a leak somewhere in there that we need to deal with.  If all of them look normal boosting the open file limit seems reasonable.  1024 is a good limit for most things, but a heavily used web server often will need more then that. , Hi [~revans2],

It seems to clear up by itself so I haven't caught it in the act.  I'll have to see if I can cron an lsof if the number of open file handles goes too high.

--Vito, Attached is lsof output showing file descriptors held open for many sockets which no longer exist., Reading through things lsof seems to report 'can't identify protocol' when the socket is closed, but the file descriptor is not closed.

see http://stackoverflow.com/questions/7911840/seeing-too-many-lsof-cant-identify-protocol

There could be other situations too.  My guess would be that for some reason a code path is being executed for thrift where the socket is not fully closed and we end up relying on the garbage collector to clean up the connection.  Java streams typically have a finalizer in them that will close the underlying file handle when called, but It takes two passes of the gc before they can be collected.  I have seen similar issues in other projects where a burst of traffic would make it difficult for them all to shut down.

I am not sure what is causing it here though.  I cannot reproduce it with the version of storm that I have been running, but I have not tried it with 0.9.0.1.]