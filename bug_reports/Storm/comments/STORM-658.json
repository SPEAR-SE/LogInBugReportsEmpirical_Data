[GitHub user caofangkun opened a pull request:

    https://github.com/apache/storm/pull/417

    STORM-658:when config topology.acker.executors is null, should not start acker bolts

    ![is null](https://cloud.githubusercontent.com/assets/1931407/6056600/2b606d70-ad4e-11e4-8c8c-b187f29d4a99.png)
    Before the patch:
    ![executors num is wrong](https://cloud.githubusercontent.com/assets/1931407/6056601/3354dc0a-ad4e-11e4-9f73-4b378bfd65e1.png)
    
    After the patch:

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/caofangkun/apache-storm storm-658

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/417.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #417
    
----
commit 6c04e1bea802d28ae92a234ddbaad2c1ed0a6835
Author: caofangkun <caofangkun@gmail.com>
Date:   2015-01-23T01:28:46Z

    Merge pull request #4 from apache/master
    
    Merger from apache/storm to caofangkun/apache-storm

commit d8c98cb1846bd07aef558bde410607b923aa5c1e
Author: caofangkun <caofangkun@gmail.com>
Date:   2015-02-04T01:09:22Z

    Merge pull request #5 from apache/master
    
    Merge from apache/storm to caofangkun/apache-storm

commit 4889f5249bbfa7e11195d55e59dbd8b6afc099ad
Author: caofangkun <caofangkun@gmail.com>
Date:   2015-02-05T07:44:00Z

    STORM-658:when config topology.acker.executors is null, should not start acker bolts

----
, Github user harshach commented on a diff in the pull request:

    https://github.com/apache/storm/pull/417#discussion_r24178428
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/common.clj ---
    @@ -312,7 +312,7 @@
         ))
     
     (defn has-ackers? [storm-conf]
    -  (or (nil? (storm-conf TOPOLOGY-ACKER-EXECUTORS)) (> (storm-conf TOPOLOGY-ACKER-EXECUTORS) 0)))
    +  (and (not (nil? (storm-conf TOPOLOGY-ACKER-EXECUTORS))) (> (storm-conf TOPOLOGY-ACKER-EXECUTORS) 0)))
    --- End diff --
    
    can you use not-nil? in util.clj
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/417#discussion_r24178855
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/common.clj ---
    @@ -312,7 +312,7 @@
         ))
     
     (defn has-ackers? [storm-conf]
    -  (or (nil? (storm-conf TOPOLOGY-ACKER-EXECUTORS)) (> (storm-conf TOPOLOGY-ACKER-EXECUTORS) 0)))
    +  (and (not (nil? (storm-conf TOPOLOGY-ACKER-EXECUTORS))) (> (storm-conf TOPOLOGY-ACKER-EXECUTORS) 0)))
    --- End diff --
    
    Or leave out both `not` and `nil`?  Would that also work here?
, Github user caofangkun commented on a diff in the pull request:

    https://github.com/apache/storm/pull/417#discussion_r24216861
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/common.clj ---
    @@ -312,7 +312,7 @@
         ))
     
     (defn has-ackers? [storm-conf]
    -  (or (nil? (storm-conf TOPOLOGY-ACKER-EXECUTORS)) (> (storm-conf TOPOLOGY-ACKER-EXECUTORS) 0)))
    +  (and (not (nil? (storm-conf TOPOLOGY-ACKER-EXECUTORS))) (> (storm-conf TOPOLOGY-ACKER-EXECUTORS) 0)))
    --- End diff --
    
    Fixed. Thanks.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/417#issuecomment-74128755
  
    -1. This breaks trident functionality if `topology.acker.executors` is `null` in `storm.yaml` and not overridden in the topology conf.
    
    I've not dug too far into the root cause, but the situation above results in `StackOverflowError`s in the MasterBatchCoordinator spout. See stack trace below.
    ```
    java.lang.StackOverflowError: null
            at clojure.lang.Numbers.multiply(Numbers.java:3663) ~[clojure-1.6.0.jar:na]
            at backtype.storm.stats$curr_time_bucket.invoke(stats.clj:29) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.stats$update_rolling_window.doInvoke(stats.clj:41) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at clojure.lang.RestFn.applyTo(RestFn.java:142) ~[clojure-1.6.0.jar:na]
            at clojure.core$apply.invoke(core.clj:628) ~[clojure-1.6.0.jar:na]
            at backtype.storm.stats$update_rolling_window_set$iter__2980__2984$fn__2985$fn__2986.invoke(stats.clj:77) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.stats$update_rolling_window_set$iter__2980__2984$fn__2985.invoke(stats.clj:76) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.6.0.jar:na]
            at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.6.0.jar:na]
            at clojure.lang.RT.seq(RT.java:484) ~[clojure-1.6.0.jar:na]
            at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.6.0.jar:na]
            at clojure.core$dorun.invoke(core.clj:2855) ~[clojure-1.6.0.jar:na]
            at clojure.core$doall.invoke(core.clj:2871) ~[clojure-1.6.0.jar:na]
            at backtype.storm.stats$update_rolling_window_set.doInvoke(stats.clj:76) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at clojure.lang.RestFn.invoke(RestFn.java:439) ~[clojure-1.6.0.jar:na]
            at clojure.lang.Atom.swap(Atom.java:65) ~[clojure-1.6.0.jar:na]
            at clojure.core$swap_BANG_.invoke(core.clj:2234) ~[clojure-1.6.0.jar:na]
            at backtype.storm.stats$emitted_tuple_BANG_.invoke(stats.clj:215) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.task$mk_tasks_fn$fn__4329.invoke(task.clj:160) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$fn__4615$fn__4630$send_spout_msg__4648.invoke(executor.clj:502) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$fn__4615$fn$reify__4657.emit(executor.clj:547) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:49) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at storm.trident.topology.MasterBatchCoordinator.sync(MasterBatchCoordinator.java:176) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at storm.trident.topology.MasterBatchCoordinator.ack(MasterBatchCoordinator.java:145) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$ack_spout_msg.invoke(executor.clj:399) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$fn__4615$fn__4630$send_spout_msg__4648.invoke(executor.clj:532) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$fn__4615$fn$reify__4657.emit(executor.clj:547) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:49) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at storm.trident.topology.MasterBatchCoordinator.sync(MasterBatchCoordinator.java:200) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at storm.trident.topology.MasterBatchCoordinator.ack(MasterBatchCoordinator.java:145) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$ack_spout_msg.invoke(executor.clj:399) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$fn__4615$fn__4630$send_spout_msg__4648.invoke(executor.clj:532) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$fn__4615$fn$reify__4657.emit(executor.clj:547) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:49) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at storm.trident.topology.MasterBatchCoordinator.sync(MasterBatchCoordinator.java:176) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at storm.trident.topology.MasterBatchCoordinator.ack(MasterBatchCoordinator.java:145) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$ack_spout_msg.invoke(executor.clj:399) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
            at backtype.storm.daemon.executor$fn__4615$fn__4630$send_spout_msg__4648.invoke(executor.clj:532) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    ```

, This Issue raises a valid point that Storm's behavior with a null value for {{topology.acker.executors}} might seem surprising.  Whether or not the Acker Executors will be used depends on the code.  If the topology components intend to make use of Acking, then it would be good for the common case to be handled. One Acker Executor per Worker seems like a good default here, so I am not too interested in changing it. I could be convinced otherwise.

What we might want to do instead is change the documentation—both the Markdown and the JavaDoc—to explicitly describe what the default behavior is.  We would want to update the description in Config.java and update our document titled "Guaranteeing Message Processing."  (Incidentally, the latter document already describes how to disable launching of the Acker Executors if you know that you do not want acking, which is to use {{0}}.)

We might also want to either update the example Topologies to actually make use of acking or else update them not to launch the Ackers at all so as to avoid possibly confusing users.

Another thought is that perhaps we should not count Executors that are devoted to running system Tasks like the Acker on the UI Topology Summary, since we have a button at the bottom that enables or disables inclusion of the system stats.  I looked at the UI code, and it seems we have been counting Acker Executors in the Topology Summary for a very long time.  So far we have simply counted all the Executors in the Topology whether or not the user asked to show System Stats.
, {code}
(defn add-acker! [storm-conf ^StormTopology ret]
  (let [num-executors (if (nil? (storm-conf TOPOLOGY-ACKER-EXECUTORS)) (storm-conf TOPOLOGY-WORKERS) (storm-conf TOPOLOGY-ACKER-EXECUTORS))
{code}

Setting null to "topology.acker.executors" is intentional (It is only way to use 1 acker per worker), but it looks like easy to be confusing (This issue is started with confusion) and it seems not be documented properly any places., [~kabhwan] I agree. I think we should document the current behavior., Hi, Fangjun, I see this issue has been inactive for a while. Does it matter if I take a shot on this? 
If it is OK, I will assign it to me and submit a pull request for updating the doc descriptions for TOPOLOGY_ACKER_EXECUTORS. Please feel free to pose if you have a different opinion., GitHub user zhuoliu opened a pull request:

    https://github.com/apache/storm/pull/628

    [STORM-658] Add doc descriptions for the TOPOLOGY_ACKER_EXECUTORS

    The current doc for ackers is outdated (it is for TOPOLOGY_ACKERS), we need to update the descriptions for TOPOLOGY_ACKER_EXECUTORS in Config.java, Guaranteeing-message-processing.md and Running-topologies-on-a-production-cluster.md.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/zhuoliu/storm 658

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/628.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #628
    
----
commit f3bfaa08d359ab2bf6374bf428bbd3658b097ad6
Author: zhuol <zhuol@yahoo-inc.com>
Date:   2015-07-10T18:38:06Z

    [STORM-658] Add doc descriptions for the TOPOLOGY_ACKER_EXECUTORS

----
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/417#issuecomment-120494584
  
    This PR is a bit stale.  Can we move to another PR #628 that adds clarifying documentation, and close this one?
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/628#issuecomment-120539193
  
    +1, good explanation.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/628#issuecomment-120601264
  
    @zhuoliu 
    Btw, I think we need to create new issue cause STORM-658 is actually wrong.
    Could you create new issue? Thanks!
, Github user zhuoliu commented on the pull request:

    https://github.com/apache/storm/pull/628#issuecomment-120642677
  
    Thanks @HeartSaVioR ! Sure, a new issue is created (storm-934). 
    https://issues.apache.org/jira/browse/STORM-934
, It is not a problem, but we need to improve documentation to make it clear., Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/417#issuecomment-120654669
  
    @caofangkun Could you please close this PR? Thanks!
, Github user caofangkun closed the pull request at:

    https://github.com/apache/storm/pull/417
]