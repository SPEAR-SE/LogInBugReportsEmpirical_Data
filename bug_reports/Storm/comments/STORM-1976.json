[cleanup-corrupt-topologies! is a legacy code written at very first version of Nimbus so it doesn't consider H/A of nimbus. It assumes that all topology codes should be available at Nimbus startup, but in many cases it doesn't.

Let me show some cases:

1. When we launch nimbus process from new node. The node never has topology code in local dir since this is the first time Nimbus process is launched. So while startup of Nimbus, Nimbus calls cleanup-corrupt-topologies!, and all topologies goes away from ZK.
(This is easy to reproduce and occurred during valid use case so IMO this is 'Blocker'.)

2. At a glance, Local BlobStore does full replications to all nimbuses which is same as before.
But when submitting topology, leader Nimbus starts to make assignment when topology.min.replication.count is achieved or it waits longer than topology.max.replication.wait.time.sec. So there's still a gap between topology assignment and full replications. If one or more follower nimbuses are crashed before replicating topology code at that time and restarted, cleanup-corrupt-topologies! removes that topology from ZK.

Btw, I didn't look deeply at the source code, but full replication seems to be started immediately when topology is submitted. I set nimbus.code.sync.freq.secs to 600 to give a delay, but full replication is started while topology code is uploading to leader nimbus. If then this gap may be small in LAN.

N1 (leader)
{code}
2016-07-17 07:51:39.128 o.a.s.d.nimbus [INFO] Received topology submission for production-topology2 with conf {"topology.max.task.parallelism" nil, "topology.submitter.principal" "", "topology.acker.executors" nil, "topology.eventlogger.executors" 0, "topology.debug" true, "storm.zookeeper.superACL" nil, "topology.users" (), "topology.submitter.user" "storm", "topology.kryo.register" nil, "topology.kryo.decorators" (), "storm.id" "production-topology2-1-1468741899", "topology.name" "production-topology2"}
...
2016-07-17 07:51:39.301 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormjar.jar/<node1>:6627-1
2016-07-17 07:51:39.346 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormconf.ser/<node1>:6627-1
2016-07-17 07:51:39.389 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormcode.ser/<node1>:6627-1
...
2016-07-17 07:51:39.400 o.a.s.d.nimbus [INFO] Created download session for production-topology2-1-1468741899-stormjar.jar with id 5b4103f0-d4cf-4ec9-9191-e2a0e06ad97c
...
2016-07-17 07:51:39.403 o.a.s.d.nimbus [INFO] desired replication count 1 achieved, current-replication-count for conf key = 1, current-replication-count for code key = 1, current-replication-count for jar key = 1
...
2016-07-17 07:51:39.415 o.a.s.d.nimbus [INFO] Created download session for production-topology2-1-1468741899-stormjar.jar with id 854cabdd-080f-4c6f-91fe-81950490e67d
2016-07-17 07:51:39.436 o.a.s.d.nimbus [INFO] Activating production-topology2: production-topology2-1-1468741899
2016-07-17 07:51:40.578 o.a.s.d.nimbus [INFO] Created download session for production-topology2-1-1468741899-stormjar.jar with id 50809e38-61b2-4533-9942-0aa28cb327c0
2016-07-17 07:51:41.058 o.a.s.d.nimbus [INFO] Created download session for production-topology2-1-1468741899-stormjar.jar with id 83949b7a-ac55-484a-a4f2-5361a90ee518
2016-07-17 07:51:41.836 o.a.s.d.nimbus [INFO] Created download session for production-topology2-1-1468741899-stormcode.ser with id c6c80b3c-99da-4557-9af5-7f49d6eaccc9
2016-07-17 07:51:41.906 o.a.s.d.nimbus [INFO] Created download session for production-topology2-1-1468741899-stormconf.ser with id 051d7ced-7f8f-4170-88a2-86a01fe49e28
2016-07-17 07:51:42.703 o.a.s.d.nimbus [INFO] Created download session for production-topology2-1-1468741899-stormcode.ser with id dd7bf0ab-6b06-46e5-9b44-f4d956812a0b
2016-07-17 07:51:42.768 o.a.s.d.nimbus [INFO] Created download session for production-topology2-1-1468741899-stormconf.ser with id cea017dc-c574-4891-a00a-3febd6bd001a
{code}

N2 (non-leader)
{code}
2016-07-17 07:51:40.526 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormjar.jar/<node2>:6627-0
2016-07-17 07:51:41.806 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormjar.jar/<node2>:6627-1
2016-07-17 07:51:41.891 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormcode.ser/<node2>:6627-0
2016-07-17 07:51:41.956 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormconf.ser/<node2>:6627-0
{code}

N3 (non-leader)
{code}
2016-07-17 07:51:40.999 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormjar.jar/<node3>:6627-0
2016-07-17 07:51:42.665 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormjar.jar/<node3>:6627-1
2016-07-17 07:51:42.735 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormcode.ser/<node3>:6627-0
2016-07-17 07:51:42.795 o.a.s.cluster [INFO] setup-path/blobstore/production-topology2-1-1468741899-stormconf.ser/<node3>:6627-0
...
{code}, cleanup-corrupt-topologies! should change the condition for cleaning up to check availability of whole nimbuses.
I'm not sure if replication count works. If it does, we can just use this. If it doesn't work, we should just remove cleanup-corrupt-topologies!., You are right, checking against replication count will not work. I had seen the issue when the replication count was 1., I didn't mean configuration of replication count. I'm saying current replicated count for specific blob.
Anyway I'm just thinking about getting rid of that code, since cleaning up corrupt topology when only startup is not making sense for now. We could eventually set stable condition for checking corrupted topology, and clean up periodically, but for now I don't have an idea., [~kabhwan] as far as I know first version of nimbus doesn't do clean up if its non-leader. From your description it seems that non-leader nimbus is calling the cleanup of the topologies. Is that right?, [~sriharsha]
Yes condition statement for checking leader seems to be deleted.
Btw, old logic also doesn't make sense since nimbus was elected to leader only when it has all topologies code locally. So effectively no-op., [~kabhwan] I think the old logic is seems ok to me. It makes assumption about the topology jar is replicated on all the nimbus nodes. Which makes sense when the user deploys the topology we immediately sync to all the nimbus nodes hence the reason we had min.num.replication , its the criteria that minimum number of nimbus nodes need to have the topology jar for the uploadJar to be successful. Not sure if the new code follows that.
, [~sriharsha] 
Yes I mean that cleanup-corrupt-topologies! was effectively no-op (so can be deleted) since leader should have all the topology codes which is actually what cleanup-corrupt-topologies! is trying to check for cleaning up. 
If a nimbus meets the condition for leader, there's always no corrupt topologies.
cleanup-corrupt-topologies! is executed only when starting up nimbus.

Btw, new code follows the criteria: waiting for replicating via topology.min.replication.count and topology.max.replication.wait.time.sec., GitHub user HeartSaVioR opened a pull request:

    https://github.com/apache/storm/pull/1571

    STORM-1976 Remove cleanup-corrupt-topologies!

    * cleanup-corrupt-topologies! was born from non-H/A Nimbus age
    * Keeping this with Nimbus H/A makes various issues which in result topology going away
    
    Please refer [STORM-1976](https://issues.apache.org/jira/browse/STORM-1976) for more details.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/HeartSaVioR/storm STORM-1976

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/1571.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1571
    
----
commit 391a8460345f29a25cdf0041368e36069709093b
Author: Jungtaek Lim <kabhwan@gmail.com>
Date:   2016-07-17T23:05:25Z

    STORM-1976 Remove cleanup-corrupt-topologies!
    
    * cleanup-corrupt-topologies! was born from non-H/A Nimbus age
    * Keeping this with Nimbus H/A makes various issues which in result topology going away

----
, GitHub user HeartSaVioR opened a pull request:

    https://github.com/apache/storm/pull/1572

    STORM-1976 Remove cleanup-corrupt-topologies! (1.x)

    For master branch: #1571 
    
    * cleanup-corrupt-topologies! was born from non-H/A Nimbus age
    * Keeping this with Nimbus H/A makes various issues which in result topology going away
    
    Please refer [STORM-1976](https://issues.apache.org/jira/browse/STORM-1976) for more details.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/HeartSaVioR/storm STORM-1976-1.x

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/1572.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1572
    
----
commit d677f54b4d6281733169e493eb3330bbde95bbe7
Author: Jungtaek Lim <kabhwan@gmail.com>
Date:   2016-07-17T22:48:55Z

    STORM-1976 Remove cleanup-corrupt-topologies!
    
    * cleanup-corrupt-topologies! was born from non-H/A Nimbus age
    * Keeping this with Nimbus H/A makes various issues which in result topology going away

----
, Github user harshach commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    +1 . build failures are not related to the patch.
, Github user revans2 commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    +1, but I would like to see a tool that can kill a corrupted topology without nimbus being involved, or instructions somewhere on how to do it by logging directly into ZooKeeper.
, For me I keep seeing this.  Min replication count was not set but the cluster was run in HA mode and something bad happened, and then HA did not recover cleanly.  This issue is that without with min replication count HA does not really work properly.  It would be nice to have a way to make this simpler for end users so they don't have something else to configure.  But the reality is that a lot of this feels like user errors and errors in documentation, not blockers for the software.  Additionally I don't think we want to "fix" min replication count not being set quickly.  I think it is a more complex problem., Github user HeartSaVioR commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    @revans2 
    Yes agreed. After removing this code we can't remove corrupted topology in normal way since no nimbus can gain leadership. It would be nice to have a tool to show replication count of topologies *without leader nimbus involved* to determine corrupted topology, and kill one if users want to.
, Github user harshach commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    @revans2 @HeartSaVioR why can't we enforce the min.replication.count by looking nimbus.seeds if there are more than one host in that list we should set the min.replication.count to 2. User can override the setting by specifying it in storm.yaml. My understanding is they are not doing it and we are relying on min.replication.count to be 1.
, Github user revans2 commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    @HeartSaVioR actually I have thought about it more and I am probably overreacting.  If you want to call these blockers  it is not that big of a deal.  I would just like the community have a follow on discussion about how we really want HA to behave in these situations now that we have had some experience with people running with HA.
, Github user revans2 commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    @harshach I don't think that would fix the issue, although it is an improvement.  If I think about it more I think what happened was that they were not running with HA/expecting HA to work, but ran into a bad situation where they really would have wanted to be running with HA, and then when they tried to recover, by bringing up a new nimbus instance, all of the topologies were killed.
, Github user harshach commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    @HeartSaVioR @revans2 are we ok with merging this in and file a follow-up JIRA to have a discussion around the HA and expectations from user.
, Github user revans2 commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    Yes I am good with merging this in.
, Github user HeartSaVioR commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    Yes I think discussion of Nimbus H/A and BlobStore would be longer, so we need to apply short-term fix first. There're lots of critical bugfixes in 1.0.2 so we shouldn't drag the release because of struggling long-term fix.
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/1572
, Github user HeartSaVioR closed the pull request at:

    https://github.com/apache/storm/pull/1571
, Github user HeartSaVioR commented on the issue:

    https://github.com/apache/storm/pull/1571
  
    It seems to be off sync due to fixing rebase conflict. Merged to master. Closing.
, Set fix version/s to 1.0.3 tentatively. It's up to vote result, and this could be 1.0.2., Github user HeartSaVioR commented on the issue:

    https://github.com/apache/storm/pull/1572
  
    Regarding tool, I filed an issue: https://issues.apache.org/jira/browse/STORM-1985
]