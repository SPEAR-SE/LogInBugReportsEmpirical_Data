[I think I was able to reproduce it on my laptop, and will start trying to dig into it, but I have jury duty this week so I have no idea what my schedule is going to be like., This might be related.  In my supervisor logs I see for a specific topology that it failed to download the jar, etc for it.

{code}
AsyncLocalizer Executor - 2 [WARN] Failed to download blob LOCAL TOPO BLOB TOPO_CONF TumblingWindowTestt2-20-1510353392 will try again in 100 ms
{code}

Looking at nimbus for it I see.

{code}
2017-11-10 16:36:32.412 o.a.s.d.n.Nimbus pool-14-thread-28 [INFO] Received topology submission for TumblingWindowTestt2
...
2017-11-10 16:36:41.834 o.a.s.d.n.Nimbus timer [INFO] Setting new assignment for topology id TumblingWindowTestt2-20-1510353392
...
2017-11-10 16:38:12.529 o.a.s.d.n.Nimbus pool-14-thread-51 [INFO] Delaying event REMOVE for 30 secs for TumblingWindowTestt2-20-1510353392
2017-11-10 16:38:12.529 o.a.s.d.n.Nimbus pool-14-thread-51 [INFO] TRANSITION: TumblingWindowTestt2-20-1510353392 KILL null true
2017-11-10 16:38:12.531 o.a.s.d.n.Nimbus pool-14-thread-51 [INFO] Adding topo to history log: TumblingWindowTestt2-20-1510353392
...
2017-11-10 16:38:42.019 o.a.s.d.n.Nimbus timer [INFO] Executor TumblingWindowTestt2-20-1510353392:[1, 1] not alive
...
2017-11-10 16:38:42.020 o.a.s.d.n.Nimbus timer [INFO] Setting new assignment for topology id TumblingWindowTestt2-20-1510353392 //No assignments
...
2017-11-10 16:38:42.533 o.a.s.d.n.Nimbus timer [INFO] Killing topology: TumblingWindowTestt2-20-1510353392
...
{code}

Only after the topology was totally killed did the supervisor start doing anything about it.

{code}
2017-11-10 16:39:36.639 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [WARN] Failed to download blob LOCAL TOPO BLOB TOPO_CONF TumblingWindowTestt2-20-1510353392 will try again in 100 ms
{code}

It was just under 3 mins from the time the worker was scheduled that the supervisor started to look at it.

I will try to see what the supervisor was doing at that time., So the supervisor was in waiting for blob localization on the slots it needed.

{code}
2017-11-10 16:39:36.629 o.a.s.d.s.Slot SLOT_6701 [INFO] STATE EMPTY msInState: 181025 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2017-11-10 16:39:36.629 o.a.s.d.s.Slot SLOT_6702 [INFO] STATE EMPTY msInState: 181026 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2017-11-10 16:39:36.633 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE EMPTY msInState: 53 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
{code}

Looks like 6700 was the bottleneck (as it was only in that state for 53 ms, not the 3 mins of the others).

It was waiting for 3 mins to kill another worker.

{code}
2017-11-10 16:39:36.580 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE KILL msInState: 179992 topo:TumblingWindowTestt500-18-1510353319 worker:null -> EMPTY msInState: 0
{code}

Which looks a bit like turtles all they way down.  It appears that the 3 min timeout for both is causing them to just miss each other.  Where the supervisor gets in a bad state that takes 3 mins to recover from, but then the test times out after 3 mins and kills the topology which causes the next topology to wait 3 mins to be launched....

I'll see if I can find the first one with issues. , I am seeing a lot of transitions in there to updating the blob.  My guess is that the blob changes right before it is killed (a.k.a it is deleted).  This is causing extra transitions to happen and slowing everything down.  Possibly even confusing the slot so it is stuck waiting for some timeout to happen., I also found the following killing off the supervisor that needs to be fixed, but I have to go so I may not be able to work on this more until tonight.

{code}
2017-11-13 08:54:22.597 o.a.s.d.s.Slot SLOT_6703 [ERROR] Error when processing event
java.io.FileNotFoundException: File '/.../storm-local/supervisor/stormdist/SlidingWindowTestw2s5-4-1510584685/stormconf.ser' does no
t exist
        at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292) ~[commons-io-2.5.jar:2.5]
        at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815) ~[commons-io-2.5.jar:2.5]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:269) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:425) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:264) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.localizer.AsyncLocalizer.getLocalResources(AsyncLocalizer.java:332) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.localizer.AsyncLocalizer.releaseSlotFor(AsyncLocalizer.java:496) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:632) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:391) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:1120) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
{code}, So it appears to be something related to the AsyncLocallizer checking if a blob is fully downloaded as part of the periodic check to see if it has changed.  When I comment out that part everything works fine, except if the files get deleted part way through the supervisor will not recover.  I am not totally sure why this is the case, but I am going to try to dig into it., OK I found it, and what a bug it is.  When checking to see if the topology jar was fully downloaded it was verifying that the resources subdirectory was created and exists.  But for this particular jar there is no resources subdirectory within the jar.  So it is never extracted.  So every 30 seconds the worker processes are killed and restarted., [~Srdo],

I just put up a pull request that I think fixes this.  Thanks for helping to debug it.]