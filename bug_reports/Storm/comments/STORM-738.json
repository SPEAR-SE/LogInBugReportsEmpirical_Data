[[~dashengju]
Let me say about heartbeat timeout issue.

First, I described a constraint from at first of PR, and you encounter this situation.
https://github.com/apache/storm/pull/286#issuecomment-58366793

Below links are describing why I introduce this constraint.
https://github.com/apache/storm/pull/286#issuecomment-61719213
https://github.com/apache/storm/pull/286#issuecomment-62295232
https://github.com/apache/storm/pull/286#issuecomment-62295379

Second, it is actually a bug. ;(
In ShellSpout heartbeat is updated whether message from subprocess is sync or not. So we can avoid constraint which I described already.
But it doesn't applied to ShellBolt. 
I will take care of it. Thanks for letting me know!, About overflow-control, I don't think it's good for ShellBolt to see subprocess memory usage cause it's on JVM.

Actually I think it is a natural behavior. If you use normal Bolt, your JVM process will have an issue unless you increase enough parallelism or worker count.
Subprocess still have a chance to launch threads or processes to take care of flooding tuples. In this situation launching threads seems not resolve this issue cause it is a parser and python (normally) uses GIL.

Could you have more Parser ShellBolt workers and see it helps?, [~kabhwan]  
With normal JVM bolt, all tuples are cached in Netty, and we can limit the jvm memory size. So it will not consume all the machine's memory.

But with python bolt, the python process's memory can not control. 
I do not think it's good for ShellBolt to see subprocess memory usage. I think we can limit Python script's queue size, when the queue is getting full, It can tell the ShellBolt's writerThread just stop send tuples., AFAIK it would be a comparison between OOME from one worker (JVM) and OOM issue from OS.
Seems like we can't avoid both things with your situation (we cannot block flooding tuples with unacked, am I right?), but definitely latter is more dangerous.

It's about the strategy, so I'd like to hear opinion from other contributors and committers., Yes, It's more dangerous in OOM issue from OS. It affect other normal topology., I thought overflow-control once more, and there seems to two different approaches to get over.

A. ShellBolt side control

We can modify ShellBolt to have sent tuple ids list, and stop sending tuples when list exceeds configured max value. In order to achieve this, subprocess should notify "tuple id is complete" to ShellBolt.

- It introduces new commands for multi-lang, "proceed" (or better name)
- ShellBolt stores in-progress-of-processing tuples list.
- Its overhead could be big, subprocess should always notify to ShellBolt when any tuples are processed. 

B. subprocess side control

We can modify subprocess to check pending queue after reading tuple.
If it exceeds configured max value, subprocess can request "delay" to ShellBolt for slowing down.
When ShellBolt receives "delay", BoltWriterRunnable should stop polling pending queue and continue polling later.

How long ShellBolt wait for resending? Its unit would be "delay time" or "tuple count". I don't know which is better yet.

- It introduces new commands for multi-lang, "delay" (or better name)
- I don't think it would be introduced soon, but subprocess can request delay based on own statistics. (ex. pending tuple count * average tuple processed time for time unit, average pending tuple count for count unit)
-- We can leave when and how much to request "delay" to user. User can make his/her own algorithm to control flooding.

In my opinion B seems to more natural cause current issue is by subprocess side so it would be better to let subprocess overcome it., Filed an issue (https://issues.apache.org/jira/browse/STORM-756) and pull request based on this idea., Please note that design constraint still exists when subprocess handles tuples which takes very long time.
I'll take care of 'very busy bolt' only., [~kabhwan]  I have two questions about heartbeat mechanism:
1) I think the heartbeat mechanism should not related to ACK or NON-ACK mechanism. I means heartbeat should not rely on  ACK or NON-ACK, or else.
2) the heartbeat design constraint(subprocess is alive but cannot process "heartbeat" tuple in time) is always happen. For example: a) if ShellBolt handle one tuple need more time, may exceed the timeout, but it handle other tuples quickly;  b)  currently, the ShellBolt handle one tuple, emit the result, and read all the tuples from stdin to get the id,  read all the tuples from stdin needs long time, which cause exceed timeout. So I think we should remove the  heartbeat design constraint. 
, Let me reply your questions.

First of all, please refer https://github.com/apache/storm/pull/286#issuecomment-61719213 for some backgrounds.

We already know that it's better to remove design constraint, by letting subprocess sending heartbeat periodically.
To tell the truth, issue lies on 'multi-lang'.
Threads should read/write stdin/stdout with synchronized, but I don't know PHP / Perl supports this.

It's up to Storm Community to make a decision, dropping some languages if it cannot support.
After decision, I'll remove design constraint and make it work. 
(Though I need to help with node.js implementation. :) ), [~dashengju]
Via STORM-738 I removed the heartbeat design constraint, by let subprocess updates local pid file periodically.
If it is accepted, whole heartbeat timeout issues should be resolved.
Could you check it makes sense?]