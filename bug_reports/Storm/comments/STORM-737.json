[We got the following exception after our topology had been up for ~2 days, and I was wondering if it might be related. 
Looks like "task" in "mk-transfer-fn" is null, making "(.add remote (TaskMessage. task (.serialize serializer tuple)))" fail on NPE (worker.clj:128, storm-core-0.9.2-incubating.jar)

java.lang.RuntimeException: java.lang.NullPointerException
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	at backtype.storm.disruptor$consume_loop_STAR_$fn__758.invoke(disruptor.clj:94) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	at backtype.storm.util$async_loop$fn__457.invoke(util.clj:431) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_72]
Caused by: java.lang.NullPointerException: null
	at clojure.lang.RT.intCast(RT.java:1087) ~[clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$mk_transfer_fn$fn__5748.invoke(worker.clj:128) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	at backtype.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__5483.invoke(executor.clj:256) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	at backtype.storm.disruptor$clojure_handler$reify__745.onEvent(disruptor.clj:58) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
	... 6 common frames omitted,java.lang.RuntimeException: java.lang.NullPointerException

Any ideas?

P.S.
Also saw it here: 
http://mail-archives.apache.org/mod_mbox/storm-user/201501.mbox/%3CCABcMBhCusXXU=V1e66wfUATGYH1euQnd1SiOG65-Tp8xLWx0ww@mail.gmail.com%3E, GitHub user HeartSaVioR opened a pull request:

    https://github.com/apache/storm/pull/521

    [STORM-737] Check task->node+port with read lock to prevent sending to closed connection

    It's based on Nathan's comment, please refer to https://github.com/apache/storm/pull/349#issuecomment-87778672
    
    https://github.com/apache/storm/commit/861a92eab8740cfc0f83ac4d7ade9a2ab04a8b9b seems to make a regression. 
    But it also introduces optimizations of sending, it shouldn't be discarded.
    
    I changed send logic to let TransferDrainer matches task to node+port so then we can still enjoy optimization of sending logic.
    
    I'm still not familiar with clojure so please review and comment if it can be optimized.
    Thanks!

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/HeartSaVioR/storm STORM-737

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/521.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #521
    
----
commit a1d7b3eb343f304565fe24fb7e0151bfbcb3824e
Author: Jungtaek Lim <kabhwan@gmail.com>
Date:   2015-04-13T22:16:37Z

    While sending tuple, check task->node+port with read lock
    
    * we can ensure task->node+port is safe within read lock
    ** refer write lock inside of mk-refresh-connections
    * Let TransferDrainer matches task to node+port
    ** So then we can still enjoy optimization of sending logic

----
, [~staslev] 
Seems like it would be better to file a new issue about your situation since root cause is not discovered., Opened STORM-770, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-98850849
  
    @nathanmarz @d2r You may want to take a look since it's based on your discussion, https://github.com/apache/storm/pull/349#issuecomment-87778672.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-98857155
  
    @HeartSaVioR , I am sorry I have been swamped with another task, and I have not had a chance to review.
    
    I had a [branch](https://github.com/apache/storm/compare/master...d2r:storm-737-prevent-send-to-invalid-socket), but I had not taken an opportunity to really test it yet.
    
    I will try to get back to this within the week.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-98942013
  
    @d2r Thanks for comment, I can wait it. :)
    
    Btw, your branch doesn't follow Nathan's comment.
    ```
    In short, the code in the write-lock is fine â€“ it's the code in the read lock that needs to be fixed. As part of that, the code looking up the node+port for a task needs to be moved back to this function and not happen before the tuple goes on the transfer queue.
    ```
    Root reason is from mk-transfer-fn. 
    It matches task and node+port but there's some latencies from adding Transfer queue to sending via Netty. Task to node+port can be changed during latency so we should delegate matching it to very close to sending, within read lock to get safely.
    
    So I'd like you to compare mine and yours when you come back. Thanks!
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101382574
  
    @HeartSaVioR 
    
    Thanks for taking a look. Well, I was trying to follow his comments. :)
    
    I noticed though, that the line in my `let` binding
    ```Clojure
    valid-node+ports (vals task->node+port)
    ```
    maybe should have been
    ```Clojure
    valid-node+ports (vals @task->node+port)
    ```
    with the `@`.
    
    `task->node+port` is actually an atom, however, so if we dereference it within the read-lock there, we should have the accurate task assignment before we add anything to the queue to be sent.  I believe this addresses the concern you cited from @nathanmarz.  The other change was to encapsulate the separate TransferDrainer#add, since it does not need to be a separate public method.
    
    So, @HeartSaVioR, since we have both looked at my branch already, do you want to take my changes into your branch and continue the discussion from there?  It seems like a smaller set of changes to begin with, and maybe it would be easier going forward?
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101446038
  
    @d2r 
    First of all, I'm sorry I did some mistakes about expression.
    I mean, your approach follows some of Nathan's comment, but not all.
    Your patch can also resolve STORM-737, with dropping some tuples when task->node+port is changed, while we can save these.
    
    @nathanmarz said 861a92e made a regression, so it means, before applying 861a92e (0666c41387fc11c0422b26ab27ebc38c30fe26af) was right.
    AFAIK mk-transfer-fn is looking inside task->node+port while it should be handled from read-lock, and that's the point of regression.
    You can find out I'm trying to revert it when you see changeset of mk-transfer-tuples-handler in that commit.
    Current PR takes same logic (yes, same) from old thing but playing with TransferDrainer.
    
    * PR
    ```
    (defn mk-transfer-tuples-handler [worker]
      (let [^DisruptorQueue transfer-queue (:transfer-queue worker)
            drainer (TransferDrainer.)
            node+port->socket (:cached-node+port->socket worker)
            task->node+port (:cached-task->node+port worker)
            endpoint-socket-lock (:endpoint-socket-lock worker)
            ]
        (disruptor/clojure-handler
          (fn [packets _ batch-end?]
            (.add drainer packets)
            
            (when batch-end?
              (read-locked endpoint-socket-lock
                (let [node+port->socket @node+port->socket
                      task->node+port @task->node+port]
                  (.send drainer task->node+port node+port->socket)))
              (.clear drainer))))))
    ```
    
    * Old (that Nathan said it was right)
    https://github.com/apache/storm/blob/0666c41387fc11c0422b26ab27ebc38c30fe26af/storm-core/src/clj/backtype/storm/daemon/worker.clj
    ```
    (defn mk-transfer-tuples-handler [worker]
      (let [^DisruptorQueue transfer-queue (:transfer-queue worker)
            drainer (ArrayList.)
            node+port->socket (:cached-node+port->socket worker)
            task->node+port (:cached-task->node+port worker)
            endpoint-socket-lock (:endpoint-socket-lock worker)
            ]
        (disruptor/clojure-handler
          (fn [packets _ batch-end?]
            (.addAll drainer packets)
            (when batch-end?
              (read-locked endpoint-socket-lock
                (let [node+port->socket @node+port->socket
                      task->node+port @task->node+port]
                  ;; consider doing some automatic batching here (would need to not be serialized at this point to remove per-tuple overhead)
                  ;; try using multipart messages ... first sort the tuples by the target node (without changing the local ordering)
                
                  (fast-list-iter [[task ser-tuple] drainer]
                    ;; TODO: consider write a batch of tuples here to every target worker  
                    ;; group by node+port, do multipart send              
                    (let [node-port (get task->node+port task)]
                      (when node-port
                        (.send ^IConnection (get node+port->socket node-port) task ser-tuple))
                        ))))
              (.clear drainer))))))
    ```
    
    So, I'd like you to review current PR, and find out issues, and go together.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101449371
  
    Please note that packets shouldn't have values which can be changed while passing to transfer flow.
    task->node+port can be changed (I mean node+port for tuple can be changed), but task id for tuple is not.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101475237
  
    > @d2r
    > First of all, I'm sorry I did some mistakes about expression.
    
    No worries.
    
    > So, I'd like you to review current PR, and find out issues, and go together.
    
    OK, sounds good to me. Let me take a look.

, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30196125
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -26,10 +26,10 @@
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    --- End diff --
    
    We should remove `add` or make it private, as per [this comment](https://github.com/apache/storm/pull/349#issuecomment-87767343).  There is no need to have separate `add` and `send` methods here.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101480982
  
    I see 4 test errors using your branch.  They all appear to be test timeouts.
    
    * integration-test/test-basic-topology
    * messaging-test/
       * test-receiver-message-order
       * test-local-transport
    * netty-integration-test/test-integration
    
    I ran `mvn test` on Linux and OSX and got the same results.  Can you please check?

, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30197303
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    If we use `task` as the key, doesn't that mean we are no longer batching tuples that go to the same `node+port`?  I think we want to do this.
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30197344
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -26,10 +26,10 @@
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    --- End diff --
    
    @d2r 
    Yes, we don't need this since we're adding tuples and sending it immediately. 
    I'll make it private and add parameter to send. Thanks!
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30197889
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    @d2r 
    Yes, TransferDrainer doesn't have to do it.
    Btw, it makes me think that we may don't need to play with TransferDrainer since its main role is grouping tuples by node+port.
    
    Could you confirm that I'm right?
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30198050
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    @d2r 
    Maybe I misunderstood your comment.
    Seems like I grouped tuples by task, not node+port. It should be grouped again from mk-transfer-tuples-handler.
    I'll fix it. Maybe TransferDrainer can help this, so please forget about last comment.
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30201415
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -26,10 +26,10 @@
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    --- End diff --
    
    @d2r 
    Looking it again, it should stores tuples to buffer unless batch-end? is true.
    0666c41 stores tuples to ArrayList, and 861a92e just stores tuples to TransferDrainer itself.
    So if we changed add() to private or remove add(), we should introduce another buffer, which role may be duplicated to TransferDrainer.
    (It may not an issue when batch-end? is always true, but I don't know.)
    
    Which one do you think is better?
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101515533
  
    @d2r I can't reproduce test failures. I'll give it a try again.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30225056
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    Yeah, maybe that part we do not need to change.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101648482
  
    > @d2r I can't reproduce test failures. I'll give it a try again.
    
    OK, please update your branch, and I will re-run the tests.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101825293
  
    @d2r 
    Modified TransferDrainer to re-group messages by destination when send has called.
    I left TransferDrainer.add() cause it plays as buffer, but I'm ready to remove Transfer.add() when we think it's better to have other buffer.
    I've done upmerging, and ran unit tests with no issue.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101826046
  
    @clockfly 
    I'd like you to have a look since I don't want to destroy your optimization, but just fix the issue.
    Thanks in advance!
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101831525
  
    We may be better to revert mk-transfer-fn to 0666c41387fc11c0422b26ab27ebc38c30fe26af as grouping by task can be handled (or doesn't need to be handled) from mk-transfer-tuples-handler.
, Please ignore this comment. I'll try other approach and talk it later., Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-101841901
  
    I made another changeset which reverts whole things to 0666c41 (without ```try-serialize-local```) but leaves TransferDrainer as buffer and grouper (by host+port).
    https://github.com/HeartSaVioR/storm/commit/6ef2f11e9eade772c8ae67a6410d537871739938
    
    If we think latter is better to move on, I'll post a new PR based on 6ef2f11e9eade772c8ae67a6410d537871739938.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30463274
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    If we leave the code here unchanged, we will not need to re-group by destination later.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30463289
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -23,40 +23,62 @@
     
     import backtype.storm.messaging.IConnection;
     import backtype.storm.messaging.TaskMessage;
    +import com.google.common.collect.Maps;
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    +    for (Integer task : taskTupleSetMap.keySet()) {
    +      addListRefToMap(this.bundles, task, taskTupleSetMap.get(task));
    +    }
    --- End diff --
    
    This does not need to be public.  It is not necessary to separately add and send.
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30466949
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    @d2r 
    Think about situation like 
    
    * tuples which consist of 
      * 3 tuples corresponding to task 1 (worker A) -- (A)
      * 5 tuples corresponding to task 2 (worker A) -- (B)
      * 2 tuples corresponding to task 3 (worker B) -- (C)
    
    We can ensure we send tuples to proper worker since we check task->node+port with read lock.
    But if we don't re-group messages by worker, we'll send tuples batched by task, not worker.
    It means, though we can send 10 tuples at a moment, we send batched tuples (A), (B) to worker A separately.
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30466962
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -23,40 +23,62 @@
     
     import backtype.storm.messaging.IConnection;
     import backtype.storm.messaging.TaskMessage;
    +import com.google.common.collect.Maps;
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    +    for (Integer task : taskTupleSetMap.keySet()) {
    +      addListRefToMap(this.bundles, task, taskTupleSetMap.get(task));
    +    }
    --- End diff --
    
    @d2r Please check my previous comment.
    https://github.com/apache/storm/pull/521#discussion_r30201415
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-102749917
  
    Nathan's comments for clarifying its real issue completely makes sense.
    But we should verify what codes we should change while applying.
    IMO it is a critical path for transfer efficiency, so it would be better to have more reviewer for this patch.

, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30511279
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    I think we both agree.
    
    I suggest we do not change L132-137, since it already groups by destination.
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30513365
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    @d2r 
    Accessing task->node+port from these lines is root of this problem since it can be changed during preparing for transfer. So we should change current implementation to not accessing task->node+port.
    It should be grouped by destination (node and port) within read lock.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30521681
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -139,12 +139,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    +                    (when (not (.get remoteMap task))
    +                      (.put remoteMap task (ArrayList.)))
    +                    (let [remote (.get remoteMap task)]
                           (.add remote (TaskMessage. task (.serialize serializer tuple)))
    -                     )))) 
    +                    ))))
    --- End diff --
    
    > I suggest we do not change L132-137, since it already groups by destination.
    
    I will follow up again, I think I see what you mean in your other comment.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-103145088
  
    @HeartSaVioR,
    
    I see you are right.  There are two race conditions here:
    
    1. If we do not check assignments in the read-lock, then we could use an invalid connection. There could be an exception if we lose this race.
    
    2. If we group by destination node+port when we equeue tuples for sending, we lose the information needed to update the destination node+port when we dequeue for sending, since assignments `task->node+port` can change in the meantime.  If we lose this race, then tuples for which there is a new assignment may be dropped unnecessarily (if we fix 1.) or sent to the wrong worker.
    
    (Formerly, I did not realize 2. was also a regression with 861a92e, so I was looking for a smaller change in this PR.)
    
    > If you think latter is better to move on, I'll post a new PR based on 6ef2f11.
    
    OK, I will take a look at your other branch.
    
    >  IMO it is a critical path for transfer efficiency, so it would be better to have more reviewer for this patch.
    
    I agree. The more, the better.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30539591
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -23,40 +23,62 @@
     
     import backtype.storm.messaging.IConnection;
     import backtype.storm.messaging.TaskMessage;
    +import com.google.common.collect.Maps;
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    +    for (Integer task : taskTupleSetMap.keySet()) {
    +      addListRefToMap(this.bundles, task, taskTupleSetMap.get(task));
    +    }
       }
       
    -  public void send(HashMap<String, IConnection> connections) {
    -    for (String hostPort : bundles.keySet()) {
    -      IConnection connection = connections.get(hostPort);
    -      if (null != connection) { 
    -        ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(hostPort);
    -        Iterator<TaskMessage> iter = getBundleIterator(bundle);
    -        if (null != iter && iter.hasNext()) {
    -          connection.send(iter);
    +  public void send(HashMap<Integer, String> taskToNode, HashMap<String, IConnection> connections) {
    +    HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundleMapByDestination = groupBundleByDestination(taskToNode);
    +
    +    for (String hostPort : bundleMapByDestination.keySet()) {
    +      if (hostPort != null) {
    --- End diff --
    
    Can `hostPort` (the key) be `null` in the map returned by `groupBundleByDestination`?
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30539645
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -23,40 +23,62 @@
     
     import backtype.storm.messaging.IConnection;
     import backtype.storm.messaging.TaskMessage;
    +import com.google.common.collect.Maps;
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    +    for (Integer task : taskTupleSetMap.keySet()) {
    +      addListRefToMap(this.bundles, task, taskTupleSetMap.get(task));
    +    }
       }
       
    -  public void send(HashMap<String, IConnection> connections) {
    -    for (String hostPort : bundles.keySet()) {
    -      IConnection connection = connections.get(hostPort);
    -      if (null != connection) { 
    -        ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(hostPort);
    -        Iterator<TaskMessage> iter = getBundleIterator(bundle);
    -        if (null != iter && iter.hasNext()) {
    -          connection.send(iter);
    +  public void send(HashMap<Integer, String> taskToNode, HashMap<String, IConnection> connections) {
    +    HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundleMapByDestination = groupBundleByDestination(taskToNode);
    +
    +    for (String hostPort : bundleMapByDestination.keySet()) {
    +      if (hostPort != null) {
    +        IConnection connection = connections.get(hostPort);
    +        if (null != connection) {
    +          ArrayList<ArrayList<TaskMessage>> bundle = bundleMapByDestination.get(hostPort);
    +          Iterator<TaskMessage> iter = getBundleIterator(bundle);
    +          if (null != iter && iter.hasNext()) {
    +            connection.send(iter);
    +          }
             }
           }
    -    } 
    +    }
       }
    -  
    +
    +  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> groupBundleByDestination(HashMap<Integer, String> taskToNode) {
    +    HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundleMap = Maps.newHashMap();
    +    for (Integer task : this.bundles.keySet()) {
    +      String hostPort = taskToNode.get(task);
    +      if (hostPort != null) {
    +        for (ArrayList<TaskMessage> chunk : this.bundles.get(task)) {
    +          addListRefToMap(bundleMap, hostPort, chunk);
    +        }
    +      }
    +    }
    +    return bundleMap;
    +  }
    +
    +  private <T> void addListRefToMap(HashMap<T, ArrayList<ArrayList<TaskMessage>>> bundles,
    --- End diff --
    
    nit: Can we rename `bundles` to something like `bundleMap`, since it is also the name of a member?
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30539714
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -129,12 +129,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    --- End diff --
    
    If we have nothing in the binding form, then we can remove the `let` here.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-103187844
  
    > I made another changeset which reverts whole things to 0666c41 (without try-serialize-local) but leaves TransferDrainer as buffer and grouper (by host+port).
    > HeartSaVioR@6ef2f11
    > 
    > If you think latter is better to move on, I'll post a new PR based on 6ef2f11.
    
    I do prefer the changes on your other branch above those in this pull request because the other branch is cleaner, but it is up to you.  Either way we will want to be careful.
    
    Thanks for looking at this.
, GitHub user HeartSaVioR opened a pull request:

    https://github.com/apache/storm/pull/557

    [STORM-737] Check task->node+port with read lock to prevent sending to closed connection

    * we can ensure task->node+port is safe within read lock
    ** refer write lock inside of mk-refresh-connections
    * don't group messages from transfer-fn cause we'll match task->node+port later
    * Let TransferDrainer groups message to node+port by referring task id
    ** So then we can still enjoy optimization of sending logic
    
    Actually it's another approach from #521, but this changeset is cleaner so I'd like to move on this PR.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/HeartSaVioR/storm STORM-737-2nd-appraoch

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/557.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #557
    
----
commit 6ef2f11e9eade772c8ae67a6410d537871739938
Author: Jungtaek Lim <kabhwan@gmail.com>
Date:   2015-05-13T22:50:45Z

    While sending tuple, check task->node+port with read lock
    
    * we can ensure task->node+port is safe within read lock
    ** refer write lock inside of mk-refresh-connections
    * don't group messages from transfer-fn cause we'll match task->node+port later
    * Let TransferDrainer groups message to node+port by referring task id
    ** So then we can still enjoy optimization of sending logic

----
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-103213278
  
    @d2r Thanks for reviewing!
    I'd like to get 6ef2f11 to pulled in, so I'm closing this PR unless we agree #521 is better than #557.
, Github user HeartSaVioR closed the pull request at:

    https://github.com/apache/storm/pull/521
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/557#discussion_r30548714
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -26,88 +26,47 @@
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    -  
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  private ArrayList<TaskMessage> buffer = new ArrayList<TaskMessage>();
    +
    +  public void addAll(ArrayList<TaskMessage> tuples) {
    +    buffer.addAll(tuples);
    --- End diff --
    
    I know you mentioned this in a comment on the former pull request, but since this is just a list do you want to remove `buffer`, `addAll()`, and `clear()` from this class and instead pass an ArrayList to the `send` method?
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/557#discussion_r30549032
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -26,88 +26,47 @@
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    -  
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  private ArrayList<TaskMessage> buffer = new ArrayList<TaskMessage>();
    +
    +  public void addAll(ArrayList<TaskMessage> tuples) {
    +    buffer.addAll(tuples);
    --- End diff --
    
    @d2r 
    Anything is fine with me. 
    We may introduce new buffer to outer side, but if it feels clearer I'll be happy to do it.
, Github user d2r commented on a diff in the pull request:

    https://github.com/apache/storm/pull/557#discussion_r30549645
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -26,88 +26,47 @@
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    -  
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  private ArrayList<TaskMessage> buffer = new ArrayList<TaskMessage>();
    +
    +  public void addAll(ArrayList<TaskMessage> tuples) {
    +    buffer.addAll(tuples);
    --- End diff --
    
    Yeah, I liked your suggestion of having the new buffer.  Before TransferDrainer was added, the buffer was just an ArrayList anyway.  I do not think it makes sense to have an ArrayList "inside" the TransferDrainer, because then we are implementing wrapper methods for `addAll` and `clear` that do not add value.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-103230182
  
    @d2r Changed to have a buffer outer side of TransferDrainer.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-103575069
  
    FYI: unit tests passed for me.
    I plan to take a look at performance using [storm-perf-test](https://github.com/yahoo/storm-perf-test)
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-103668907
  
    So far I think I'm seeing a performance regression in terms of throughput, but I want to let all the test cases run to be sure. I will have more information tomorrow.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-103777141
  
    I'm also seeing a performance regression. IMO resolving issue could introduce performance drop slightly, but shouldn't too much.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-103843197
  
    Since current approach do more works within read lock, it may be necessary to test performance with previous approach, too.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-103904248
  
    I'm definitely seeing a performance regression. In a core storm topology I'm seeing a drop in throughput and slight increase in latency. In a trident topology I'm seeing a significant increase in latency (2x).
    
    I'll try testing with #521.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-103928314
  
    Testing with #521 applied to 0.10.x-branch I'm actually seeing a performance ***improvement***.
    
    With core storm topologies there's an increase in throughput and a small reduction in latency. With trident, throughput is the same and latency is slightly lower.
    
    I'll run my fault tolerance tests next to see if there's any change in behavior
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-103992667
  
    @HeartSaVioR,
    #521 passed my fault tolerance test (which randomly kills workers and tests for data loss).
    
    I'd suggest closing this pull request, and reopening #521 to address @d2r's comments there.
    

, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/557#issuecomment-104039348
  
    @ptgoetz Thanks for testing! I'll move on to #521.
, Github user HeartSaVioR closed the pull request at:

    https://github.com/apache/storm/pull/557
, GitHub user HeartSaVioR reopened a pull request:

    https://github.com/apache/storm/pull/521

    [STORM-737] Check task->node+port with read lock to prevent sending to closed connection

    It's based on Nathan's comment, please refer to https://github.com/apache/storm/pull/349#issuecomment-87778672
    
    https://github.com/apache/storm/commit/861a92eab8740cfc0f83ac4d7ade9a2ab04a8b9b seems to make a regression. 
    But it also introduces optimizations of sending, it shouldn't be discarded.
    
    I changed send logic to let TransferDrainer matches task to node+port so then we can still enjoy optimization of sending logic.
    
    I'm still not familiar with clojure so please review and comment if it can be optimized.
    Thanks!

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/HeartSaVioR/storm STORM-737

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/521.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #521
    
----
commit a1d7b3eb343f304565fe24fb7e0151bfbcb3824e
Author: Jungtaek Lim <kabhwan@gmail.com>
Date:   2015-04-13T22:16:37Z

    While sending tuple, check task->node+port with read lock
    
    * we can ensure task->node+port is safe within read lock
    ** refer write lock inside of mk-refresh-connections
    * Let TransferDrainer matches task to node+port
    ** So then we can still enjoy optimization of sending logic

commit 85af195049fd1229acc62a1b8638415c06b6cf9d
Author: Jungtaek Lim <kabhwan@gmail.com>
Date:   2015-05-13T21:21:32Z

    Change TransferDrainer to re-group msg by destination when sending

commit 52bd47b31505db9352e299178c85e990c3f10235
Author: Jungtaek Lim <kabhwan@gmail.com>
Date:   2015-05-13T21:43:28Z

    Merge branch 'master' into STORM-737

----
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-104039782
  
    Since #557 has performance regression, we picked this PR.
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30750154
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -23,40 +23,62 @@
     
     import backtype.storm.messaging.IConnection;
     import backtype.storm.messaging.TaskMessage;
    +import com.google.common.collect.Maps;
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    +    for (Integer task : taskTupleSetMap.keySet()) {
    +      addListRefToMap(this.bundles, task, taskTupleSetMap.get(task));
    +    }
       }
       
    -  public void send(HashMap<String, IConnection> connections) {
    -    for (String hostPort : bundles.keySet()) {
    -      IConnection connection = connections.get(hostPort);
    -      if (null != connection) { 
    -        ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(hostPort);
    -        Iterator<TaskMessage> iter = getBundleIterator(bundle);
    -        if (null != iter && iter.hasNext()) {
    -          connection.send(iter);
    +  public void send(HashMap<Integer, String> taskToNode, HashMap<String, IConnection> connections) {
    +    HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundleMapByDestination = groupBundleByDestination(taskToNode);
    +
    +    for (String hostPort : bundleMapByDestination.keySet()) {
    +      if (hostPort != null) {
    --- End diff --
    
    @d2r 
    No, we removed it into groupBundleByDestination(), so we don't need to check again. Thanks!
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30750199
  
    --- Diff: storm-core/src/jvm/backtype/storm/utils/TransferDrainer.java ---
    @@ -23,40 +23,62 @@
     
     import backtype.storm.messaging.IConnection;
     import backtype.storm.messaging.TaskMessage;
    +import com.google.common.collect.Maps;
     
     public class TransferDrainer {
     
    -  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
    +  private HashMap<Integer, ArrayList<ArrayList<TaskMessage>>> bundles = new HashMap();
       
    -  public void add(HashMap<String, ArrayList<TaskMessage>> workerTupleSetMap) {
    -    for (String key : workerTupleSetMap.keySet()) {
    -      
    -      ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(key);
    -      if (null == bundle) {
    -        bundle = new ArrayList<ArrayList<TaskMessage>>();
    -        bundles.put(key, bundle);
    -      }
    -      
    -      ArrayList tupleSet = workerTupleSetMap.get(key);
    -      if (null != tupleSet && tupleSet.size() > 0) {
    -        bundle.add(tupleSet);
    -      }
    -    } 
    +  public void add(HashMap<Integer, ArrayList<TaskMessage>> taskTupleSetMap) {
    +    for (Integer task : taskTupleSetMap.keySet()) {
    +      addListRefToMap(this.bundles, task, taskTupleSetMap.get(task));
    +    }
       }
       
    -  public void send(HashMap<String, IConnection> connections) {
    -    for (String hostPort : bundles.keySet()) {
    -      IConnection connection = connections.get(hostPort);
    -      if (null != connection) { 
    -        ArrayList<ArrayList<TaskMessage>> bundle = bundles.get(hostPort);
    -        Iterator<TaskMessage> iter = getBundleIterator(bundle);
    -        if (null != iter && iter.hasNext()) {
    -          connection.send(iter);
    +  public void send(HashMap<Integer, String> taskToNode, HashMap<String, IConnection> connections) {
    +    HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundleMapByDestination = groupBundleByDestination(taskToNode);
    +
    +    for (String hostPort : bundleMapByDestination.keySet()) {
    +      if (hostPort != null) {
    +        IConnection connection = connections.get(hostPort);
    +        if (null != connection) {
    +          ArrayList<ArrayList<TaskMessage>> bundle = bundleMapByDestination.get(hostPort);
    +          Iterator<TaskMessage> iter = getBundleIterator(bundle);
    +          if (null != iter && iter.hasNext()) {
    +            connection.send(iter);
    +          }
             }
           }
    -    } 
    +    }
       }
    -  
    +
    +  private HashMap<String, ArrayList<ArrayList<TaskMessage>>> groupBundleByDestination(HashMap<Integer, String> taskToNode) {
    +    HashMap<String, ArrayList<ArrayList<TaskMessage>>> bundleMap = Maps.newHashMap();
    +    for (Integer task : this.bundles.keySet()) {
    +      String hostPort = taskToNode.get(task);
    +      if (hostPort != null) {
    +        for (ArrayList<TaskMessage> chunk : this.bundles.get(task)) {
    +          addListRefToMap(bundleMap, hostPort, chunk);
    +        }
    +      }
    +    }
    +    return bundleMap;
    +  }
    +
    +  private <T> void addListRefToMap(HashMap<T, ArrayList<ArrayList<TaskMessage>>> bundles,
    --- End diff --
    
    @d2r OK, it's better than shadowing fields. Thanks!
, Github user HeartSaVioR commented on a diff in the pull request:

    https://github.com/apache/storm/pull/521#discussion_r30750378
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/worker.clj ---
    @@ -129,12 +129,12 @@
                       (.add local pair) 
     
                       ;;Using java objects directly to avoid performance issues in java code
    -                  (let [node+port (get @task->node+port task)]
    -                    (when (not (.get remoteMap node+port))
    -                      (.put remoteMap node+port (ArrayList.)))
    -                    (let [remote (.get remoteMap node+port)]
    +                  (let []
    --- End diff --
    
    @d2r Oh, right. I'll reflect it. Thanks!
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-104045430
  
    @d2r @ptgoetz Applied @d2r's comments.
, Github user d2r commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-106038982
  
    Tested with https://github.com/yahoo/storm-perf-test with the following arguments:
    ```
    --ack --bolt 4 --name test -l 1 -n 1 --workers 4 --spout 3 --testTimeSec 900
    -c topology.max.spout.pending=1092 --messageSize 10
    ```
    
    OK, I found both the data-rate and the latency are improved with this patch in my tests:
    
    master at 512d3def:
    Throughput in MB/s:
      max: 0.1519711812
      99:  0.1510783919
      90:  0.1276066783
      50:  0.1100190481
      min: 0.07258733114
    Complete Latency/10m: 265ms
    
    This PR 85c5096e merged to master at 512d3def:
    Throughput in MB/s:
      max: 0.1760864258
      99:  0.1707911174
      90:  0.1550458272
      50:  0.1419607798
      min: 0.1128451029
    Complete Latency/10m: 204ms
    
    I am also fine with the changes.
    
    +1
    
    Thank you for your patience, @HeartSaVioR.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-106086856
  
    @d2r No Problem, thanks for reviewing and taking performance test!
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-106087173
  
    @ptgoetz Could you take a look in order to complete current PR? Thanks in advance!
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/storm/pull/521#issuecomment-106087952
  
    I'm +1. As I mentioned on the other pull request this patch actually improves performance while the alternate approach created a performance regression.
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/521
, It was already resolved.]