[Your configuration looks fine. Could you post some more details about your topology? e.g. do you have tuple failures when you look in Storm UI, does the spout consume properly if you just have it emit to a dummy bolt?

I'd also recommend you try enabling debug logging for the org.apache.storm.kafka.spout package, it will likely help you figure out what's going on., The topology has 39 bolts.
There are no tuple failures in the UI.
We tried enabling debug logging, but there also we didn't see any abnormal behaviour as such. Its polling every 200 ms like it should and even when we push messages to kafka, it logs - "Polled [0] records from Kafka.".
The problem is not sequential. Even if a message is not consumed, some different message is unaffected by it. It gets consumed properly inspite of the last unconsumed message.
So somehow storm is missing a message and incrementing the offset.
I changed the pollOffSet to Uncommitted_Earliest, but there was no change.
Can u please tell us something more which can help out with the issue?


Also, how can I set TOPOLOGY_BACKPRESSURE_ENABLE as false for the kafkaSpout.
I tried - 
KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(kafkaProperties.getProperty("bootstrap.servers"), stormProperties.getProperty("TOPIC"))
                .setFirstPollOffsetStrategy(KafkaSpoutConfig.FirstPollOffsetStrategy.LATEST)
                .setRecordTranslator(new MessageDeserializer(), arguments)
                .setProp(Config.TOPOLOGY_BACKPRESSURE_ENABLE, false)
                .build();

It didn't seem to work. 
, The properties you pass to the KafkaSpoutConfig are the KafkaConsumer properties documented at https://kafka.apache.org/documentation/#newconsumerconfigs. The TOPOLOGY_BACKPRESSURE_ENABLE setting is a Storm parameter, so you should set it in your Storm config (you're likely creating a Config object when you create your topology).

{quote}
Even if a message is not consumed, some different message is unaffected by it. It gets consumed properly inspite of the last unconsumed message.
{quote}
I'm not sure I follow. Do you mean that if you put e.g. message "a" and "b" into partition 0, then "b" is emitted, but "a" isn't?

Are you producing into Kafka with acks, so you're sure the message is actually written to Kafka?

In order to narrow it down to the spout, you might want to try running a KafkaConsumer manually and verify that it doesn't drop messages. Then try using a test topology where you have only the spout and a logging bolt, and check whether messages are dropped here.

Finally, do the offsets that are skipped contain null values? The spout will skip these by default https://github.com/apache/storm/blob/v1.2.1/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java#L655., We debugged it a little more and there are multiple situations we are facing randomly.
The spout randomly stops consuming messages from kafka.

The kafka spout config looks like this - 

 KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(kafkaProperties.getProperty("bootstrap.servers"), stormProperties.getProperty("TOPIC"))
                .setFirstPollOffsetStrategy(KafkaSpoutConfig.FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST)
                .setRecordTranslator(new MessageDeserializer(), new Fields("msg"))
                .setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE)
                .setProp(ConsumerConfig.GROUP_ID_CONFIG, "message")
                .build();


* There are times when the message is not consumed by the spout, but when we kill the topology and restart it, the messages which weren't consumed get 
   consumed automatically.
* Sometimes, the same set of messages is getting consumed multiple times.
* Sometimes, even after restarting the topology, the previous messages are not getting consumed at all.
* There are also times, when if we keep the topology running, some/all (randomly) of the missed messages get processed.

Either something related to committing the messages or getting the messages from the partition is screwing this up.

I can't seem to figure out why this is happening so randomly.

And how/where are the offsets stored ?

Can u please tell something which would help in solving this?, Just to make it clear to me, how are you determining that a message was consumed or not? If you're not already doing it, please try to use the this log line to check what the spout emits [https://github.com/apache/storm/blob/d156d25d991311eaa1f5131d3dc34787f87ce684/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L488.]

 

Your spout configuration uses the NO_GUARANTEE processing guarantee, which means messages may be processed 0, 1, or more times. If you need all messages to be processed, you shouldn't use this value. When the spout is configured to use this value, it will commit messages at arbitrary times once they've been emitted, which means you can lose messages if e.g. messages disappear on your network, or a transient error occurs. Use AT_LEAST_ONCE instead (and ensure acking is enabled in your topology), to guarantee that all messages are processed at least once. That way, if a message disappears or fails in the topology, it will be retried a configurable number of times.

 

Committed offsets are stored in the Kafka __consumer_offsets topic. You can check the committed offset via the kafka-consumer-groups.sh command in your Kafka install. The script may not work in earlier versions of Kafka, I believe someone else mentioned having to upgrade to Kafka 0.11 or 1.x to get it to work.

 

Some of what you describe could be caused by the spout restarting, e.g. processing the same set of messages multiple times. Given that you're using NO_GUARANTEE, I'd only expect this if the spout worker was restarting though. I'm not sure what would be causing the rest of what you describe.

 

How large is your Kafka cluster? Which version are you using? Is the version you have installed as brokers the same version you are using for Storm?]