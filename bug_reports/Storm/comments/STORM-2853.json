[I'm having the same issue. It's pretty troubling. Any help would be appreciated., The topology I am using to re-create this issue., The issue happens because the RingBuffer in a [DisruptorQueue|https://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java] fills up and when publishing threads are trying to claim a slot they effectively get stuck doing _LockSupport.parkNanos(1L)_.

The whole scenario looks like this. When a topology is active some objects and tuples get published to the DisruptorQueue and they are consumed at the same time. After deactivating the topology the tick tuples (maybe some other kind of tuples as well) are still published and consumed. But then when the worker process is killed and restored the tuples are still published but are no longer consumed. Then the RingBuffer fills up and publishing threads cause CPU spike. This happens because if the topology starts up as deactivated the bolts are not started. That behaviour is caused by the code here:
[https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/executor.clj#L744]

In my opinion it is a bug, since the behaviour of a deactivated topology is not consistent before and after a JVM's restart.

A permanent fix would be potentially to remove the sleeping part all toghether.

A quick workaround mitigating the issue could be to check the RingBuffer state and stop trying publishing if it is full, e.g. by adding this code:
_while (_buffer.remainingCapacity() == 0) {_
 _Utils.sleep(1);_
_}_
here [https://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java#L517]

Any comments or other considerations would be appreciated, [~vorin]

Makes sense. The code might only consider about startup of topology, which is expected to be finished within small period, but it triggers the issue when there's a case worker is restarting while topology is inactive. Not sure it really did, since the code was placed at really older version, before 0.9.0 I guess, donated to Apache incubator. Nathan's repo is no longer exist so I can't track even why the code was placed for now, so I'm not 100% sure we know the possible risks of getting rid of the sleeping part.

The thing which would make the issue more complicated is, we can't disable tick tuple while topology is in inactive state, since from bolt's perspective there might be remaining tuples to process and it needs tick tuple to take some action like flush.

The only way what I can think about for now is defer activating tick tuple until all tasks in executor are completed calling open()/prepare(). Even if it makes sense, not sure whether it is easy to address or not. Need to take a look at., [~vorin]

Fortunately the fix turned out to be really simple. I submitted pull requests against master and 1.x-branch. Please take a look at if you are interested on the patch., I'm very interested as it affects our production env. Thanks for the quick fix. I'll test it and update you when I'm done.

BTW I'm trying to figure out how the storm repo is organised. What's the difference between the master and e.g. 1.x-branch? Currently the most likely explanation for me is that the master branch contains various additional projects, whereas e.g. 1.x-branch contains only what in the final distro?

Also the 2 PRs you raised. One is for the 1.x-branch in the storm-core, which is the project I was working with so far. The other is for the master branch in the storm-client project, which seems to mirror some of the storm-core functionality but without Clojure part, am I right? Am I right that the storm-client project is not included in standard storm distro? I'm trying to understand that structure so I can properly build a patched version of the storm binaries.

Is there any doc that would help me understand that repo/code structure? Didn't find anything useful in the Doc folder

Thanks, [~vorin]

Storm 2.0.0 was started for porting Clojure to Java, which was one of major change for merging JStorm. While merging JStorm is not happened, we had many voices (including me) who are in favor of get rid of Clojure in various reasons, so we went ahead and ported most of things to Java in master branch.(Regarding merging JStorm, we can restart merging works at any time since they donate the code to ASF, but huge divergence between twos are not easy to cover.)

We also have several improvements in Storm 2.0.0, and what you see is one of them: we broke storm-core down into multiple modules in upcoming Storm 2.0.0, "storm-client" which is related to client topology side (worker) interfaces and implementations (will have much less dependencies than current), "storm-server" which is related to daemon side interfaces and implementations, "storm-webserver" which is related to HTTP service and UI. We still keep "storm-core" since we didn't port back some tests yet.

So if you are really brave to test Storm 2.0.0 SNAPSHOT out, please check out master branch and build your own dist. Most of the time you would want to check out 1.x-branch to test out the change of latest 1.x version line., Merged the patch into master, 1.x, 1.1.x, 1.0.x branches.

[~vorin], I merged the patch into branches to make sure they're included to current ongoing RCs. Please reopen the issue if the patch doesn't resolve your issue. Thanks in advance!, [Jungtaek Lim|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kabhwan] I tested the patch. In our use case it resolves the issue. Thank you.

My only reservation is that we removed one of the symptoms but the original inconsistency/root cause remains. Meaning that the behavior of a deactivated topology is not consistent before and after a JVM's restart. 
 As an example. As you pointed out when a topology is deactivated we want to keep tick tuples coming as there might be remaining tuples to process/flush. Then let's assume that the worker process dies/is killed before the processing/flush can finish. In that case stateful bolts normally would resume processing using those tick tuples but currently they won't have a chance to do it.
 Well, it's just a consideration. I do understand the bigger picture perspective here and that the risks of making the bigger functional change dealing with the root cause could be too big.

Also thanks for the explanation. It cleared up a lot. Quite an interesting story as well with Clojure and JStorm :-), [~vorin]

I guess you're saying stateful implemented by user side: I don't think they can use intermediate result while crash occurs, since the result is not guaranteed to be not corrupted. If you're saying stateful supported by Storm, it doesn't leverage tick tuple from bolt side. Please elaborate your example a bit if my explanation doesn't fall into your case., [Jungtaek Lim|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kabhwan] 

Yes, stateful bolts implemented by a user. Our current application fortunately wouldn't run into such a situation. So it is more of a theoretical scenario.

So one can imagine a case e.g. of a bolt that emits tuples in batches. Let's assume it stores them somewhere e.g. in an external cache. Then when the topology is deactivated it could want to flush the current batch (e.g. after a timeout which I can imagine is triggered by a tick tuple in most implementations). So if the JVM restart happens before the timeout, the flush will not happen until the topology is activated and receives the first tick tuple. Whereas it would happen if the JVM hasn't restarted, that's the inconsistency. It's is an edge case scenario of course.]