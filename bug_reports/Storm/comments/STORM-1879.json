[The nimbus and supervisor logs for the period. The undead worker is running ZendeskTicketTopology-127-1464780171. The initial attempt at shutting it down is at 2016-06-01 13:44:39.955 in the supervisor log., I am seeing the exact same issue. It is almost trivial to reproduce just by killing all Storm processes on one of my five workers, including the supervisor which is immediately restarted by runit.

I will attach some additional supervisor logs which include DEBUG log messages for {{org.apache.storm.daemon.supervisor}} and {{org.apache.storm.util}}. Note that I have added an additional log message in order to confirm that indeed the {{pids}} for the worker are empty in the {{shutdown-worker}} function (https://github.com/apache/storm/blob/v1.0.1/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj#L277). The added line is this:
{code}(log-message "Killing the following pids for " (:supervisor-id supervisor) ":" id " : " (seq pids)){code}


, The trouble starts here:

bq. 2016-06-14 18:31:04.465 o.a.s.d.supervisor [INFO] Shutting down ee56fb9d-2657-4d3f-b52a-1ae4abae85f7:

leading to:

{quote}2016-06-14 18:31:04.471 o.a.s.util [DEBUG] Rmr path /var/lib/storm/storm-local/workers//heartbeats
2016-06-14 18:31:04.471 o.a.s.util [DEBUG] Rmr path /var/lib/storm/storm-local/workers//pids
2016-06-14 18:31:04.471 o.a.s.util [DEBUG] Rmr path /var/lib/storm/storm-local/workers/{quote}

I am still trying to figure out why this happens, but I think an {{assert}} is in order somewhere, Seems like the problem is this:

 {quote}2016-06-14 18:31:00.741 o.a.s.d.supervisor [INFO] Launching worker with assignment {:storm-id "realtime_stats-4-1465921242", :executors [[85 85] [39 39] [128 128]], :resources #object[org.apache.storm.generated.W
orkerResources 0x58f5aa7a "WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)"]} for this supervisor ee56fb9d-2657-4d3f-b52a-1ae4abae85f7 on port 6702 with id a2566676-8c17-4314-8eec-b4a5cdeb17c9
 {quote}

and this

bq. 2016-06-14 18:31:04.464 o.a.s.d.supervisor [DEBUG] Worker a2566676-8c17-4314-8eec-b4a5cdeb17c9 is :not-started: nil at supervisor time-secs 1465921864

This last line is right after the assignment from nimbus changed. The worker on port 6702 has different executors and therefore should be killed. But the currently running worker on port 6702 with id a2566676-8c17-4314-8eec-b4a5cdeb17c9 was only launched ~3.7s ago, and has not written any heartbeat files yet. Here https://github.com/apache/storm/blob/v1.0.1/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj#L536 the {{port->worker-id}} map is created from the worker heartbeats and there will be no entry for port 6702., I attached a patch that implements a preliminary workaround. Works for me., [~nico.meyer] we're seeing the same thing. Specifically:

{quote}
2016-06-14 18:31:04.465 o.a.s.d.supervisor [INFO] Shutting down ee56fb9d-2657-4d3f-b52a-1ae4abae85f7:
{quote}

Its shutting down a worker without a worker ID.  Below are the logs from one of our supervisors:

{quote}
2016-06-15 14:18:58.349 o.a.s.d.supervisor [INFO] Shutting down 0274fa9c-c271-4a76-928e-28955db4ee34:
2016-06-15 14:18:58.349 o.a.s.config [INFO] GET worker-user
2016-06-15 14:18:58.350 o.a.s.config [WARN] Failed to get worker user for . #error {
 :cause /var/storm/storm-local/workers-users (Is a directory)
 :via
 [{:type java.io.FileNotFoundException
   :message /var/storm/storm-local/workers-users (Is a directory)
   :at [java.io.FileInputStream open0 FileInputStream.java -2]}]
 :trace
 [[java.io.FileInputStream open0 FileInputStream.java -2]
  [java.io.FileInputStream open FileInputStream.java 195]
  [java.io.FileInputStream <init> FileInputStream.java 138]
  [clojure.java.io$fn__9189 invoke io.clj 229]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9201 invoke io.clj 258]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9163 invoke io.clj 165]
  [clojure.java.io$fn__9115$G__9091__9122 invoke io.clj 69]
  [clojure.java.io$reader doInvoke io.clj 102]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [clojure.lang.AFn applyToHelper AFn.java 154]
  [clojure.lang.RestFn applyTo RestFn.java 132]
  [clojure.core$apply invoke core.clj 632]
  [clojure.core$slurp doInvoke core.clj 6653]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [org.apache.storm.config$get_worker_user invoke config.clj 239]
  [org.apache.storm.daemon.supervisor$shutdown_worker invoke supervisor.clj 281]
  [org.apache.storm.daemon.supervisor$kill_existing_workers_with_change_in_components invoke supervisor.clj 536]
  [org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9078 invoke supervisor.clj 595]
  [org.apache.storm.event$event_manager$fn__8630 invoke event.clj 40]
  [clojure.lang.AFn run AFn.java 22]
  [java.lang.Thread run Thread.java 745]]}
2016-06-15 14:18:58.362 o.a.s.config [INFO] REMOVE worker-user
2016-06-15 14:18:58.362 o.a.s.d.supervisor [INFO] Shut down 0274fa9c-c271-4a76-928e-28955db4ee34:
{quote}

On another supervisor its a different error:

{quote}
2016-06-15 14:17:45.472 o.a.s.d.supervisor [INFO] Worker 4cee7f5c-2c87-48af-a44d-c1568e472f12 failed to start
2016-06-15 14:17:45.473 o.a.s.d.supervisor [INFO] Worker 96f8b5d5-a4e0-4fe2-90a8-7de85a5dd993 failed to start
2016-06-15 14:17:45.504 o.a.s.d.supervisor [INFO] Shutting down and clearing state for id 414f723a-683c-4fd8-9b57-b8742a2ddade. Current supervisor time: 1466000265. State: :disallowed, Heartbeat: {:time-secs 1466000264, :storm-id "<snipped>", :executors [[12 12] [54 54] [156 156] [42 42] [72 72] [24 24] [144 144] [162 162] [186 186] [66 66] [120 120] [102 102] [18 18] [6 6] [96 96] [150 150] [48 48] [30 30] [-1 -1] [114 114] [84 84] [90 90] [60 60] [126 126] [138 138] [36 36] [108 108] [180 180] [132 132] [168 168] [78 78] [174 174]], :port 6701}
2016-06-15 14:17:45.504 o.a.s.d.supervisor [INFO] Shutting down 0274fa9c-c271-4a76-928e-28955db4ee34:414f723a-683c-4fd8-9b57-b8742a2ddade
2016-06-15 14:17:45.505 o.a.s.config [INFO] GET worker-user 414f723a-683c-4fd8-9b57-b8742a2ddade
2016-06-15 14:17:45.507 o.a.s.config [WARN] Failed to get worker user for 414f723a-683c-4fd8-9b57-b8742a2ddade. #error {
 :cause /var/storm/storm-local/workers-users/414f723a-683c-4fd8-9b57-b8742a2ddade (No such file or directory)
 :via
 [{:type java.io.FileNotFoundException
   :message /var/storm/storm-local/workers-users/414f723a-683c-4fd8-9b57-b8742a2ddade (No such file or directory)
   :at [java.io.FileInputStream open0 FileInputStream.java -2]}]
 :trace
 [[java.io.FileInputStream open0 FileInputStream.java -2]
  [java.io.FileInputStream open FileInputStream.java 195]
  [java.io.FileInputStream <init> FileInputStream.java 138]
  [clojure.java.io$fn__9189 invoke io.clj 229]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9201 invoke io.clj 258]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9163 invoke io.clj 165]
  [clojure.java.io$fn__9115$G__9091__9122 invoke io.clj 69]
  [clojure.java.io$reader doInvoke io.clj 102]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [clojure.lang.AFn applyToHelper AFn.java 154]
  [clojure.lang.RestFn applyTo RestFn.java 132]
  [clojure.core$apply invoke core.clj 632]
  [clojure.core$slurp doInvoke core.clj 6653]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [org.apache.storm.config$get_worker_user invoke config.clj 239]
  [org.apache.storm.daemon.supervisor$shutdown_worker invoke supervisor.clj 281]
  [org.apache.storm.daemon.supervisor$sync_processes invoke supervisor.clj 427]
  [clojure.core$partial$fn__4527 invoke core.clj 2492]
  [org.apache.storm.event$event_manager$fn__8630 invoke event.clj 40]
  [clojure.lang.AFn run AFn.java 22]
  [java.lang.Thread run Thread.java 745]]}
2016-06-15 14:17:45.517 o.a.s.config [INFO] REMOVE worker-user 414f723a-683c-4fd8-9b57-b8742a2ddade
2016-06-15 14:17:45.517 o.a.s.d.supervisor [INFO] Shut down 0274fa9c-c271-4a76-928e-28955db4ee34:414f723a-683c-4fd8-9b57-b8742a2ddade
2016-06-15 14:17:45.518 o.a.s.d.supervisor [INFO] Shutting down and clearing state for id 2eb07a90-a8b7-4a81-929c-93d6edb3c2ef. Current supervisor time: 1466000265. State: :disallowed, Heartbeat: {:time-secs 1466000265, :storm-id "<snipped>", :executors [[41 41] [125 125] [137 137] [53 53] [65 65] [101 101] [149 149] [161 161] [-1 -1] [5 5] [29 29] [89 89] [173 173] [77 77] [185 185] [113 113] [17 17]], :port 6700}
2016-06-15 14:17:45.518 o.a.s.d.supervisor [INFO] Shutting down 0274fa9c-c271-4a76-928e-28955db4ee34:2eb07a90-a8b7-4a81-929c-93d6edb3c2ef
2016-06-15 14:17:45.518 o.a.s.config [INFO] GET worker-user 2eb07a90-a8b7-4a81-929c-93d6edb3c2ef
2016-06-15 14:17:45.519 o.a.s.config [WARN] Failed to get worker user for 2eb07a90-a8b7-4a81-929c-93d6edb3c2ef. #error {
 :cause /var/storm/storm-local/workers-users/2eb07a90-a8b7-4a81-929c-93d6edb3c2ef (No such file or directory)
 :via
 [{:type java.io.FileNotFoundException
   :message /var/storm/storm-local/workers-users/2eb07a90-a8b7-4a81-929c-93d6edb3c2ef (No such file or directory)
   :at [java.io.FileInputStream open0 FileInputStream.java -2]}]
 :trace
 [[java.io.FileInputStream open0 FileInputStream.java -2]
  [java.io.FileInputStream open FileInputStream.java 195]
  [java.io.FileInputStream <init> FileInputStream.java 138]
  [clojure.java.io$fn__9189 invoke io.clj 229]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9201 invoke io.clj 258]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9163 invoke io.clj 165]
  [clojure.java.io$fn__9115$G__9091__9122 invoke io.clj 69]
  [clojure.java.io$reader doInvoke io.clj 102]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [clojure.lang.AFn applyToHelper AFn.java 154]
  [clojure.lang.RestFn applyTo RestFn.java 132]
  [clojure.core$apply invoke core.clj 632]
  [clojure.core$slurp doInvoke core.clj 6653]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [org.apache.storm.config$get_worker_user invoke config.clj 239]
  [org.apache.storm.daemon.supervisor$shutdown_worker invoke supervisor.clj 281]
  [org.apache.storm.daemon.supervisor$sync_processes invoke supervisor.clj 427]
  [clojure.core$partial$fn__4527 invoke core.clj 2492]
  [org.apache.storm.event$event_manager$fn__8630 invoke event.clj 40]
  [clojure.lang.AFn run AFn.java 22]
  [java.lang.Thread run Thread.java 745]]}
2016-06-15 14:17:45.525 o.a.s.config [INFO] REMOVE worker-user 2eb07a90-a8b7-4a81-929c-93d6edb3c2ef
2016-06-15 14:17:45.525 o.a.s.d.supervisor [INFO] Shut down 0274fa9c-c271-4a76-928e-28955db4ee34:2eb07a90-a8b7-4a81-929c-93d6edb3c2ef
2016-06-15 14:17:45.526 o.a.s.d.supervisor [INFO] Shutting down and clearing state for id 96f8b5d5-a4e0-4fe2-90a8-7de85a5dd993. Current supervisor time: 1466000265. State: :not-started, Heartbeat: nil
2016-06-15 14:17:45.526 o.a.s.d.supervisor [INFO] Shutting down 0274fa9c-c271-4a76-928e-28955db4ee34:96f8b5d5-a4e0-4fe2-90a8-7de85a5dd993
2016-06-15 14:17:45.526 o.a.s.config [INFO] GET worker-user 96f8b5d5-a4e0-4fe2-90a8-7de85a5dd993
2016-06-15 14:17:45.527 o.a.s.config [INFO] REMOVE worker-user 96f8b5d5-a4e0-4fe2-90a8-7de85a5dd993
2016-06-15 14:17:45.527 o.a.s.d.supervisor [INFO] Shut down 0274fa9c-c271-4a76-928e-28955db4ee34:96f8b5d5-a4e0-4fe2-90a8-7de85a5dd993
{quote}

[~kabhwan] have you seen this before?, Hi folks,

I came here by accident, and to my surprise I had the very exact same problem that [~Srdo] and [~kevinconaway] is experiencing.

I even created a separate JIRA ticket for Kevin's issue:
https://issues.apache.org/jira/browse/STORM-1915

So it seems the two things (no worker ID) and unclean shutdown are related?, [~revans2] have you seen this one or STORM-1915 before?, I'm encountering this problem too, where a worker task fails due to some reason, and then the supervisor gets stuck in this mode where it repeatedly, but unsuccessfully, tries to update the blob.
Disk activity goes to 100% (doing just write accesses), and the whole topology basically stalls.
I have to stop the topology and restart it, and hope the problem does not recur too quickly.
My environment is on a single Windows 8.1 laptop on a LocalCluster, with one spout, a total of 7 bolt instances (4 different bolts), with all ACKing turned off (ACK executors = 0)., I have not seen either of these issue on our clusters.

I don't have a lot of time right now to try and reproduce it.  I don't know how much, if any critical information is going to be in the memory of nimbus/the supervisor for your topology, but I don't suspect that it will be very much.  If any of you are OK with taking a heap dump of both nimbus and the supervisor that is causing the issues when this happens it would probably be really helpful.

The errors above happen occasionally because the cleanup of the files does not always coincidence with shooting the worker so the worker could be relauched/come up after the supervisor removed the config file it needs., I am not sure about the problem Jeyendran Balakrishnan described, but I already worked out what causes the other problem.
I described my findings in one of my last comments. Maybe I didn't describe it very well, so let me try to summarize it differently:

The problem occurs if {{kill-existing-workers-with-change-in-components}} tries to kill a worker that has been started but has not yet written a single heartbeat, so {{read-allocated-workers}} returns a state of {{:not-started}} and also no information about the port this worker is assigned to. Therefore in this line https://github.com/apache/storm/blob/v1.0.1/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj#L536 the second argument to {{shutdown-worker}} will be {{nil}}.
This leads to:
# {{pids}} is empty (https://github.com/apache/storm/blob/v1.0.1/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj#L277), therefore the worker is not killed
# {{try-cleanup-worker}} is also called with an {{id}} of {{nil}} (https://github.com/apache/storm/blob/v1.0.1/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj#L303)
# which will delete the local state files for all workers, since {{org.apache.storm.config/worker-root conf nil}} returns for example '/var/lib/storm/workers/' and this path is deleted (https://github.com/apache/storm/blob/v1.0.1/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj#L265)

The heartbeat files of the workers that are still running will eventually be recreated at the heartbeat, but the PID files are gone for good at that point, so even stopping the topology won't help. Only manually killing the worker processes will solve the problem.


P.S: I also attached a workaround patch., Sorry for late participating. I have been struggling with other works.

I'm suspecting that many issues from supervisor are from race condition (sync-supervisor and sync-processes).

One of supervisor test is intermittently failing ([STORM-1933|https://issues.apache.org/jira/browse/STORM-1933]), and after digging I found that supervisor has race condition which can create various issues.
(What [~nico.meyer] pointed out seems to be same to what STORM-1933 shows.)

I submitted a [patch|https://github.com/apache/storm/pull/1528] to [STORM-1934|https://issues.apache.org/jira/browse/STORM-1934] so I'd be really happy if you applies my patch and see it works. , [~Srdo] [~nico.meyer] [~kevinconaway] [~fogetti]
Sorry to mention so lately, but could anyone apply my patch to see it resolves what you're affected?
Please mention me with your storm version if you want to receive modified core jar instead of building your own.
Thanks in advance., Sorry [~kabhwan], I've been out of the office for a while, and for some reason I don't get mail notifications from JIRA. 1.0.2 release seems close, so we'll keep an eye on whether this issue pops up after we upgrade. Thanks for taking a look at this., [~kabhwan] did this make it in to 1.0.2?, [~kevinconaway] I guess yes, since STORM-1934 is applied to 1.0.2. I just forgot to close related issues.
Please try out 1.0.2 and see it works without the symptom, and reopen if it's not. Thanks!
, Resolving this since STORM-1934 was merged.]