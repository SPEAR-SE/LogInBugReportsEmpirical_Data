[GitHub user vesense opened a pull request:

    https://github.com/apache/storm/pull/405

    Update PartitionManager.java

    fix bug [STORM-643] KafkaUtils repeat fetch messages which offset is out
    of range

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vesense/storm 0.9.3-branch

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/405.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #405
    
----
commit 9280a948efaf28ab4de019060435d46731abd375
Author: vesense <best.wangxin@163.com>
Date:   2015-02-02T06:24:14Z

    Update PartitionManager.java
    
    fix bug [STORM-643] KafkaUtils repeat fetch messages which offset is out
    of range

----
, Github user pershyn commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-75567762
  
    Hi Xin,
    
    Thank you for the good finding and patch.
    
    I have applied proposed patch on pure storm-0.9.3 branch, deployed and testet, and indeed it seems to fix the issue by skipping failed offsets. 
    But, in case there are lots of failed messages that have outdated offset -> the spout will slowly ask kafka for each of them and then discarding filed message one at a time.
    
    In logs this will look like following: (some log warnings records were added for debugging reasons).
    Note, that "new offset" doesn't change.
    
    ```
    2015-02-23T14:19:30.080+0100 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [58094410182]
    2015-02-23T14:19:30.083+0100 s.k.PartitionManager [WARN] Using new offset: 58094849835
    2015-02-23T14:19:30.083+0100 s.k.PartitionManager [WARN] Removing the failed offset that is out of range: 58094410182
    2015-02-23T14:19:30.186+0100 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [58094410183]
    2015-02-23T14:19:30.189+0100 s.k.PartitionManager [WARN] Using new offset: 58094849835
    2015-02-23T14:19:30.189+0100 s.k.PartitionManager [WARN] Removing the failed offset that is out of range: 58094410183
    2015-02-23T14:19:30.291+0100 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [58094411596]
    2015-02-23T14:19:30.293+0100 s.k.PartitionManager [WARN] Using new offset: 58094849835
    2015-02-23T14:19:30.293+0100 s.k.PartitionManager [WARN] Removing the failed offset that is out of range: 58094411596
    2015-02-23T14:19:30.396+0100 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [58094411837]
    ```
    
    So, I have added some logic to skip all the outdated failed offsets in one step, see code snippet below:
    
    ```
                //fix bug [STORM-643] : remove this offset from failed list when it is OutOfRange
                if (had_failed) {
                    // For the case of EarliestTime it would be better to discard
                    // all the failed offsets, that are earlier than actual EarliestTime
                    // offset, since they are anyway not there.
                    // These calls to broker API will be then saved.
    
                    // In case of LatestTime - it is a question, if we still need to try out and
                    // reach those that are failed (they still may be available).
                    // But, by moving to LatestTime we are discarding messages in kafka queue.
                    // Since it is configured so, assume that it is ok for user to loose information
                    // and user cares about newest messages first.
                    // It makes sense not to do exceptions for those that are failed and discard them as well.
    
                    SortedSet<Long> omitted = failed.headSet(_emittedToOffset);
    
                    // Use tail, since sortedSet maintains its elements in ascending order
                    // Using tailSet will set a 'range' on original implementation
                    // so we couldn't then add objects that are out of range.
                    // For that reason we copy tail into new Set, where range is not set.
                    failed = new TreeSet<Long>(failed.tailSet(_emittedToOffset));
                    LOG.warn("Removing the failed offsets that are out of range: {}", omitted);
                }
    ```
    
    So, then outdated offsets are skipped at once, and we save several (hundreds) calls to kafka API:
    
    ```
    2015-02-23T15:07:21.913+0100 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [8786892024]
    2015-02-23T15:07:21.915+0100 s.k.PartitionManager [WARN] Using new offset: 8789372723
    2015-02-23T15:07:21.916+0100 s.k.PartitionManager [WARN] Removing the failed offsets that are out of range: [8786892024, 8786892114, 8786892125, 8786892127, 8786892170, 8786892207, 8786892217, 8786892317, 8786892405, 8786892444, 8786892453, 8786892469, 8786892478, 8786892549, 8786892614, 8786892667, 8786892918, /* ... some omitted ... */ 8786977572, 8786977944, 8786986501, 8786991794, 8786991797, 8786994497, 8787001536]
    2015-02-23T15:07:32.759+0100 s.k.ZkCoordinator [INFO] Task [7/8] Refreshing partition manager connections
    ```
    
    If you are agree with such improvement I would be happy if you extend the pull request with this logic.
    
    Best regards and Thanks,
    Michael
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-75902724
  
    Hi Michael,
    
    Thank you for your improvement.
    
    It would be better to skip all the outdated failed offsets in one step and I have extended the pull request.
    
    Best regards and Thanks,
    Xin Wang
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-76148389
  
    @ptgoetz can you please review this. Thanks.
, Github user 2new commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-78835043
  
    +1
, Is this problem solved by https://issues.apache.org/jira/browse/STORM-586?

See the change to PartitionManager.java at the related pull request https://github.com/apache/storm/pull/339/files:

{code}
         try {
             msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
-        } catch (UpdateOffsetException e) {
-            _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
+        } catch (TopicOffsetOutOfRangeException e) {
+            _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, kafka.api.OffsetRequest.EarliestTime());
             LOG.warn("Using new offset: {}", _emittedToOffset);
             // fetch failed, so don't update the metrics
return;
{code}

That is, {{_spoutConfig}} was replaced by {{kafka.api.OffsetRequest.EarliestTime()}}., Github user miguno commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-94007128
  
    @vesense As [I said in the JIRA ticket](https://issues.apache.org/jira/browse/STORM-643?focusedCommentId=14494138&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14494138): is it possible that this problem was already solved by [STORM-586](https://issues.apache.org/jira/browse/STORM-586) or [STORM-511](https://issues.apache.org/jira/browse/STORM-511)?
, Github user tpiscitell commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-94448772
  
    @miguno I don't think either of those JIRAs fixed this issue. The problem here is that PartitionManager.fill() will always attempt to fetch any failed tuples first:
    ```
            // Are there failed tuples? If so, fetch those first.
            if (had_failed) {
                offset = failed.first();
            } else {
                offset = _emittedToOffset;
            }
    ```
    However, even with the patches for those two JIRAs, the `failed` list is never pruned. Instead PartitionManager.fill() just update `_emitedToOffset` and returns:
    
    ```
            } catch (TopicOffsetOutOfRangeException e) {
                _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
                LOG.warn("Using new offset: {}", _emittedToOffset);
                // fetch failed, so don't update the metrics
                return;
            }
    ```
    Just to be called again, and the process repeats:
    ```
        public EmitState next(SpoutOutputCollector collector) {
            if (_waitingToEmit.isEmpty()) {
                fill();
            }
    ```
, Github user slora commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-120259980
  
    I think we are facing this issue too. Is this patch going to be merged in the short term?
    
    Thx!!
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-120415203
  
    @slora which version of storm are you using
, Github user slora commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-120419097
  
    Faced this issue with 0.9.4 and 0.9.5.
    Thx!!
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-120979666
  
    +1
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121066991
  
    @vesense 
    Could you make a pull request based on master branch? 
    We can cherry-pick to other branches (for example, 0.10.x-branch, 0.9.x-branch) if we want.
    Thanks!
, Github user alexsobrino commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121146615
  
    +1
, Github user ellull commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121180650
  
    We are also facing this issue, so +1
, Github user tedxia commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121188995
  
    In branch 0.10.0, the failed tuple has been managered by ExponentialBackoffMsgRetryManager, should we also make this change to 0.10.0 
, Github user mvalleavila commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121189676
  
    We are reproducing the issue too, +1
    Thx!
, Github user 2new commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121215831
  
    Faced this issue with 0.9.5. Is this patch going to be merged in the short term?
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121473131
  
    @miguno Sorry I took so long to respond. Like @tpiscitell already explained, this problem are not solved by STORM-586 and STORM-511.

, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121473865
  
    @HeartSaVioR @tedxia This PR is for branch 0.9.x. 0.9.x is very different from 0.10.x and master, so when we go the cherry-pick there are a lot of conflicts. In branch 0.10.0, the failed tuple has been managered by ExponentialBackoffMsgRetryManager, so we should test and verify whether we can reproduce the issue in 0.10.0.

, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121620839
  
    @vesense 
    Agreed on not merging to master unless we confirm that 0.10.0 and / or 0.11.0 have this issue.
    
    But, at least this should be targeted to 0.9.x-branch, not 0.9.3-branch.
    Could you create a new pull request which points to 0.9.x-branch? Thanks!
, GitHub user vesense opened a pull request:

    https://github.com/apache/storm/pull/635

    [STORM-643] KafkaUtils repeatedly fetches messages whose offset is out of range

    STORM-643 fixing for 0.9.x-branch

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vesense/storm patch-2

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/635.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #635
    
----
commit 9221dba1eee59686ec6a1bcfd93d4ce542b071e9
Author: Xin Wang <best.wangxin@163.com>
Date:   2015-07-16T02:06:28Z

    Fix incorrect WARN log

commit a6557a565429b2e12fc0659c6484fc50f90b5cd5
Author: Xin Wang <best.wangxin@163.com>
Date:   2015-07-16T02:19:41Z

    Remove outdated failed offsets

----
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-121808744
  
    @HeartSaVioR Thanks. I have created a new PR(https://github.com/apache/storm/pull/635) for 0.9.x-branch. Can this be merged into the apache branch now?

, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-122193841
  
    @HeartSaVioR
    Could you take a look? Thx.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/635#issuecomment-122462058
  
    +1. I'll merge it.
    
    Please note that we're considering only 0.9.x now, so we should try to reproduce and fix from 0.10.x and master later.
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/635
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-122464841
  
    @vesense I merged #635 to 0.9.x-branch. Could you close the PR? Thanks!
, Thanks [~vesense], I merged into 0.9.x-branch.

Please note that we should try to reproduce this to 0.10.x-branch / master, and fix it if such behavior occurs., Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-122466082
  
    Will do. Thanks.
, Github user vesense closed the pull request at:

    https://github.com/apache/storm/pull/405
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/635#issuecomment-122467741
  
    Thanks for reminding. Yes, I'm working on it.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/635#issuecomment-122468927
  
    @vesense Though I don't know about storm-kafka deeply, seems like ExponentialBackoffMsgRetryManager doesn't care about it. It just delays retrying failed tuples, and doesn't remove failed tuples with invalid offset.
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/635#issuecomment-122479359
  
    @HeartSaVioR Yeah, you are right. We can reproduce the issue with 0.10.x branch too. And I will make a new PR based on master branch.
, GitHub user vesense opened a pull request:

    https://github.com/apache/storm/pull/642

    [STORM-643] KafkaUtils repeatedly fetches messages whose offset is out of range

    STORM-643 fixing based on master branch

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vesense/storm patch-4

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/642.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #642
    
----
commit 1a0a3c2bfe4ace4c3febb14fccbda8af4e61cfa5
Author: Xin Wang <best.wangxin@163.com>
Date:   2015-07-18T03:39:51Z

    STORM-643 fixing based on master branch

commit 9400850f849d59354d4c6a2d7786f1e23e268cf2
Author: Xin Wang <best.wangxin@163.com>
Date:   2015-07-18T03:46:08Z

    STORM-643 fixing based on master branch

----
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/635#issuecomment-122479965
  
    @HeartSaVioR
    I have created a new PR(https://github.com/apache/storm/pull/642) based on master branch. Could you take a look? Thx.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122480095
  
    +1
, Github user 2new commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122485021
  
    +1
, Reopen this issue cause it is reproducible from master branch.
I'll add 0.10.0 to affected version, and mark issue as resolve again when we merge https://github.com/apache/storm/pull/642 into master / 0.10.x-branch., Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122625519
  
    @vesense 
    Seems like your patch removes one invalid offset per each fill().
    
    We may apply same optimization here.
    When TopicOffsetOutOfRangeException occurs, we can remove invalid offsets from ExponentialBackoffMsgRetryManager at once.
    
    Though we should loop and examine each records to find them, it would be better than current.
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122638602
  
    @HeartSaVioR Thanks. I will fix that.
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122669976
  
    @HeartSaVioR
    I have completed the optimization. Could you take a look? Thx.
, Github user lazyval commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122858859
  
    Minor complain, but [next time]  it would be handy to keep commit messages a bit more descriptive
    
    <img width="465" alt="storm 2015-07-20 14-42-47" src="https://cloud.githubusercontent.com/assets/235297/8775105/97f3d9c4-2eed-11e5-84c7-07d467a1dab5.png">

, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122872920
  
    @lazyval Thank you for pointing out my mistake. I'm sorry for that. It won't happen next time.
    Thanks again.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122913837
  
    @vesense Since it contains too many commits compared to changed lines, I will try to squash commits into one and merge.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122917914
  
    Forgot to give +1. :)
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-122919759
  
    @vesense 
    Thanks for quick work! I merged this into master and 0.10.x-branch respectively.
    
    - master: https://github.com/apache/storm/commit/f091743d9ebc935c9bbbc210095884c3d819632b
    - 0.10.x-branch: https://github.com/apache/storm/commit/05bd1e36697fe7038284833032da262afb82ac99
    
    I squashed your commits into one before merging, so Github can't recognize this PR to closed.
    Could you close this PR?
, It is merged to master and 0.10.x now, but I don't want to change fix version since it is already set to lower version, which should be released., Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-123132069
  
    @HeartSaVioR OK. thx.
, Github user vesense closed the pull request at:

    https://github.com/apache/storm/pull/642
, Github user slora commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-123578961
  
    First of all, thanks @vesense for your work here!
    
    Unfortunately , we're experiencing the same issues with 0.9.6 (includes @vesense  patch). The topology worked well for 6h, but then KafkaSpout seems to be trying to retrieve the same (different for each worker) outdated offset repeatedly. Is anyone else having the same issue? The worker logs are full of this lines:
    ```
    [WARN] Got fetch request with offset out of range: [11024170]; retrying with default start offset time from configuration. configured start offset time: [-2]
    [WARN] Using new offset: 11322905
    ```
    I couldn't find in the logs the message that logs which failed offsets have been deleted:
    ```java
    LOG.warn("Removing the failed offsets that are out of range: {}", omitted);
    ```
, Github user vesense commented on the pull request:

    https://github.com/apache/storm/pull/405#issuecomment-123581897
  
     Maybe you compile your project including the storm-kafka.
     So, add {code}<scope>provided</scope>{code} to POM(storm-kafka dependency).
     And just try again.
, Github user prokopowicz commented on the pull request:

    https://github.com/apache/storm/pull/642#issuecomment-141141208
  
    we've seen a related problem when a custom scheme returns an empty list of lists via generateTuples.  The code in partitionManager does not ack the offset in that case.  It should be made clear that a scheme should not return an empty list of lists, but null, if it deserializes no objects.  Or, this code in PartitionManager should be fixed to treat an empty list of lists in the same way it treats a null list.
    
    ```java
                if (tups != null) {
                    if(_spoutConfig.topicAsStreamId) {
                        for (List<Object> tup : tups) {
                            collector.emit(_spoutConfig.topic, tup, new KafkaMessageId(_partition, toEmit.offset));
                        }
                    } else {
                        for (List<Object> tup : tups) {
                            collector.emit(tup, new KafkaMessageId(_partition, toEmit.offset));
                        }
                    }
                    break;
                } else {
                    ack(toEmit.offset);
                }
    ```
, I'm using this ver  storm-kafka 0.95.and got a large number of logs like below .
and set up 	spoutConf.forceFromStart = true;
the root cause is  this issue?






2015-11-28T12:39:28.843+0800 s.k.PartitionManager [WARN] Using new offset: 26147475
2015-11-28T12:39:28.850+0800 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [9852196]; retrying with default start offset time from configuration. configured start offset time: [-2]
2015-11-28T12:39:28.850+0800 s.k.PartitionManager [WARN] Using new offset: 26147475
2015-11-28T12:39:28.851+0800 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [9852196]; retrying with default start offset time from configuration. configured start offset time: [-2]
2015-11-28T12:39:28.852+0800 s.k.PartitionManager [WARN] Using new offset: 26147475
2015-11-28T12:39:28.853+0800 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [9852196]; retrying with default start offset time from configuration. configured start offset time: [-2]
2015-11-28T12:39:28.853+0800 s.k.PartitionManager [WARN] Using new offset: 26147475
2015-11-28T12:39:28.855+0800 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [9852196]; retrying with default start offset time from configuration. configured start offset time: [-2]
2015-11-28T12:39:28.855+0800 s.k.PartitionManager [WARN] Using new offset: 26147475, Yes. You can fix it by upgrading your Storm to 0.9.6., thx ]