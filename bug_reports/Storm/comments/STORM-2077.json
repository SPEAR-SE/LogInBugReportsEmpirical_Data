[[~tobiasmaier], have you turned on acker ?, The problem seems to be the following piece of code:

{code}
public class KafkaSpout<K, V> extends BaseRichSpout {
//...
private void doSeekRetriableTopicPartitions() {
//...
           if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                kafkaConsumer.seekToEnd(toArrayList(rtp));    // Seek to last committed offset <-- Indeed seeks to end of partition
            }
{code}

However, this code seeks to the end of the partition, not to the last committed offset. The following change seems to repair the issue:

{code}
           if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                OffsetAndMetadata committed = kafkaConsumer.committed(rtp);
                if(committed == null) { 
                   // Nothing yet committed
                    kafkaConsumer.seekToBeginning(toArrayList(rtp));
                } else {
                    // seek to entry after last committed offset
                    kafkaConsumer.seek(rtp, committed.offset() + 1);
                }
{code}, By the way: this issue is not assigned to the right component: it should be storm-kafka-client. Opening a new Issue., Is this ticket related to STORM-2087 ?, Quoting [~db3f] from STORM-2229 (provides additional detail on a proposed fix):

When the topology fails a tuple, it is never resent by the KafkaSpout. This can easily be shown by constructing a small topology failing every tuple.

Apparent reason:

{code}
public class KafkaSpout<K, V> extends BaseRichSpout {
//...
private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                kafkaConsumer.seekToEnd(toArrayList(rtp));    // Seek to last committed offset <== Does seek to end of partition
            }
        }
    }
{code}

The code seeks to the end of the partition instead of seeking to the first uncommited offset.

Preliminary fix (worked for me, but needs to be checked by an expert)

{code}
    private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                OffsetAndMetadata committed = kafkaConsumer.committed(rtp);
                if(committed == null) {
                    // No offsets commited yet for this partition - start from beginning 
                    kafkaConsumer.seekToBeginning(toArrayList(rtp));
                } else {
                   // Seek to first uncommitted offset
                    kafkaConsumer.seek(rtp, committed.offset() + 1);
                }
            }
        }
    }
{code}
, I think the patch is incorrect since the consumer has already seeked to the correct position in the ConsumerRebalanceListener. If the committed is not found, we should just ignore the this partition., Just to throw some attention at: open / pending Pull-Request from (possible dup) STORM-2087 :
https://github.com/apache/storm/pull/1679

Perhaps the patch above is no (more) needed but the PR should be forced, As ist stands, the currenmt Version in git has a different patch for this: 

{code}
    kafkaConsumer.seek(rtp, acked.get(rtp).committedOffset + 1);
{code}
 
Im going to test the patch., The current fix works form me. As far as I am concerned, this issue could be closed., I think this solution provides alternative way to set offset but there's nothing wrong with seekToEnd because it probably worked as designed and here's why. seekToEnd uses OffsetResetStrategy.LATEST to set end offset and what this means is that if there's an offset maintained for the consumer partition by Kafka (Zookeeper or special Kafka topic called consumer_offsets) then seekToEnd goes to latest committed offset; otherwise when there's no existing consumer offset it goes to the end of partition. There are many ways that Kafka can loose tracking of consumer offsets but you can verify it by setting expire offset parameter to some ridiculously short period. This is probably what happened.
The alternative solution also works because it sets offset explicity and I hope out of range cases are handled.
]