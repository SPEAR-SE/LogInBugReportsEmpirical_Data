[HDFS bolt does support tick tuples to periodically flush data at a configurable interval.  Does that address this concern?, GitHub user darionyaphet opened a pull request:

    https://github.com/apache/storm/pull/1566

    [STORM-1971] HDFS Timed Synchronous Policy

    [STORM-1971 HDFS Timed Synchronous Policy](https://issues.apache.org/jira/browse/STORM-1971)
    
    When the data need to be wrote to HDFS is not very large in quantity . 
    
    We need a timed synchronous policy to flush cached date into HDFS periodically.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/darionyaphet/storm STORM-1971

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/1566.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1566
    
----
commit 86e036a8544040557f6b514e0d27f119c6473727
Author: darionyaphet <darion.yaphet@gmail.com>
Date:   2016-07-14T13:34:45Z

    STORM-1971 : HDFS Timed Synchronous Policy

----
, Hi , I checked default.yaml and found topology.tick.tuple.freq.secs is null . So if I don't specify this item in config the tick tuple will not send to bolts ., [~darion] There is a default value for the tick tuples, 15 seconds.
https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java#L57

The tick tuple approach guarantees a sync at a defined interval.  A solution based on SyncPolicy can't do that because it can only trigger when a tuple is being processed., Github user dossett commented on a diff in the pull request:

    https://github.com/apache/storm/pull/1566#discussion_r71151878
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/sync/CountSyncPolicy.java ---
    @@ -26,7 +26,7 @@
      * have been processed.
      */
     public class CountSyncPolicy implements SyncPolicy {
    -    private int count;
    +    private final int count;
    --- End diff --
    
    Great catch to make that final. I would be +1 on that change in a separate PR.
, Github user darionyaphet commented on a diff in the pull request:

    https://github.com/apache/storm/pull/1566#discussion_r71182808
  
    --- Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/sync/CountSyncPolicy.java ---
    @@ -26,7 +26,7 @@
      * have been processed.
      */
     public class CountSyncPolicy implements SyncPolicy {
    -    private int count;
    +    private final int count;
    --- End diff --
    
    @dossett OK fixed :)
,  [~dossett]  When I read the code, I find that each tuple is written to HDFS one by one when it is processed even though filesystem sync is called at Tick_tuple_frequency. Shouldn't the write happen only for the tuplebatch at periodic intervals(may be sync interval) for minimizing the network cost and for higher throughput? Why is each tuple written one by one to hdfs? 
https://github.com/apache/storm/blob/f48d7941b10483e87a30b4849321c4dc0844a5a5/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java#L152 
             , Writing each tuple doesn't necessarily result in higher network costs since caching can happen in HDFS code.  (I am not an HDFS expert, feel free to correct me if I'm wrong).  I view the sync as an outer limit on when the data is guaranteed to have been flushed and persisted to HDFS.

It is possible that an alternative approach (batching the writes) would result in better performance -- I'd be interested in seeing benchmarks for that., Thanks for your reply.    I think overhead is very high in the current case. Writing a message of x size y times to a hdfs cluster vs write a single message of xy size to hdfs cluster.  HDFS is best at large streaming reads and writes  What are the advantages of current implementation(one message write) over the proposed one(batching writes)?    , I did not create that approach (although I did carry it over in a re-write) but one advantage that comes to mind is that data will potentially appear in HDFS sooner.  Also, not all implementations of AbstractHdfsBolt may be able to support the "write one big message" approach.  A text file bolt certainly could, but what about Sequence File or Avro?  I don't know off the top of my head.

If you pursue that and are able to contribute a patch, I will definitely be available for review., Can we say that data will be in HDFS sooner?  Readers can see the latest message writes only after sync. Hence, i feel that it is safe to write a batch and do a sync immediately during every tick.   As you said, this will work for text files and i need to investigate Avro and Sequence. I am ready to contribute and I will take this up very soon.   Thanks]