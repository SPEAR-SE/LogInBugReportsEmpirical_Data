[[~knusbaum] and [~redsanket] can you two jump on this and take a look.  This should be simple to reproduce so we can see where the issues are comming from.  I suspect that it is probably in the blob store code as that is a big rewrite to work with nimbus HA, where as the heartbeat code we have been running with for a long time., [~danielschonfeld] I am curious about what is nature of the topologies and are you using nimbus HA? along with it.
Have you have started using blobstore and making a significant number of updates to a blob.
It will be really helpful for me to know these aspects., [~danielschonfeld] I think [~revans2] is correct. I suspect since supervisor is trying to update blobs for every 30 secs and there are around 10 supervisors, a part of code written for nimbus HA is making way too many connections. If you don't mind, I would like to assign this to myself and come up with a patch so that you can test it on your environment and let me know if it worked., GitHub user redsanket opened a pull request:

    https://github.com/apache/storm/pull/933

    [STORM-1376] Zk slowing down due to many connections

    Apologies, uneccessary creation of client and closing of connections puts load on zookeeper.
    Having a single client connection to perform reads helps

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/redsanket/storm zk-slowing-down

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/933.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #933
    
----
commit f27e5bce2c7fe2e06cef7746227a01099d7d32d3
Author: Sanket <schintap@untilservice-lm>
Date:   2015-12-08T22:58:28Z

    zk slowing down due to many connections

commit c511a7e5581c2aff5c4ddd25c5507950fea08005
Author: Sanket <schintap@untilservice-lm>
Date:   2015-12-08T23:12:02Z

    removing close method as shutdown does that

----
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-163268480
  
    @redsanket did you reproduce the original issue?  Do we need any synchronization around zk client now that we don't create a new one each time?
, Github user redsanket commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-163330056
  
    @revans2 I have launched around 5 topologies on 5 supervisor nodes and could see lot of connections being made to zookeeper and event processing slowing down. But I could not observe zookeeper slowing down to a point it could not respond. I will launch more topologies to make sure it really slows down. I realized in this scenario that it is only performing reads and as per zookeeper implementation I presume connections are thread safe and hence one connection to read information would be sufficient I guess. An interesting point came up with synchronization here, I looked at the localizer code and I have seen downloads to the supervisor are performed by locking but updates are not therefore to support nimbus HA checkForUpdate and checkForDownload might have to be synchronized. Will make sure by running it with more topologies. Thanks for the review
, Github user redsanket commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-163520769
  
    @revans2 Would Automatic backpressure make a difference, as it is turned on now by default and do we need to use pace maker along with it as I guess this change improves to remove the number of connections and speed up a little but on the whole it is not doing good when compared to 0.10.x branch when automatic backpressure is on and running with no pace maker
, [~danielschonfeld] Can you let me know if you are abserving high cpu usage for log writer processes for your topologies, Github user redsanket commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-163988918
  
    STORM-1215 has done some changes to Async Logging, I removed those commits and tested it again and now 0.10.x and 0.11 current version are doing similar
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/storm/pull/933#discussion_r47546102
  
    --- Diff: conf/defaults.yaml ---
    @@ -179,7 +179,7 @@ task.refresh.poll.secs: 10
     task.credentials.poll.secs: 30
     
     # now should be null by default
    -topology.backpressure.enable: true
    +topology.backpressure.enable: false
    --- End diff --
    
    Do we want this off by default?
, Github user ppoulosk commented on a diff in the pull request:

    https://github.com/apache/storm/pull/933#discussion_r47546588
  
    --- Diff: storm-core/test/clj/backtype/storm/nimbus_test.clj ---
    @@ -1238,10 +1238,11 @@
       (testing "nimbus-data uses correct ACLs"
         (let [scheme "digest"
               digest "storm:thisisapoorpassword"
    -          auth-conf {STORM-ZOOKEEPER-AUTH-SCHEME scheme
    +          auth-conf (merge (read-storm-config)
    +                    {STORM-ZOOKEEPER-AUTH-SCHEME scheme
                          STORM-ZOOKEEPER-AUTH-PAYLOAD digest
                          STORM-PRINCIPAL-TO-LOCAL-PLUGIN "backtype.storm.security.auth.DefaultPrincipalToLocal"
    -                     NIMBUS-THRIFT-PORT 6666}
    +                     NIMBUS-THRIFT-PORT 6666})
    --- End diff --
    
    I know that the code was already there, but is it possible to use an ephemeral port here?
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-164538102
  
    Overall the patch looks good, just curious why we turned off backpressure by default.
, Github user rfarivar commented on a diff in the pull request:

    https://github.com/apache/storm/pull/933#discussion_r47548775
  
    --- Diff: conf/defaults.yaml ---
    @@ -179,7 +179,7 @@ task.refresh.poll.secs: 10
     task.credentials.poll.secs: 30
     
     # now should be null by default
    -topology.backpressure.enable: true
    +topology.backpressure.enable: false
     backpressure.disruptor.high.watermark: 0.9
     backpressure.disruptor.low.watermark: 0.4
    --- End diff --
    
    How about adding a comment describing why the choice of 0.4 and 0.9?
, Github user rfarivar commented on a diff in the pull request:

    https://github.com/apache/storm/pull/933#discussion_r47549090
  
    --- Diff: storm-core/test/clj/backtype/storm/nimbus_test.clj ---
    @@ -1238,10 +1238,11 @@
       (testing "nimbus-data uses correct ACLs"
         (let [scheme "digest"
               digest "storm:thisisapoorpassword"
    -          auth-conf {STORM-ZOOKEEPER-AUTH-SCHEME scheme
    +          auth-conf (merge (read-storm-config)
    +                    {STORM-ZOOKEEPER-AUTH-SCHEME scheme
                          STORM-ZOOKEEPER-AUTH-PAYLOAD digest
                          STORM-PRINCIPAL-TO-LOCAL-PLUGIN "backtype.storm.security.auth.DefaultPrincipalToLocal"
    -                     NIMBUS-THRIFT-PORT 6666}
    +                     NIMBUS-THRIFT-PORT 6666})
    --- End diff --
    
    Ditto.
, Github user redsanket commented on a diff in the pull request:

    https://github.com/apache/storm/pull/933#discussion_r47574811
  
    --- Diff: conf/defaults.yaml ---
    @@ -179,7 +179,7 @@ task.refresh.poll.secs: 10
     task.credentials.poll.secs: 30
     
     # now should be null by default
    -topology.backpressure.enable: true
    +topology.backpressure.enable: false
    --- End diff --
    
    Oh Im sorry I was testing few scenarios might have come along with my last commit. Will change it back
, Github user hustfxj commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-164609684
  
    yes, others are good, but why we turned off backpressure by default.
, Github user zhuoliu commented on a diff in the pull request:

    https://github.com/apache/storm/pull/933#discussion_r47592998
  
    --- Diff: conf/defaults.yaml ---
    @@ -179,7 +179,7 @@ task.refresh.poll.secs: 10
     task.credentials.poll.secs: 30
     
     # now should be null by default
    -topology.backpressure.enable: true
    +topology.backpressure.enable: false
     backpressure.disruptor.high.watermark: 0.9
     backpressure.disruptor.low.watermark: 0.4
    --- End diff --
    
    This low/high watermark default values are now more of empirical values based on a limited set of topologies. We may need more experiments and theoretical analysis (based on representative queue draining speed, callback response time etc.) for choices of these watermark percentages.
, Github user redsanket commented on a diff in the pull request:

    https://github.com/apache/storm/pull/933#discussion_r47598080
  
    --- Diff: storm-core/test/clj/backtype/storm/nimbus_test.clj ---
    @@ -1238,10 +1238,11 @@
       (testing "nimbus-data uses correct ACLs"
         (let [scheme "digest"
               digest "storm:thisisapoorpassword"
    -          auth-conf {STORM-ZOOKEEPER-AUTH-SCHEME scheme
    +          auth-conf (merge (read-storm-config)
    +                    {STORM-ZOOKEEPER-AUTH-SCHEME scheme
                          STORM-ZOOKEEPER-AUTH-PAYLOAD digest
                          STORM-PRINCIPAL-TO-LOCAL-PLUGIN "backtype.storm.security.auth.DefaultPrincipalToLocal"
    -                     NIMBUS-THRIFT-PORT 6666}
    +                     NIMBUS-THRIFT-PORT 6666})
    --- End diff --
    
    All the test cases in this file have a similar port, will create a JIRA to tackle this issue separately as I feel it is not a part of this change
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-165224292
  
    +1
, Github user ppoulosk commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-165224934
  
    @redsanket, Agreed.  thanks for filing the Jira.  +1, NB
, Github user knusbaum commented on the pull request:

    https://github.com/apache/storm/pull/933#issuecomment-165582546
  
    +1
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/933
]