[it seems that storm still use an old version of disruptor, the project has moved from google to gitub, and the artefactId changed since.

I expect that if we use the last version of disruptor, it will benefits of this commit
https://github.com/LMAX-Exchange/disruptor/commit/8870a417bff5aed07825fc366b8f470d3561c838#diff-3a4074986e8b9e9f3a9cd82470d357ea, Seeing the same with Storm 0.9.3:

88% sun.misc.Unsafe.park(boolean, long) :native
71% java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) :215
70% java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(long, java.util.concurrent.TimeUnit) :2163,2152
Execution on lines: 2163,2152
70% com.lmax.disruptor.BlockingWaitStrategy.waitFor(long, com.lmax.disruptor.Sequence, com.lmax.disruptor.Sequence[], com.lmax.disruptor.SequenceBarrier, long, java.util.concurrent.TimeUnit) :87
70% com.lmax.disruptor.ProcessingSequenceBarrier.waitFor(long, long, java.util.concurrent.TimeUnit) :54
70% backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(com.lmax.disruptor.EventHandler) :97,99
70% backtype.storm.disruptor$consume_batch_when_available.invoke(java.lang.Object, java.lang.Object) :80
, will be fix in next versions
https://issues.apache.org/jira/browse/STORM-350, We found leaking tuples from STORM-350, so it was reverted and seems like no one retry it except me. ;)
So suggestion based on Disruptor 2.10.1 is still vaild., What's the status of this issue?

Our Vagrant environment is killing our laptops because the activity is low there and we suspect it's due to this bug., Github user errordaiwa commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-119814511
  
    @HeartSaVioR  done!
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-119832755
  
    See the comment @ storm-core/src/clj/backtype/storm/disruptor.clj
    
    `;; :block strategy requires using a timeout on waitFor (implemented in DisruptorQueue), as sometimes the consumer stays blocked even when there's an item on the queue.`
    
    So with this change the bolts might have to wait for up to one sec (in the worst case) before they see the tuple after its added to the queue.

, Github user errordaiwa commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120209941
  
    @arunmahadevan you are right. But I think at least this value should be configurable to tradeoff latency VS CPU usage.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120241702
  
    Letting users configure disruptor queue wait timeout makes sense for me. Better than origin PR.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120252655
  
    Great, +1.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120260564
  
    After digging further I suspect the volatile `numWaiters` @ https://github.com/LMAX-Exchange/disruptor/blob/2.10.1/code/src/main/com/lmax/disruptor/BlockingWaitStrategy.java 
    could be the culprit. If a volatile is updated by multiple threads we could be losing updates which would manifest as consumers not being waken up when there is an item in the queue.
    
    @errordaiwa  lmax-disruptor 2.10.4 appear to have a fix for this issue. Can you try with this version and call the waitFor without any timeout ?
    

, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120267465
  
    I've found some commits which seems to fix race conditions of BlockingWaitStrategy.
    
    https://github.com/LMAX-Exchange/disruptor/commit/80744acd26767eb382c0b7214587c4d26891541f
    https://github.com/LMAX-Exchange/disruptor/commit/e47dfc4ab61373289fa27d98d38d44bfddb4d5e4
    https://github.com/LMAX-Exchange/disruptor/commit/8ee839119751156552d31390c0ff559598d954c4
    
    It could be more.
    I also think that we can apply lmax-disruptor to 2.10.4 before applying this PR, and check it resolves our issue.
, Github user errordaiwa commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120416512
  
    @arunmahadevan  I open an issue [Race condition in 2.10.1 release](https://github.com/LMAX-Exchange/disruptor/issues/119) and the author of Disruptor queue confirm this fix. But I can't reproduce this bug using 2.10.1. Do you have any idea on how to reprodue it?
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120605067
  
    @errordaiwa @arunmahadevan 
    We may create unit test to check,
    
    - make sure that there's no available data from disruptor queue
    - subscriber thread waits for new data from disruptor queue with wait time 1 sec
    - other thread immediately (or less than 1 sec) publish new data to disruptor queue
    
    Subscriber thread should awake immediately (not waiting whole wait time) when publishing.
, Github user errordaiwa commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120778560
  
    @HeartSaVioR I do unit test for both ver2.10.1 and 2.10.4, but no latency was found. May be the condition to reproduce the bug is very strict. So should we apply this pr or upgrade version of disruptor queue?
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120808061
  
    @errordaiwa 
    Did you test it with multi-threaded producers? 
    
    Btw, I think we can take a benefit from adopting 2.10.4, since changeset contains some fixes for race.
    https://github.com/LMAX-Exchange/disruptor/compare/2.10.1...2.10.4
    It would be better to create new issue for changing Disruptor to 2.10.4 cause we should run some tests (unit tests / performance tests / etc.) so that we'll be sure about its safety.
, Github user errordaiwa commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120817734
  
    @HeartSaVioR 
    No, I test it with just one producer.
    
    And here is the new issue [Storm-935](https://issues.apache.org/jira/browse/STORM-935).
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-120818894
  
    @errordaiwa with 2.10.4 you may try the single argument waitFor(long sequeuce) in DisruptrorQueue.java and observe the CPU usage with a high bolt count like what you mentioned in the issue. Also see if the bolts are receiving the tuples without a delay.
, Github user HeartSaVioR commented on the pull request:

    https://github.com/apache/storm/pull/625#issuecomment-121765284
  
    Since #630 contains #625 now, I think we can close this PR.
    @errordaiwa Please close PR, thanks!
, Github user errordaiwa closed the pull request at:

    https://github.com/apache/storm/pull/625
, Thanks [~xingyu] for great work.
I merged into master, 0.10.x, 0.9.x branches respectively.

And thanks all users/contributors for reporting!]