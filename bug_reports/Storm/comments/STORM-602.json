[Further information on the issue:
1) When starting the topology, if the hadoop nodes are not available, you get "worker died" message and HdfsBolt and the entire topology die.
java.lang.RuntimeException: ("Worker died")
......
2) If the topology is running and then the hadoop nodes become unavailable, you get connection refused error. When hadoop nodes become available, the HdfsBolt never recovers. It keeps giving the following error:
 org.apache.storm.hdfs.bolt.HdfsBolt - write/sync failed.
 All datanodes ....... are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1008) ~[hadoop-hdfs-2.2.0.jar:na]
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:823) ~[hadoop-hdfs-2.2.0.jar:na]
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:475) ~[hadoop-hdfs-2.2.0.jar:na]
If you restart the topology, everything is OK and HdfsBolt can write to the hdfs nodes., We're experiencing the same problem, especially the second issue. It seems that updating {{hadoop-client}} and {{hadoop-hdfs}} in the [storm-hdfs pom.xml|https://github.com/apache/storm/blob/v0.9.3/external/storm-hdfs/pom.xml] to {{2.5.2}} (our Hadoop is Hadoop 2.5.0-cdh5.2.0) fixed the issue.

Could someone verify this?, The second problem ("When hadoop nodes become available, the HdfsBolt never recovers.") has been resolved by STORM-969., My tentative conclusion is that the first error is not a defect.  If the hadoop cluster is down at startup, failing the topology seems reasonable.  Without being able to establish a connection to HDFS and open a file there's no guarantee that the submitted configuration would ever work when Hadoop comes back online.  Failing at the outset seems like the better option.

Other thoughts?, If nobody objects, I will close this ticket as "not a problem" based on my previous comment.]