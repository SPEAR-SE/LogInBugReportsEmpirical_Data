[I've started working on a fix because the topology I'm working on is behaving unstable, which I'm suspecting is because of this. I'll submit a pull request when the code is tested.

Branch is at: https://github.com/eshioji/storm/tree/STORM-839, GitHub user eshioji opened a pull request:

    https://github.com/apache/storm/pull/565

    STORM-839

    This fixes the reported deadlock between `disruptor-worker-transfer-queue` thread and `client-worker` thread, which seem to have been introduced by STORM-329.
    
    After reviewing the `v0.9.4` code, my conclusion was that the background flushing task can be removed without real change in the behavior. By doing this, the synchronization that was involved in the deadlock can be removed altogether.
    
    My reasoning is as follows:
     - One has three option to deal with Netty's buffer filling up:
      1. Discard incoming new messages
      2. Block client thread until there is space (back pressure)
      3. Keep buffering up until OOME is thrown
     - My guess is that the `v0.9.4` code attempted to implement option (i), but actually the behavior is option (iii). When Netty's `Channel.isWritable` returns false in `Client.send`, the thread avoids writing to the `Channel` and leaves messages in the `Channel.messageBatch` field, which the background task flushes when the `Channel.isWritable` starts to return true again. However, if `Client.send` is called again before this (which should be common), the content of `Channel.messageBatch` is written and buffered on the `Channel` anyways, because `flushMessages` does not check `Channel.isWritable`.
     - AFAIU we like option (ii), but this requires more work. Between option (i) and option (iii), IMO (iii) is superior because if OOME happens, the user can reduce MAX_PENDING_TUPLES to prevent this. Discarding messages would be harder to diagnose.
     - If we are content with option (ii), we can remove the background flushing task and related synchronization altogether, removing the source of deadlock. We'd simply write and buffer onto the unbounded buffer of `Channel`, and if there are too many pending messages, the worker will die of OOME, and the user should reduce this with indirect means like reducing MAX_PENDING_TUPLES until option (ii) is implemented in the future.
    
    Thoughts @miguno ?
    
    
    
     

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/eshioji/storm STORM-839

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/565.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #565
    
----
commit 98f4e619d54052b73a309d23ab7214953e4c7774
Author: Sriharsha Chintalapani <mail@harsha.io>
Date:   2015-02-05T18:08:05Z

    STORM-130: Supervisor getting killed due to java.io.FileNotFoundException: File '../stormconf.ser' does not exist.
    
    Signed-off-by: P. Taylor Goetz <ptgoetz@gmail.com>

commit a1e5893e1b94c224d39fedf11583b216c21351c8
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-02-24T20:46:12Z

    update changelog for STORM-130

commit 62788f295bb1fb1cc83b99c30f82beb40eea5f25
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-02-24T23:03:40Z

    port STORM-329 fix to 0.9.x

commit 81016c2ed7222da99138bc9971e335533d4cb518
Author: Michael G. Noll <mnoll@verisign.com>
Date:   2015-02-16T09:01:27Z

    Track how many messages are being dropped when a connection is unavailable
    
    Signed-off-by: P. Taylor Goetz <ptgoetz@gmail.com>

commit 97a76fc896de508f015dbe32f1473ddbf10d736b
Author: Michael G. Noll <mnoll@verisign.com>
Date:   2015-02-16T09:03:07Z

    Clarify name of method for dropping messages
    
    Signed-off-by: P. Taylor Goetz <ptgoetz@gmail.com>

commit 9138d9fc255639b4d0d43657379ce467591e8ef2
Author: Michael G. Noll <mnoll@verisign.com>
Date:   2015-02-16T09:07:35Z

    Change log level for intentionally dropping messages from WARN to ERROR
    
    This change makes the log level for dropping messages consistent in
    Client.java.
    
    Signed-off-by: P. Taylor Goetz <ptgoetz@gmail.com>

commit 6b06d8468ff5e743fb12b85dd84fe0931041c2c3
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-02-24T23:18:43Z

    add STORM-329 to changelog

commit e63fb2af9086e2b2e688662ca42a4b4d0112274b
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2015-03-03T00:06:58Z

    STORM-693: when bolt fails to write tuple, it should report error instead of silently acking.
    
    Signed-off-by: P. Taylor Goetz <ptgoetz@gmail.com>

commit 92836de540ec8ab90d7591b96ba02126e80b5c3a
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T14:59:56Z

    add STORM-693 to changelog

commit c19e482b70f18d690ad165c78551860506486095
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2015-02-20T19:56:22Z

    STORM-682: supervisor should handle worker state corruption gracefully.
    
    Signed-off-by: P. Taylor Goetz <ptgoetz@gmail.com>

commit f0de11a20fe2f20dc1dc2f485549e0dc342f8680
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T15:05:30Z

    add STORM-682 to changelog

commit 835a410c879dc1eb02d9670410f65fe0be6f28c6
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2015-01-14T20:27:35Z

    STORM-559: ZkHosts in README should use 2181 as port.
    
    Signed-off-by: P. Taylor Goetz <ptgoetz@gmail.com>

commit 30e0be8616c89cb1f8a51fcf462f76a075e6e964
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T15:11:16Z

    add STORM-559 to changelog

commit b1bbacb7134d17ff47c2e8b8857a66244a4d1d4f
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T15:28:11Z

    add missing import in supervisor.clj

commit edf596bac8feab0c8721f7de94474e5549858355
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T16:21:38Z

    [maven-release-plugin] prepare release v0.9.4

commit 48d10e20eb3c750fc41fcf0bef3d49501cf6d5a4
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T16:21:45Z

    [maven-release-plugin] prepare for next development iteration

commit 41f44f9914d4f27d0db3f211a85f88301533f09b
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T17:59:39Z

    Revert "[maven-release-plugin] prepare for next development iteration"
    
    This reverts commit 48d10e20eb3c750fc41fcf0bef3d49501cf6d5a4.

commit 233603c3cbd729fdfabd2759bfa7705811996aa4
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T18:00:11Z

    Revert "[maven-release-plugin] prepare release v0.9.4"
    
    This reverts commit edf596bac8feab0c8721f7de94474e5549858355.

commit 61e1b5c3e226122143c91bbd7527b605f8ed8727
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T18:05:51Z

    add Apache license header to ConnectionWithStatus.java

commit 00091d7952681a39281aa171adfad133a5e26330
Author: P. Taylor Goetz <ptgoetz@gmail.com>
Date:   2015-03-18T18:13:57Z

    [maven-release-plugin] prepare release v0.9.4

commit ed8ab3ec194f19c75fc2f5c000609204f04b50e8
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-28T19:42:05Z

    Simplified the flow and removed the lock that was causing the deadlock

commit 91b8eb3840432e47b79f40abebec8304627732a8
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-28T19:46:17Z

    Bump version

commit b7d84bdc7fd3de34f45a94131cdbb6bfbd3763dc
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-28T21:27:31Z

    Remove background flushing because it doesn't seem necessary. Netty's Channel queues up written data on an unbounded buffer. The background flushing seems to have been added to avoid this, but in practice it was probably doing it anyways because flushMessages(), which is called by send() doesn't check for isWritable. Moreover, queuing on an unbounded buffer seems fine because back pressure is provided by MAX_PENDING_TUPLE. If OOME occurs due to this buffer overflowing, it seems reasonable that one has to reduce MAX_PENDING_TUPLE, rather than Storm trying to cope with it by dropping messages.

commit 679e42bc1e38f51c2759667b03cb45322c6a793b
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-28T21:31:35Z

    Change to a SNAPSHOT version for deployment purposes

commit 27a92e2aa3488c0203f500306e0583ff9e7e1e82
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-29T09:32:16Z

    Remove (now) dead comment and code

----
, Github user eshioji commented on the pull request:

    https://github.com/apache/storm/pull/565#issuecomment-106765335
  
    Argh sorry I should have opened this against Storm v0.9.4, I worked off v0.9.4 tag.
, Github user eshioji closed the pull request at:

    https://github.com/apache/storm/pull/565
, GitHub user eshioji opened a pull request:

    https://github.com/apache/storm/pull/566

    [STORM-839] Deadlock hazard in backtype.storm.messaging.netty.Client

    (I accidentally did a PR against master with the same content, please ignore that one)
    
    This fixes the reported deadlock between `disruptor-worker-transfer-queue` thread and `client-worker` thread, which seem to have been introduced by STORM-329.
    
    After reviewing the `v0.9.4` code, my conclusion was that the background flushing task can be removed without real change in the behavior. By doing this, the synchronization that was involved in the deadlock can be removed altogether.
    
    My reasoning is as follows:
     - One has three option to deal with Netty's buffer filling up:
      1. Discard incoming new messages
      2. Block client thread until there is space (back pressure)
      3. Keep buffering up until OOME is thrown
     - My guess is that the `v0.9.4` code attempted to implement option (i), but actually the behavior is option (iii). When Netty's `Channel.isWritable` returns false in `Client.send`, the thread avoids writing to the `Channel` and leaves messages in the `Channel.messageBatch` field, which the background task flushes when the `Channel.isWritable` starts to return true again. However, if `Client.send` is called again before this (which should be common), the content of `Channel.messageBatch` is written and buffered on the `Channel` anyways, because `flushMessages` does not check `Channel.isWritable`.
     - AFAIU we like option (ii), but this requires more work. Between option (i) and option (iii), IMO (iii) is superior because if OOME happens, the user can reduce MAX_PENDING_TUPLES to prevent this. Discarding messages would be harder to diagnose.
     - If we are content with option (ii), we can remove the background flushing task and related synchronization altogether, removing the source of deadlock. We'd simply write and buffer onto the unbounded buffer of `Channel`, and if there are too many pending messages, the worker will die of OOME, and the user should reduce this with indirect means like reducing MAX_PENDING_TUPLES until option (ii) is implemented in the future.
    
    Thoughts @miguno ?
    
    
    
     

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/eshioji/storm STORM-839

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/566.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #566
    
----
commit ed8ab3ec194f19c75fc2f5c000609204f04b50e8
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-28T19:42:05Z

    Simplified the flow and removed the lock that was causing the deadlock

commit 91b8eb3840432e47b79f40abebec8304627732a8
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-28T19:46:17Z

    Bump version

commit b7d84bdc7fd3de34f45a94131cdbb6bfbd3763dc
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-28T21:27:31Z

    Remove background flushing because it doesn't seem necessary. Netty's Channel queues up written data on an unbounded buffer. The background flushing seems to have been added to avoid this, but in practice it was probably doing it anyways because flushMessages(), which is called by send() doesn't check for isWritable. Moreover, queuing on an unbounded buffer seems fine because back pressure is provided by MAX_PENDING_TUPLE. If OOME occurs due to this buffer overflowing, it seems reasonable that one has to reduce MAX_PENDING_TUPLE, rather than Storm trying to cope with it by dropping messages.

commit 679e42bc1e38f51c2759667b03cb45322c6a793b
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-28T21:31:35Z

    Change to a SNAPSHOT version for deployment purposes

commit 27a92e2aa3488c0203f500306e0583ff9e7e1e82
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-29T09:32:16Z

    Remove (now) dead comment and code

commit 09bf6e1b5d9d351f2a60cd9a32e0239752cf437a
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-05-29T10:23:46Z

    Merge branch '0.9.x-branch' into STORM-839
    
    Conflicts:
    	examples/storm-starter/pom.xml
    	external/storm-hbase/pom.xml
    	external/storm-hdfs/pom.xml
    	external/storm-kafka/pom.xml
    	pom.xml
    	storm-buildtools/maven-shade-clojure-transformer/pom.xml
    	storm-core/pom.xml
    	storm-dist/binary/pom.xml
    	storm-dist/source/pom.xml

----
, Github user eshioji commented on the pull request:

    https://github.com/apache/storm/pull/566#issuecomment-106768270
  
    @Hailei Thanks for the comment. Could you review my reasoning in the [PR comment above](https://github.com/apache/storm/pull/566#issue-82369808)?  
, Github user Hailei commented on the pull request:

    https://github.com/apache/storm/pull/566#issuecomment-106778432
  
    >we can remove the background flushing task and related synchronization altogether, removing the source of deadlock. We'd simply write and buffer onto the unbounded buffer of Channel
    +1 I agree with you.flushMessages don't check isWritable,So the last message batch check it is not useful.
    Block client thread until there is space (back pressure) is better.It can snap out of OOM.

, Github user eshioji commented on the pull request:

    https://github.com/apache/storm/pull/566#issuecomment-106792295
  
    @Hailei I see. 
    
    > Block client thread until there is space (back pressure) is better.It can snap out of OOM.
    
    Agreed, but my understanding is that this needs quite a bit more work. At the least, the change can't be contained within `Client.java`. 
, Github user miguno commented on the pull request:

    https://github.com/apache/storm/pull/566#issuecomment-106795826
  
    > My reasoning is as follows:
    > 
    > * One has three option to deal with Netty's buffer filling up:
    >     1. Discard incoming new messages
    >     2. Block client thread until there is space (back pressure)
    >     3. Keep buffering up until OOME is thrown
    >
    > My guess is that the v0.9.4 code attempted to implement option (i), but actually the behavior is option (iii).
    
    The code's intention was actually (iii).  As you described back pressure (ii) was not picked because this will require a significant amount of work, which was thus out of scope for fixing STORM-329.  The reason (iii) was preferred over (i) was also as you described -- if (and only if) users have enabled acking = guaranteed message processing for a topology, they can prevent OOM errors from happening by setting [`topology.max.spout.pending`](https://github.com/apache/storm/blob/master/conf/defaults.yaml#L180).
    
    I'll have to look at your code in more detail before commenting.  I'll also ping @ptgoetz, @clockfly, @tedxia, and @revans2 who were involved in this significant patch (sorry for the spam, folks!).
, Github user eshioji commented on the pull request:

    https://github.com/apache/storm/pull/566#issuecomment-106800904
  
    Just as a heads up, I'm seeing STORM-763 as well (300 ERROR messages per second per machine) in our cluster. My initial gut feeling is that maybe reconnect isn't getting enough run time from the ScheduledExecutor. Depending on the results I might create another PR for 763.
, Github user eshioji commented on the pull request:

    https://github.com/apache/storm/pull/566#issuecomment-107148613
  
    Hi @miguno , thanks for taking the time. I think I have a [fix for STORM-763](https://github.com/apache/storm/pull/568) which builds on this PR. Let me know what you think!
, GitHub user eshioji opened a pull request:

    https://github.com/apache/storm/pull/616

    Storm 763/839 0.11.x

    This is a port of PR #568 . It fixes STORM-839 (Deadlock) and STORM-763 (Establish Netty reconnects asynchronously and reduce verbosity of error logs)
    
    @revans2 This is the PR for master. I ran the performance test against master's HEAD and the results were in-line with the results I got in #568 . I'll create a separate PR for 0.10.x

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/eshioji/storm STORM-763_0.11.x

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/616.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #616
    
----
commit b1411aa63383801913cb1340a8b51c5bb46db0ba
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-07-01T12:54:33Z

    This fixes STORM-763 and STORM-839

commit 935e87accc85340549f44effa8a675e950747faa
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-07-02T17:56:23Z

    Remove obsolete TODO

commit 7af467723b5c4b0beedc40626a8c56bc7c3e0d21
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-07-03T00:02:00Z

    Bring back removal of client from context upon closing

----
, GitHub user eshioji opened a pull request:

    https://github.com/apache/storm/pull/617

    Storm 763/839 0.10.x

    This is a port of PR #568 from 0.9.x to 0.10.x. It fixes STORM-839 (Deadlock) and STORM-763 (Establish Netty reconnects asynchronously and reduce verbosity of error logs)
    
    The content is equivalent to the PR for 0.11.x (#616)
    
    @revans2 This is the PR for 0.10.x. I also ran the performance test for this and the results were again inline with what I got with 0.9.x. I get build failure on my machine but I get the same failure with 0.10.x HEAD, so I'm guessing it's unrelated to this change. 

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/eshioji/storm STORM-763_0.10.x

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/617.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #617
    
----
commit 286bacdf49937d1e8576eff27dfc887824ffdbbb
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-07-02T18:01:01Z

    This fixes STORM-763 and STORM-839

commit a2502c3bc3bcd4caf3800bb645058abb61d2a071
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-07-02T23:51:32Z

    Merge remote-tracking branch 'upstream/0.10.x-branch' into STORM-763_0.10.x
    
    # Conflicts:
    #	storm-core/src/jvm/backtype/storm/messaging/netty/Client.java

commit f5db06ce2809c3b66f2d797f979f8c40133c2f60
Author: Enno Shioji <eshioji@gmail.com>
Date:   2015-07-03T00:02:00Z

    Bring back removal of client from context on closing

----
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/616
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/617
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/566
, Thanks [~eshioji],

I merged this into master, branch-0.10.x and branch-0.9.x.  You may want to talk to [~ptgoetz] about doing a 0.9.6 release.]