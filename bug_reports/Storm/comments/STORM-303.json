[Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12329583
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/DynamicBrokersReader.java ---
    @@ -75,6 +74,8 @@ public GlobalPartitionInformation getBrokerInfo() {
                         LOG.error("Node {} does not exist ", path);
                     }
                 }
    +				} catch(java.net.SocketTimeoutException e) {
    --- End diff --
    
    The indentation seems to be off here.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12329609
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/DynamicBrokersReader.java ---
    @@ -52,15 +51,15 @@ public DynamicBrokersReader(Map conf, String zkStr, String zkPath, String topic)
                                 Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL))));
                 _curator.start();
             } catch (Exception ex) {
    -            LOG.error("can't connect to zookeeper");
    +            LOG.error("Couldn't connect to zookeeper", ex);
             }
         }
     
         /**
          * Get all partitions with their current leaders
          */
    -    public GlobalPartitionInformation getBrokerInfo() {
    -        GlobalPartitionInformation globalPartitionInformation = new GlobalPartitionInformation();
    +    public GlobalPartitionInformation getBrokerInfo() throws java.net.SocketTimeoutException {
    --- End diff --
    
    Do we want to import java.net.SocketTimeoutException as it is used more then once? (very minor)
, Github user brndnmtthws commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12329833
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/DynamicBrokersReader.java ---
    @@ -75,6 +74,8 @@ public GlobalPartitionInformation getBrokerInfo() {
                         LOG.error("Node {} does not exist ", path);
                     }
                 }
    +				} catch(java.net.SocketTimeoutException e) {
    --- End diff --
    
    True.  I'll fix that.
, Github user brndnmtthws commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12329854
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/DynamicBrokersReader.java ---
    @@ -52,15 +51,15 @@ public DynamicBrokersReader(Map conf, String zkStr, String zkPath, String topic)
                                 Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL))));
                 _curator.start();
             } catch (Exception ex) {
    -            LOG.error("can't connect to zookeeper");
    +            LOG.error("Couldn't connect to zookeeper", ex);
             }
         }
     
         /**
          * Get all partitions with their current leaders
          */
    -    public GlobalPartitionInformation getBrokerInfo() {
    -        GlobalPartitionInformation globalPartitionInformation = new GlobalPartitionInformation();
    +    public GlobalPartitionInformation getBrokerInfo() throws java.net.SocketTimeoutException {
    --- End diff --
    
    Probably a good idea.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12329867
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java ---
    @@ -159,12 +171,17 @@ public static ByteBufferMessageSet fetchMessages(KafkaConfig config, SimpleConsu
             for (int errors = 0; errors < 2 && msgs == null; errors++) {
                 FetchRequestBuilder builder = new FetchRequestBuilder();
                 FetchRequest fetchRequest = builder.addFetch(topic, partitionId, offset, config.fetchSizeBytes).
    -                    clientId(config.clientId).build();
    +                    clientId(config.clientId).maxWait(config.fetchMaxWait).build();
                 FetchResponse fetchResponse;
                 try {
                     fetchResponse = consumer.fetch(fetchRequest);
                 } catch (Exception e) {
    -                if (e instanceof ConnectException) {
    +                if (e instanceof ConnectException ||
    --- End diff --
    
    This is a java 7 specific construct.  Currently we have JDK6 compatibility listed in the pom.  If we want to move to Java7, which I personally am OK with, but I know others are not, we need to discuss this on the mailing list.
, Github user brndnmtthws commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12329890
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java ---
    @@ -159,12 +171,17 @@ public static ByteBufferMessageSet fetchMessages(KafkaConfig config, SimpleConsu
             for (int errors = 0; errors < 2 && msgs == null; errors++) {
                 FetchRequestBuilder builder = new FetchRequestBuilder();
                 FetchRequest fetchRequest = builder.addFetch(topic, partitionId, offset, config.fetchSizeBytes).
    -                    clientId(config.clientId).build();
    +                    clientId(config.clientId).maxWait(config.fetchMaxWait).build();
                 FetchResponse fetchResponse;
                 try {
                     fetchResponse = consumer.fetch(fetchRequest);
                 } catch (Exception e) {
    -                if (e instanceof ConnectException) {
    +                if (e instanceof ConnectException ||
    --- End diff --
    
    Ah, okay.  JDK6 is fine too.  I can repair this.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12329897
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java ---
    @@ -147,80 +146,97 @@ public EmitState next(SpoutOutputCollector collector) {
     
         private void fill() {
             long start = System.nanoTime();
    -        ByteBufferMessageSet msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, _emittedToOffset);
    +        long offset;
    +        final boolean had_failed = !failed.isEmpty();
    +
    +        // Are there failed tuples? If so, fetch those first.
    +        if (had_failed) {
    +            offset = failed.first();
    +        } else {
    +            offset = _emittedToOffset + 1;
    +        }
    +
    +        ByteBufferMessageSet msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
             long end = System.nanoTime();
             long millis = (end - start) / 1000000;
             _fetchAPILatencyMax.update(millis);
             _fetchAPILatencyMean.update(millis);
             _fetchAPICallCount.incr();
    -        int numMessages = countMessages(msgs);
    -        _fetchAPIMessageCount.incrBy(numMessages);
    +        if (msgs != null) {
    +            int numMessages = 0;
     
    -        if (numMessages > 0) {
    -            LOG.info("Fetched " + numMessages + " messages from: " + _partition);
    -        }
    -        for (MessageAndOffset msg : msgs) {
    -            _pending.add(_emittedToOffset);
    -            _waitingToEmit.add(new MessageAndRealOffset(msg.message(), _emittedToOffset));
    -            _emittedToOffset = msg.nextOffset();
    -        }
    -        if (numMessages > 0) {
    -            LOG.info("Added " + numMessages + " messages from: " + _partition + " to internal buffers");
    -        }
    -    }
    -
    -    private int countMessages(ByteBufferMessageSet messageSet) {
    -        int counter = 0;
    -        for (MessageAndOffset messageAndOffset : messageSet) {
    -            counter = counter + 1;
    +            for (MessageAndOffset msg : msgs) {
    +                final Long cur_offset = msg.offset();
    +                if (!had_failed || failed.contains(cur_offset)) {
    +                    numMessages += 1;
    +                    _pending.add(cur_offset);
    +                    _waitingToEmit.add(new MessageAndRealOffset(msg.message(), cur_offset));
    +                    _emittedToOffset = Math.max(cur_offset, _emittedToOffset);
    +                    if (had_failed) {
    +                        failed.remove(cur_offset);
    +                    }
    +                }
    +            }
    +            _fetchAPIMessageCount.incrBy(numMessages);
             }
    -        return counter;
         }
     
         public void ack(Long offset) {
    -        _pending.remove(offset);
    +        if (!_pending.isEmpty() && _pending.first() < offset - _spoutConfig.maxOffsetBehind) {
    +            // Too many things pending!
    +            _pending.headSet(offset).clear();
    +        } else {
    +            _pending.remove(offset);
    +        }
    +        numberAcked++;
         }
     
         public void fail(Long offset) {
    -        //TODO: should it use in-memory ack set to skip anything that's been acked but not committed???
    -        // things might get crazy with lots of timeouts
    -        if (_emittedToOffset > offset) {
    -            _emittedToOffset = offset;
    -            _pending.tailSet(offset).clear();
    +        if (offset < _emittedToOffset - _spoutConfig.maxOffsetBehind) {
    +            LOG.info(
    +                    "Skipping failed tuple at offset=" + offset +
    +                            " because it's more than maxOffsetBehind=" + _spoutConfig.maxOffsetBehind +
    +                            " behind _emittedToOffset=" + _emittedToOffset
    +            );
    +        } else {
    +            LOG.debug("failing at offset=" + offset + " with _pending.size()=" + _pending.size() + " pending and _emittedToOffset=" + _emittedToOffset);
    +            failed.add(offset);
    +            numberFailed++;
    +            if (numberAcked == 0 && numberFailed > _spoutConfig.maxOffsetBehind) {
    +                throw new RuntimeException("Too many tuple failures");
    +            }
             }
         }
     
         public void commit() {
    -        long lastCompletedOffset = lastCompletedOffset();
    -        if (lastCompletedOffset != lastCommittedOffset()) {
    -            LOG.info("Writing last completed offset (" + lastCompletedOffset + ") to ZK for " + _partition + " for topology: " + _topologyInstanceId);
    -            Map<Object, Object> data = ImmutableMap.builder()
    +        LOG.debug("Committing offset for " + _partition);
    +        long committedTo;
    +        if (_pending.isEmpty()) {
    +            committedTo = _emittedToOffset;
    +        } else {
    +            committedTo = _pending.first() - 1;
    +        }
    +        if (committedTo != _committedTo) {
    +            LOG.debug("Writing committed offset to ZK: " + committedTo);
    +
    +            Map<Object, Object> data = (Map<Object, Object>) ImmutableMap.builder()
                         .put("topology", ImmutableMap.of("id", _topologyInstanceId,
                                 "name", _stormConf.get(Config.TOPOLOGY_NAME)))
    -                    .put("offset", lastCompletedOffset)
    +                    .put("offset", committedTo)
                         .put("partition", _partition.partition)
                         .put("broker", ImmutableMap.of("host", _partition.host.host,
                                 "port", _partition.host.port))
                         .put("topic", _spoutConfig.topic).build();
                 _state.writeJSON(committedPath(), data);
    -            _committedTo = lastCompletedOffset;
    -            LOG.info("Wrote last completed offset (" + lastCompletedOffset + ") to ZK for " + _partition + " for topology: " + _topologyInstanceId);
    -        } else {
    -            LOG.info("No new offset for " + _partition + " for topology: " + _topologyInstanceId);
    +
    +            //LOG.info("Wrote committed offset to ZK: " + committedTo);
    --- End diff --
    
    Either remove this or change it to debug.
, Github user brndnmtthws commented on the pull request:

    https://github.com/apache/incubator-storm/pull/94#issuecomment-42316449
  
    I'll update the PR as per the comments above, but I can't do this today.
, Github user revans2 commented on the pull request:

    https://github.com/apache/incubator-storm/pull/94#issuecomment-42316654
  
    I am not a Kafka expert in any way, I just did a quick pass through the code.
, Github user brndnmtthws commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12705493
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java ---
    @@ -159,12 +171,17 @@ public static ByteBufferMessageSet fetchMessages(KafkaConfig config, SimpleConsu
             for (int errors = 0; errors < 2 && msgs == null; errors++) {
                 FetchRequestBuilder builder = new FetchRequestBuilder();
                 FetchRequest fetchRequest = builder.addFetch(topic, partitionId, offset, config.fetchSizeBytes).
    -                    clientId(config.clientId).build();
    +                    clientId(config.clientId).maxWait(config.fetchMaxWait).build();
                 FetchResponse fetchResponse;
                 try {
                     fetchResponse = consumer.fetch(fetchRequest);
                 } catch (Exception e) {
    -                if (e instanceof ConnectException) {
    +                if (e instanceof ConnectException ||
    --- End diff --
    
    Actually, it looks like this is okay with Java 6, unless I'm mistaken.
, Github user brndnmtthws commented on the pull request:

    https://github.com/apache/incubator-storm/pull/94#issuecomment-43248872
  
    Updated as per comments above, with a couple other minor fixes.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/94#discussion_r12745379
  
    --- Diff: external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java ---
    @@ -159,12 +171,17 @@ public static ByteBufferMessageSet fetchMessages(KafkaConfig config, SimpleConsu
             for (int errors = 0; errors < 2 && msgs == null; errors++) {
                 FetchRequestBuilder builder = new FetchRequestBuilder();
                 FetchRequest fetchRequest = builder.addFetch(topic, partitionId, offset, config.fetchSizeBytes).
    -                    clientId(config.clientId).build();
    +                    clientId(config.clientId).maxWait(config.fetchMaxWait).build();
                 FetchResponse fetchResponse;
                 try {
                     fetchResponse = consumer.fetch(fetchRequest);
                 } catch (Exception e) {
    -                if (e instanceof ConnectException) {
    +                if (e instanceof ConnectException ||
    --- End diff --
    
    You are correct.  I though I saw the java 7 ```catch ConnectException | SocketTimeoutException | ...``` But I could be mistaken.  It looks fine now
, Github user revans2 commented on the pull request:

    https://github.com/apache/incubator-storm/pull/94#issuecomment-43348089
  
    The code looks good to me, but I am not a kafka expert and would really like someone else with more kafka knowledge to take a look at this too. I am +1.  
, Github user brndnmtthws commented on the pull request:

    https://github.com/apache/incubator-storm/pull/94#issuecomment-43372409
  
    Updated as per @wurstmeister's comments.
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/incubator-storm/pull/94#issuecomment-43641973
  
    +1
, Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-storm/pull/94
]