[[~ptgoetz] 
Providing more log lines may be helpful to determine what things were occurred.
For example, is there any log messages "connection attempt %s to %s failed" before "closing Netty Client"?
, Strange... Let's focus that Client seems to be closed but worker still call send() to that Client.
There're methods which closes the Client...

1.  mk-refresh-connection

mk-refresh-connection replaces cached-task->node+port first, and closes remove-connections, so send() shouldn't be called to that Client.

2. Context.term()

It means that worker is in progress of shutdown, so eventually send() shouldn't be called.

3. Connect.run()

Connect.run() only calls Client.close() when closing is true, which means that either 1 or 2 should occur before this one.

Please comment new paths here which I'm missing., I also checked without any success. One possible cause could be context returning a stale value of client. (host-port --> connections) map inside the context class is not thread safe. There are synchronized blocks but they do not guarantee visibility consistency. 
However, other than shutdown, I could not see any scenario wherein the put and get to this map is called from different thread. , Lowering priority to major and removing from 1.0 release epic since I (and others) are unable to reliably reproduce., Similar problem as following.

I got everything well when I ran topology in Eclipse LocalCluster. 

I used Kafka Spout/StateUpdater in Trident State.

2016-03-29 16:40:36.461 b.s.m.n.Client [ERROR] connection attempt 12 to Netty-Client-bt-199-036.bta.net.cn/202.106.199.36:6702 failed: org.apache.storm.shade.org.jboss.netty.channel.ConnectTimeoutException: connection timed out: bt-199-036.bta.net.cn/202.106.199.36:6702
2016-03-29 16:40:36.859 STDIO [ERROR] 三月 29, 2016 4:40:36 下午 org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer
警告: An exception was thrown by TimerTask.
java.lang.RuntimeException: Giving up to scheduleConnect to Netty-Client-bt-199-036.bta.net.cn/202.106.199.36:6702 after 12 failed attempts. 0 messages were lost
	at backtype.storm.messaging.netty.Client$Connect.run(Client.java:511)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:546)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$Worker.notifyExpiredTimeouts(HashedWheelTimer.java:446)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:395)
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:745)
2016-03-29 16:40:44.466 b.s.m.n.Client [ERROR] connection attempt 1 to Netty-Client-bt-199-036.bta.net.cn/202.106.199.36:6703 failed: org.apache.storm.shade.org.jboss.netty.channel.ConnectTimeoutException: connection timed out: bt-199-036.bta.net.cn/202.106.199.36:6703
2016-03-29 16:40:54.665 b.s.m.n.Client [ERROR] connection attempt 2 to Netty-Client-bt-199-036.bta.net.cn/202.106.199.36:6703 failed:, Connect.run() only calls Client.close() when closing is true. We see the connections calls client.close by Connect.run() from the log. So we should call the close() previously. Thus the log should print the "closing Netty Client {}" when calls close() previously. [~ptgoetz]   can you find the a little earlier log about close()?, Maybe this is a problem due to incompatibility of different Zookeeper versions. I solved my problem by removing Zookeeper dependencies in my $STORM_HOME/extlib. 

Previously, I used Zookeeper-3.4.7 as my zk cluster and imported zookeeper-3.4.6.jar from kafka_2.11-0.8.2.2/lib, and the problem came into being. After I removed the wrong version zk jar, everything went ok with me. 

, Zookeeper does have netty as a dependency, but in the versions we have been using it is not configured to be on.  Perhaps newer versions of zookeeper have it packaged with it somehow?, [~Johnbaba], unfortunately I don't. I ran into it when testing on a virtualized cluster that I've since destroyed. Next time I'll be better at saving off the log files. :), I have been facing a similar issue in storm. keep getting a error like 
[ERROR] discarding 1 messages because the Netty client to Netty-Client-/{host}:{port} is being closed

I have seen it appear when the storm nodes had connectivity issues
1. connectivity issues within storm nodes
2. connectivity issues between kafka spout and kafka broker.
Even after the connectivity is recovered , the issue still remains., That could possibly be because the topology workers are getting restarted frequently. It usually happens when there is an uncaught exception in the topology. , Even if the topology workers are getting restarted, why should it lead to discarding a message. Even when the worker is back up,i see these errors and some missing messgaes. , Is it certain that this problem affects version 1.0.0? Because I observed the same problem  in 0.9.6 when workers restarted in short succession. That often happens if one worker crashes since nimbus tends to reassign tasks right around the time the crashed worker is restarted by the supervisor (most likely because by default both heartbeat timeouts are 30 second).
I had a patch that solved it, but it is no longer necessary for 1.0.0., I meet same problem in 0.9.6. Can you tell me, how you slove the problem in that version?, Hi,Nico Meyer,the same problem appears in version 0.10.0.The situation is similar to what you say.Can I know how you solve this problem?I am Looking forward to your replay.Thanks a lot., Patch for 0.9.6, Hi sun, sorry for the long delay, but I forgot my password and the reset function didn't work for me last week.

I attached our patch against 0.9.6 (created with 'git diff') that seemed to do the trick, but no guarantees. We upgraded to 1.0.2 by now., [~nico.meyer]

Because of the Log 

{code:java}
2016-02-18 08:46:59.116 o.a.s.m.n.Client [INFO] closing Netty Client Netty-Client-/192.168.202.6:6701
{code}
,
the method {color:red}close(){color} in backtype.storm.messaging.netty.Client is invoked.

So your Patch maybe to not working because reconnectingAllowed will always return {color:red}FALSE{color}.

{code:java}
public void run(Timeout timeout)
      throws Exception
    {
      if (Client.this.reconnectingAllowed())
      {
{code}

[STORM-2561|https://issues.apache.org/jira/browse/STORM-2561]
, [~sunny.davy]

I don't remember the details to be honest, since I wrote this over a year ago. And we moved on to Storm 1.0.2 by now, which does not have this problem any more.
All I can say is, that we used the patch in production for quite some time and did not ever observe the lockup problem again., [~nico.meyer]

Okey, maybe I will upgrade to 0.10.2 firstly. :), [~sunny.davy]
I would propose upgrading to 1.0.3 directly. It is a little bit of work to fix all the namespaces, and some other minor things. But there are a lot of other improvements in the 1.x version that warrant the time spent in my opinion., [~nico.meyer]

:)  I will review the diff between 0.10.x and 1.0.x , thank you, Hi ,

I also got similar error. I am using storm 1.1.1 . Please advise.

2017-10-10 13:17:47.674 o.a.s.m.n.Client client-boss-1 [ERROR] connection attempt 199 to Netty-Client-/192.168.2.195:6702 failed: java.net.ConnectException: Connection refused: /192.168.2.195:6702
2017-10-10 13:17:47.674 o.a.s.u.StormBoundedExponentialBackoffRetry client-boss-1 [WARN] WILL SLEEP FOR 738ms (MAX)
2017-10-10 13:17:48.474 o.a.s.m.n.Client client-boss-1 [ERROR] connection attempt 200 to Netty-Client-/192.168.2.195:6702 failed: java.net.ConnectException: Connection refused: /192.168.2.195:6702
2017-10-10 13:17:48.475 o.a.s.u.StormBoundedExponentialBackoffRetry client-boss-1 [WARN] WILL SLEEP FOR 741ms (MAX)
2017-10-10 13:17:48.972 o.a.s.m.n.Client refresh-connections-timer [INFO] closing Netty Client Netty-Client-/192.168.2.195:6702
2017-10-10 13:17:48.977 o.a.s.m.n.Client refresh-connections-timer [INFO] waiting up to 600000 ms to send 0 pending messages to Netty-Client-/192.168.2.195:6702
2017-10-10 13:17:49.271 STDIO client-schedule-service-1 [ERROR] Oct 10, 2017 1:17:49 PM org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer
WARNING: An exception was thrown by TimerTask.
java.lang.RuntimeException: Giving up to scheduleConnect to Netty-Client-/192.168.2.195:6702 after 200 failed attempts. 0 messages were lost
	at org.apache.storm.messaging.netty.Client$Connect.run(Client.java:606)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:546)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$Worker.notifyExpiredTimeouts(HashedWheelTimer.java:446)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:395)
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:748)
2017-10-10 13:19:55.636 o.a.s.m.n.StormServerHandler Netty-server-localhost-6702-worker-1 [ERROR] server errors in handling the request
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_131]
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_131]
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_131]
	at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_131]
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) ~[?:1.8.0_131]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64) [storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [storm-core-1.1.1.jar:1.1.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2017-10-10 13:28:14.369 o.a.s.m.n.StormServerHandler Netty-server-localhost-6702-worker-1 [ERROR] server errors in handling the request
java.io.IOException: Connection reset by peer, [~saurav689],

In 1.1.1 The only time that "Giving up to schedule" is thrown, is when the netty client is closing.  The only time that it is closing is when the scheduling has changed and we no longer have a need to that client, which is what you have in your logs with the refresh-connections-timer.

From the logs it looks like your topology had a worker scheduled to be on 192.168.2.195:6702, but that worker never came up for some reason.  You didn't include the logs so I cannot tell.  After some time nimbus rescheduled the worker to be on a different host/port.  At that point you got an exception while we were closing the client. 

The later logs for Netty-server-localhost-6702-worker-1 indicate that a worker that was connected to this worker broke the connection.  They are most likely not related to the first one.

Did your topology eventually recover?  Did you ever look at the logs for 192.168.2.195:6702 to try and see why it didn't come up?  In 2.x we have added in better logging so hopefully we would be able to see which worker disconnected from the server.
, Hello,

FYI - We get same error about one time per month in our production Storm cluster.

Our by-pass is to monitor our topologies using Storm web services and restart impacted topologies when we detect they are impacted (poor's man by pass...)

 

Best regards,

Alexandre Vermeerbergen

 

 , Looks similar to STORM-3055]