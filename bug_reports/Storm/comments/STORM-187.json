[try to set "storm.messaging.netty.max_retries" to a smaller number (e.g. 10). The current setting will cause sleep_time to exceed the maximum integer and evaluate to a negative value., Hi,

Thanks for the response. Now I am not getting the mentioned error but the topology is not working. I see to much traces like this:

2014-01-02 16:29:40 b.s.m.n.Client [INFO] Reconnect ... [10]
2014-01-02 16:29:40 b.s.m.n.Client [WARN] Remote address is not reachable. We will close this client.

Any idea?

Thanks

, Have you verified any network problems ?, Yes, it is all fine. As I mentioned the topologies work fine with ZMQ but when I put those lines in the storm.yaml they don't work., More log traces:

2014-01-03 12:38:08 b.s.m.n.Client [WARN] Remote address is not reachable. We will close this client.
2014-01-03 12:38:09 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: Client is being closed, and does not take requests any more
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:90) ~[storm-core-0.9.0.1.jar:na]
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:61) ~[storm-core-0.9.0.1.jar:na]
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:62) ~[storm-core-0.9.0.1.jar:na]
    at backtype.storm.disruptor$consume_loop_STAR_$fn__2975.invoke(disruptor.clj:74) ~[storm-core-0.9.0.1.jar:na]
    at backtype.storm.util$async_loop$fn__444.invoke(util.clj:403) ~[storm-core-0.9.0.1.jar:na]
    at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
    at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
Caused by: java.lang.RuntimeException: Client is being closed, and does not take requests any more
    at backtype.storm.messaging.netty.Client.send(Client.java:109) ~[storm-netty-0.9.0.1.jar:na]
    at backtype.storm.daemon.worker$mk_transfer_tuples_handler$fn__5867$fn__5868.invoke(worker.clj:304) ~[storm-core-0.9.0.1.jar:na]
    at backtype.storm.daemon.worker$mk_transfer_tuples_handler$fn__5867.invoke(worker.clj:293) ~[storm-core-0.9.0.1.jar:na]
    at backtype.storm.disruptor$clojure_handler$reify__2962.onEvent(disruptor.clj:43) ~[storm-core-0.9.0.1.jar:na]
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:87) ~[storm-core-0.9.0.1.jar:na]
    ... 6 common frames omitted

I tried to map the hostnames of the different components of the storm architecture (nimbus, zookepers and workers) to their IPs in /etc/hosts just in case, but the result is the same., I was running a storm topology and found the same issue:
WARNING: An exception was thrown by a user handler while handling an exception event ([id: 0x655862ff] EXCEPTION: java.net.ConnectException: Connection refused)
java.lang.IllegalArgumentException: timeout value is negative
        at java.lang.Thread.sleep(Native Method)
        at backtype.storm.messaging.netty.Client.reconnect(Client.java:78)
        at backtype.storm.messaging.netty.StormClientHandler.exceptionCaught(StormClientHandler.java:108)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
        at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:109)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:701)

Is netty production ready?


, Just found out that this issue happens when I directly kill supervisor and nimbus without killing the topology from the storm ui. Also when I start the topology with the same name the error persists, but if I change the topology name the topology starts to work again., Am getting the same error Jose posted on 3/01. Here is how I configured storm :

{quote}
storm.messaging.transport: "backtype.storm.messaging.netty.Context"
storm.messaging.netty.server_worker_threads: 1
storm.messaging.netty.client_worker_threads: 1
storm.messaging.netty.buffer_size: 5242880
storm.messaging.netty.max_retries: 10
storm.messaging.netty.max_wait_ms: 1000
storm.messaging.netty.min_wait_ms: 100
{quote}

Any idea?, Can anyone please guide me to run storm in remote mode? what are all the changes that I must do in yaml file to run in remote mode. should I want to add any changes in Netty folder interms of IP address. 

Thank you,
Priya, Go through the following links and I think that should get you started
https://github.com/nathanmarz/storm/wiki/Running-topologies-on-a-production-cluster
http://www.michael-noll.com/tutorials/running-multi-node-storm-cluster/







-- 
Regards,
Piyush Rai
, Thanks Piyush for your reply. I am running storm on windows. The above link consists of linux commands. And also I don't have any clear idea about the dependencies. It will be very useful if you please help me

Thanks,
Priya
, https://github.com/apache/incubator-storm/pull/41, Github user easyreal commented on the pull request:

    https://github.com/apache/incubator-storm/pull/41#issuecomment-39538772
  
    it does not resolve the underlying problems:"netty reconnect"ï¼š
    https://issues.apache.org/jira/browse/STORM-187 
    I want to know what could cause this problem,because i found the same issue in our project
    thank you
, Any chance this is being released soon? Saw discussion a few weeks ago and have tracked down to a point where EC2 nodes won't work with Netty as the transport until this is fixed. Enough changes with this release that I'm getting tempted to cut a 0.9.1.1 release to move forward., Is there any info on when there is going to be a release with this fix included? We are facing the situation where we can't reliably scale our storm setup to multiple instances in our production environment due to this bug. So I'd appreciate any info on a release or a temporary workaround. 

Thanks, I agree. A new minor version (0.9.1.1-incubating?) needs to be released with this fix included, since this bug affects the reliability and fault tolerance of the whole Storm cluster!, [~SoAG] a temporary workaround is to merge https://github.com/apache/incubator-storm/pull/41 into 0.9.1-incubating and build your own Storm release. See https://github.com/apache/incubator-storm/blob/master/DEVELOPER.md for build instructions., Technically I pulled the apach-0.9.1 tag and included the fix. This worked perfectly after running mvn clean package -DskipTests=true in the main repo and then in the storm-dist folder. One thing interesting I found is that I also needed to grab the libthrift out of the new package. , Cool thanks a lot [~jreichhold] and [~dschiavu]. I am going to give this a try now.]