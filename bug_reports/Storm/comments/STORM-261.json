[All storm services are designed to be fail fast and should be continuously managed by something like supervosord. The reason workers don't self destruct is because you would then depend on the supervisor being alive. Once scheduled, workers can survive a supervisor death and restart. 

This would require heartbeats from the supervisors and closing if missing or require workers to communicate with zookeeper looking for reassignments. 

Furthermore a worker cannot easily kill itself. Currently workers are killed externally through a kill 9 from the supervisor. This absolves any risk of non jvm termination due to thread leaks or such. 


I believe when the supervisor is restarted it should try to kill any rescheduled tasks on that machine while it was offline. , This issue was created to consider the case when the supervisor is unable to be restarted, yet its workers are still running.  If there is nothing left to supervise the workers, then a worker will go on participating in the topology (communicating with other legitimate workers) without knowing it should stop., I haven't actually verified this, but when it's scheduled out of the topology, shouldn't it stop receiving new data? In that case, the only data it outputs would be older buffered data, or if there was a thread (or similar)-type tick outputting tuples. Otherwise, it should just be sitting there, wasting resources, but not outputting data...I think?, I'm +1 on this. This kind of redundancy can only be beneficial. , I'm just not sure how this can be accomplished without breaking the idea that workers can survive a supervisor death/restart (short of having every worker heartbeat to zookeeper)., It doesn't suicide if the supervisor is down, but if Nimbus reassigns that worker. In that case the supervisor would just kill the worker immediately on startup anyway., But isn't that the exact scenario this is supposed to address, when the supervisor is down, and cannot be started?, RIght now it doesn't do that. I'm saying that's the behavior resolving this issue would have., bq. I haven't actually verified this, but when it's scheduled out of the topology, shouldn't it stop receiving new data?

We ran into this issue while running some tests. Someone accidentally had brought up a supervisor on the same node as nimbus.  They took the supervisor back down, and then noticed that things were a bit out of whack.  There was a word count topology running completely on a node that wasn't a part of the cluster any more.  Out of curiosity I rebalanced the topology to see what would happen, and now there were two copies of the topology running.  Looking at the logs both appeared to be processing data. 

I marked this as minor because like [~jmlogan] stated before I didn't see much of a way this would cause problems in the real world.  Thinking about it further I can see some use cases where if a spout is left active it could be causing problems, like consuming data that is never fully processes, or by continuing to process data after the topology has been killed.]