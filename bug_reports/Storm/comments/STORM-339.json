[This is not specific to the Netty transport, although it does not help because it does buffer a lot more than the zeromq code did.  If you don't have acking enabled there is no flow control in storm, and if you have not properly sized your components the tuples will be buffered in memory and eventually OOM or be shot by the supervisor because GC took too long and the heartbeats stopped coming.

I'm not sure that there is a really good way to fix this totally without acking., There are 3 methods for implementing protection against OOM without need to acknowledge every message. Storm in ack mode has 10x lower throughput.

See end of http://docs.jboss.org/hornetq/2.2.5.Final/user-manual/en/html/queue-attributes.html#queue-attributes.address-settings

1) use ring buffer for receiving messages. If messages are processed too slowly newly arriving message will replace older unprocessed message. This is not a flow control - just protection against OOM. (type DROP)

2) implement flow control messages, something simple like XON/XOFF protocol (http://en.wikipedia.org/wiki/Software_flow_control) should suffice (type BLOCK)

3) save messages to disk instead of throwing them away (type PAGE)

for inspiration see http://docs.jboss.org/hornetq/2.2.5.Final/user-manual/en/html/flow-control.html, 1)DROP is not a good way because if your components are not properly sized, there will be many dropped tuples and there is no good way to measure the max throughput  your topology can bear without significant dropping.
2)BLOCK is not a good way either because it will cause potential dead loop problem in transport layer. It will cause the whole topology hang.
3)PAGE is not a good way as it will continuous  increase latency and there is no good way to measure the max throughput your topology can bear with all tuples executed in reasonable latency.

Maybe the best way is to implement a flow control mechanism like spout pending but much simple one.
, DROP is what zeromq did by default and there has been some discussion about supporting that for netty as well.   Adding flow control to storm sounds like a reasonable solution.  

[~hsn] I am curious where you got your 10x decrease in throughput numbers? All of my throughput testing I have done with acking enabled to avoid these types of issues., I have 10k msg/sec throughput without ack, 100 per sec with ack, using different number of unconfirmed messages did not change anything. This made storm unusable for me and i am exploring different solution., We are hitting the same issue with Storm 0.10.0, heap balloons past our -Xmx4G setting very fast and eventually worker gets killed from an OOM Exception. Heap dump shows millions of byte[] arrays in Netty Server that correspond to our serialized tuples being passed around. Issue goes away as soon as we enable ackers but of course we pay the slow throughput penalty ]