[Hi [~roshan_naik],

Do you mean [https://github.com/intel-hadoop/storm-benchmark] ?  I'm the main committer to this project and we've previously express our desire to contribute in STORM-642. The project now a bit out-dated and needs to add support for latest storm releases. 

How did the perf number change ? Basically, storm-benchmark is a suite of native Storm topologies and the metrics data are collected from nimbus. , Hi [~mauzhang],
    Yes thats it. I first observed that perf difference issue when working on STORM-1632, but was not able to get to the bottom of it. The storm native topology mentioned here : https://github.com/apache/storm/pull/1217#issuecomment-201074919

I can try to locate the benchmark-specific version of the topology but its a straightforward rewrite.

The storm native showed a  difference of ~12% when doing a A/B test (with and without the fix)
The benchmark specific version of the topology .. it was 25%  as noted in the description of STORM-1632.



IMO..  briefly ignoring the perf diff issue, it would be good to go ahead and see what we can incorporate from that benchmark . In this jira my goal is to add a few topologies for perf testing... not to create a benchmarking tool/framework itself. In that sense its not conflicting with STORM-642. 

*side note:* If we are adding a benchmarking framework, it would be good if it can run standard Storm topologies directly and not require topologies to be written specifically for it., I would love to see more performance tests/topologies in storm.  I wrote one https://github.com/apache/storm/blob/master/examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java but it is just a word count topology, and it requires acking to be enabled for it to work properly.  We also did a storm benchmark as part of https://github.com/yahoo/streaming-benchmarks.  And then there is the https://github.com/yahoo/storm-perf-test which is outdated, and too many people use it to test the general performance of storm instead of using it as stress test on the messaging layer, which is the only thing it is really good at.

I think ThroughputVsLatency is the correct model that we want to follow for these types of tests.  Trying to capture the processing latency at a given throughput.  It can easily be adapted to other topologies, and with a small amount of work should be able to support an OK measure of latency without the need for acking, assuming that the clocks among the different nodes are close to being in sync with one another. The intel benchmarks are great in their diversity of workloads, but I would also like to see something that is more real world too., Thanks, very good suggestions. Maybe not a proper place to ask, but do you have plans to move the yahoo streaming benchmarks forward ? It has not been updated for months, and it seems no one is looking at opened pull requests., Sorry, yes I want to move it forward.  Life happens, but I will try to make time soon to pull in the current pull requests., Merged by [~sriharsha].

NOTE: for 1.x-branch the version of module is set to 1.1.0-SNAPSHOT whereas others are 1.1.1-SNAPSHOT. This is fine when we rollback the release for 1.1.0 RC2 cancelation.]