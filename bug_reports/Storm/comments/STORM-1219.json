[GitHub user arunmahadevan opened a pull request:

    https://github.com/apache/storm/pull/893

    STORM-1219: Fix HDFS and Hive bolt flush/acking

    HDFS and Hive bolts were setting the default tick tuple interval in the
    prepare() method, which was not taking effect. Correctly set a default
    and return from getComponentConfiguration if the user does not
    explicitly set a value.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/arunmahadevan/storm hdfs-hive-bolt-fix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/storm/pull/893.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #893
    
----
commit e36bc47acd43ebd567630c253d5f8f422fd21863
Author: Arun Mahadevan <aiyer@hortonworks.com>
Date:   2015-11-19T07:17:33Z

    STORM-1219: Fix HDFS and Hive bolt flush/acking
    
    HDFS and Hive bolts were setting the default tick tuple interval in the
    prepare() method, which was not taking effect. Correctly set a default
    and return from getComponentConfiguration if the user does not
    explicitly set a value.

----
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-158125939
  
    +1
, Github user dossett commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-158126017
  
    Was the prior approach not working because getComponentConfiguration() is called before prepare() ?
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-158126825
  
    @dossett it appears that the component configuration is merged with the global storm config. This happens when the topology is constructed and serialized. The merged config is then passed to `prepare`, so if the map passed to prepare is updated, it does not seem to have any effect.
, Github user dossett commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-158146049
  
    Interesting, this must have been a regression when that particular code was refactored from HdfsBolt up to AbstractHdfsBolt.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-164343943
  
    @harshach @dossett any other comments? Can we merge ?
, Github user dossett commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-164517610
  
    It's still not clear to me why the original approach doesn't work, but that's probably a problem with my understanding of storm internals.
    
    +1
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165166066
  
    @arunmahadevan I might be overlooking something but if the default tick tuple is at 1secs and this will cause the tuples to be flushed much more frequently without even reaching to batchsize. This behavior can create lot of small files in hdfs and also for hive.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165166518
  
    I think the default should be at least 15secs than 1
, Github user dossett commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165168520
  
    @harshach I don't believe flushing would force early file rotations.  On second thought, I do agree that 15 seconds would be better than 1 second.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165169297
  
    @dossett It does for hive. It creates small files , which eventually will be merged by compactor. But its not good to have small files especially if your compactor runs like every hour.
, Github user dossett commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165169501
  
    @harshach Ah, thanks, I was only thinking of the hdfs bolt side.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165185880
  
    @harshach with 15 secs, the throughput was very low since the spout could not emit more tuples after hitting max.spout.pending. The low value (1s) default also makes sure that the value is not more than topology.message.timeout.secs in case user changes the default timeout (if the value is more it would cause unnecessary duplicates). There seems to be no way to get the message timeout value in getComponentConfiguration and adjust the tick tuple interval accordingly.
    
    The 1s is only the default, the user can change it via HiveOptions if compaction is a concern.
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165193639
  
    @arunmahadevan throughput should control by batch size not by tickTupleinterval. If the tuples are not appearing the hive table or hdfs file, one should lower the batch size or configure the tickTupleInterval to lower but making this default is not ideal. Most often users won't configure them and can create adverse effects in production.  15 secs will at least guarantee the tuples won't be timing out and not create smaller files.
, Github user arunmahadevan commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165342711
  
    @harshach yes its a trade off. Updated default to 15s 
, Github user harshach commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-165359965
  
    +1
, Github user revans2 commented on the pull request:

    https://github.com/apache/storm/pull/893#issuecomment-170617085
  
    +1
, Github user asfgit closed the pull request at:

    https://github.com/apache/storm/pull/893
, Thanks [~arunmahadevan],

I merged this into master.]