[I have started to work on this but after looking through the security code I am not clear about the issue completely. Currently we have some code in AutoTGT that attempts auto login to hadoop if hadoop is in the classPath. Is this task asking for implementation to ensure we check if HDFS and HBASE is in class path and if yes, fetch delegation tokens and store it? Shouldn't this be part of HDFS and HBASE bolts, where the actual interactions would happen? Or this issue is just for creating a helper so clients that use HDFS and HBASE can get delegation token using this helper?, Sorry I did throw up a design or explain this very well.

IAutoCredentials right now only has APIs to fetch credentials on the client side, and then use them on the topology side to populate a Subject.  For this we need to extend it so that it could also fetch the credentials on nimbus when the topology is submitted (if it is configured to do so).

So for an HDFS delegation token I would expect the code to do something like the following.

# The client would submit a topology with a config set that says it wants hdfs delegation tokens. i.e. topology.autohdfs.namenodes=["hdfs://foo.com/","hdfs://bar.com"]
# Nimbus before if finished submitting the topology it runs the configured IAutoCredentials instances (Probably need a new config for this, or extend ICredentialsRenewer to also take the topology config).
## AutoHDFS would look at the config and become a proxy user for the different NameNodes to fetch the delegation token and put it into the credentials.
# On the topology side AutoHDFS would take the delegation token and populate it into the UGI.

Periodically Nimbus would run AutoHDFS as an ICredentialsRenewer.  If the token needs to be renewed it would connect to the name node and renew it.  If the renewal period is about to expire it would fetch a new delegation token, and replace the old one in the credentials map.

For HBase it should be similar, but replace hdfs with hbase.  I have heard though that for this to work properly hbase may need a fix too.  Some people I have talked to have indicated that the RPC layer of HBase caches the delegation token, so even if it is updated in the UGI it will not be used to make new connections to HBase, but I don't know for sure.
, GitHub user Parth-Brahmbhatt opened a pull request:

    https://github.com/apache/incubator-storm/pull/189

    STORM-346: added AutoHDFS class that will get hdfs delegation tokens on behalf of users, push it to workers and renew the delegation tokens automatically.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Parth-Brahmbhatt/incubator-storm STORM-346

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-storm/pull/189.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #189
    
----
commit fe5f41aa8332700d3f98422cb7d986fc47289bcd
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-05-21T16:03:11Z

    STORM-216: Added Authentication and Authorization.
    
    This is an upmerged version of https://github.com/yahoo/incubator-storm/tree/security

commit ce6e5d26384d7f5d831b35e4eff126fe214981d1
Author: Derek Dagit <derekd@yahoo-inc.com>
Date:   2014-05-22T18:34:23Z

    rename test for consistent capitalization

commit 698bb9c9788b82d4127d861fb3ecf06a06b683c2
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-05-28T13:45:22Z

    Merge branch 'master' into security

commit 6592b8209c49a98db15b3d6d228f488aa6c2e623
Author: Kishor Patil <patik523@yahoo.com>
Date:   2014-06-09T15:42:39Z

    Add missing ACLs to error znodes and remove auto vivification of error znodes

commit cf2e8b7ee06b455a90bd4b3bfd53facef1369612
Author: Sriharsha Chintalapani <mail@harsha.io>
Date:   2014-06-10T22:01:33Z

    Storm 344. (Security) nimbus renew-credentials not calling ICredentialsRenewer.renew

commit 0a98bee214b46ed20b566a9b49c3eca2895f9fd5
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-11T16:07:59Z

    Merge branch 'master' into security-upmerge
    
    Conflicts:
    	.gitignore
    	storm-core/src/clj/backtype/storm/daemon/drpc.clj
    	storm-core/src/clj/backtype/storm/daemon/executor.clj
    	storm-core/src/clj/backtype/storm/daemon/logviewer.clj
    	storm-core/src/clj/backtype/storm/daemon/worker.clj
    	storm-core/src/clj/backtype/storm/timer.clj
    	storm-core/src/clj/backtype/storm/ui/core.clj
    	storm-core/src/clj/backtype/storm/ui/helpers.clj
    	storm-core/src/clj/backtype/storm/util.clj
    	storm-core/src/jvm/backtype/storm/Config.java
    	storm-core/src/jvm/backtype/storm/utils/Utils.java

commit 118b9221b492ed8b91e6633c3cfb748bc1b82790
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-11T16:10:04Z

    Merge branch 'master' into security

commit 2131a0aeb9074b2c83a09d7515ff8e8ae86f6eaf
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-12T15:58:13Z

    Added back in the user to the web ui.

commit 41615b3c4b174077ac1c729af4aef32e5b79d3c5
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-12T15:59:46Z

    Merge branch 'master' into security

commit bc91ed88d77e392f38c406d143e7ac37bc634564
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-12T16:01:35Z

    Added license to UI template.

commit a762f1c5f99a7a9e77038399f0f14ae03b1414c7
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-12T17:48:51Z

    Merge branch 'STORM-344' of https://github.com/harshach/incubator-storm into STORM-344
    
    STORM-344: (Security) nimbus renew-credentials not calling ICredentialsRenewer.renew

commit 92e3a5742374a3a7c3aae20cbeda32ce7b033526
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-12T21:09:57Z

    Merge branch 'master' into security-upmerge
    
    Conflicts:
    	storm-core/src/clj/backtype/storm/LocalCluster.clj
    	storm-core/src/clj/backtype/storm/cluster.clj
    	storm-core/src/clj/backtype/storm/config.clj
    	storm-core/src/clj/backtype/storm/daemon/drpc.clj
    	storm-core/src/clj/backtype/storm/testing.clj
    	storm-core/src/clj/backtype/storm/testing4j.clj
    	storm-core/src/clj/backtype/storm/thrift.clj
    	storm-core/src/clj/backtype/storm/ui/core.clj
    	storm-core/src/clj/backtype/storm/util.clj
    	storm-core/src/clj/backtype/storm/zookeeper.clj

commit ab7784e49d251ca4da967c6ec6bc340cc7f940aa
Author: Kishor Patil <patik523@yahoo.com>
Date:   2014-06-17T15:19:00Z

    Force free a slot in bad-state

commit d1ba4fc4acdadd5e5e138395bdc5892dfdb88bff
Author: Derek Dagit <derekd@yahoo-inc.com>
Date:   2014-06-17T15:56:51Z

    Do not clean up user file when rmr is unsuccessful

commit 87cdbf5fdf5bfb49b983604542283f05123d0d51
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-17T18:32:18Z

    Merge branch 'STORM-357' of https://github.com/d2r/incubator-storm into STORM-357
    
    STORM-357: Cleans workers-users file only when rmr is successful

commit ea946d04dcb6df8e65dbf16500a361eaaba13432
Author: Kishor Patil <kpatil@yahoo-inc.com>
Date:   2014-06-18T23:58:33Z

    Show node details on errors for STORM-360 on security

commit 79089ad0da80e38eb36b7ea91be8b43795dc4efb
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-19T21:04:02Z

    Merge tag 'v0.9.2-incubating' into security
    
    [maven-release-plugin]  copy for tag v0.9.2-incubating
    
    Conflicts:
    	storm-core/pom.xml

commit f20df7d52d52abc9bc03a0cf45388241927cff5a
Author: Kishor Patil <kpatil@yahoo-inc.com>
Date:   2014-06-20T20:35:16Z

    Fixing coding style and component template

commit c545b9d638067c0ae4528e16f14e67c56e0dd47e
Author: Kishor Patil <kpatil@yahoo-inc.com>
Date:   2014-06-20T23:01:00Z

    Fix nimbus use of doto

commit d7c1d1d0a909079a370ed35aaac91668eef33a22
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-23T14:31:41Z

    Merge branch 'node-on-error-security' of https://github.com/kishorvpatil/incubator-storm into STORM-360-security
    
    STORM-360: Add node details for Error Topology and Component pages on security

commit 65aee65af54dd29434af8f5ee403233b597561b6
Author: Robert (Bobby) Evans <bobby@apache.org>
Date:   2014-06-23T15:18:38Z

    Merge branch 'master' into security
    
    Conflicts:
    	storm-core/src/clj/backtype/storm/cluster.clj
    	storm-core/src/clj/backtype/storm/ui/core.clj
    	storm-core/test/clj/backtype/storm/cluster_test.clj

commit 28c168fd7d0272f88d586f6f572eab937b874f22
Author: Kishor Patil <kpatil@yahoo-inc.com>
Date:   2014-06-24T19:12:56Z

    Add check for empty table before sorting on security

commit 3c6930dfe4447b6077916b9f9a07b062141b5305
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2014-07-07T18:36:49Z

    AutoHDFS for getting HDFS delegation token and auto renew.

commit 00e80e9a132764d4b73737d2f7a52282e5247856
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2014-07-07T18:43:59Z

    Merge remote-tracking branch 'upstream/security' into security

commit e04c37356c96d9851c00542c739d053e4bf36481
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2014-07-07T18:49:42Z

    Revert "AutoHDFS for getting HDFS delegation token and auto renew."
    
    This reverts commit 3c6930dfe4447b6077916b9f9a07b062141b5305.

commit 1094762bf9c3ae339500a3a4500d742367c33e63
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2014-07-11T18:50:07Z

    STORM-346: added AutoHDFS class that will get hdfs delegation tokens on behalf of users, push it to workers and renew the delegation tokens automatically.

----
, Github user ptgoetz commented on the pull request:

    https://github.com/apache/incubator-storm/pull/189#issuecomment-48771543
  
    @Parth-Brahmbhatt Is this pull request meant for the security branch? Currently it's pointing to master.
, Github user Parth-Brahmbhatt closed the pull request at:

    https://github.com/apache/incubator-storm/pull/189
, GitHub user Parth-Brahmbhatt opened a pull request:

    https://github.com/apache/incubator-storm/pull/190

    STORM-346: added AutoHDFS class that will get hdfs delegation tokens on behalf of users, push it to workers and renew the delegation tokens automatically.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Parth-Brahmbhatt/incubator-storm STORM-346

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-storm/pull/190.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #190
    
----
commit 3c6930dfe4447b6077916b9f9a07b062141b5305
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2014-07-07T18:36:49Z

    AutoHDFS for getting HDFS delegation token and auto renew.

commit 00e80e9a132764d4b73737d2f7a52282e5247856
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2014-07-07T18:43:59Z

    Merge remote-tracking branch 'upstream/security' into security

commit e04c37356c96d9851c00542c739d053e4bf36481
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2014-07-07T18:49:42Z

    Revert "AutoHDFS for getting HDFS delegation token and auto renew."
    
    This reverts commit 3c6930dfe4447b6077916b9f9a07b062141b5305.

commit 1094762bf9c3ae339500a3a4500d742367c33e63
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   2014-07-11T18:50:07Z

    STORM-346: added AutoHDFS class that will get hdfs delegation tokens on behalf of users, push it to workers and renew the delegation tokens automatically.

----
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15085803
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,298 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +    private static final String CONF_KEYTAB_KEY = "keytab";
    +    private static final String CONF_USER_KEY = "user";
    +
    +    private Map conf;
    +
    +    public void prepare(Map conf) {
    +        this.conf = conf;
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private Object getConfiguration() {
    --- End diff --
    
    I think I would rather have hdfs-site.xml be configured properly rather then trying to have storm know enough about Hadoop configuration to make it work.  Configuring Hadoop is a real pain, and I think this will just end up being a losing battle.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15086078
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,298 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +    private static final String CONF_KEYTAB_KEY = "keytab";
    +    private static final String CONF_USER_KEY = "user";
    +
    +    private Map conf;
    +
    +    public void prepare(Map conf) {
    +        this.conf = conf;
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private Object getConfiguration() {
    +        try {
    +            final String hdfsUser = (String) conf.get(Config.HDFS_USER);
    +            final String hdfsUserKeyTab = (String) conf.get(Config.HDFS_USER_KEYTAB);
    +
    +            /**
    +             *  Configuration configuration = new Configuration();
    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +             *  configuration.set(CONF_USER_KEY, hdfsUser);
    +             */
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            Object configuration = configurationClass.newInstance();
    +
    +            Method setMethod = configurationClass.getMethod("set", String.class, String.class);
    +            setMethod.invoke(configuration, CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +            setMethod.invoke(configuration, CONF_USER_KEY, hdfsUser);
    +            /**
    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml
    +             * and core-site.xml in the class path which should set these configuration.
    +             * setMethod.invoke(configuration, "hadoop.security.authentication", "KERBEROS");
    +             * setMethod.invoke(configuration,"dfs.namenode.kerberos.principal",
    +             *                                "hdfs/zookeeper.witzend.com@WITZEND.COM");
    +             * setMethod.invoke(configuration, "hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +             */
    +
    +            setMethod.invoke(configuration, "hadoop.security.authentication", "KERBEROS");
    +            setMethod.invoke(configuration, "dfs.namenode.kerberos.principal","hdfs/zookeeper.witzend.com@WITZEND.COM");
    +            setMethod.invoke(configuration, "hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +
    +            //UserGroupInformation.setConfiguration(configuration);
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            Method setConfigurationMethod = ugiClass.getMethod("setConfiguration", configurationClass);
    +            setConfigurationMethod.invoke(null, configuration);
    +            return configuration;
    +        }  catch (Exception e) {
    +            throw new RuntimeException(e);
    +        }
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private void login(Object configuration) {
    +        try {
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            final Class securityUtilClass = Class.forName("org.apache.hadoop.security.SecurityUtil");
    +            Method loginMethod = securityUtilClass.getMethod("login", configurationClass, String.class, String.class);
    +            loginMethod.invoke(null, configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);
    +        } catch (Exception e) {
    +           throw new RuntimeException("Failed to login to hdfs .", e);
    +        }
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken() throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    +             *  Configuration configuration = new Configuration();
    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +             *  configuration.set(CONF_USER_KEY, hdfsUser);
    +             *  UserGroupInformation.setConfiguration(configuration);
    +             *  if(UserGroupInformation.isSecurityEnabled) {
    +             *      SecurityUtil.login(configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);
    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +             *      Credentials credential= proxyUser.getCredentials();
    +             *      fs.addDelegationToken(hdfsUser, credential);
    +             * }
    +             * and then return the credential object as a bytearray.
    +             */
    +            Object configuration = getConfiguration();
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod("isSecurityEnabled");
    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);
    +            if(isSecurityEnabled) {
    +                login(configuration);
    +
    +                final URI nameNodeURI = URI.create((String) conf.get(Config.HDFS_NAMENODE_URL));
    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);
    +                final String hdfsUser = (String) conf.get(Config.HDFS_USER);
    +
    +                Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +
    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +                Class fileSystemClass = Class.forName("org.apache.hadoop.fs.FileSystem");
    +                Method getMethod = fileSystemClass.getMethod("get", URI.class, configurationClass, String.class);
    +                Object fileSystem = getMethod.invoke(null, nameNodeURI, configuration, topologySubmitterUser);
    +
    +                //UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +                Method getCurrentUserMethod = ugiClass.getMethod("getCurrentUser");
    +                final Object ugi = getCurrentUserMethod.invoke(null);
    +
    +                //UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +                Method createProxyUserMethod = ugiClass.getMethod("createProxyUser", String.class, ugiClass);
    +                Object proxyUGI = createProxyUserMethod.invoke(null, topologySubmitterUser, ugi);
    +
    +                //Credentials credential= proxyUser.getCredentials();
    +                Method getCredentialsMethod = ugiClass.getMethod("getCredentials");
    +                Object credentials = getCredentialsMethod.invoke(proxyUGI);
    +
    +                //fs.addDelegationToken(hdfsUser, credential);
    +                Class credentialClass = Class.forName("org.apache.hadoop.security.Credentials");
    +                Method addDelegationTokensMethod = fileSystemClass.getMethod("addDelegationTokens", String.class,
    +                        credentialClass);
    +                addDelegationTokensMethod.invoke(fileSystem, hdfsUser, credentials);
    +
    +
    +                ByteArrayOutputStream bao = new ByteArrayOutputStream();
    +                ObjectOutputStream out = new ObjectOutputStream(bao);
    +                Method writeMethod = credentialClass.getMethod("write", DataOutput.class);
    +                writeMethod.invoke(credentials, out);
    +                out.flush();
    +                out.close();
    +
    +                LOG.info(bao.toString());
    +                return bao.toByteArray();
    +            } else {
    +                throw new RuntimeException("Security is not enabled for HDFS");
    +            }
    +        } catch (Exception ex) {
    +            throw new RuntimeException("Failed to get delegation tokens." , ex);
    +        }
    +    }
    +
    +    @Override
    +    public void populateCredentials(Map<String, String> credentials) {
    --- End diff --
    
    Populate credentials is called on the gateway by the end user.  As such there is no reason to use a proxyUser get a delegation token, because I can just get it.  In fact the proxyUser should fail, because regular users are not authorized to act as proxy users.
    
    The idea was to change some of how nimbus used an ICredentialsRenewer, or have a different interface so that nimbus can ask AutoHDFS to populate the credentials at the very beginning. I'll try to explain better in the more general comments.
, Github user revans2 commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-49372008
  
    I really guess I didn't explain things very well in the JIRA.  So the current way IAutoCredentials and ICredentialsRenewer work with nimbus are.
    
    IAutoCredentials will run on the gateway as the topology user.  It gets handed a map that it can place any credentials in needs into.
    Those credentials are uploaded to nimbus, which places them in zookeeper.
    Periodically nimbus will download the serialized credentials and hand them to ICredentialsRenewer which will run as the nimbus user. If it modifies the the credentials they are pushed to zookeeper.
    
    At the same time the worker running as the topology user will download the credentials from zookeeper and pass them to IAutoCredentials to the passed in Subject with them. This repeats any time the credentials change in zookeeper.
    
    
    What I would like to see is something where IAutoCredentials has an option of not running on the gateway, or it is a noop on the gateway.  Instead after the credentials are submitted to nimbus, something similar to, or an extended version of ICredentialsRenewer would run as the nimbus user and get the credentials on behalf of the topology user.
    
    The rest of the process would stay the same, except if a token is about to expire completely.  In that case it would fetch a brand new token.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15125867
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,298 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +    private static final String CONF_KEYTAB_KEY = "keytab";
    +    private static final String CONF_USER_KEY = "user";
    +
    +    private Map conf;
    +
    +    public void prepare(Map conf) {
    +        this.conf = conf;
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private Object getConfiguration() {
    +        try {
    +            final String hdfsUser = (String) conf.get(Config.HDFS_USER);
    +            final String hdfsUserKeyTab = (String) conf.get(Config.HDFS_USER_KEYTAB);
    +
    +            /**
    +             *  Configuration configuration = new Configuration();
    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +             *  configuration.set(CONF_USER_KEY, hdfsUser);
    +             */
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            Object configuration = configurationClass.newInstance();
    +
    +            Method setMethod = configurationClass.getMethod("set", String.class, String.class);
    +            setMethod.invoke(configuration, CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +            setMethod.invoke(configuration, CONF_USER_KEY, hdfsUser);
    +            /**
    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml
    +             * and core-site.xml in the class path which should set these configuration.
    +             * setMethod.invoke(configuration, "hadoop.security.authentication", "KERBEROS");
    +             * setMethod.invoke(configuration,"dfs.namenode.kerberos.principal",
    +             *                                "hdfs/zookeeper.witzend.com@WITZEND.COM");
    +             * setMethod.invoke(configuration, "hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +             */
    +
    +            setMethod.invoke(configuration, "hadoop.security.authentication", "KERBEROS");
    +            setMethod.invoke(configuration, "dfs.namenode.kerberos.principal","hdfs/zookeeper.witzend.com@WITZEND.COM");
    +            setMethod.invoke(configuration, "hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +
    +            //UserGroupInformation.setConfiguration(configuration);
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            Method setConfigurationMethod = ugiClass.getMethod("setConfiguration", configurationClass);
    +            setConfigurationMethod.invoke(null, configuration);
    +            return configuration;
    +        }  catch (Exception e) {
    +            throw new RuntimeException(e);
    +        }
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private void login(Object configuration) {
    +        try {
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            final Class securityUtilClass = Class.forName("org.apache.hadoop.security.SecurityUtil");
    +            Method loginMethod = securityUtilClass.getMethod("login", configurationClass, String.class, String.class);
    +            loginMethod.invoke(null, configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);
    +        } catch (Exception e) {
    +           throw new RuntimeException("Failed to login to hdfs .", e);
    +        }
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken() throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    +             *  Configuration configuration = new Configuration();
    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +             *  configuration.set(CONF_USER_KEY, hdfsUser);
    +             *  UserGroupInformation.setConfiguration(configuration);
    +             *  if(UserGroupInformation.isSecurityEnabled) {
    +             *      SecurityUtil.login(configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);
    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +             *      Credentials credential= proxyUser.getCredentials();
    +             *      fs.addDelegationToken(hdfsUser, credential);
    +             * }
    +             * and then return the credential object as a bytearray.
    +             */
    +            Object configuration = getConfiguration();
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod("isSecurityEnabled");
    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);
    +            if(isSecurityEnabled) {
    +                login(configuration);
    +
    +                final URI nameNodeURI = URI.create((String) conf.get(Config.HDFS_NAMENODE_URL));
    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);
    +                final String hdfsUser = (String) conf.get(Config.HDFS_USER);
    +
    +                Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +
    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +                Class fileSystemClass = Class.forName("org.apache.hadoop.fs.FileSystem");
    +                Method getMethod = fileSystemClass.getMethod("get", URI.class, configurationClass, String.class);
    +                Object fileSystem = getMethod.invoke(null, nameNodeURI, configuration, topologySubmitterUser);
    +
    +                //UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +                Method getCurrentUserMethod = ugiClass.getMethod("getCurrentUser");
    +                final Object ugi = getCurrentUserMethod.invoke(null);
    +
    +                //UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +                Method createProxyUserMethod = ugiClass.getMethod("createProxyUser", String.class, ugiClass);
    +                Object proxyUGI = createProxyUserMethod.invoke(null, topologySubmitterUser, ugi);
    +
    +                //Credentials credential= proxyUser.getCredentials();
    +                Method getCredentialsMethod = ugiClass.getMethod("getCredentials");
    +                Object credentials = getCredentialsMethod.invoke(proxyUGI);
    +
    +                //fs.addDelegationToken(hdfsUser, credential);
    +                Class credentialClass = Class.forName("org.apache.hadoop.security.Credentials");
    +                Method addDelegationTokensMethod = fileSystemClass.getMethod("addDelegationTokens", String.class,
    +                        credentialClass);
    +                addDelegationTokensMethod.invoke(fileSystem, hdfsUser, credentials);
    +
    +
    +                ByteArrayOutputStream bao = new ByteArrayOutputStream();
    +                ObjectOutputStream out = new ObjectOutputStream(bao);
    +                Method writeMethod = credentialClass.getMethod("write", DataOutput.class);
    +                writeMethod.invoke(credentials, out);
    +                out.flush();
    +                out.close();
    +
    +                LOG.info(bao.toString());
    +                return bao.toByteArray();
    +            } else {
    +                throw new RuntimeException("Security is not enabled for HDFS");
    +            }
    +        } catch (Exception ex) {
    +            throw new RuntimeException("Failed to get delegation tokens." , ex);
    +        }
    +    }
    +
    +    @Override
    +    public void populateCredentials(Map<String, String> credentials) {
    --- End diff --
    
    completely my bad, I did not intend to keep these config lines uncommented. I had to keep them uncommented for testing as my hdfs-site.xml settings were not being picked up by the code due to some class path issue. Removing them.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15125886
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,298 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +    private static final String CONF_KEYTAB_KEY = "keytab";
    +    private static final String CONF_USER_KEY = "user";
    +
    +    private Map conf;
    +
    +    public void prepare(Map conf) {
    +        this.conf = conf;
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private Object getConfiguration() {
    --- End diff --
    
    completely my bad, I did not intend to keep these config lines uncommented. I had to keep them uncommented for testing as my hdfs-site.xml settings were not being picked up by the code due to some class path issue. Removing them.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15126024
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,298 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +    private static final String CONF_KEYTAB_KEY = "keytab";
    +    private static final String CONF_USER_KEY = "user";
    +
    +    private Map conf;
    +
    +    public void prepare(Map conf) {
    +        this.conf = conf;
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private Object getConfiguration() {
    +        try {
    +            final String hdfsUser = (String) conf.get(Config.HDFS_USER);
    +            final String hdfsUserKeyTab = (String) conf.get(Config.HDFS_USER_KEYTAB);
    +
    +            /**
    +             *  Configuration configuration = new Configuration();
    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +             *  configuration.set(CONF_USER_KEY, hdfsUser);
    +             */
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            Object configuration = configurationClass.newInstance();
    +
    +            Method setMethod = configurationClass.getMethod("set", String.class, String.class);
    +            setMethod.invoke(configuration, CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +            setMethod.invoke(configuration, CONF_USER_KEY, hdfsUser);
    +            /**
    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml
    +             * and core-site.xml in the class path which should set these configuration.
    +             * setMethod.invoke(configuration, "hadoop.security.authentication", "KERBEROS");
    +             * setMethod.invoke(configuration,"dfs.namenode.kerberos.principal",
    +             *                                "hdfs/zookeeper.witzend.com@WITZEND.COM");
    +             * setMethod.invoke(configuration, "hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +             */
    +
    +            setMethod.invoke(configuration, "hadoop.security.authentication", "KERBEROS");
    +            setMethod.invoke(configuration, "dfs.namenode.kerberos.principal","hdfs/zookeeper.witzend.com@WITZEND.COM");
    +            setMethod.invoke(configuration, "hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +
    +            //UserGroupInformation.setConfiguration(configuration);
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            Method setConfigurationMethod = ugiClass.getMethod("setConfiguration", configurationClass);
    +            setConfigurationMethod.invoke(null, configuration);
    +            return configuration;
    +        }  catch (Exception e) {
    +            throw new RuntimeException(e);
    +        }
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private void login(Object configuration) {
    +        try {
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            final Class securityUtilClass = Class.forName("org.apache.hadoop.security.SecurityUtil");
    +            Method loginMethod = securityUtilClass.getMethod("login", configurationClass, String.class, String.class);
    +            loginMethod.invoke(null, configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);
    +        } catch (Exception e) {
    +           throw new RuntimeException("Failed to login to hdfs .", e);
    +        }
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken() throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    +             *  Configuration configuration = new Configuration();
    +             *  configuration.set(CONF_KEYTAB_KEY, hdfsUserKeyTab);
    +             *  configuration.set(CONF_USER_KEY, hdfsUser);
    +             *  UserGroupInformation.setConfiguration(configuration);
    +             *  if(UserGroupInformation.isSecurityEnabled) {
    +             *      SecurityUtil.login(configuration, CONF_KEYTAB_KEY, CONF_USER_KEY);
    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +             *      Credentials credential= proxyUser.getCredentials();
    +             *      fs.addDelegationToken(hdfsUser, credential);
    +             * }
    +             * and then return the credential object as a bytearray.
    +             */
    +            Object configuration = getConfiguration();
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod("isSecurityEnabled");
    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);
    +            if(isSecurityEnabled) {
    +                login(configuration);
    +
    +                final URI nameNodeURI = URI.create((String) conf.get(Config.HDFS_NAMENODE_URL));
    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);
    +                final String hdfsUser = (String) conf.get(Config.HDFS_USER);
    +
    +                Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +
    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +                Class fileSystemClass = Class.forName("org.apache.hadoop.fs.FileSystem");
    +                Method getMethod = fileSystemClass.getMethod("get", URI.class, configurationClass, String.class);
    +                Object fileSystem = getMethod.invoke(null, nameNodeURI, configuration, topologySubmitterUser);
    +
    +                //UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +                Method getCurrentUserMethod = ugiClass.getMethod("getCurrentUser");
    +                final Object ugi = getCurrentUserMethod.invoke(null);
    +
    +                //UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +                Method createProxyUserMethod = ugiClass.getMethod("createProxyUser", String.class, ugiClass);
    +                Object proxyUGI = createProxyUserMethod.invoke(null, topologySubmitterUser, ugi);
    +
    +                //Credentials credential= proxyUser.getCredentials();
    +                Method getCredentialsMethod = ugiClass.getMethod("getCredentials");
    +                Object credentials = getCredentialsMethod.invoke(proxyUGI);
    +
    +                //fs.addDelegationToken(hdfsUser, credential);
    +                Class credentialClass = Class.forName("org.apache.hadoop.security.Credentials");
    +                Method addDelegationTokensMethod = fileSystemClass.getMethod("addDelegationTokens", String.class,
    +                        credentialClass);
    +                addDelegationTokensMethod.invoke(fileSystem, hdfsUser, credentials);
    +
    +
    +                ByteArrayOutputStream bao = new ByteArrayOutputStream();
    +                ObjectOutputStream out = new ObjectOutputStream(bao);
    +                Method writeMethod = credentialClass.getMethod("write", DataOutput.class);
    +                writeMethod.invoke(credentials, out);
    +                out.flush();
    +                out.close();
    +
    +                LOG.info(bao.toString());
    +                return bao.toByteArray();
    +            } else {
    +                throw new RuntimeException("Security is not enabled for HDFS");
    +            }
    +        } catch (Exception ex) {
    +            throw new RuntimeException("Failed to get delegation tokens." , ex);
    +        }
    +    }
    +
    +    @Override
    +    public void populateCredentials(Map<String, String> credentials) {
    --- End diff --
    
    Sorry for missing this, I thought the server side implementation of submitTopology is invoking populatecreds so I assume that code will be executed by nimbus and thus ended up creating a proxyUser.
, Github user Parth-Brahmbhatt commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-49469937
  
    The simplest alternative seems to be no implementation for IAutoCredentials needed for AutoHDFS to work. In other words users will not  have to specify any class for "topology.auto-credentials" config for auto hdfs to work.
    
    User will specify AutoHDFS.java as "nimbus.credential.renewers.classes" and AutoHDFS will only implement ICredentialsRenewer. In the prepare phase of AutoHDFS.java, which should be called on nimbus startup, we can get the HDFS credentials.
    
    I have one clarifying question. The ICredentialsRenewer implementations seems to be loaded by reading "nimbus.credential.renewers.classes" config at startup by nimbus. If I understand correctly this means if we use ICredentialsRenewer the users who have a running nimbus and wants to use AutoHDFS will have to change the config and restart the nimbus. Is that acceptable? 
    
    

, Github user Parth-Brahmbhatt commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-49474731
  
    Here is what I have so far:
    
    User can specify AutoHDFS.java as "nimbus.credential.renewers.classes" and AutoHDFS will only implement ICredentialsRenewer. In the prepare phase of AutoHDFS.java, which should be called on nimbus startup, we can get the HDFS credentials. However, I don't think the topology submitter user will be available at that time so we will not be able to get the token on behalf of the user but only as nimbus which I feel is unacceptable. 
    
    In order to actually get the credentials as topology submitter user, we either need a new Interface that will run on nimbus when a topology is submitted as part of submitTopologyWithOpts implementation or we can add getCredentialForUser(Map conf) method to ICredentialsRenewer interface and call that as part of submitTopologyWithOpts. I personally prefer not to pollute the ICredentialsRenewer interface. Let me know if you have better alternatives or prefer one over another.
    
    I have one last question. The ICredentialsRenewer implementations seems to be loaded by reading "nimbus.credential.renewers.classes" config at startup by nimbus. This means the users who have a running nimbus and wants to use AutoHDFS or any other implementation of ICredentialsRenewer will have to change the config and restart the nimbus. Is that acceptable? 
    

, Github user Parth-Brahmbhatt commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-49766164
  
    I have added a new interface that will execute on nimbus during submitTopology operation so nimbus can get credentials on behalf of the user.  I also had to change the ICredentialsRenewer so we can pass the topology configuration to the renew methods in addition to the credentials map. 
, Github user Parth-Brahmbhatt commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-50082444
  
    Added a unit test on nimbus to verify that the credentials will be populated as part of submit topology. I did not find any equivalent unit test for renewers so I added renewer test as well, I think I found a bug in nimbus's renew-credentials method. When old and new credentials do not match, the method calls 
    (.set-credentials! storm-cluster-state id new-creds) which seems to be missing a parameter topology-conf so I changed it to (.set-credentials! storm-cluster-state id new-creds topology-conf). 
    
    The test passed after this change but please confirm that this is indeed a bug and let me know if you want to file a separate JIRA for it.
, Github user revans2 commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-50180758
  
    Yes that does appear to be a bug. Good catch.  This is why we need more unit tests :)
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15414285
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/nimbus.clj ---
    @@ -1046,7 +1047,10 @@
                                     (dissoc storm-conf STORM-ZOOKEEPER-TOPOLOGY-AUTH-SCHEME STORM-ZOOKEEPER-TOPOLOGY-AUTH-PAYLOAD))
                     total-storm-conf (merge conf storm-conf)
                     topology (normalize-topology total-storm-conf topology)
    +                nimbus-autocred-plugins (AuthUtils/getNimbusAutoCredPlugins total-storm-conf)
    --- End diff --
    
    I would prefer to see this generated at the startup of nimbus, and stored in the nimbus data structure.  Here every time a topology is submitted we have to create new instances of the plugins and never clean them up.  Also by using the total-storm-conf, it allows the topology to override the list of plugins, potentially loading a class on the classpath that administrators had disabled on purpose.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15414374
  
    --- Diff: storm-core/src/jvm/backtype/storm/Config.java ---
    @@ -1186,7 +1193,15 @@
          */
         public static final String TOPOLOGY_ISOLATED_MACHINES = "topology.isolate.machines";
         public static final Object TOPOLOGY_ISOLATED_MACHINES_SCHEMA = Number.class;
    -    
    +
    +    /**
    +     * HDFS information, used to get the delegation token on behalf of the topology
    +     * submitter user and renew the tokens. see {@link backtype.storm.security.auth.kerberos.AutoHDFS}
    +     * kerberos principal name with realm should be provided.
    +     */
    +    public static final Object HDFS_PRINCIPAL = "topology.hdfs.user";
    --- End diff --
    
    can we name this TOPOLOGY_HDFS_PRINCIPAL? just so it is clear it is for the topology, even in the clojure/java code.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15414455
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/INimbusCredentialPlugin.java ---
    @@ -0,0 +1,39 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package backtype.storm.security;
    +
    +import java.util.Map;
    +
    +/**
    + * Nimbus auto credential plugin that will be called on nimbus host
    + * during submit topology option. User can specify a list of implementation using config key
    + * nimbus.autocredential.plugins.classes.
    + */
    +public interface INimbusCredentialPlugin {
    --- End diff --
    
    Can we add in a prepare and make this extend Shutdownable?  I can see some plugins wanting to initialize things and cleanup at the end.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15414517
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/AuthUtils.java ---
    @@ -109,6 +111,27 @@ public static IPrincipalToLocal GetPrincipalToLocalPlugin(Map storm_conf) {
         }
     
         /**
    +     * Get all the Nimbus Auto cred plugins that users want to use.
    +     * @param topologyConf topologyConfiguration to use.
    +     * @return nimbus auto credential plugins.
    +     */
    +    public static Collection<INimbusCredentialPlugin> getNimbusAutoCredPlugins(Map topologyConf) {
    --- End diff --
    
    Again I am not sure we want this to come from the topology conf.  I would rather have a list of plugins that the administrator has approved, and let those plugins look at the topology conf to decide if they are going to do anything or not.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15414609
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    --- End diff --
    
    Perhaps this should be in a different package.  Even though we do use kerberos, perhaps backtype.storm.security.auth.hadoop would be more appropriate.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15414654
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    --- End diff --
    
    A comment would be better then logging.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15414687
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    --- End diff --
    
    I really prefer line comments to block comments in the middle of a function.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15414806
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    +             *  if(UserGroupInformation.isSecurityEnabled) {
    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +             *      Credentials credential= proxyUser.getCredentials();
    +             *      fs.addDelegationToken(hdfsUser, credential);
    +             * }
    +             * and then return the credential object as a bytearray.
    +             *
    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml
    +             * and core-site.xml in the class path which should set these configuration.
    +             * configuration.set("hadoop.security.authentication", "KERBEROS");
    --- End diff --
    
    A lot of this information should probably be in a new section of the SECURITY.md to describe how to use this correctly, and the changes that are needed on the NameNode to enable nimbus to user proxy users.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15417146
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    +             *  if(UserGroupInformation.isSecurityEnabled) {
    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +             *      Credentials credential= proxyUser.getCredentials();
    +             *      fs.addDelegationToken(hdfsUser, credential);
    +             * }
    +             * and then return the credential object as a bytearray.
    +             *
    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml
    +             * and core-site.xml in the class path which should set these configuration.
    +             * configuration.set("hadoop.security.authentication", "KERBEROS");
    +             * configuration.set("dfs.namenode.kerberos.principal",
    +             *                                "hdfs/zookeeper.witzend.com@WITZEND.COM");
    +             * configuration.set("hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +             * anf the ticket cache must have the hdfs user's creds.
    +             */
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            Object configuration = configurationClass.newInstance();
    +
    +            //UserGroupInformation.isSecurityEnabled
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod("isSecurityEnabled");
    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);
    +
    +            if(isSecurityEnabled) {
    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);
    +                final String hdfsUser = (String) conf.get(Config.HDFS_PRINCIPAL);
    +
    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +                Class fileSystemClass = Class.forName("org.apache.hadoop.fs.FileSystem");
    +                Object defaultNameNodeURI = fileSystemClass.getMethod("getDefaultUri", configurationClass).invoke(null, configuration);
    --- End diff --
    
    It might be nice in the future to allow for more then just the default URI, but we could easily do that on a different JIRA. no reason to hold up this pull request.
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15417222
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    +             *  if(UserGroupInformation.isSecurityEnabled) {
    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +             *      Credentials credential= proxyUser.getCredentials();
    +             *      fs.addDelegationToken(hdfsUser, credential);
    +             * }
    +             * and then return the credential object as a bytearray.
    +             *
    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml
    +             * and core-site.xml in the class path which should set these configuration.
    +             * configuration.set("hadoop.security.authentication", "KERBEROS");
    +             * configuration.set("dfs.namenode.kerberos.principal",
    +             *                                "hdfs/zookeeper.witzend.com@WITZEND.COM");
    +             * configuration.set("hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +             * anf the ticket cache must have the hdfs user's creds.
    +             */
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            Object configuration = configurationClass.newInstance();
    +
    +            //UserGroupInformation.isSecurityEnabled
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod("isSecurityEnabled");
    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);
    +
    +            if(isSecurityEnabled) {
    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);
    +                final String hdfsUser = (String) conf.get(Config.HDFS_PRINCIPAL);
    +
    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +                Class fileSystemClass = Class.forName("org.apache.hadoop.fs.FileSystem");
    +                Object defaultNameNodeURI = fileSystemClass.getMethod("getDefaultUri", configurationClass).invoke(null, configuration);
    --- End diff --
    
    Actually could we just pull that URI out from the topology conf.  That way they have a flag to turn this on or off?
, Github user revans2 commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-50187860
  
    It is looking really good.  Juts a few comments about the lifecycle of the plugins, and some about documentation.  I think this is really close.  Great work.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15417963
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/nimbus.clj ---
    @@ -1046,7 +1047,10 @@
                                     (dissoc storm-conf STORM-ZOOKEEPER-TOPOLOGY-AUTH-SCHEME STORM-ZOOKEEPER-TOPOLOGY-AUTH-PAYLOAD))
                     total-storm-conf (merge conf storm-conf)
                     topology (normalize-topology total-storm-conf topology)
    +                nimbus-autocred-plugins (AuthUtils/getNimbusAutoCredPlugins total-storm-conf)
    --- End diff --
    
    I have a few questions:
    * Shouldn't the plugin be cleared up as soon as the scope of submit topology is over?
    * If we generate these instances at startup then it means anyone wanting to use it will have to change the nimbus config and restart nimbus. Right now the restart is required because renewers are designed that way. I could follow that pattern however I think this approach is better because no restart is required as long as all the hdfs configuration files are already in the class path. 
    
    If you like to stick to this approach I can change the code so the list of plugins are loaded at startup and stored in nimbus data structure and their populateCred method is invoked as part of submit topology. 
    
    We need some way of identifying who the topology submitter is. Without that information we don't know for whom are we getting the creds. I can pass just the topology config  and I think any future implementations of the interface would also need that. I could make the config immutable thus ensuring the user code can not modify it, does that work ?
, Github user revans2 commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15419109
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/nimbus.clj ---
    @@ -1046,7 +1047,10 @@
                                     (dissoc storm-conf STORM-ZOOKEEPER-TOPOLOGY-AUTH-SCHEME STORM-ZOOKEEPER-TOPOLOGY-AUTH-PAYLOAD))
                     total-storm-conf (merge conf storm-conf)
                     topology (normalize-topology total-storm-conf topology)
    +                nimbus-autocred-plugins (AuthUtils/getNimbusAutoCredPlugins total-storm-conf)
    --- End diff --
    
    Why does the plugin need to be cleaned up as soon as the submission is over?  Creating an object each time a method is called feels like a lot of overhead to me.  It also makes it so that the plugin does not have the option to cache things and amortize the overhead across multiple calls.  Not that the code currently needs to worry about it, as the current code is more or less just a static function.
    
    > If we generate these instances at startup then it means anyone wanting to use it will have to change the nimbus config and restart nimbus.
    
    Yes, I can see that.  But that is kind of what I want.  It becomes a balancing  act between convenience/insecurity and security/inconvenience.  I just feel nervous that anything on the classpath the user gets to decide to load it into memory. It feels like it is too open ended, but honestly if you feel strongly about it I will not push it.  I would just want the config changed so that it is prefixed with topology instead of nimbus so that it is obvious that it is a topology specific config.

, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15419864
  
    --- Diff: storm-core/src/clj/backtype/storm/daemon/nimbus.clj ---
    @@ -1046,7 +1047,10 @@
                                     (dissoc storm-conf STORM-ZOOKEEPER-TOPOLOGY-AUTH-SCHEME STORM-ZOOKEEPER-TOPOLOGY-AUTH-PAYLOAD))
                     total-storm-conf (merge conf storm-conf)
                     topology (normalize-topology total-storm-conf topology)
    +                nimbus-autocred-plugins (AuthUtils/getNimbusAutoCredPlugins total-storm-conf)
    --- End diff --
    
    I can see how this can be a security hole when the user passes his own implementation and act as nimbus to get delegation tokens as some other user. Good catch. I will go with your approach. 
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15428650
  
    --- Diff: storm-core/src/jvm/backtype/storm/Config.java ---
    @@ -1186,7 +1193,15 @@
          */
         public static final String TOPOLOGY_ISOLATED_MACHINES = "topology.isolate.machines";
         public static final Object TOPOLOGY_ISOLATED_MACHINES_SCHEMA = Number.class;
    -    
    +
    +    /**
    +     * HDFS information, used to get the delegation token on behalf of the topology
    +     * submitter user and renew the tokens. see {@link backtype.storm.security.auth.kerberos.AutoHDFS}
    +     * kerberos principal name with realm should be provided.
    +     */
    +    public static final Object HDFS_PRINCIPAL = "topology.hdfs.user";
    --- End diff --
    
    done.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15428652
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/INimbusCredentialPlugin.java ---
    @@ -0,0 +1,39 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +package backtype.storm.security;
    +
    +import java.util.Map;
    +
    +/**
    + * Nimbus auto credential plugin that will be called on nimbus host
    + * during submit topology option. User can specify a list of implementation using config key
    + * nimbus.autocredential.plugins.classes.
    + */
    +public interface INimbusCredentialPlugin {
    --- End diff --
    
    done.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15428704
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/AuthUtils.java ---
    @@ -109,6 +111,27 @@ public static IPrincipalToLocal GetPrincipalToLocalPlugin(Map storm_conf) {
         }
     
         /**
    +     * Get all the Nimbus Auto cred plugins that users want to use.
    +     * @param topologyConf topologyConfiguration to use.
    +     * @return nimbus auto credential plugins.
    +     */
    +    public static Collection<INimbusCredentialPlugin> getNimbusAutoCredPlugins(Map topologyConf) {
    --- End diff --
    
    this is now a nimbus data configuration just like renewers.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15428705
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    --- End diff --
    
    done.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15428709
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    --- End diff --
    
    done.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15428712
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    --- End diff --
    
    done.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15428727
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    +             *  if(UserGroupInformation.isSecurityEnabled) {
    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +             *      Credentials credential= proxyUser.getCredentials();
    +             *      fs.addDelegationToken(hdfsUser, credential);
    +             * }
    +             * and then return the credential object as a bytearray.
    +             *
    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml
    +             * and core-site.xml in the class path which should set these configuration.
    +             * configuration.set("hadoop.security.authentication", "KERBEROS");
    --- End diff --
    
    added the configuration information to SECURITY.MD file.
, Github user Parth-Brahmbhatt commented on a diff in the pull request:

    https://github.com/apache/incubator-storm/pull/190#discussion_r15428733
  
    --- Diff: storm-core/src/jvm/backtype/storm/security/auth/kerberos/AutoHDFS.java ---
    @@ -0,0 +1,254 @@
    +/**
    + * Licensed to the Apache Software Foundation (ASF) under one
    + * or more contributor license agreements.  See the NOTICE file
    + * distributed with this work for additional information
    + * regarding copyright ownership.  The ASF licenses this file
    + * to you under the Apache License, Version 2.0 (the
    + * "License"); you may not use this file except in compliance
    + * with the License.  You may obtain a copy of the License at
    + *
    + * http://www.apache.org/licenses/LICENSE-2.0
    + *
    + * Unless required by applicable law or agreed to in writing, software
    + * distributed under the License is distributed on an "AS IS" BASIS,
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    + * See the License for the specific language governing permissions and
    + * limitations under the License.
    + */
    +
    +package backtype.storm.security.auth.kerberos;
    +
    +import backtype.storm.Config;
    +import backtype.storm.security.INimbusCredentialPlugin;
    +import backtype.storm.security.auth.IAutoCredentials;
    +import backtype.storm.security.auth.ICredentialsRenewer;
    +import org.slf4j.Logger;
    +import org.slf4j.LoggerFactory;
    +
    +import javax.security.auth.Subject;
    +import javax.xml.bind.DatatypeConverter;
    +import java.io.*;
    +import java.lang.reflect.Method;
    +import java.net.URI;
    +import java.util.Collection;
    +import java.util.HashMap;
    +import java.util.Map;
    +
    +/**
    + * Automatically get HDFS delegation tokens and push it to user's topology. The class
    + * assumes that HDFS configuration files are in your class path.
    + */
    +public class AutoHDFS implements IAutoCredentials, ICredentialsRenewer, INimbusCredentialPlugin {
    +    private static final Logger LOG = LoggerFactory.getLogger(AutoHDFS.class);
    +    public static final String HDFS_CREDENTIALS = "HDFS_CREDENTIALS";
    +
    +    public void prepare(Map conf) {
    +       LOG.debug("no op.");
    +    }
    +
    +    @SuppressWarnings("unchecked")
    +    private byte[] getHDFSCredsWithDelegationToken(Map conf) throws Exception {
    +
    +        try {
    +            /**
    +             * What we want to do is following:
    +             *  if(UserGroupInformation.isSecurityEnabled) {
    +             *      FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +             *      UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    +             *      UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(topologySubmitterUser, ugi);
    +             *      Credentials credential= proxyUser.getCredentials();
    +             *      fs.addDelegationToken(hdfsUser, credential);
    +             * }
    +             * and then return the credential object as a bytearray.
    +             *
    +             * Following are the minimum set of configuration that needs to be set,  users should have hdfs-site.xml
    +             * and core-site.xml in the class path which should set these configuration.
    +             * configuration.set("hadoop.security.authentication", "KERBEROS");
    +             * configuration.set("dfs.namenode.kerberos.principal",
    +             *                                "hdfs/zookeeper.witzend.com@WITZEND.COM");
    +             * configuration.set("hadoop.security.kerberos.ticket.cache.path", "/tmp/krb5cc_1002");
    +             * anf the ticket cache must have the hdfs user's creds.
    +             */
    +            Class configurationClass = Class.forName("org.apache.hadoop.conf.Configuration");
    +            Object configuration = configurationClass.newInstance();
    +
    +            //UserGroupInformation.isSecurityEnabled
    +            final Class ugiClass = Class.forName("org.apache.hadoop.security.UserGroupInformation");
    +            final Method isSecurityEnabledMethod = ugiClass.getDeclaredMethod("isSecurityEnabled");
    +            boolean isSecurityEnabled = (Boolean)isSecurityEnabledMethod.invoke(null);
    +
    +            if(isSecurityEnabled) {
    +                final String topologySubmitterUser = (String) conf.get(Config.TOPOLOGY_SUBMITTER_USER);
    +                final String hdfsUser = (String) conf.get(Config.HDFS_PRINCIPAL);
    +
    +                //FileSystem fs = FileSystem.get(nameNodeURI, configuration, topologySubmitterUser);
    +                Class fileSystemClass = Class.forName("org.apache.hadoop.fs.FileSystem");
    +                Object defaultNameNodeURI = fileSystemClass.getMethod("getDefaultUri", configurationClass).invoke(null, configuration);
    --- End diff --
    
    added.
, Github user Parth-Brahmbhatt commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-51368333
  
    2 week ping.
, Github user revans2 commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-51483071
  
    Sorry, looking at the changes now. I need to make more time for reviews.
, Github user revans2 commented on the pull request:

    https://github.com/apache/incubator-storm/pull/190#issuecomment-51485317
  
    Looks good tests pass, I'll merge this in. +1
, Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-storm/pull/190
, Thanks Parth,

I merged this into branch security.  Sorry I took so long to review your changes.  They look great.]