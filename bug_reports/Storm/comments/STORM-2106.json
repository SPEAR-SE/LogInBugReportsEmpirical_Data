[Are you sure this is an issue? I get that the spout will restart at the failed offset if it crashes or partitions are reassigned and another spout has to restart at the last committed offset, but as far as I can tell it should progress past the failed tuple when it's reemitted or the retry service decides to delay it. doSeekRetriableTopicPartitions only seeks on those partitions that have failed tuples ready to retry (i.e. whose retry delays have expired).

If you have a sequence of offsets x...y and x has failed, x-1 was committed and everything up to y has been acked, doSeekRetriableTopicPartitions should seek to x and retry it. The offsets between x and y will then be skipped by emitTupleIfNotEmitted because they've already been acked. On the next call to nextTuple after skipping ]x...y[, the spout should poll Kafka again at y because waitingToEmit is empty (x was emitted and the others were already acked). If x gets acked there's no problem. If x fails again and the retry service decides to delay it, doSeekRetriableTopicPartitions won't seek back to x until the backoff expires. In the meantime new offsets get processed. When x's retry delay expires, the spout will just seek back to x, emit it again and skip past ]x...y...z[ (z being the last message it managed to ack before seeking back to x).

There are a few other factors in play here like maxUncommittedOffsets, but I don't really see the issue you're describing., [~Srdo] I don't think it quite works that way. In the case you provided, I agree with you that x+1..y will be skipped while x is processing. However in doSeekRetriableTopicPartitions we seek to the next offset that may be committed based on acked messages. So in this case, since x is still processing it seeks back to x and filters out all of x-y again because x is processing and the rest are acked. The result is nothing gets emitted until x finishes processing. 

For me the batch size kafka provide (~300) was << maxUncommittedOffsets which severely crippled the throughput of my topology. 

I agree that we should poll kafka again at y and that is why I made this ticket. 

----
Code snippet: 
{code}
    private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                kafkaConsumer.seek(rtp, acked.get(rtp).committedOffset + 1);    // Seek to last committed offset
            }
        }
    }
{code}


, [~jfenc91] I don't think it works that way. doSeekRetriableTopicPartitions only seeks to the committed offset if retryService.retriableTopicPartitions returns x's TopicPartition (since the for-loop otherwise doesn't include x's TopicPartition), which it only does if there are failed tuples ready for retry on that TopicPartition. When x is ready for retry, retryService.retriableTopicPartitions will return x's TopicPartition, the consumer seeks to x and the consumer is polled. x should then be emitted. When x is emitted, emitTupleIfNotEmitted removes the messageId from retryService, which should prevent retryService.retriableTopicPartitions from returning x's TopicPartition until x (or some other message on x's TopicPartition) fails again.

While x is still processing the spout shouldn't seek back to x (and doesn't as far as I can tell). Is it possible that you were limited by something else, like maxSpoutPending, retrying a large number of tuples or partition reassignments/worker restarts? 

See https://github.com/apache/storm/blob/master/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L294 and https://github.com/apache/storm/blob/master/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutRetryExponentialBackoff.java#L164

edit: I suppose if you have a failed tuple reasonably far back in the tuple stream with a bunch of acked tuples you need to skip, you might end up wasting a bunch of time waiting for Storm to skip the acked tuples because Storm waits 1 ms between calls to nextTuple by default if nothing is emitted. Maybe that was why calling emitTupleIfNotEmitted in a loop made a difference (assuming it made a difference)?, [~Srdo] Thank you so much for the explanation here! You are 100% correct. As it turns out, I did this to myself in hacking around some exceptions caused by rebalance. I'll close the ticket. Thanks again for your time here!, My pleasure :)]