[I have use JConsole to monitor the OOM problem and found for 5.10, something in "tenured gen" can not got released after GC. And the "tenured gen" will grow up and up and finally caused the problem. (Actually, each time after the full GC performed, the tenured gen can release some memory but it still keep going up)

Again, the GC situation is correct in 5.5.1, Bellow is result from Eclipse Memory Analyzer

Problem Suspect 1
One instance of "com.sun.jmx.mbeanserver.JmxMBeanServer" loaded by "<system class loader>" occupies 182,127,056 (57.45%) bytes. The instance is referenced by org.apache.activemq.broker.jmx.ManagementContext @ 0xf142bc0 , loaded by "java.net.URLClassLoader @ 0xef3bad0". The memory is accumulated in one instance of "java.util.HashMap$Entry[]" loaded by "<system class loader>".

Problem Suspect 2
74,663 instances of "javax.management.ObjectName", loaded by "<system class loader>" occupy 62,369,904 (19.67%) bytes. These instances are referenced from one instance of "java.lang.Object[]", loaded by "<system class loader>"


Possible Memory Waste
Duplicate Strings
Found 146 occurrences of char[] with at least 10 instances having identical content. Total size is 8,593,000 bytes.

Top elements include:
•22 × \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u... (59,338 bytes)
•4,355 × JMS Client id of the Connection the Producer is on... (120 bytes)
•4,355 × User Name used to authorize creation of this Produ... (120 bytes)
•4,355 × The name of the destionation the Producer is on. (112 bytes)
•4,355 × Is the producer configured for Async Dispatch (104 bytes) , What GC strategy are you using?  And which JVM implementation?  (Hotspot, OpenJDK, IBM, etc.)  And are you sure that your "full GCs" were actually *full* GCs rather than just GCs that collected from Old Gen?

Using G1GC in Hotspot 7u21, I've seen behavior like what you described (with an upward-sloping sawtooth pattern for Old Gen usage), but once a real full GC happened, Old Gen dropped to nearly nothing and then started the pattern again, which makes me think that what I saw was a poor implementation of the G1GC algorithm rather than anything wrong with ActiveMQ.  Did you actually let things run until you hit an OutOfMemoryError?  If not, you're just speculating that a memory leak actually existed, because the GC strategy is under no obligation to collect everything that can be GCed at any point in time., Hi there,

We also use AMQ 5.10 and we are monitoring our process using JMX. we also experience such OOM and the MAT reports about the same results: 

Problem Suspect 1

One instance of "com.sun.jmx.mbeanserver.Repository" loaded by "<system class loader>" occupies 1,050,891,224 (33.43%) bytes. The instance is referenced by org.apache.activemq.broker.TransportConnection$2 @ 0x7bb1427b8 Async Exception Handler , loaded by "java.net.URLClassLoader @ 0x7000430b8". The memory is accumulated in one instance of "java.util.HashMap$Node[]" loaded by "<system class loader>".



Problem Suspect 2

756,250 instances of "javax.management.ObjectName", loaded by "<system class loader>" occupy 1,365,859,400 (43.45%) bytes. 

Keywords
javax.management.ObjectName


here is our process command line :

/usr/local/panorama/java/bin/java -Xms512m -Xmx3072m -XX:+HeapDumpOnOutOfMemoryError  -Djava.util.logging.config.file=logging.properties -Djava.security.auth.login.config=/usr/local/panorama/activemq/conf/login.config -Djava.rmi.server.hostname=10.x.x.22 -Dcom.sun.management.jmxremote.port=11099 -Dcom.sun.management.jmxremote.password.file=/usr/local/panorama/etc/jmx/activemq_jmx.pass -Dcom.sun.management.jmxremote.access.file=/usr/local/panorama/etc/jmx/activemq_jmx.access -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote -Djava.awt.headless=true -Djava.io.tmpdir=/usr/local/panorama/activemq/tmp -Dactivemq.classpath=/usr/local/panorama/activemq/conf; -Dactivemq.home=/usr/local/panorama/activemq -Dactivemq.base=/usr/local/panorama/activemq -Dactivemq.conf=/usr/local/panorama/activemq/conf -Dactivemq.data=/usr/local/panorama/activemq/data -jar /usr/local/panorama/activemq/bin/activemq.jar start 


we do not specify GC algo, so i guess its the default for :

java version "1.8.0_45"
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode) 

, The latest release is 5.12.0.  I'd advise you to use that and if you still think you have a leak condition then create a test that can be used to reproduce and analyse the problem.  , Hi Daniel:
 I suggest you to try 5.12. I have test that several days ago and found this issue not occur in V5.12, Thank you both, 
I will do just that (move to 5.12).
keep you posted (for better or worse).

, Unfortunately, this memory issue is reproduced with 5.12.
I noticed that one of the Advisory topics has a very large number of enqueue messages but zero dequeue.
could this be the cause ?
what are the Advisory items ?, The advisory bit sounds normal, the client subscribes to the Topic to be informed of temp destination create / destroy etc.  Am not seeing any concrete evidence of leaks presented here.  , Actually, when I do the test, I set advisorySupport="false". But even after this setting, I can reproduce the issue in V5.9 and V5.10. But for V5.12, I can not reproduce it with same setting. , I am happy to say that we found the cause of the leak (well, sort of). we found some third party code that probably not closing sessions properly, and then all messages sent on that session were not released (that's a guess, not a fact). 
The only troubling thing here is that the heap dump clearly showed a big Map of ObjectName which was very misleading in regards to finding the cause. 

Could someone elaborate on why there are so many ObjectName objects when such bug occur (producer not properly using the AMQ session) ?

, The ObjectName instance most likely represent the JMX objects registered to allow for gathering statistics on connections, and their associated producers and consumers etc.  If you have some misbehaving client code that is opening and not closing down these resources then you are likely to see things like this.  Exploring the JMX tree via JConsole or the like would give you more insight into what is happening on your Broker instance and is the very reason for their existence.  

It doesn't sound to me like there is a bug here.  I am going to close this as not a problem, if you find a test case that shows some actual issue please open a new issue to track it.  , Problem seems to be due to bad code on the client side not closing down connections. ]