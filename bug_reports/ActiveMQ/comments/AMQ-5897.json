[activemq.xml files for the 4 nodes in the sample cluster, and the client code to run.

The consumer is called ActiveMQFailOverDurableMessageListener.java, and I run it using a discovery transport url, like this: 

/usr/bin/java -cp . com.xifin.jms.tools.activemq.ActiveMQFailOverDurableMessageListener 'discovery:(multicast://default)?maxReconnectAttempts=10' id

The producer is called ActiveMQFailOverMessageSender.java, and I run it like this: 

/usr/bin/java -cp . com.xifin.jms.tools.activemq.ActiveMQFailOverMessageSender tcp://brokerhost:61612 58
, ah... so durable topics across network of brokers can be confusing as messages can get orphaned on a broker depending on where the durable sub is.  Maybe a little bit related to this? https://issues.apache.org/jira/browse/AMQ-4000

Or to this?

http://blog.christianposta.com/activemq/activemq-clustering-durable-subscribers-and-virtual-topics-to-the-rescue/

, The problem I encountered sounds exactly like the issue you described in your blog post ("virtual topics to the rescue").  Not so much like the AMQ-4000 issue though.

The Virtual Topic solution sounds neat, however I am not interested in going off the JMS reservation and writing custom code to solve this problem.  If I understand correctly, to use Virtual Topics I need to rewrite my consumers to connect to a specially named queue instead of a topic.  This approach will not be portable if we plug-in a different JMS broker.

We're now looking into using (yep!) ActiveMQ's JDBC Persistence to satisfy our requirements; our performance needs are not tremendous so I am hopeful it will suffice.  It performs as expected for fail-over scenarios, so that is good.

I appreciate your response on this issue!  Thanks. 

Is the jira protocol for me to now change the status of this issue to something like "As Designed" or somesuch?
, Thanks for closing the loop!

So you'll have all of the brokers in the network of brokers attach to the same database instance? 

, The use of a network seems to be the root cause of the lost messages during failover, so we are abandoning the use of a network. Instead we will use Master/Slave shared persistence (a single master broker alive at any one time, with N slaves ready to take over in the event of a master failure).

I had hoped to use JDBC persistence but it did not perform well enough for us under a reasonable load (6+ secs on average to deliver msgs) so we are instead using kahadb which is much faster of course.

We will have hundreds, if not thousands of connections to the single master; I am hoping that will not be an issue.]