[When this problem occurs, do you see the journal files in the kahadb folder are all sequential?

Also, in what state are the destinations?  Are there any queues, or durable topic subscriptions, with old messages sitting around?

BTW, the following log setting will get more details (it does generate a lot of output when there are a lot of files):

{{log4j.logger.org.apache.activemq.store.kahadb.MessageDatabase=TRACE}}
, This issue may have been introduced by the fix for AMQ-5542., This was caught in production.

* >14,000 files were pending removal
* all files from the first appeared to remain (i.e. there were no gaps in the file numbers detected)
* according to the webconsole, all of the destinations were processing messages fine
* browsing the messages in the webconsole, no old messages were pending on any destination
* disconnecting clients and allowing messages to flow across the NOB, all messages were consumed but the kahadb files did not cleanup
* restarting the broker, the kahadb files cleaned up normally
* capturing the MessageDatabase logging, one destination showed up as holding the very first data file
* that destination also showed up in the log files with this warning:
{{Non existent message update attempt rejected. Destination: QUEUE://<SCRUBBED>, Message id: <SCRUBBED>}}

So, there is a chance this occurrence is related to that warning.  Looking at the code, that warning appears to indicate an attempt to update an index with a message when that message is not found in the StoredDestination (MessageDatabase.updateIndex)., May relate to AMQ-5394; unclear right now., The broker configuration with this issue has {{persistJMSRedelivered=true}}., CORRECTION: the destination holding the first data file was not the same one in the error message - the latter contained the former, hence the mis-identification., The following warning message stops occurring when persistJMSRedelivered is set to false:
{code}
Non existent message update attempt rejected. Destination: QUEUE://<SCRUBBED>, Message id: <SCRUBBED>
{code}
, We have also observed that in some cases restarting the broker or simply browsing a queue through the web console triggered a cleanup., I noticed this issue happening as well after upgrading some brokers to 5.11.1.  I'm not really sure what's going on yet.  I have persistJMSRedelivered set to true and I'm going to try setting it to false to see if that helps.
, I find something very interesting concerning this problem.

I can reproduce it when I set producerFlowControl="false" in in policyEntry on activemq.xml 
, {code:title=Trace log with producerFlowControl="false"}
 2015-07-28 20:56:16,908 [eckpoint Worker] TRACE MessageDatabase                - Last update: 8:17837754, full gc candidates set: [1, 2, 3, 4, 5, 6, 7, 8]
 2015-07-28 20:56:16,909 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after producerSequenceIdTrackerLocation:8, [1, 2, 3, 4, 5, 6, 7]
 2015-07-28 20:56:16,909 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after ackMessageFileMapLocation:8, [1, 2, 3, 4, 5, 6, 7]
 2015-07-28 20:56:16,909 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after tx range:[null, null], [1, 2, 3, 4, 5, 6, 7]
 2015-07-28 20:56:16,911 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after dest:0:longTerm, [2, 3, 4, 5, 6, 7]
 2015-07-28 20:56:16,911 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after dest:0:shortTerm, [2, 3, 4, 5, 6, 7]
 2015-07-28 20:56:16,912 [eckpoint Worker] TRACE MessageDatabase                - gc candidates: [2, 3, 4, 5, 6, 7]
 2015-07-28 20:56:16,912 [eckpoint Worker] TRACE MessageDatabase                - not removing data file: 2 as contained ack(s) refer to referenced file: [1, 2]
 2015-07-28 20:56:16,913 [eckpoint Worker] TRACE MessageDatabase                - not removing data file: 3 as contained ack(s) refer to referenced file: [2, 3]
 2015-07-28 20:56:16,913 [eckpoint Worker] TRACE MessageDatabase                - not removing data file: 4 as contained ack(s) refer to referenced file: [3, 4]
 2015-07-28 20:56:16,913 [eckpoint Worker] TRACE MessageDatabase                - not removing data file: 5 as contained ack(s) refer to referenced file: [4, 5]
 2015-07-28 20:56:16,914 [eckpoint Worker] TRACE MessageDatabase                - not removing data file: 6 as contained ack(s) refer to referenced file: [5, 6]
 2015-07-28 20:56:16,914 [eckpoint Worker] TRACE MessageDatabase                - not removing data file: 7 as contained ack(s) refer to referenced file: [6, 7]
 2015-07-28 20:56:16,914 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint done.
{code}

{code:title=Trace log with producerFlowControl="true"}
 2015-07-28 20:59:47,203 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint started.
 2015-07-28 20:59:47,393 [eckpoint Worker] TRACE MessageDatabase                - Last update: 7:17837754, full gc candidates set: [1, 2, 3, 4, 5, 6, 7]
 2015-07-28 20:59:47,394 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after producerSequenceIdTrackerLocation:7, [1, 2, 3, 4, 5, 6]
 2015-07-28 20:59:47,394 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after ackMessageFileMapLocation:7, [1, 2, 3, 4, 5, 6]
 2015-07-28 20:59:47,394 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after tx range:[null, null], [1, 2, 3, 4, 5, 6]
 2015-07-28 20:59:47,395 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after dest:0:longTerm, [2, 3, 4, 5, 6]
 2015-07-28 20:59:47,395 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after dest:0:shortTerm, [2, 3, 4, 5, 6]
 2015-07-28 20:59:47,396 [eckpoint Worker] TRACE MessageDatabase                - gc candidates: [2, 3, 4, 5, 6]
 2015-07-28 20:59:47,396 [eckpoint Worker] DEBUG MessageDatabase                - Cleanup removing the data files: [2, 3, 4, 5, 6]
 2015-07-28 20:59:47,427 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint done.
{code}
, We have seen this behavior a lot following upgrades to 5.11.1; however, in every case the log files are cleaned up when the persistent messages they reference are consumed.

What you are experiencing is a direct result of AMQ-5542.  If you read that ticket you will understand why log files are being retained longer than in previous releases.  Unfortunately, the accumulation of journal logs tends to snowball under a variety of conditions (small message sizes, mixing fast and slow consumers, etc).  Anyone experiencing this may want to look at mKahaDB as a possible solution., You're right Brian, producerFlowControl true or false is just a border effect., As a follow up to my post a month ago, I don't think persistJMSRedelivered has anything to do with it.  After more investigation I agree with [~brianjohnson].  The issue seems to just be related to a variety of conditions.  mKahaDB has helped in my case., We encountered the same problem in our performance test environments. I don't think this bug is only encountered when you have a mix of  fast and slow consumers sharing the same kahadb store. We are using ActiveMQ 5.11.1 in our environments.
The behaviour we noticed was that ActiveMQ wrote the messages to a new kahadb file (journal log) after every 32 mb (default configuration). This  lead to the message and its corresponding ack ending up in different files. It resulted in a chain of such journal logs which couldn't be cleaned up by ActiveMQ. It eventually filled up the disk causing ActiveMQ to shutdown
We do not have any 'slow' producers/consumers, All the responses during our performance tests were within 80 milliseconds. It implies that the issue is easily reproduced under high load.  
, [~nands] would it be possible to turn the sequence of events of your pref test into something small in a unit type test? If a trivial perf scenario can produce this behaviour we need to invest in some mitigation., Gary, It was not easy for us to capture the sequence of events in to a smaller test. The question is, When activemq creates a new journal log after the threshold limit of 32 MB, What happens to the messages in the journal log for which it has not received an ACK yet. Does it move these messages to the new journal., Messages are not moved, so if an message is not ACK'd yet then that log file must be retained until it is., We experience this in production all the time. During heavy load there's always some messages without ACK when rolling journal log to a new file and therefore the old journal log is never deleted - instead infinite chain of log files just keeps on growing. Assuming for example the case that there's single undelivered message in the first one (e.g. DLQ). So because of this single msg and heavy load nothing gets deleted., We have this condition persisting even after all messages are consumed. To clarify, there are no pending messages on any of the queues, but still several hundreds of kahadb files on disk. Once we restart ActiveMQ, it does clean up the files. Perhaps the data in memory tells it to hold on to the files, but during restart when it goes through all the entries in KahaDB, it figures out that the files are no longer needed. Since we moved to multiKahaDB the occurrence is much less frequent, but it still happens from time to time. It only started after we moved to 5.11.1.
, Hi, I am facing same issue: lot of piled up kahadb log files that don't get deleted. It causes CPU peaks. Had to delete it manually, since none of above mentioned workarounds didn't help. Do you have any idea if this has been fixed, or how can I configure activemq to disregard irregularities in old log files (we have log files old 6 months, if some messages didn't get dispatched before 6 months we, and there is no appropriate ACK for them, I don't care) and force old log file deletion?, Hey Lukáš, did you manage to find workaround for this? It would be really good if you can share possible solution . . .  , Some recent fixes in this area around cleaning out logs files, would be good to retest with the v5.13.1 and see how that works out.  A reproducer for the issue will really help if you can figure out a test case that can trigger it.  , We've seen this issue too in production. i.e. A noticeable increase in disk usage on 5.11. We compared behaviour with 5.9 and 5.13 and found 5.11 to be the odd one out. I am wondering why the closure says "cannot reproduce". We are currently working on a test case we can share., If you can contribute back a test case which shows an issue (on the current version, ie 5.13.x) then this can always be reopened., 5.13.1 still has it.

It would be nice to see if AMQ-6203 fix will resolve this one., We're experiencing the same behavior on 5.11.1, or our customers... And it happens when there are slow consumers for whatever reason. In particular circumstances it happens regularly on the customers' sites. , We've got this problem with 5.13.3 and I suspect it's when the broker or consumer is too busy to send/process the ACK.

Those suffering - are your consumers ACKing each message in turn, or are they using the default of receiving 1,000 messages (JMS) and ACKing the last? Might help narrow the scope of the problem.

Is there an MBean I can execute a command on to force clean-up or must I restart the entire ActiveMQ process (thus forcing a restart of connections)?, The problem seems to have been fixed after upgrade from 5.13.1 to 5.14.3]