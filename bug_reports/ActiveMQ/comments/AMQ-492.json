[Maybe jumped the gun again.  Looking a little further I found that the NetworkConnector member failover=true, so if it is explicitly set off, then the multicast discovery transport works more like I expected.

But then that still leave to question what would the purpose of "failover" mode be with used of multicast discovery.  And if it has a purpose then all the failover properties should be exposed and passed through the multicast discovery again uri.
, Okay, walking back out onto the limb again.  While setting the failover value to false on the network connector forces use of the TcpTransportConnector, I am not sure that the bridge is fully established again after the missing broker is seen found.  There are entries in the log such as:

2006-01-17 11:16:37,882 [Discovery: null] INFO  NetworkConnector               - Establishing network connection between vm://roadrashA?network=true and null at tcp://harley:61616
2006-01-17 11:16:37,913 [Discovery: null] INFO  TransportConnector             - Accepting connection on: vm://roadrashA
2006-01-17 11:16:38,038 [Discovery: null] INFO  DemandForwardingBridge         - Starting a network connection between vm://roadrashA#0 and tcp://harley/172.16.12.88:61616 has been established.
2006-01-17 11:17:07,101 [Discovery: null] DEBUG AbstractConnection             - Sync error occurred: javax.jms.InvalidClientIDException: No clientID specified for connection disconnect request

at the time of the initial connection with the remote broker.  However when the connection is broken then re-established, the only entries in the log indicating such is:

2006-01-17 11:18:14,820 [.16.12.88:32972] DEBUG TcpTransport                   - TCP consumer thread starting
2006-01-17 11:18:15,007 [.16.12.88:32972] DEBUG AbstractConnection             - Async error occurred: javax.jms.InvalidClientIDException: No clientID specified for connection request

I will run some additional tests, but logs do not look promising.
, There was a question of why use failover??  If you don't expect a node in the cluster to be taken offline for an extended period of time, using reliable helps the stablebility between the brokers since by using failover, if a node in the cluster a transient outage, the rest of the cluster will see him as still being online.

I think the gist of the last comment is that that broker was not reconnecting when a clustered broker is bounced.

If so, then this is related to AMQ-639 which is now fixed.  , Hi,

We have been experiencing a strange issue with our setup of paired brokers (using incubator-activemq-4.1).
Running our brokers with the Rootlogger set to DEBUG level we experience the same issues with the FailoverTransport going into a very tight endless loop of reconnection attempts upon shutdown of one broker of a pair.

Our environment:

We have two servers (dev1 and dev2) where we have a broker running on each. They both share the same Oracle datasource. Starting the broker on either server, this broker becomes the Master-broker on the datasource, whereas the second broker will report "Attempting to acquire the exclusive lock to become the Master broker" upon startup.

This is all fine. The problem occurs however when attempting to shutdown the Master-broker where we expect the slave-broker to become the new Master-broker. Running under DEBUG-level, the Master-broker hangs indefinitely.

Changing to log-level INFO fixes the problem.

Please advise if there is some bug connected to the FailoverTransport if the log-level happens to be DEBUG., Hi Trym could you attach a test case to this issue?  I have not seen this behavior so far.  A test case would help us validate the problem., 4.x broker is EOL]