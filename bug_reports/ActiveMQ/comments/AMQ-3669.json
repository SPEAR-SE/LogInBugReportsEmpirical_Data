[One more information: If we set 
{noformat}<policyEntry topic=">" producerFlowControl="false" memoryLimit="2mb" />{noformat}
we get 

Memory percent used: 100

after inserting 5111 messages. Console tells

 INFO | Started SelectChannelConnector@0.0.0.0:8161
 INFO | Usage Manager Memory Limit (1048576) reached on queue://created.static.for.persistent. Producers will be throttled to the rate at which messages are removed from this destination to prevent flooding it. See http://activemq.apache.org/producer-flow-control.html for more info

Another indication that no messages are spooled to disk?
, We ran into a similar problem. Disabling flow control for the mirror topics helped.

I'm still not exactly sure what happens -- AMQ-3671 suggest that the mirrored Messages are cached in the mirrored Topic, even if no subscribers are present. I'm not sure about that, since I did not observe an increase in memomry usage in the same amount as I piped messagues through my mirrored queues. Anyway, it DOES seem like a bug to me that flow control is applied in this case.

{code}
  <destinationPolicy>
    <policyMap>
      <policyEntries>
        ...
        <policyEntry topic="*.qmirror" producerFlowControl="false"/>
      </policyEntries>
    </policyMap>
  </destinationPolicy>
{code}, I was able to determine what the problem was with this issue.

It is that when we initially forward the message to the mirrored topic.  The message memory usage instance is set to the topic.

When the message is sent to the queue, the memory usage instance is not set to the queue memory usage but it still uses the topic memory usage.

The message setRegionDestination method is used to set the message memory usage but this is only performed if and only if the current message memory usage instance is null.  This is not the case when forwarding the message to the queue and that is why the memory usage of the topic gets affected causing the producer flow control to throttle the topic/queue when that is not the case.
, This patch contains a test and the fix to this issue.

btw, this patch was applied against trunk and I have not tried to apply this on an earlier version., Patch applied with thanks. ]