[do you have a test case for this?, {quote}
do you have a test case for this?
{quote}
Regrettably, no., a code change like that without a test to validate is just a good start. To have it committed to trunk it needs a test to validate: 
1) the issue still exists
2) to protect the fix into the future

I guess a slow client ack consumer can fill the prefetch and an appropriate ttl can have them expire, but I wonder what type of client would cause this behavior is real life, until it acks there will be no further dispatch.
There is an existing check for message expiry on dispatch and on receiving an ack., hmmm i am wondering if i am seeing the same issue here, unfortunately the original stack trace or indication of why the user thinks this is a bug was not included but i am seeing the same sort of thing and the broker produced the following stack trace:

2010-09-05 09:07:51,844 | ERROR | Failed to page in more queue messages  | org.apache.activemq.broker.region.Queue | Queue:CORE-IN-QUEUE
java.lang.OutOfMemoryError: Java heap space
                at java.util.Arrays.copyOf(Unknown Source)
                at java.util.concurrent.CopyOnWriteArrayList.add(Unknown Source)
                at org.apache.activemq.broker.region.PrefetchSubscription.dispatch(PrefetchSubscription.java:630)
                at org.apache.activemq.broker.region.PrefetchSubscription.dispatchPending(PrefetchSubscription.java:592)
                at org.apache.activemq.broker.region.PrefetchSubscription.add(PrefetchSubscription.java:158)
                at org.apache.activemq.broker.region.Queue.doActualDispatch(Queue.java:1548)
                at org.apache.activemq.broker.region.Queue.doDispatch(Queue.java:1500)
                at org.apache.activemq.broker.region.Queue.pageInMessages(Queue.java:1585)
                at org.apache.activemq.broker.region.Queue.iterate(Queue.java:1219)
                at org.apache.activemq.thread.DedicatedTaskRunner.runTask(DedicatedTaskRunner.java:98)
                at org.apache.activemq.thread.DedicatedTaskRunner$1.run(DedicatedTaskRunner.java:36)

we are using 5.3.1 by the way, and i will hopefully upgrade to 5.4.1 next week and retry

in our case we have very slow consumers in certain conditions, because they recieve the message and post it out to a web endpoint, and at various points during the year the web endpoint is taken offline, causing us the above problem.... 

we do have a test case here, but it is kind of explicitly linked into stopping and starting our services, and pulling the IP stack down, so it is kind of difficult to repost that one, and it takes about 2 days for it to actually kill the system after 300 messages per second, I tried to reproduce this issue with a test case. The test case is committed at apache as it works ok: http://svn.apache.org/viewvc/activemq/trunk/activemq-core/src/test/java/org/apache/activemq/usecases/ExpiredMessagesWithNoConsumerTest.java?view=diff&r1=1031135&r2=1031136&pathrev=1031136
It is a variant of a slow consumer test, in the original the consumer blocks and only acks when all the messages in the queue have expired. So it will have received 600 (prefetch) messages and they will be in the prefetchsubscription.dispatched list on the broker. The test verifies that all of these messages expire.
The variation of that test to validate this issue adds another batch of messages with no expiry and validates that the consumer can receive them, so it proves that expiring messages in the dispatched list does not block the consumer.

Could you look at the test case to see if any of the options used in the test do not match your use case?, After performing more tests I also observed the appearance of OutOfMemoryError. I would like to raise the theoretical case that OutOfMemoryError caused some thread to die that was supposed to perform maintenance for the dispatched list. Leading to a situation wherein the expired messages clogged the processing of the dispatched list.

If that would be the case, then the current fix is redundant and the real problem is what caused that OutOfMemoryError., yes, with an OOM all bets are off as any thread can die and it is not possible to restart them when there is no heap. 
There is a bunch of information related to memory usage and trouble shooting at: http://activemq.apache.org/javalangoutofmemory.html, The addition of the inactive destination cleanup and the auto sweep for expired messages should resolve this issue.]