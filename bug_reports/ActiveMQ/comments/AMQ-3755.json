[Some more information: we run our broker mostly with default settings, except:
 * ProducerFlowControl set to disabled for all queues and topics;
 * AsyncSend set to true;
 * CopyMessageOnSend set to false.

Clients are connecting through a failover(tcp) connection. , Do you have a test case that reproduces the problem?  Have you tried this against a 5.6-SNAPSHOT build to see if its already been resolved?, We do not have a reproducible test case, as we do not know what triggers this. We see this in our busiest production environment only. Obviously, we cannot test with a snapshot version in that deployment. For now, these stack traces are the only thing we have., From the stack trace it appears as though you VMTransport is configured for sync mode and it has reached its limit on the pending buffer (LinkedBlockingQueue) and that stalls the send operation until the broker has time to process the backed up messages.  You can provide you config for the client uris to lets us see what you have setup.  Since your producers and consumer appear to be sharing the same Connection instance they will block on the MutexTransport which is what you see in the stack trace.  As far as I can tell things are working as designed.  , We had a hunch we had to move to a multiple VMTransport implementation, however, I have some follow-up questions:

From the documentation it is not clear what the limitations of the VMTransport exactly are, it talks about a JMS Client, but that is a bit of a vague notion inside the same JVM. It seems, one of the effects of this limitation is that the receiveNoWait call will start blocking, which is not obvious from the documentation.

Furthermore, if we move to multiple VMTransports, will the backed up broker be able to continue without blocking or will the bottleneck then surface inside the broker?

It will take some time to get this modified and get it deployed in our production environment., VMTransport doesn't cause the locking issue, that exists anytime that you use a single Connection for both send and receive operations.  As far as your other issues it depends somewhat on your configurations which you haven't shared so I can't really speak to that.  , Not able to reproduce this locally and from the stack trace it doesn't appear as though its an actually issue.  Setting the asyncQueueDepth option to a higher value should allow the client's to continue sending.  Also should try a later 5.6-SHAPSHOT as there has been a few fixes in VMTransport.]