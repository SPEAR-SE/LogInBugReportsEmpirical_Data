[SVN revision 658637 makes closing a socket in a separate thread optional.
The property to set is closeAsync which is true by default.

To disable this in a broker add it as a transport option to the transport connector address 
e.g.  

<transportConnectors>
            <transportConnector name="openwire" uri="tcp://localhost:61616?transport.closeAsync=false"/>
</transportConnectors>
, For what it's worth, we saw this (problems with lots of CLOSE_WAIT sockets) only when testing with client with a tight loop that opened and closed lots of connections (instead of pooling), and only on Windows (where it would fail unable to get an available port after 4000-5000 connections).

Another solution seemed to be to alter the Transport to set SO_LINGER to any positive value., CLOSE_WAIT is a kernel tuning parameter, how long the connection stays there depends on your OS.
On linux I don't even know if you can control it, but I know you can on windows and solaris.
problem with it being a kernel parameter, is that it then affects all the system, any program using TCP connections, I stand corrected, it is just that solaris used to misname the parameter :)

http://www.sean.de/Solaris/soltune.html#tcp_close_wait_interval

close_Wait on a busy system can stick around if the application hasn't called close (server too busy, or a bug)

here is a good summary
http://www.sunmanagers.org/pipermail/sunmanagers/2006-January/039225.html, The transport.closeAsync=false seems to work. We haven't had ActiveMQ running out of file descriptors for a few days now.

I did see the systemload spike to 3+ a few times though, so the original issue which started the whole running-out-of-descriptors thingy still seem to exist. With a maximum heap of 512MB it ran out of memory and started doing full GC's every few seconds. With 1024MB heap it had less trouble with the increased load, but ran to gc's where it ran had 900MB consumed and reduced it to only 500MB rather than 300MB -> 30MB.

I guess that's a different issue?, The issue - or a similar one - reoccured yesterday.

It appears ActiveMQ 5.1/5.2-snapshot copes much worse with queues that are filled faster than consumed (or not consumed at all) than 5.0.

With 5.0 I could fill up two queues to over 80k small messages, but 5.2-snapshot refused to accept connections after about 5120 message on one queue and 3690 on the other.

The attached tgz is a set of two php5 scripts. Just extract it somewhere and adjust the $brokerUri in 'producerserver.php' to fit your needs. Then you should be able to run it and get an overloaded activemq within a minute or so.

Running is simple, make sure you have a CLI-version of php 5 (in debian that is 'apt-get install php5-cli') and run: 'php producerserver.php'

That script will then include the default Stomp-implementation for php and connect to ActiveMQ using Stomp. It forks off $max child-processes, until you stop the script, which send two messages to two seperate queues.  If you need any special connection-parameters, have a look at the Stomp.php StompConnection's 'getInstance'-method, but the above should work with a default ActiveMQ-install.

Our ActiveMQ is started with a slightly adjusted activemq.xml (ssl and xmmp connections are not started, camel-section is empty, resource management is commented out) but mostly left to the defaults. Our ACTIVEMQ_OPTS = "-XX:+UseParallelOldGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xms1024M -Xmx1024M -Dorg.apache.activemq.UseDedicatedTaskRunner=true"

Although normally you'd have at least one consumer for those queue's, its easier and faster to reproduce the issue without one. That way you should get 'ERROR TransportConnector             - Could not accept connection : Too many open files' pretty fast., I can only support what Arien said, we moved from 5.0 to 5.1 because of several smaller issues which are fixed, however there is something completly problematic going on...

We have data collectors for network traffic/disk/cpu usage spread cross the datacenter which will all report into an ActiveMQ instance, each collector has its own queue, so being it approx. 15K queus and on each queue about 3-10 messages per minute coming in. that sounds not much, but in certain scenarios the Apps/DB Server picking up the data is not fast enough to pick them in a timely fashion, so they might queue up for up to 2 days. 

We had issues with 5.0 that it died from time to time (approx. once a week), but with 5.1 it does not even stand for 12 hours. 

As it is run in a VM, we extend the original setup of 1 GB Memory already to 4 GB, but even that didn't help, we always run either out of File Descriptors or out of memory.

This is really a serious issue, because we need to decide whether we switch back to 5.0 or even back to a BEA or IBM tool, as the instabilities are going on for a few months now..., Guys, 

i had the similar issue with 5.0 and 5.0 to resolve the File Descriptors and CLOSE_WAIT sockets. i tuned the JVM parameters and Solaris parmeter. 

1. i changed the ulimit -n to 1024 for the separate activemq user account we created to run AMQ. that basically changes the maximum file handles allowed per user. 

2. we are using these JVM parameters  -Xmx2048M -XX:SurvivorRatio=4 -XX:+UseParallelGC -Xms2048M -Xss512k. This configuration works and we don't get any out of File Descriptors problem any more. 

There are reasons behind why we chose this route, if you want to know the details let me know. 

BTW: Out-of-Memory is because a major bug in AMQ versions related to how they remove the old messages. The Old Gen memory pool keeps on growing because AMQ does not release the memory for the messages when the life cycle is complete for those messages (produced /consumed and removed from the queue). 

Hope this helps.

Thanks!, I figured out why the CLOSE_WAIT's reoccured.

It is caused by the default memory limit of 5MB per queue, which is configured in the default activemq.xml. As soon as the limit is hit, activemq just has the producers wait untill space frees up.
But if there is no consumption, or the consumption isn't fast enough, ActiveMQ will eventually have more producers than available file descriptors. In my case I got 930 CLOSE_WAIT's for 1024 available descriptors in total. I also had 930 stacks ending like this:

 - java.lang.Object.wait(long) @bci=0 (Compiled frame; information may be imprecise)
 - org.apache.activemq.usage.MemoryUsage.waitForSpace(long) @bci=44, line=85 (Compiled frame)
 - org.apache.activemq.broker.region.Queue.send(org.apache.activemq.broker.ProducerBrokerExchange, org.apache.activemq.command.Message) @bci=259, line=395 (Interpreted frame)
 - org.apache.activemq.broker.region.AbstractRegion.send(org.apache.activemq.broker.ProducerBrokerExchange, org.apache.activemq.command.Message) @bci=42, line=350 (Compiled frame)
 - org.apache.activemq.broker.region.RegionBroker.send(org.apache.activemq.broker.ProducerBrokerExchange, org.apache.activemq.command.Message) @bci=142, line=437 (Compiled frame)


So each of the 930 threads is waiting for someone to make some room, but since they prevent any other connection from entering the system, the memory will never be freed again. Possibly, even if there is a consumer which picks up consumption after a while, the fact that it ran out of file descriptors might cause it to a unpredictable and dangerous situation.

After removing the memory limit for seperate queues, I reran my "overload producer" and was able to produce several tens of thousands of messages. But it wasn't really gone after that. Similar behaviour seems to occur as soon as you run out of the systemUsage's memoryUsage and/or when the disk or temporary spaces fill up, even though it by no means had reached the disk limit yet, but possibly the temporary limit.

The problem here is of course that there might not be a solution to it. But it may help if the logfile gets error messages about hitting memory limits, rather than just leaving the user in the dark?, Arjen,

I have also been experiencing the same problems. I am running a near identical setup to yours with ActiveMQ 5.0.0. I switched to 5.1.0 temporarily, but it had so many more problems, that I had to revert back.

I believe the problem is actually within the Stomp client itself. I've been searching around about the CLOSE_WAIT socket issue (in general) and it appears to be a problem caused by the code, not the server. Basically, some where in your code, the socket never gets closed. It can theoretically remain in a CLOSE_WAIT state forever if this happens.

We monitor our ActiveMQ sockets every minute (using lsof) and about once every few hours, we see it spike from around 220 open connections to numbers like 7,000 or 8,000. It doesn't build up or anything, just spikes to extremely high numbers.

In some cases, the queue actually dies (overloaded from too many sockets) and in some cases it actually recovers and flushes everything out. 

So, I think the problem lies somewhere inside the Stomp client rather than in ActiveMQ itself. Have you gotten anywhere else with this lately?, Joel,

The clients close their connection, as they should. If not, the sockets would not be in 'CLOSE_WAIT' but in some other state (TIME_WAIT?).

Switching to the 5.2 development build with the non-asynchronously closing sockets works fine once I had removed the limits on memory usage., Arjen,

I rewrote my activemq.xml from scratch and modified a lot of the memory usage parameters (like you said) and I'm no longer having these issues. 

I don't really want to switch to a dev build of 5.2, I'll wait for it come out out officially (unless you strongly feel it's much better :D), Going to mark this issue as resolved, Calling PHP 'socket_shutdown' and 'socket_close' functions to disconnect rather than using 'fclose' which might leaving linger sockets on the ActiveMQ server. Try an older version of http://stomp.codehaus.org/Stomp+JMS using fclose.
]