[The issue can be easily reproduced in the following scenario:
1. Start 3 brokers. Let it be 0, 1, 2
2. In brokers 1 and 2 enable conduit duplex network connector to 0
3. Connect tcp:// producer for topic A to 0 (jmeter)
4. Connect vm:// consumer for topic A to 1 (simple jms message forwarder in camel: from jms, to jms)
5. Connect vm:// producer for topic B to 1 (simple jms message forwarder in camel: from jms, to jms)
6. Connect vm:// consumer for topic B to 2 (simple jms message forwarder in camel: from jms, to jms)
7. Connect vm:// producer for topic C to 2 (simple jms message forwarder in camel: from jms, to jms)
8. Connect tcp:// consumer for topic C to 0 (jmeter)
In summary, we just put bi-directional load to each of network bridges where each bridge has to both send and receive messages and acks.
Since vm:// and camel embedded in the same JVM are faster than tcp:// and peer broker, bridge very quickly fills heap with responseCallback's used to handle ack and reference message body. This isnt controlled by broker memory limits or producer flow control and bridges die on OOM.

Additionally, VMTransport will also keep references to asynchronously handled messages. Since vm:// is faster than tcp://, this is another source of quick OOM where producer flow control and memory limits have no any effect. While queue is bounded, default size is 2000 and with 1MB messages you quickly run out of heap (doc doesnt seem to explain if and how it could be adjusted). Wouldnt be a problem if it could be controlled by broker memory limits and producer flow control., Attached patch with workaround for those who face OOM in network bridge. This is hardly a solution (might be lower vm performance on small messages, might be less delivery guarantees over bridges) but at least it allows JVM not to crash with OOM., the prefetch should limit the number of messages pending an ack at one time. 

try{code}<networkConnector .... prefetchSize="1" .../>{code}
, Prefetch was already set to 1 for all destinations via per-destination policy. However, at appears that this has no effect over bridge. Reducing prefetchSize from default 1000 to 1 directly on networkConnector fixes the problem with heap exhaustion. However, one may assume that amount of prefetched data should also be limited by resource usage constraints while it doesn't seem to be so.

{noformat}
    <amq:destinationPolicy>
        <amq:policyMap>
            <amq:policyEntries>
                <amq:policyEntry topic=">" producerFlowControl="true" memoryLimit="30mb" topicPrefetch="1" blockedProducerWarningInterval="30">
                    <amq:pendingMessageLimitStrategy>
                        <amq:constantPendingMessageLimitStrategy limit="-1"/>
                    </amq:pendingMessageLimitStrategy>
                </amq:policyEntry>                    
                <amq:policyEntry queue=">" producerFlowControl="true" memoryLimit="30mb" queuePrefetch="1" blockedProducerWarningInterval="30">
                    <amq:pendingQueuePolicy>
                        <amq:vmQueueCursor/>
                    </amq:pendingQueuePolicy>
                </amq:policyEntry>
            </amq:policyEntries>
        </amq:policyMap>
    </amq:destinationPolicy>
    
    <amq:systemUsage>
        <amq:systemUsage sendFailIfNoSpaceAfterTimeout="60000">
            <amq:memoryUsage>
                <amq:memoryUsage limit="350 mb" />
            </amq:memoryUsage>
            <amq:tempUsage>
                <amq:tempUsage limit="300 mb" />
            </amq:tempUsage>
        </amq:systemUsage>
    </amq:systemUsage>    
{noformat}, glad that the networkConnector prefetch value is working as expected.

Dispatch or prefetch of messages can exceed the limits by max pageSize num messages.
Note however, that paging in for dispatch (for persistent messages, default store cursor) is limited by the value of systemUsage.memoryLimit - not the per destination limits.
This is to ensure we can dispatch messages when destination resources are maxed out and producers are blocked.
What value do you use for the -Xmx jvm argument in your scenario?
I think their is a case to be made for limiting the prefetch base on resource usage, the difficulty is that can lead to starved consumers b/c the limits are shared across all destinations. If they are not shared, then a single destination can get starved.

It is not a trivial problem to solve but I think it warrants a jira issue. If you have some unit tests that can simply demonstrate the issue that would be a great help.

Also, out of curiosity, why do you use the vmQueueCursor? In most cases, I find the default store cursor works great.

 


, We use 1,5G by default for Xmx but not all of this is for broker. Broker is just one of the subsystems. I'm not sure that broker can reliably deduct preferable prefetch in non-standalone mode as you dont know how increased usage will impact other heap users.

Regarding, cursor we just follow example in http://activemq.apache.org/message-cursors.html. Not really using queues so can't tell much how it affected things.

After we had set prefetch explicitly on the bridge we have no problems anymore with memory usage growth. It was surprise that prefetch on destinations and memory usage limit on broker did not work that forced us to get code modified and to create this issue. Since there is a safer and more convenient way to prevent memory usage growth i would like to ask you to close the issue. Thanks for clarification and all the trouble.]