[There is a bunch more trace level logging in 5.3, if you can reproduce with TRACE level logging with 5.3.x and attach the log that may help. Ideally of course, if you can reproduce with a test case that would be great!, Hi Gary

I understand that more traces are needed. Unfortunately, this happens sometimes on our production system (30 servers on 2 sites). And you can see there is 3 days between the launch of the MOM-tpnocp09v and its first stop. 
Putting "Trace" level traces in these conditions will be very difficult. Because the volume of traces will be very heavy. 

But I will contact the production managers to see what can be done.

I suspect the ConcurrentHashMap<String,RemoteBrokerData> in the class MulticastDiscoveryAgent, is not correctly updated when the bridge is stopped in an "Unknown" state.

 Eric-AWL, Hi

I looked at MulticastDiscoveryAgent.java source. The "doRecovery" method is the only one that tells the connection must be reestablished. The indicator "failed" is put to false as soon as the recovery conditions are filled and even if the connection is not really reestablished (that seems to be my case) . As soon as "failed" indicator is false, the recovery and the reconnection (fireServiceAddEvent) can't be retried. Only "serviceFailed" method can put "failed" to true again.

Just by looking to this java source, it seems that a situation where the connection is not re-established, the indicator "failed" is false, the multicast heartbeat frames are received, is possible .... Only the call to "serviceFailed" can put the failed indicator to true (or delete the object entry in the hashmap). Is it possible that serviceFailed is not called although the connection is down ?

(I think that it should be safer that "failed" be an atomic boolean since I believe serviceFailed method could be called by another thread)

Eric-AWL, It seems that the "... has been established" trace can only be written if the remoteBrokerNameKnownLatch.await() condition is filled.

contdown is called into DemandForwardingBridge.serviceRemoteBrokerInfo
serviceRemoteBrokerInfo is called into DemandForwardingBridgeSupport.serviceRemoteCommand (only if disposed.get() is false)., A part of a stack trace which shows that "Multicast Discovery Agent Notifier" thread is blocked on the remoteBrokerNameKnownLatch., We are in a full embedded broker configuration.

I attach a part of an old stack trace (I delete all our own threads and the queueThreads). I think this trace was done (in march) just when one of our embedded broker doesn't succeed to reconnect to another one.

This trace is not exactly the same as the explanation since processes are not exactly the one. But this stack trace shows that "Multicast Discovery Agent Notifier" is blocked on the remoteBrokerNameKnownLatch.await() into the "bridge.start()" call (DiscoveryNetworkConnector.onServiceAdd)

            localBroker.start();
            remoteBroker.start();
            if (configuration.isDuplex() && duplexInitiatingConnection == null) {
                // initiator side of duplex network
                remoteBrokerNameKnownLatch.await();
            }

Corresponding "serviceFailed" call, can not be called (no exception) until this latch is released, which can only be done if serviceRemoteBrokerInfo call is done, what can be impossible if distant embedded broker is failed.

            try {
                bridge.start();
            } catch (Exception e) {
                ServiceSupport.dispose(localTransport);
                ServiceSupport.dispose(remoteTransport);
                LOG.warn("Could not start network bridge between: " + localURIName + " and: " + uri + " due to: " + e);
                LOG.debug("Start failure exception: " + e, e);
                try {
                    discoveryAgent.serviceFailed(event);
                } catch (IOException e1) {
                    LOG.debug("Discovery agent failure while handling failure event: " + e1.getMessage(), e1);
                }
                return;
            }

While "serviceFailed" is not called and multicast frames are received, it seems impossible to try to reconnect to this distant broker even if it is shortly restarted ?

, I'm sure that I identified a Latch problem in Multicast Network Discovery mechanism on Duplex connection 

The multicast notifier thread is blocked. here the trace (on Fuse-5.3.0.5)

"Notifier-MulticastDiscoveryAgent-listener:DiscoveryNetworkConnector:NOCSupervisorP5-ADMIN-OUT-IN:BrokerService[SIBBusModule-NOCP5-tpnocp08s-bus]" daemon prio=10 tid=0x0000000044ff2400 nid=0x1389 waiting on condition [0x0000000044c26000..0x0000000044c26b90] 
   java.lang.Thread.State: WAITING (parking) 
        at sun.misc.Unsafe.park(Native Method) 
        - parking to wait for  <0x00002aaab3dd66f0> (a java.util.concurrent.CountDownLatch$Sync) 
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158) 
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747) 
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:905) 
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1217) 
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207) 
        at org.apache.activemq.network.DemandForwardingBridgeSupport.start(DemandForwardingBridgeSupport.java:231) 
        at org.apache.activemq.network.DiscoveryNetworkConnector.onServiceAdd(DiscoveryNetworkConnector.java:114) 
        at org.apache.activemq.transport.discovery.multicast.MulticastDiscoveryAgent$2.run(MulticastDiscoveryAgent.java:484) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619) 

The problem appears when the network is quickly and alternatively on/off between the two components. 
The bridge is created in one direction, but the answer can not be received. 

The thread is blocked on the CountDownLatch. Even if multicast frames are received, the component can not establish a new network connection. 

Here are an corresponding activemq trace 

When it is OK : 
2010-06-22 22:56:24,500 [-tpnocp08s-bus]] INFO  DiscoveryNetworkConnector      - Establishing network connection from vm://SIBBusModule-NOCP5-tpnocp08s-bus to tcp://tpnocp11v-bus.vdm.priv.amm.noc:14101?useLocalHost=false 
2010-06-22 22:56:26,083 [nocp08s-bus#160] INFO  DemandForwardingBridge         - Network connection between vm://SIBBusModule-NOCP5-tpnocp08s-bus#160 and tcp://tpnocp11v-bus.vdm.priv.amm.noc/10.18.126.30:14101(SIBBusSupervisor-tpnocp11v-bus) has been established. 

2010-06-22 22:57:34,807 [-tpnocp08s-bus]] INFO  DemandForwardingBridge         - SIBBusModule-NOCP5-tpnocp08s-bus bridge to SIBBusSupervisor-tpnocp11v-bus stopped 

2010-06-22 22:57:34,811 [-tpnocp08s-bus]] INFO  DiscoveryNetworkConnector      - Establishing network connection from vm://SIBBusModule-NOCP5-tpnocp08s-bus to tcp://tpnocp11v-bus.vdm.priv.amm.noc:14101?useLocalHost=false 
2010-06-22 22:57:39,064 [nocp08s-bus#162] INFO  DemandForwardingBridge         - Network connection between vm://SIBBusModule-NOCP5-tpnocp08s-bus#162 and tcp://tpnocp11v-bus.vdm.priv.amm.noc/10.18.126.30:14101(SIBBusSupervisor-tpnocp11v-bus) has been established. 

2010-06-22 22:58:42,578 [-tpnocp08s-bus]] INFO  DemandForwardingBridge         - SIBBusModule-NOCP5-tpnocp08s-bus bridge to SIBBusSupervisor-tpnocp11v-bus stopped 

When it is KO : "Unknown" 

2010-06-22 22:58:42,648 [-tpnocp08s-bus]] INFO  DiscoveryNetworkConnector      - Establishing network connection from vm://SIBBusModule-NOCP5-tpnocp08s-bus to tcp://tpnocp11v-bus.vdm.priv.amm.noc:14101?useLocalHost=false 
2010-06-22 22:59:18,031 [18.126.30:14101] WARN  DemandForwardingBridge         - Network connection between vm://SIBBusModule-NOCP5-tpnocp08s-bus#164 and tcp://tpnocp11v-bus.vdm.priv.amm.noc/10.18.126.30:14101 shutdown due to a remote error: java.net.SocketException: Connection reset 
2010-06-22 22:59:18,033 [NetworkBridge  ] INFO  DemandForwardingBridge         - SIBBusModule-NOCP5-tpnocp08s-bus bridge to Unknown stopped 


Here is the other side corresponding activemq trace 

activemq-server.log:2010-06-22 22:55:44,295 [26.190.27:40517] INFO  TransportConnection            - Created Duplex Bridge back to SIBBusModule-NOCP5-tpnocp08s-bus 

activemq-server.log:2010-06-22 22:56:24,438 [26.190.27:40517] INFO  DemandForwardingBridge         - SIBBusSupervisor-tpnocp11v-bus bridge to SIBBusModule-NOCP5-tpnocp08s-bus stopped 

activemq-server.log:2010-06-22 22:56:26,135 [26.190.27:40518] INFO  TransportConnection            - Created Duplex Bridge back to SIBBusModule-NOCP5-tpnocp08s-bus 
activemq-server.log:2010-06-22 22:56:26,135 [ocp11v-bus#1770] INFO  DemandForwardingBridge         - Network connection between vm://SIBBusSupervisor-tpnocp11v-bus#1770 and tcp:///10.26.190.27:40518(SIBBusModule-NOCP5-tpnocp08s-bus) has been established. 

activemq-server.log:2010-06-22 22:57:34,818 [26.190.27:40518] INFO  DemandForwardingBridge         - SIBBusSupervisor-tpnocp11v-bus bridge to SIBBusModule-NOCP5-tpnocp08s-bus stopped 

activemq-server.log:2010-06-22 22:57:39,153 [26.190.27:40519] INFO  TransportConnection            - Created Duplex Bridge back to SIBBusModule-NOCP5-tpnocp08s-bus 
activemq-server.log:2010-06-22 22:57:39,153 [ocp11v-bus#1806] INFO  DemandForwardingBridge         - Network connection between vm://SIBBusSupervisor-tpnocp11v-bus#1806 and tcp:///10.26.190.27:40519(SIBBusModule-NOCP5-tpnocp08s-bus) has been established. 

activemq-server.log:2010-06-22 22:58:44,328 [26.190.27:40519] INFO  DemandForwardingBridge         - SIBBusSupervisor-tpnocp11v-bus bridge to SIBBusModule-NOCP5-tpnocp08s-bus stopped 
, What could happen if we add

         if (configuration.isDuplex() && duplexInitiatingConnection == null) {
                // initiator side of duplex network
                remoteBrokerNameKnownLatch.countDown();
            }

into the stop() method of DemandForwardingBridgeSupport class ?

, We put a no-Duplex Configuration instead of a Duplex Configuration and it seemed to work better.... But
today during a network problem (alternatively on/off) our process doesn't resist ....

We have 
- a thread dump which shows 85 StartLocalBridge Threads waiting for the same latch into the DemandForwardingBridgeSupport.StartLocalBridge method :

 protected void startLocalBridge() throws Exception {
        if (localBridgeStarted.compareAndSet(false, true)) {
            synchronized (this) {
                if (LOG.isTraceEnabled()) {
                    LOG.trace(configuration.getBrokerName() + " starting local Bridge, localBroker=" + localBroker);
                }
                remoteBrokerNameKnownLatch.await();
                ...
}

- 960 CLOSE_WAIT
- a file descriptor limit

Will the transport.closeAsync=false flag be helpful here ?

Eric-AWL, If I resume my view of the problem

When the problem occurs
- Network links between 2 brokers, are quickly and alternatively on/off. In french, we say that the network is "bagotting" :-)
- The DemandForwardingBridgeSupport.stop() method is called before the start() method  (including sons threads start methods) is fully exectuted

Consequences
- With NO DUPLEX connection, network of brokers are re-established, but some threads are created and blocked on the RemoteBrokerNameKnownLatch latch. 
In this case, the latch is awaited in the startLocalBridge() method which is called by a dedicated thread. So, a major problem occured when ressources are consumed if this kind of network faults are frequent, and the number of network connections is important. 
- With DUPLEX connection,  the latch is awaited in the start() method itself.  The main network connector thread is concerned. So the network connector is completely blocked.

I currently try to
- add a RemoteBrokerNameLatch.countDown at the end of the stop() method
- test the disposed AtomicBoolean value to correctly break the starting process in start() for DUPLEX, and in startLocalBridge() for NO-DUPLEX.

I think it will be better for DUPLEX since network connector thread will be freed, but I don't know if the son thread will be correctly destroyed.

Eric-AWL , Hi

I think I now have programmed a clean correction for this specific latch problem. And I can give you the sources. With these sources, the network connector is not ever blocked even if a network fault comes during the start'() process.

But, before giving you these sources, I have relative questions, with duplex TCP connections :

Imagine a brief network fault

On the network connector side of the DUPLEX connection, the fault is detected, the TCP bridge is closed. Socket.close() is executed, but the socket is in a CLOSE_WAIT state.
The "transport connector side" of the DUPLEX connection is not aware that the first connection is down while its InactivityMonitor Thread doesn't signal it.

Then, on the network connector side, the bridge wants to be built again, and the network is now ON. The "transport connector side" creates a second active bridge for the same DUPLEX connection.
So, the "transport connector" side temporarily sees 2 distant broker for the same one, while the InactivityMonitor thread doesn't signal the death of the first TCP connection.

Is this true ?

Eric-AWL
, Great if you have a fix.

to your question, that sounds true to me, but I would have thought that the second bridge creation attempt is considered distinct because it occurs on a new connection., I propose you this patch in the tar file (for ActiveMQ 5.3.2 version)

2 modifications are brought :
- the RemoteBrokerNameKnownLatch is countdown at the end of the stop() to free the connector thread even if it doesn't receive the correct remote broker info. That can appear when network is successively and quickly on/off. In the case of a DUPLEX connection, the network connector was then totally blocked. In the case of a not duplex connection, some dead threads were up.
- the second modification tries to stop an old invalid duplex Transport Connection when a new Duplex Transport Connection is required by the same broker for the same transport Connector

I tried to realize a JUNIT test which simulates a lot of "close socket" even during bridge start process. It seems to work, but I didn't succeed in simulating the second modification. 
the JUNIT test include a "tcpfaulty" transport with two socket factories. In the future I will try to put SocketProxy code into the ServerSocketTstFactory.

This situation where a second duplex connection tried to be established before the first one was clearly dead, appeared in my own (not JUNIT) tests , when a brief networkl fault occured and when only the first modification was implemented.

All modifications are clearly indicated by 
// Eric-AWL AMW-2774 Beginning
// Eric-AWL AMW-2774 End

Sorry for the poor english. (I'm french)

Eric-AWL
, I applied just the test component of your patch to trunk, 5.4-SNAPSHOT and ran the test and it worked just fine without any mods to the src in main/java. Is this expected? Or do you only get intermittent failures with the test. I wonder if the issue is resolved already on trunk? Can you validate trunk?, Hi Gary

It's very difficult to simulate quick network faults. With my JUNIT test, I simulate close() immediately or some seconds later (with a random value). When the close() is done immediatly, I succeeded  in  validating DUPLEX network of brokers and that nothing was blocked in this situation with my patch :

2010-07-26 14:09:20,001 [ce[SpokeBroker]] INFO  DiscoveryNetworkConnector      - Establishing network connection from vm://SpokeBroker to tcpfaulty://localhost.localdomain:61617
2010-07-26 14:09:20,035 [ocalport=32972]] INFO  SocketTstFactory               - Trying to close client socket Socket[addr=localhost.localdomain/127.0.0.1,port=61617,localport=32972] immediatly
2010-07-26 14:09:20,036 [ocalport=32972]] INFO  SocketTstFactory               - Client socket Socket[addr=localhost.localdomain/127.0.0.1,port=61617,localport=32972] is closed.
2010-07-26 14:09:20,037 [127.0.0.1:61617] WARN  DemandForwardingBridge         - Network connection between vm://SpokeBroker#8 and tcpfaulty://localhost.localdomain/127.0.0.1:61617 shutdown due to a remote error: java.net.SocketException: Socket closed
2010-07-26 14:09:20,038 [NetworkBridge  ] INFO  DemandForwardingBridge         - SpokeBroker bridge to Unknown stopped

In this kind of situation (bridge to Unknown stopped), I experimented on 5.3.0-05 fuse production environment, that the network of connector thread was completely blocked on the latch, with Duplex connections.

I'm not sure that my JUNIT test demonstrates the problem on 5.3.0-05. It helped me to debug my own patch.

I don't try my JUNIT test on 5.3.0-5 fuse version. I'm going to verify that my JUNIT test sometimes shows the problem with the 5.3.0-5 core jar.

I can look at 5.4-snapshot source code to see if something is already changed about this latch on the trunk.

I will tell you my results.

Eric-AWL
, yea, I understand this is difficult to test but the effort is worth it to protect the changes. One thought that may help is the use of SockeProxy to simulate a network failure or slow network. There is an example usage in http://svn.apache.org/viewvc/activemq/trunk/activemq-core/src/test/java/org/apache/activemq/usecases/BrokerQueueNetworkWithDisconnectTest.java?view=log - see the org.apache.activemq.usecases.BrokerQueueNetworkWithDisconnectTest#onSend use of socketProxy. This gave me a test case for a similar issue some time back so it may be worth a look, it is not unlike what you do in TcpFaulty but is very deterministic which may help., Yes, I saw this JUNIT test, and I will try to adapt SocketProxy into the TCPFaultyServerSocketFactory, to simulate pause in the accept().

I can't imagine that this JUnit test doesn't work with 5.3.0-05-fuse, but we expiremented this latch situation with network faults regularly (stack traces was taken some hours after the frozen state was discovered of the network connector, and these traces always show the "await" on this latch with duplex connection).

Eric-AWL
, At first view, on the 5.4-SNAPSHOT, with the last modification, the remoteBrokerNameKnownLatch is not countdown in the stop() method of the DemandForwardingBridge. I don't think that the network connector freeze in DUPLEX connection, is resolved.

Eric-AWL, Hi Gary

You will find here a new version of the my own org.apache.activemq.transport.tcp.SocketTstFactory.java far more deterministic (no random)

In fact, in this one, the duplex network connection is forcely closed in my "bagot thread", after 0 ms, then 1 ms, then 2 ms, then ...., then 11 ms, then 31 ms, then 51 ms, then 71 ms, .....

With my patch on the 5.3.2, and my test server, the JUNIT test succeeds (I think always), With the 5.3.2 version, the JUNIT test doesn't succeed (I think never).

On my test server (a good one with 2 CPUs), it's when the connection is closed between 1 and 3 ms, after the connect() call that the network connector is frozen. If the close connection is done immediately, or after 3 ms, the network connector continues to live.

I can't test on 5.4-trunk since I don't have a SVN with HTTPS support on my test server.

I hope this will help you to test on 5.4 and to validate my patch.

Eric-AWL, I didn't receive any mail to signal that I just added a new attachment with a comment on this thread. This comment is just to signal this if necessary !!!

Sorry if you receive 2 mails. 
Eric-AWL, I've encountered similar problem though in a different case.
Imho it is far more easily reproducible.
The case is when you try to add a DiscoveryNetworkConnector between the brokers with a TCP connection set to the broker that has SSL enabled [not TCP].

The operation is blocking [on a DemandForwardingBridgeSupport.start() -> remoteBrokerNameKnownLatch.await(); operation] and new threads are being created, all blocked on the same operation :
DiscoveryNetworkConnector.onServiceAdd()
DemandForwardingBridgeSupport.start()
CountDownLatch.await().

Here are the logs that appears during the situation described above:
https://issues.apache.org/activemq/secure/attachment/19504/activemq-bug-2774-tcpToSsl.log, Yes, with DUPLEX connection, the network connector thread is completely frozen into the start() call. With not Duplex connection, son threads are created and are blocked on the same latch, but the connector is not frozen.

The problem is the same. As we are far more worried by the DUPLEX problem, I choose to demonstrate the bug with DUPLEX connection.

If you have an environment test with source codes, perhaps you can use my patch (only 3 class to change) and tell us what happens on your configuration ???

I can provide you the activemq-all-5.3.2.jar with my patch.

Eric-AWL, The jar with the patch, in the tar file, there are 3 changes for ActiveMQ classes, 6 new test classes and one ressource file (that declares a new transport that simulate network faults)


Changes

- activemq-core/src/main/java/org/apache/activemq/broker/TransportConnection.java
- activemq-core/src/main/java/org/apache/activemq/network/DiscoveryNetworkConnector.java
- activemq-core/src/main/java/org/apache/activemq/network/DemandForwardingBridgeSupport.java

test classes

- activemq-core/src/test/java/org/apache/activemq/transport/tcp/TcpFaultyTransportFactory.java
- activemq-core/src/test/java/org/apache/activemq/transport/tcp/TcpFaultyTransport.java
- activemq-core/src/test/java/org/apache/activemq/transport/tcp/TcpFaultyTransportServer.java
- activemq-core/src/test/java/org/apache/activemq/transport/tcp/ServerSocketTstFactory.java
- activemq-core/src/test/java/org/apache/activemq/transport/tcp/SocketTstFactory.java
- activemq-core/src/test/java/org/apache/activemq/usecases/MulticastDiscoveryOnFaultyNetworkTest.java

Test ressource
- activemq-core/src/test/resources/META-INF/services/org/apache/activemq/transport/tcpfaulty


Then, I send SocketTstFactory.java again to change the network faults generation algorithm.

Eric-AWL, Your solution passed all our JUNIT tests so it seems to get rid of the problem while not interfering current features., patch applied with thanks to r980014
Great to have the test case and additional validation, that is a some what messy area of the code base.

Just for future reference, svn diff is a great way to produce a patch as it just captures the differences. Makes it alot eastier to apply a patch to trunk when it was created from a branch.
Can you cast an eye over the diffs in the commit log just to be sure to be sure it is all there. thanks again. , All seems OK. And I think that "tcpfaulty" transport can be reused to verify a lot of things...

Be careful, I don't exactly know all consequences of my patch on cluster failover mechanisms (transportInterupted and transportResumed).

Atos Wordline works with ActiveMQ on a VERY VERY HUGE project in France (an "electricity smart metering" project). And this patch was very important for us.

I'm not in general development teams but in production teams. That's why I don't have a good SVN development environment.

Thank you
Eric-AWL (Atos Worldline).]