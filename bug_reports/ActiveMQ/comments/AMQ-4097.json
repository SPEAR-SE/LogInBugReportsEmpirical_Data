[We also encountered this error in our load testing lab when we had an actual network disruption.  We repeated the scenario by disconnecting and reconnecting the network cable.  If we had a long break between the disconnection and reconnection, our network of brokers establish connections just fine.  If we had a very short break (less than a minute) between the disconnection and reconnection while the load is high, we encountered this issue.  Below is the error in the log.

2012-11-15 08:37:08,953 WARN   [ActiveMQ Transport: tcp://n4-perform70/10.40.100.17:61613] [DemandForwardingBridgeSupport:363] (   ) Network connection between vm://local-amq-broker#68046 and tcp://n4-perform70/10.40.100.17:61613 shutdown due to a remote error: javax.jms.InvalidClientIDException: Broker: amq-broker - Client: center-to-amq-broker_outbound already connected from tcp://10.47.103.25:51353

We are not using the duplex connection.  We're using ActiveMQ 5.6.0.
regards., Hi Ron,
We still encountered the same issue after upgrading to ActiveMQ 5.7.0.  You mentioned about "A special routing/firewall is used which can affect timing but not order of TCP packets".  Could you elaborate on that?  Did you run socat as the special router/firewall?  How did you affect the timing of the TCP packets?  I'm interested in reproducing the issue in a controlled environment.  Also, did you find out more about this issue?
Thanks., Hi Edwin,

the problem is still there with ActiveMQ 5.8.0.

We are using something a little bit more powerful than socat, but from a network point of view it is just two socats in a row with some additional routing based on the IP of the connecting broker.

I made some extensive tests and I found that the error is actually correct. Under load the broker is just too slow to tear down the previous connection if the disconnect and connect happen very fast. Sometimes it could take the broker minutes to completely tear down a connection.

Unfortunately the code around creating and destroying connections is a quite complicated beast of threads, singletons, per-connection objects, per-configured-bridge objects and shared state that I don't think there is an easy solution to our problem.

To solve my problem I added a connection delay into my "socat", so that there needs to be a 30 second delay after each connect or disconnect before a new connection can be tried.

Regards,
Ron, Hi Ron,
Thank you very much for sharing your findings.  Do you think "initialReconnectDelay" attribute on the network connector(in static discovery) would help, instead of introduce the delay in the "socat"?
Thanks again!
-Edwin, Hi Edwin,
"initialReconnectDelay" may work if you have only one broker connecting. We have more than 40 brokers connecting to a central broker, so unless they have all different "initialReconnectDelay" settings several brokers will reconnect at the same time causing high load and there slowing down the connection teardown.
Regards,
Ron, Hi Ron,

I am also facing the same issue in 5.6.0.
you said you solved the problem set the property for socat, i did not find this property anywhere can you please
tell me where I can set up the socat value to reconnect the client again with Broker
 "To solve my problem I added a connection delay into my "socat"

Thanks,
Vee, Hi Vee,

I'm not using the actual socat but only something like socat, which was developed by myself. As this is company property I can't disclose it. You may be able to duplicate it by looking into the proxy example of netty.

Regards,
Ron, Hello,
I just want to respond to my earlier question.  I applied the "initialReconnectDelay" attribute.  Our production system has been running ok for more than two months now.  We don't have that many brokers to connect to a central broker at the same time.  When the network outage happens, it's usually between two brokers.  Hope the information helps for others.
Regards.
-Edwin, Hi,

To resolve this, the network connector name between your broker must be different. Example :
<networkConnector name="main-nc_$\{hostname\}" ...

With this configuration, i don't have this error message.

Regards
Amine, Hi Amine,

what you wrote is probably a prerequisite to get it working at all. We are usually encoding source and destination hostnames in the network-connector name to be on the safe side.

Anyway, the problem also occurs under heavy load with distinct network connector names.

Regards,
Ron, Hi Ron

??Also occurs under heavy load??

What do you mean by "heavy load" ? A lot of amq broker or a lot of message (the rate) ? or both ?

Regards,
Amine, Hi Amine,

usually that meant, that the broker has a lot of things to do. Especially broker logins/logouts seem to be quite critical. We have one environment with about 50 connected remote brokers which have about 10000 consumers shared among them. These are all connected over one leased line, so when that line breaks down, all remote brokers are disconnected due to missing keepalive. That causes a cascade of teardowns which can take minutes, especially if the line comes back and the remote brokers try to login again.

We do not have problems with a high number of messages to be sent in either direction or one logout/login.

Regards,
Ron, Hi Ron,

What about the "allowLinkStealing" option (recently added) for the TransportConnector? Don't you think it can solve this problem?

Thanks., Hi Ron, everyone,

I meet the same problem here, and thinking use allowLinkStealing option. 
Do you have any feedback about this solution ?
, Thanks for Edwin's experience and solution! 
I encountered the same problem as you did . AMQ 5.8 and usually two brokers(bridge type) using. I will apply the "initialReconnectDelay" attribute to my conf. Hopes everything works well later. 

Regards, 
Hao]