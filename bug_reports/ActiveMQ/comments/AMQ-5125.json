[Test program, Default configuration file as shipped with ActiveMQ. LevelDb enabled., The last minutes of the Java VM of the ActiveMQ., Recommend you test against a recent 5.10-SNAPSHOT which contains a great many fixes for LevelDB, you can make it simpler by turning your test into a JUnit test and drop it into the ActiveMQ source or create a simple maven project that allows you to quickly change version numbers. , The problem seems to exist in the recent 5.10 snapshot as well. After a few minutes of brokering, the system comes to a halt. I attached the thread dumps of the broker and the clients. The broker seems to hang in a rollback.

I observed that the number of LevelDB log files does *not* steadily increase anymore as it did in 5.9.0. 

Regarding the operating system: I ran reproduced the issue on Windows 7 with the attached test program. We observed the problem on Linux as well, but I haven't ran my test program on this platform., Same issue happening to us not every time but quite often on very small load and even with just one rollback. The all threads are BLOCKED. The version is  5.9.0., We  have a similar issue for which I posted a message on the user forums. There is a performance test attached there which can be used to reproduce this.

http://activemq.2283324.n4.nabble.com/ActiveMQ-message-dequeuing-hangs-td4681366.html, Any chance you can see if having producers/consumers on separate connections make a difference?, I believe the issue that this ticket was raised against is the same one described here (same link as above), which badavis has analysed in his recent post:

http://activemq.2283324.n4.nabble.com/ActiveMQ-message-dequeuing-hangs-td4681366.html

I've gone through the code, and I agree with his findings.

I've attached a patch to 5.9.2 based on this, in which LevelDBStore uses an internal private lock variable instead of _this_, which removes the deadlock that can happen when the LocalTransaction synchronises externally on the transactionStore during a rollback.

Using this patch, the original (attached) test runs for hours without deadlocking, as do my own tests that previously deadlocked after a few thousand messages.

Thanks,

James

, Nice!!! 


-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
, Reviewed all the mailing list discussion and the threads dumps and the analysis is correct, the lock cannot be the LevelDBStore object since that can lead to  a lock in the hawtDispatch serialization thread runner if a task is modifying mutable state and needs to protect data that also accessible on the public store API. 

Patch applied with thanks. , Awesome that this is getting fixed.  Thank you James for fixing this.  I apologize if this is the wrong form for this, I am new to this type of development.  The other issue I noticed from my analysis is the GCPolling job and the rollback job got scheduled to the same thread at the same time in the hawtdispatcher.  It just so happens that the GCPoll job got ran first.  My system had many dispatch threads available but all the jobs only went to the one dispatch thread causing the deadlock.  Could you tell me the reasoning this is not a concern?  I personally would like to understand this better.  Thank you.

Brian, The reason is simple, the serial execution of tasks protects from concurrent modification of mutable state in the LeveDB store.  If the tasks executed on multiple threads concurrently then the code would need to be drastically more protective of all mutable state in the store and you'd see a lot more locking and most likely threading related bugs. The reason for the small number of synchronized areas is to protect those bits that are exposed by the persistence adapter API which need to read or modify some things that are also modified on the dispatch thread.  ]