[any indication of an earlier error? Guess it could be ages ago when that entry was written or any time in between.
The exception indicates that the locator size value is < 4, which it should not be or course., The previous errors are:

2013-12-18 21:12:00,482 [ActiveMQ Journal Checkpoint Worker] ERROR MessageDatabase - KahaDB failed to store to Journal
java.io.FileNotFoundException: /activemq/data/kahadb/db-43153.log (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
        at org.apache.kahadb.journal.DataFile.openRandomAccessFile(DataFile.java:70)
        at org.apache.kahadb.journal.DataFileAppender.processQueue(DataFileAppender.java:324)
        at org.apache.kahadb.journal.DataFileAppender$2.run(DataFileAppender.java:203)
2013-12-18 21:12:25,510 [ActiveMQ Journal Checkpoint Worker] INFO DefaultIOExceptionHandler - Stopping the broker due to exception, java.io.FileNotFoundException: /activemq/data/kahadb/db-43153.log (Too many open files)
java.io.FileNotFoundException: /activemq/data/kahadb/db-43153.log (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
        at org.apache.kahadb.journal.DataFile.openRandomAccessFile(DataFile.java:70)
        at org.apache.kahadb.journal.DataFileAppender.processQueue(DataFileAppender.java:324)
        at org.apache.kahadb.journal.DataFileAppender$2.run(DataFileAppender.java:203)

FWIW, we had "Too many open files" errors because we had too many sockets in CLOSE_WAIT state., do you use checksumJournalFiles (defaults to true in 5.9) and checkForCorruptJournalFiles on kahaDB?
These should allow corrupt journal data to be ignored on restart - but there would be some message loss on the failed batch write if
the sends are not replayed by clients (failover openwire clients would replay)., We do not set these parameters (yet). Is there a significant performance penalty in enabling checksumJournalFiles?, with large queue depths like you have, there will be an impact on
startup as each journal data file will need to be read to verify batch
checksums. So multiple random reads per data file. So the restart
impact will be significant if IO random read performance is low.
the runtime overhead is minimal - generating an alder32 check on an
~8k buffer in memory.




-- 
http://redhat.com
http://blog.garytully.com
, OK, we will try these options.

Is there any way to tell ActiveMQ to check the current corrupted KahaDB store and use the files that are sane? I guess that checkForCorruptJournalFiles does nothing if there are no checksums in the first place, right?, yep, the checksum is a prerequisit, Is there any tool to attempt to recover messages from these corrupted KahaDB files that do not have cheksums?, I got the same problem after I encoutered "Too many open files" error and raised the ulimit and then restarted activemq 5.7.0, No reproducer and the broker version is quite old 5.5.x, Just wanted to let you know I've seen this with 5.11.1 as well.
So the bug is still there but as it is with some, they're hard to reproduce.]