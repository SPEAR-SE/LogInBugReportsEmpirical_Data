[I've implemented a fix for this and will release it today.

My fix was per discussion on the list.

The design takes RegionBroker and the inactiveDestinationsPurgeLock and breaks it up by granularity based on the ActiveMQDestination.

This way two destinations with the same physical name don't acquire the same lock.  This makes purge WAY more concurrent.

I've implemented a ChunkedGranularReentrantReadWriteLock which internally uses 1024 ReentrantReadWriteLocks so there's a 1/1024 chance that a given queue will use the same lock as another queue.  

This isn't perfect but in practice this should be MORE than enough concurrency to solve the 80% solution.

A more correct model would be to acquire a lock per queue name but this opens up some complex concurrency issues regarding cleaning up ReentrantReadWriteLocks for queues as they disappear.  For now we're using a simple interface so if someone wants to improve upon this in the future they can just change the implementation., My changes are here:

https://github.com/spinn3r/activemq

the branch is burton-concurrent-destination-gc 

... it's testing now but I'll post the results here when the tests pass., Additional unit tests here:

https://github.com/spinn3r/activemq-gc-purge-lockup

Which I can contribute as well. It's showing that maximum createConsumer latency went down from about 60000ms to about 2ms.

, My branch fails testing but so does activemq-5.11.x.  My isolated tests show that I've fixed the bug but now I want to get it to test before I can submit a patch. ]