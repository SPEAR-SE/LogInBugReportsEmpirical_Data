[Hi Kevin,

The maxInactivityDuration setting has to match on both sides of the connection.  If they do then there is no need to make KeepAlives request a reply since the other side will send a KeepAlive if his side of the connection remains idle for too long.

Could you attache the broker configuration scripts you using?  Thanks!, So, in the case of the client setting the maxInactivityDuration value on its connector url, you are saying that the broker must have a maxInactivityDuration setting (with the same value) on the transportConnector url?

Not quite sure why you would want to make this restriction.  Seems problematic with regards to potential timing issues.

Also, if you set the maxInactivityDuration on the transportConnector url, then when you have two brokers connected to each other, they continuously drop connections and resubscribe.  If you also set maxInactivityDuration on the networkConnector url for each broker, then the brokers still drop connections and get whacked out with subcriptions.

I'll attach broker XML configs in a bit., Attached are my broker XML configs.  We run embedded brokers, but have external access to them (i.e. the tcp connectivity).

I've tried with and without the maxInactivityDuration setting on the networkConnector url., Hi Kevin,

Yeah, I agree, making sure everybody is using the same setting is a bit of a problem.  I working on a change right now that will allow the setting to be negociated by the 2 sides of the connection when it's initialy established.

If the server setting is x and the client setting is y, then the inactivity duration for the established connection will be calculated at min(x,y).  This way the client and server do not have to have their configs match up exactly.

I'll update the issue once the change is commited.

Regards,
Hiram, However, that sounds like both sides would still have to explicitly make the setting, right?, The change has been commited.

You now configure the maxInactivityDuration on the wireFormat.   So on tcp connection, it would look like:

tcp://localhost:61616?wireFormat.maxInactivityDuration=10000

, I'll try this out tomorrow after the SNAPSHOT build., Tried this out yesterday and it seems to work perfectly.  I was able to freeze brokers and have them disconnect from each other, but recover after unfreezing.  Same for clients.

Nondurable messages are still queued to a disconnected broker, but that can be mitigated by having the asyncDispatch and an eviction strategy to your liking.  I'd rather see the subscriptions dropped, and no further queuing, but maybe that wasn't doable., We could always raise another feature request JIRA for the subscription dropping; also for nondurable messaging we have the option to drop old messages if consumers slow down.

So is this issue resolved now?, I'm very happy with the eviction policy features.  We will make *extensive* use of that (btw, we may want to implement an eviction  policy at some point - any way to do that ?).  However, I can raise a feature request for the subscription dropping.

One thing I forgot to mention in my comment above is that the broker networkConnector url config changed again.  You now have to explicitly put on failover if you want broker-to-broker connections to use failover.  That dropped off at one point (I was trying to find where you and I conversed about that - it was some other JIRA entry), where you would get double failover transport decorators, thus causing problems with shutdown.  You said that static would do the same thing.

Anyway, not a big deal, but I believe you had updated some documentation regarding this.  So, you might want to revisit that again., Doh!   Yes, issue resolved!]