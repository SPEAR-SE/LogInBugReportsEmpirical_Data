[Can you describe the nature of your usecase?
The fix for https://issues.apache.org/jira/browse/AMQ-5212 may be what you need. It narrows down the dlq processing on duplicates from the store. You would need to verify against a 5.11-SNAPSHOT to confirm., I'm not sure how to describe the usercase. I checked the messages in DLQ and they are from a same queue. The queue is populated by activemq component in apache camel and consumed by another java application., Github user asfgit closed the pull request at:

    https://github.com/apache/activemq/pull/36
, The use case and broker configuration are not given.  There is not enough information to investigate this., This happens again today. Please check  my broker configuration below:
<broker xmlns="http://activemq.apache.org/schema/core" brokerName="localhost" dataDirectory="${activemq.data}">

        <destinationPolicy>
            <policyMap>
              <policyEntries>
                <policyEntry topic=">" >
                    <!-- The constantPendingMessageLimitStrategy is used to prevent
                         slow topic consumers to block producers and affect other consumers
                         by limiting the number of messages that are retained
                         For more information, see:

                         http://activemq.apache.org/slow-consumer-handling.html

                    -->
                  <pendingMessageLimitStrategy>
                    <constantPendingMessageLimitStrategy limit="1000"/>
                  </pendingMessageLimitStrategy>
                </policyEntry>
              </policyEntries>
            </policyMap>
        </destinationPolicy>


        <!--
            The managementContext is used to configure how ActiveMQ is exposed in
            JMX. By default, ActiveMQ uses the MBean server that is started by
            the JVM. For more information, see:

            http://activemq.apache.org/jmx.html
        -->
        <managementContext>
            <managementContext createConnector="false"/>
        </managementContext>

        <!--
            Configure message persistence for the broker. The default persistence
            mechanism is the KahaDB store (identified by the kahaDB tag).
            For more information, see:

            http://activemq.apache.org/persistence.html
        -->
        <persistenceAdapter>
            <!--<levelDB directory="${activemq.data}/leveldb"/>-->
		<kahaDB directory="${activemq.data}/kahadb"/>
        </persistenceAdapter>


          <!--
            The systemUsage controls the maximum amount of space the broker will
            use before disabling caching and/or slowing down producers. For more information, see:
            http://activemq.apache.org/producer-flow-control.html
          -->
          <systemUsage>
            <systemUsage>
                <memoryUsage>
                    <memoryUsage percentOfJvmHeap="70" />
                </memoryUsage>
                <storeUsage>
                    <storeUsage limit="100 gb"/>
                </storeUsage>
                <tempUsage>
                    <tempUsage limit="50 gb"/>
                </tempUsage>
            </systemUsage>
        </systemUsage>

        <!--
            The transport connectors expose ActiveMQ over a given protocol to
            clients and other brokers. For more information, see:

            http://activemq.apache.org/configuring-transports.html
        -->
        <transportConnectors>
            <!-- DOS protection, limit concurrent connections to 1000 and frame size to 100MB -->
            <transportConnector name="openwire" uri="tcp://0.0.0.0:61616?maximumConnections=5000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="amqp" uri="amqp://0.0.0.0:5672?maximumConnections=1000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="stomp" uri="stomp://0.0.0.0:61613?maximumConnections=1000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="mqtt" uri="mqtt://0.0.0.0:1883?maximumConnections=1000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="ws" uri="ws://0.0.0.0:61614?maximumConnections=1000&amp;wireFormat.maxFrameSize=104857600"/>
	    <transportConnector name="nio" uri="nio://0.0.0.0:62626?maximumConnections=10000&amp;wireFormat.maxFrameSize=104857600"/>
        </transportConnectors>

        <!-- destroy the spring context on shutdown to stop jetty -->
        <shutdownHooks>
            <bean xmlns="http://www.springframework.org/schema/beans" class="org.apache.activemq.hooks.SpringContextHook" />
        </shutdownHooks>

    </broker>, This usually can be fixed by restarting activemq, I saw same warnings in my broker today. Broker is running with ActiveMQ 5.10. and in my configuration I am using discardingDLQplugin. 

       <amq:plugins>
            <amq:discardingDLQBrokerPlugin dropAll="true"/>          
        </amq:plugins>


Thanks,
Anuj
, We're seeing this too on 5.10. If there's anything we can do to help, let us know.
We've upgraded from 5.9.0, which was rock stable., This is our broker definition
{code}
    <bean id="broker" class="com.atlassian.bamboo.amq.BambooBrokerService"
          init-method="start" destroy-method="stop">
      <property name="useJmx" value="${bamboo.broker.useJmx}" />
      <property name="brokerName" value="bamboo" />
      <property name="deleteAllMessagesOnStartup" value="${bamboo.ignore.server.state.on.restart}" />
      <property name="persistent" value="true" />
      <property name="dataDirectory" value="${bamboo.home}/jms-store" />
      <property name="destinationPolicy">
        <amq:policyMap>
          <amq:policyEntries>
            <amq:policyEntry queue=">" producerFlowControl="false">
              <amq:deadLetterStrategy>
                <amq:sharedDeadLetterStrategy processExpired="false"/>  <!-- discard expired messages: -->
              </amq:deadLetterStrategy>
            </amq:policyEntry>
          </amq:policyEntries>
        </amq:policyMap>
      </property>
      <property name="systemUsage">
        <amq:systemUsage>
          <amq:memoryUsage>
            <amq:memoryUsage limit="128 mb"/>
          </amq:memoryUsage>
          <amq:storeUsage>
            <amq:storeUsage limit="100 gb"/>
          </amq:storeUsage>
          <amq:tempUsage>
            <amq:tempUsage limit="50 gb"/>
          </amq:tempUsage>
        </amq:systemUsage>
      </property>

    </bean>

  <bean id="connectionFactory" class="org.apache.activemq.pool.PooledConnectionFactory" destroy-method="stop" depends-on="broker">
    <constructor-arg>
      <bean class="com.atlassian.bamboo.v2.build.agent.BambooActiveMQConnectionFactory">
        <constructor-arg value="vm://bamboo"/>
        <property name="prefetchPolicy">
          <amq:prefetchPolicy queuePrefetch="1"/>
        </property>
        <property name="sendTimeout" value="${activemq.sync.response.timeout}"/>
      </bean>
    </constructor-arg>
  </bean>
{code}, can you validate a 5.11 snapshot. There has been a bunch of refactoring to the queue/cursor/store interaction with a view to resolving the occurrence of duplicates from the store. With concurrentStoreAndDispatch=true for kahadb(true by default) there is the possibility of duplicates when the cache becomes exhausted and the cursor reverts to the store. These are now identified and suppressed they don't clutter the DLQ., Thank you for you response. The problem is that I'd have to push it out to production (we can't reproduce it on our test servers).
Is current 5.11-SNAPSHOT stable enough to do that? Is 5.11 final far away?, 5.11 is in very good shape. Looking to close it down in the next week or so., OK, we'll upgrade as soon as it's available. It will take some time before it ends up on production and then we should wait for at least two weeks before confirming the fix - the rate of occurrence is roughly once per week., I am seeing:
2014-12-23 07:19:32,491 | WARN  | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@4eed1516:xstomp.request,batchResetNeeded=false,size=1,cacheEnabled=false,maxBatchSize:1,hasSpace:false,pendingCachedIds.size:0,lastSyncCachedId:null,lastSyncCachedId-seq:null,lastAsyncCachedId:null,lastAsyncCachedId-seq:null,store=org.apache.activemq.store.memory.MemoryMessageStore@2f037fd5 - cursor got duplicate from store ID:ip-10-140-168-52-43583-1419289565347-1:1:4:1:46539 seq: 974372 | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ NIO Worker 11233
2014-12-23 07:19:32,492 | WARN  | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@4eed1516:xstomp.request,batchResetNeeded=false,size=1,cacheEnabled=false,maxBatchSize:1,hasSpace:false,pendingCachedIds.size:0,lastSyncCachedId:null,lastSyncCachedId-seq:null,lastAsyncCachedId:null,lastAsyncCachedId-seq:null,store=org.apache.activemq.store.memory.MemoryMessageStore@2f037fd5 - cursor got duplicate from store ID:ip-10-45-179-71-55811-1419289516957-1:1:7:1:46574 seq: 974373 | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ NIO Worker 11233
2014-12-23 07:19:32,492 | WARN  | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@4eed1516:xstomp.request,batchResetNeeded=false,size=1,cacheEnabled=false,maxBatchSize:1,hasSpace:false,pendingCachedIds.size:0,lastSyncCachedId:null,lastSyncCachedId-seq:null,lastAsyncCachedId:null,lastAsyncCachedId-seq:null,store=org.apache.activemq.store.memory.MemoryMessageStore@2f037fd5 - cursor got duplicate from store ID:ip-10-140-168-52-43583-1419289565347-1:1:9:1:46540 seq: 974374 | org.apache.activemq.broker.region.cursors.AbstractStoreCursor | ActiveMQ NIO Worker 11233
2014-12-23 07:19:32,493 | WARN  | duplicate message from store ID:ip-10-140-168-52-43583-1419289565347-1:1:7:1:46411, redirecting for dlq processing | org.apache.activemq.broker.region.Queue | ActiveMQ NIO Worker 11233
2014-12-23 07:19:32,495 | WARN  | duplicate message from store ID:ip-10-140-168-52-43583-1419289565347-1:1:7:1:46414, redirecting for dlq processing | org.apache.activemq.broker.region.Queue | ActiveMQ NIO Worker 11233
2014-12-23 07:19:32,496 | WARN  | duplicate message from store ID:ip-10-140-168-52-43583-1419289565347-1:1:4:1:46405, redirecting for dlq processing | org.apache.activemq.broker.region.Queue | ActiveMQ NIO Worker 11233
2014-12-23 07:19:32,497 | WARN  | duplicate message from store ID:ip-10-140-168-52-43583-1419289565347-1:1:9:1:46406, redirecting for dlq processing | org.apache.activemq.broker.region.Queue | ActiveMQ NIO Worker 11233

My confifuration is different though. I run 2 static brokers. 
I see these messages on both 5.10.0 and apache-activemq-5.11-20141222.222708-183. 5.9.0 works fine under exactly the same test hardware and configuration. It is highly reproducible.

, Feel free to contact me for the details. I will be glad if I can be of any help., @Alex - can you work on a unit test case. There are a bunch of related test in the activemq-unit-test module that you could use for inspiration.
The warnings in themselves are not a bad thing, there may actually be some duplicate sends under various conditions that need to be suppressed by the cursor if the producer audit is exhausted or a message is replayed across a network.
When you suggest that it works fine in 5.9, do you use the same metric? as in all messages sent are consumed? 
A valid test would need to verify each message is received once and once only., Gary,

I will check the unit test module but I am afraid it might prove hard to replicate our environment.
Maybe it will be more productive if I work with you or someone assigned to this issue to understand what exactly is happening and then proceed with the test.

Here is some info:
1) Our production system consists of about half a dozen of servers running clients producing messages, then another set of about 10 consuming the load while producing some messages for consumption by themselves and another set of about 5 servers running apache storm. (The numbers change as we scale). 8 queues are created across the board.
2) We've ran into some reliability issues with the network of brokers while using AMQ 5.8.0 (transient) so we resorted to a single beefed up server.
3) Now we are on 5.10.0 and it is able to keep up with the current load but the load tends to grow and we would want to have some failover capability too.
4) I am running a load test for the same configuration as in production with 5, 7, 6 servers and again a beefy AMQ 5.10.0 server works fine.
5) When I add another AMQ server using static connector everything seems to work fine under lighter load. As load grows, even before the message consumption starts falling behind I am seeing a flood of the "duplicate" messages. They of course are not a problem by themselves but the DLQ fills up really quickly and AMQs die horrible out of memory death. Exactly the same behavior under the same setup and load is observed for the 5 or .11-SNAPSHOT (I basically swap the activemq link in the /opt folder)
6) Keeping the whole system the same, when I switch to 5.9.0 the issue disappears - no "duplicate" in the log, DLQ does not get populated.

So the question is what gives? The consumers seem to process all the messages at least it shouldn't be different from 5.9 to 5.10 to 5.11. Do you believe something has changed in 5.10 and it might be a legit behavior?

Let me know. I can provide additional details and assist with debugging if needed.
, Same problem over here with 5.10.0. Happens on a topic for us.
DLQ suddenly starts filling up with these messages:

2015-01-06 04:58:58,331 [10.29.0.4:46476] WARN  Topic                          - duplicate message from store ID:svlumipmq01.x.y.be-59515-1420515005092-3:975:-1:1:107, redirecting for dlq processing
2015-01-06 04:58:58,475 [10.29.0.4:46476] WARN  AbstractStoreCursor            - TopicStorePrefetch(m2mGatewayEventClient,m2mGatewayDurableEventSubscription) ID:svlumipweb01.x.y.be-47894-1419262838671-5:1:1:3 - org.apache.activemq.broker.region.cursors.TopicStorePrefetch@7ec6ea1b:m2mbox.IT-LUMI-9-MB-Herdi1.event.cogen.level/changed,batchResetNeeded=false,storeHasMessages=true,size=0,cacheEnabled=true,maxBatchSize:200,hasSpace:true - cursor got duplicate: ID:svlumipmq01.x.y.be-59515-1420515005092-3:975:-1:1:109, 4, @Alex - there was a change of behaviour in 5.10 - to treat duplicates from the store a as problem - in the past, they were ignored by the cursor but could get redispatched on a restart or if the audit was exhausted, they would be redispatched as duplicates. in 5.10, if the page in logic from the store gets a duplicate if does the DLQ thing. So in some ways the difference between 5.9 and 5.10 is expected.
What is unknown is what exact scenario causes the behaviour you see.
The trigger for the duplicates I tracked in the past was queue memory limit being reached with concurrent producers to the same destination, some of which were transactional. The interleaving could leave the cursor and store out of sync, such that when the cursor needed to page in from the store, it would use the wrong start point.
Your trigger seems to be related to the network bridge, which would introduce a non transacted client ack consumer and non transacted producer. Maybe you can simulate that at load without the bridge.
We need to determine the sequence of events that lead to the duplicate from the store to understand if it is legitimate or not and if not, to ensure we don't fill the DLQ in this case., @Christian - can you verify against 5.11-SNAPSHOT and if the problem persists try and build a simple reproducible test case? There are a bunch of tests around https://issues.apache.org/jira/browse/AMQ-5266 - peek at the activity tab. They may give inspiration., @Gary: I managed to reproduce the problem with a local instance, maybe the following info might give you some insights.

- started a client publishing messages to a topic continuously 
- our camel application was consuming from that topic
- while everything was running ok, I restarted activemq
- the duplicate message warnings started appearing immediately after startup
- even after killing the client, the broker seems to be 'stuck' in this state. Each single message will trigger the warning from then on.
- not all 'duplicate' messages end up in the DLQ. Seems to be timing related. (Looks like the faster the machine, the less messages end up in the DLQ.)  
- reproducable both with an oracle and a kahadb store

I connected a debugger and noticed that the handling thread enters AbstractStoreCursor.recoverMessage() twice, hence the duplicate warning because it already exists in the audit map. PrefetchSubscription.add() seems to trigger the 2 executions, once through pending.addMessageLast(node) and once through dispatchPending()...

Here are 2 stacktraces showing how it enters that method.
{code}

ActiveMQ Transport: tcp:///0:0:0:0:0:0:0:1:56106@1883@10055 daemon, prio=5, in group 'main', status: 'RUNNING'
	  at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.recoverMessage(AbstractStoreCursor.java:90)
	  at org.apache.activemq.broker.region.cursors.TopicStorePrefetch.recoverMessage(TopicStorePrefetch.java:77)
	  at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.addMessageLast(AbstractStoreCursor.java:194)
	  at org.apache.activemq.broker.region.cursors.StoreDurableSubscriberCursor.addMessageLast(StoreDurableSubscriberCursor.java:198)
	  at org.apache.activemq.broker.region.PrefetchSubscription.add(PrefetchSubscription.java:159)
	  at org.apache.activemq.broker.region.DurableTopicSubscription.add(DurableTopicSubscription.java:272)
	  at org.apache.activemq.broker.region.policy.SimpleDispatchPolicy.dispatch(SimpleDispatchPolicy.java:48)
	  at org.apache.activemq.broker.region.Topic.dispatch(Topic.java:717)
	  at org.apache.activemq.broker.region.Topic.doMessageSend(Topic.java:510)
	  at org.apache.activemq.broker.region.Topic.send(Topic.java:441)
	  at org.apache.activemq.broker.region.AbstractRegion.send(AbstractRegion.java:424)
	  at org.apache.activemq.broker.region.RegionBroker.send(RegionBroker.java:445)
	  at org.apache.activemq.broker.jmx.ManagedRegionBroker.send(ManagedRegionBroker.java:297)
	  at org.apache.activemq.broker.BrokerFilter.send(BrokerFilter.java:147)
	  at org.apache.activemq.broker.CompositeDestinationBroker.send(CompositeDestinationBroker.java:96)
	  at org.apache.activemq.broker.TransactionBroker.send(TransactionBroker.java:307)
	  at org.apache.activemq.broker.BrokerFilter.send(BrokerFilter.java:147)
	  at org.apache.activemq.broker.MutableBrokerFilter.send(MutableBrokerFilter.java:152)
	  at org.apache.activemq.broker.TransportConnection.processMessage(TransportConnection.java:496)
	  at org.apache.activemq.command.ActiveMQMessage.visit(ActiveMQMessage.java:756)
	  at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:294)
	  at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:148)
	  at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:45)
	  at org.apache.activemq.transport.mqtt.MQTTInactivityMonitor.onCommand(MQTTInactivityMonitor.java:123)
	  at org.apache.activemq.transport.mqtt.MQTTTransportFilter.sendToActiveMQ(MQTTTransportFilter.java:91)
	  at org.apache.activemq.transport.mqtt.MQTTProtocolConverter.sendToActiveMQ(MQTTProtocolConverter.java:132)
	  at org.apache.activemq.transport.mqtt.MQTTProtocolConverter.onMQTTPublish(MQTTProtocolConverter.java:566)
	  at org.apache.activemq.transport.mqtt.MQTTProtocolConverter.onMQTTCommand(MQTTProtocolConverter.java:175)
	  at org.apache.activemq.transport.mqtt.MQTTTransportFilter.onCommand(MQTTTransportFilter.java:79)
	  at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
	  at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:214)
	  at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:196)
	  at java.lang.Thread.run(Thread.java:745)
{code}


{code}

ActiveMQ Transport: tcp:///0:0:0:0:0:0:0:1:56106@1883@10055 daemon, prio=5, in group 'main', status: 'RUNNING'
	 blocks ActiveMQ Transport: tcp:///127.0.0.1:56056@61616@9345
	 blocks ActiveMQ BrokerService[localhost] Task-178@8926
	  at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.recoverMessage(AbstractStoreCursor.java:103)
	  at org.apache.activemq.broker.region.cursors.TopicStorePrefetch.recoverMessage(TopicStorePrefetch.java:77)
	  at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.recoverMessage(AbstractStoreCursor.java:85)
	  at org.apache.activemq.store.kahadb.KahaDBStore$KahaDBTopicMessageStore$5.execute(KahaDBStore.java:931)
	  at org.apache.activemq.store.kahadb.disk.page.Transaction.execute(Transaction.java:779)
	  at org.apache.activemq.store.kahadb.KahaDBStore$KahaDBTopicMessageStore.recoverNextMessages(KahaDBStore.java:905)
	  at org.apache.activemq.store.ProxyTopicMessageStore.recoverNextMessages(ProxyTopicMessageStore.java:115)
	  at org.apache.activemq.broker.region.cursors.TopicStorePrefetch.doFillBatch(TopicStorePrefetch.java:113)
	  at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.fillBatch(AbstractStoreCursor.java:277)
	  at org.apache.activemq.broker.region.cursors.AbstractStoreCursor.hasNext(AbstractStoreCursor.java:162)
	  at org.apache.activemq.broker.region.cursors.StoreDurableSubscriberCursor.hasNext(StoreDurableSubscriberCursor.java:255)
	  at org.apache.activemq.broker.region.PrefetchSubscription.dispatchPending(PrefetchSubscription.java:646)
	  at org.apache.activemq.broker.region.DurableTopicSubscription.dispatchPending(DurableTopicSubscription.java:278)
	  at org.apache.activemq.broker.region.PrefetchSubscription.add(PrefetchSubscription.java:161)
	  at org.apache.activemq.broker.region.DurableTopicSubscription.add(DurableTopicSubscription.java:272)
	  at org.apache.activemq.broker.region.policy.SimpleDispatchPolicy.dispatch(SimpleDispatchPolicy.java:48)
	  at org.apache.activemq.broker.region.Topic.dispatch(Topic.java:717)
	  at org.apache.activemq.broker.region.Topic.doMessageSend(Topic.java:510)
	  at org.apache.activemq.broker.region.Topic.send(Topic.java:441)
	  at org.apache.activemq.broker.region.AbstractRegion.send(AbstractRegion.java:424)
	  at org.apache.activemq.broker.region.RegionBroker.send(RegionBroker.java:445)
	  at org.apache.activemq.broker.jmx.ManagedRegionBroker.send(ManagedRegionBroker.java:297)
	  at org.apache.activemq.broker.BrokerFilter.send(BrokerFilter.java:147)
	  at org.apache.activemq.broker.CompositeDestinationBroker.send(CompositeDestinationBroker.java:96)
	  at org.apache.activemq.broker.TransactionBroker.send(TransactionBroker.java:307)
	  at org.apache.activemq.broker.BrokerFilter.send(BrokerFilter.java:147)
	  at org.apache.activemq.broker.MutableBrokerFilter.send(MutableBrokerFilter.java:152)
	  at org.apache.activemq.broker.TransportConnection.processMessage(TransportConnection.java:496)
	  at org.apache.activemq.command.ActiveMQMessage.visit(ActiveMQMessage.java:756)
	  at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:294)
	  at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:148)
	  at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:45)
	  at org.apache.activemq.transport.mqtt.MQTTInactivityMonitor.onCommand(MQTTInactivityMonitor.java:123)
	  at org.apache.activemq.transport.mqtt.MQTTTransportFilter.sendToActiveMQ(MQTTTransportFilter.java:91)
	  at org.apache.activemq.transport.mqtt.MQTTProtocolConverter.sendToActiveMQ(MQTTProtocolConverter.java:132)
	  at org.apache.activemq.transport.mqtt.MQTTProtocolConverter.onMQTTPublish(MQTTProtocolConverter.java:566)
	  at org.apache.activemq.transport.mqtt.MQTTProtocolConverter.onMQTTCommand(MQTTProtocolConverter.java:175)
	  at org.apache.activemq.transport.mqtt.MQTTTransportFilter.onCommand(MQTTTransportFilter.java:79)
	  at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)
	  at org.apache.activemq.transport.tcp.TcpTransport.doRun(TcpTransport.java:214)
	  at org.apache.activemq.transport.tcp.TcpTransport.run(TcpTransport.java:196)
	  at java.lang.Thread.run(Thread.java:745)
{code}
, Hi,

We are using 5.10 broker. 
We saw this error messages. 
[20150107 15:01:52.282 EST (ActiveMQ Transport: tcp:///10.79.26.196:41859@61613) org.apache.activemq.broker.region.cursors.AbstractStoreCursor#recoverMess
age 103 WARN] - TopicStorePrefetch(cas_stuff_price_info_to_jsh,new_golden_price_message) ID:kepler47.com-49402-1420406276070-1:22507:-1:1 - org
.apache.activemq.broker.region.cursors.TopicStorePrefetch@7524aad9:pricing.goldenPriceFeed,batchResetNeeded=false,storeHasMessages=true,size=52586,cacheEn
abled=false,maxBatchSize:200,hasSpace:true - cursor got duplicate: ID:helium16.nyc-49994-1420473196066-5:6690:1:1:4257, 4 

From the source code of org.apache.activemq.broker.region.cursors.AbstractStoreCursor#recoverMess
age it looks like duplicates messages got stored in kahadb for same destination. The topic for which it happend has a durable subscriber. 
I want to understand in what cases kahadb stores duplicate messages ? Is this a bug? 


Thanks,
Anuj
, In our case after getting quite a lot of "cursor got duplicate" in a DQL, we start to observe "Problem retrieving message for browse" error with NullPointerException for that DLQ when trying to browse the queue. Restart helps.

2015-01-14 20:26:00,659 | ERROR | Problem retrieving message for browse | org.apache.activemq.broker.region.Queue | ActiveMQ Broker[dcdng] Scheduler
java.lang.NullPointerException, Bouncing into the same issue on activemq 5.9.0

{code}
JBossFuse:admin@root> list -t 0 | grep active
[ 131] [Active     ] [Created     ] [       ] [   50] activemq-osgi (5.9.0.redhat-610379)
[ 137] [Active     ] [            ] [       ] [   50] activeio-core (3.1.4)
[ 139] [Active     ] [Created     ] [       ] [   50] activemq-karaf (5.9.0.redhat-610379)
[ 207] [Active     ] [            ] [       ] [   60] activemq-camel (5.9.0.redhat-610379)
{code}

{code}
fuseesb.log.2015-04-30.gz:13:15:35,941 | WARN  | 344189615-182566 | AbstractStoreCursor              | 131 - org.apache.activemq.activemq-osgi - 5.9.0.redhat-610379 | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@8ec707b:Pim.Plu.Hybris,batchResetNeeded=false,storeHasMessages=false,size=5,cacheEnabled=true,maxBatchSize:5,hasSpace:true - cursor got duplicate: ID:prd-esb-inno1-37535-1429248180616-5:4:6:1:3, 4
fuseesb.log.2015-04-30.gz:13:15:36,003 | WARN  | 344189615-182566 | AbstractStoreCursor              | 131 - org.apache.activemq.activemq-osgi - 5.9.0.redhat-610379 | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@8ec707b:Pim.Plu.Hybris,batchResetNeeded=false,storeHasMessages=false,size=6,cacheEnabled=true,maxBatchSize:6,hasSpace:true - cursor got duplicate: ID:prd-esb-inno1-37535-1429248180616-5:5:7:1:62, 4
fuseesb.log.2015-04-30.gz:13:15:36,066 | WARN  | 344189615-182566 | AbstractStoreCursor              | 131 - org.apache.activemq.activemq-osgi - 5.9.0.redhat-610379 | org.apache.activemq.broker.region.cursors.QueueStorePrefetch@8ec707b:Pim.Plu.Hybris,batchResetNeeded=false,storeHasMessages=false,size=7,cacheEnabled=true,maxBatchSize:7,hasSpace:true - cursor got duplicate: ID:prd-esb-inno1-60724-1429933192681-1:3:5:1:837, 4
{code}

Occurs both for topics as queues. 

We are using mysql as store, with 2 nodes in failover. , For the durable topic sub case this scenario is captured in https://issues.apache.org/jira/browse/AMQ-6562 and the "cursor got duplicate from store" warn is now suppressed.]