[For some reason these steps did not reproduce the problem on our working servers (I've setup this one separate environment). Additional step "helped": restart any slave between steps 2 and 3 (before restarting masters). Additionally, if you restart 2 slaves one at a time instead of one between steps 2 and 3 then it's enough to restart only one master to reproduce a problem (i.e. step 3 was not required)., I have reported something very similar or exactly the same on issue AMQ-5553 .
On that ticket I have attached my levelDb data and configuration which might help get information about this issue.

This issue leaves my replicas all entering the infinite loop as when each of them becomes the master each enters the infinite loop. I have replicated this issue on 5.10 and 5.11. Artem, you say you have steps that can easily replicate this issue? I have been able to replicate them but seems mostly random.
, Hello Pablo.
The steps outlined above _always_ reproduce the problem for me. The steps in description alone always do it on a separate environment I've setup in Google Compute Engine. And the steps with corrrections in the first comment do the same in testing environment of our internal project. Not sure why the first variant does not "work" there though.

<offtopic>The number and severity of bugs we've encountered when using Replicated LevelDB (f.e. see AMQ-5619, AMQ-5612, AMQ-5611) made us switch to the old solution with JDBC and HA MySQL (Google provides that). Previously we changed to Replicated LevelDB because change of master occurred rather regularly (like once a day) with JDBC and made queue counters reset. But now we'll see that as an improvement upon the current situation - at least there were no cases when store was corrupted beyond repair.</offtopic>, Thanks Artem, I will try to reproduce it again and try to find if there exists a work around or a way to recover. 

Sadly Replicated LevelDB currently is in a completely unusable state as this types of issues are very common and could greatly impact production systems as right now the only way to fix this is to delete all storage data from the cluster and support staff would not be able to handle this situation on time.

I will check if I can get clearer information about this issue and see what I can find out, but my Scala skills are pitiful.

Let me investigate too the alternative you proposed, sounds it could be useful for the moment., Hello Pablo,

Have you had to chance to come up with a work around?


Thx!, Actually no, and my team discarded the usage of Replicated Level DB
But on a certain hope, another team from my company started using Replicated Level DB and until now they haven't seen this issue occur.
They are running 5.13.3  and Java version of LevelDB. I think they have stumbled with other issues related to Zookeeper but not this one.

Important question, is this issue still occurring to you?

Thanks,
Pablo , Hello Pablo,

Yes, we have amq 5.14 and zook 3.4.8. 3 - 3 instances of each with master/slave.
We found the very same repl. leveldb corruption that the ticket reporter had. Our db was corrupted beyond repair., Another data point: My team also encountered this issue recently, in June 2016, while evaluating replicated LevelDB storage for our application. We were using AMQ 5.13.3 and ZK 3.4.8 with 3 instances. Like Jonathan G, our database seemed to be corrupted beyond repair, causing us to eliminate replicated LevelDB as an option., I have done some digging around.. and it seems that it will sooner or later will stop the loop.. eventually at least.
I dont have the setup of Replicated LevelDB at hand right now but from I can see is that is basically lost track about the last position on the records of LevelDB.

From the logs in my case it would try to iterate from 0 to the last position it knows exists. So in my case it would iterate until 32610993430.
The fix from AMQ-5300 seems not to be executed as this replay of the logs of LevelDB is being triggered by a call which bypasses its fix. The fix basically is not to start replaying from 0 but from the last record registered.

{noformat}
2015-01-29 19:42:55,740 -0600 WARN  56426 [ActiveMQ BrokerService[mailsystemBroker] Task-2] LevelDBClient  - Could not load message seq: 162542 from DataLocator(11e952fc, 1754)
2015-01-29 19:42:55,740 -0600 WARN  56426 [ActiveMQ BrokerService[mailsystemBroker] Task-2] RecordLog  - No reader available for position: 11e999ce, log_infos: {32610993430=LogInfo(/m2/tomcat7.0/work/navigator-mail-mq/6009/work/repDB/0000000797c44516.log,32610993430,0)}
2015-01-29 19:42:55,740 -0600 WARN  56426 [ActiveMQ BrokerService[mailsystemBroker] Task-2] LevelDBClient  - Could not load message seq: 162552 from DataLocator(11e999ce, 1754)
2015-01-29 19:42:55,740 -0600 WARN  56426 [ActiveMQ BrokerService[mailsystemBroker] Task-2] RecordLog  - No reader available for position: 11e9a7f8, log_infos: {32610993430=LogInfo(/m2/tomcat7.0/work/navigator-mail-mq/6009/work/repDB/0000000797c44516.log,32610993430,0)}
2015-01-29 19:42:55,740 -0600 WARN  56426 [ActiveMQ BrokerService[mailsystemBroker] Task-2] LevelDBClient  - Could not load message seq: 162554 from DataLocator(11e9a7f8, 1754)
2015-01-29 19:42:55,740 -0600 WARN  56426 [ActiveMQ BrokerService[mailsystemBroker] Task-2] RecordLog  - No reader available for position: 11e9b622, log_infos: {32610993430=LogInfo(/m2/tomcat7.0/work/navigator-mail-mq/6009/work/repDB/0000000797c44516.log,32610993430,0)}
2015-01-29 19:42:55,741 -0600 WARN  56427 [ActiveMQ BrokerService[mailsystemBroker] Task-2] LevelDBClient  - Could not load message seq: 162556 from DataLocator(11e9b622, 1754)
{noformat}

I lost most of my log files from this issue but if someone can attach then if possible at Trace level and if possible with a LevelDB included I may be able to take a look deeper. My Scala is not the best but I think I can help track the root of the issue, We're using version 5.13.4, and have the same issues. We have a three node cluster  using replicatedLevelDB persistence, and sometimes we shutdown one of the nodes to do failover testing, and from time to time, we got the error "  WARN  | No reader available for position: 0" on the master, and then we had to delete all the LevelDB folders on the three nodes to make it work again.

I think it's a critical bug, hopefully someone can look further into it.

Thanks in advance.

, LevelDB has been deprecated and is no longer supported., Hey [~cshannon] could you do me a favor and point me to a place where I can find the discussion in which LevelDB was decided to be deprecated? Mailing List archive or something similar would be great.

Thanks, The thread is here http://activemq.2283324.n4.nabble.com/DISCUSS-LevelDB-deprecation-tp4719227.html
]