[After enabling DEBUG logging, I found the following output after 
* starting the server clean
* wget --no-http-keep-alive -q -O - "http://localhost:8161/api/message/TEST?type=queue&clientId=GETID&readTimeout=1000"
* curl -d "body=message" "http://localhost:8161/api/message/TEST?type=queue"

{code}
2013-12-17 23:28:52,121 | DEBUG | Async client internal exception occurred with no exception listener registered: java.lang.IllegalStateException: IDLE,initial | org.apache.activemq.ActiveMQConnection | ActiveMQ Session Task-1
java.lang.IllegalStateException: IDLE,initial
        at org.eclipse.jetty.server.AsyncContinuation.dispatch(AsyncContinuation.java:408)
        at org.eclipse.jetty.server.AsyncContinuation.resume(AsyncContinuation.java:815)
        at org.apache.activemq.web.MessageServlet$Listener.onMessageAvailable(MessageServlet.java:395)
        at org.apache.activemq.ActiveMQMessageConsumer.dispatch(ActiveMQMessageConsumer.java:1383)
        at org.apache.activemq.ActiveMQSessionExecutor.dispatch(ActiveMQSessionExecutor.java:131)
        at org.apache.activemq.ActiveMQSessionExecutor.iterate(ActiveMQSessionExecutor.java:202)
        at org.apache.activemq.thread.PooledTaskRunner.runTask(PooledTaskRunner.java:129)
        at org.apache.activemq.thread.PooledTaskRunner$1.run(PooledTaskRunner.java:47)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
2013-12-17 23:28:52,121 | DEBUG | Sent! to destination: queue://TEST message: ActiveMQTextMessage {commandId = 0, responseRequired = false, messageId = ID:turtle-52404-1387319318754-3:2:1:1:1, originalDestination = null, originalTransactionId = null, producerId = null, destination = queue://TEST, transactionId = null, expiration = 0, timestamp = 1387319332114, arrival = 0, brokerInTime = 0, brokerOutTime = 0, correlationId = null, replyTo = null, persistent = true, type = null, priority = 5, groupID = null, groupSequence = 0, targetConsumerId = null, compressed = false, userID = null, content = null, marshalledProperties = null, dataStructure = null, redeliveryCounter = 0, size = 0, properties = null, readOnlyProperties = f
{code}

Looks to me that the consuming session from the first wget is still there, in IDLE state -- but still is able to consume the message. Hope this helps -- I can't do any further testing or even debugging without a deeper understanding of ActiveMQ's internal workings., Another thing: Setting the prefetch size to 1 as suggested in [Apache ActiveMQ â„¢ -- REST|http://activemq.apache.org/rest.html] didn't help. There's just one consumer here anyway, I guess., It also seems that sending multiple messages after the timed-out consumption, only the first of those messages is lost.

Here's debug output from the rest servlet (org.apache.activemq.web.MessageServlet).

*get with no messages on queue*
{noformat}
DEBUG | Getting local client [GETID]
DEBUG | Receiving message(s) from: queue://TEST with timeout: 1000
DEBUG | Received 0 message(s)
DEBUG | Getting local client [GETID]
DEBUG | Receiving message(s) from: queue://TEST with timeout: 1000
{noformat}

*send 3 messages, get 3 times*
{noformat}
DEBUG | Sending message to: queue://TEST with text: message
DEBUG | Sending message to: queue://TEST with text: message
DEBUG | Sending message to: queue://TEST with text: message
DEBUG | Getting local client [GETID]
DEBUG | Receiving message(s) from: queue://TEST with timeout: 1000
DEBUG | Received 1 message(s)
DEBUG | Getting local client [GETID]
DEBUG | Receiving message(s) from: queue://TEST with timeout: 1000
DEBUG | Received 1 message(s)
DEBUG | Getting local client [GETID]
DEBUG | Receiving message(s) from: queue://TEST with timeout: 1000
DEBUG | Received 0 message(s)
DEBUG | Getting local client [GETID]
DEBUG | Receiving message(s) from: queue://TEST with timeout: 1000
{noformat}

Note only 2 messages were received., Watching the stats using the web console, the stats update as follows:

|| stat || on normal send || on send after get timed out ||
| Dequeue Counter | no change | +1 |
| Message count waiting acknowledge | +1 | no change |
| Dispatched Counter | +1 | +1 |
| Enqueue Counter | +1 | +1 |

So, it appears the consumer is acknowledging the message immediately.
, Attached is a patch that eliminates the erroneous consumption; the problem is that the continuation defined for the web request times out, but the availableMessageListener for the consumer is never updated with that fact.  So, when the next message comes in, the listener consumes it with the intent that the continuation of the web request will receive it.

I'm going to review this code more carefully for race conditions.  I'm convinced there are races on the use of MessageConsumer's if more than one web request comes in for the same consumer (e.g. using the same clientId).  There may be more, including in this patch., Thank you very much for the fix, I can confirm it works with the above test case. With just one client, no messages are lost.

Here's another test case involving two distinct client IDs, but no actual race:
{code}
#
# Two clients with IDs "GETID-1" and "GETID-2"... both timeout since there are no messages yet
#
wget --no-http-keep-alive -q -O - "http://localhost:8161/api/message/TEST?type=queue&clientId=GETID-1&readTimeout=1000"
wget --no-http-keep-alive -q -O - "http://localhost:8161/api/message/TEST?type=queue&clientId=GETID-2&readTimeout=1000"

# Send a message:
curl -d "body=$((++i))" http://localhost:8161/api/message/TEST?type=queue; echo ": $i"
Message sent

#
# Receive using first client ID, OK
#
wget --no-http-keep-alive -q -O - "http://localhost:8161/api/message/TEST?type=queue&clientId=GETID-1&readTimeout=1000"
1

#
# Send a message again:
#
curl -d "body=$((++i))" http://localhost:8161/api/message/TEST?type=queue; echo ": $i"
Message sent

#
# Receive using first client ID, no response, not OK
#
wget --no-http-keep-alive -q -O - "http://localhost:8161/api/message/TEST?type=queue&clientId=GETID-1&readTimeout=1000"

# ... but with client ID 2:
wget --no-http-keep-alive -q -O - "http://localhost:8161/api/message/TEST?type=queue&clientId=GETID-2&readTimeout=1000"
2
{code}

Seems like round-robin still happens even after a client disconnected because of the read timeout. Hope this helps., That's what I would expect with a prefecth > 0 because even prefetch = 1 means a single message will be dispatched, or pushed, to the client "proactively" - i.e. without any call to the receive method.

Let me look at how prefetch works in the REST client., OK, so bad news - prefetch = 0 leads to web requests that don't appear to timeout.

Prefetch = 1 works as I expected - one of the two clients gets a single message preloaded.  Also tested with prefeth = 2 and it works as expected., So each new client ID creates a new subscription, which remains even after the client disconnects or timeouts and will be dispatched messages if prefetch > 0. Tested this with 3 clients (and without readTimeout), and all messages were distributed equally. Nice.

Unfortunatelly, it's still pretty easy to provoke message loss when using an (admitedly unreasonable) low read Timeout. But what may happen, timing-wise, in an artifical test scenario may eventually happen in production even with natural timeouts. And avoiding message loss is paramount, IMHO.

So here's another one:
{code}
#
# start receiving messages in background with readTimeout=1, clientId "GETID-1"
#
while true; do wget --no-http-keep-alive -q -O - "http://localhost:8161/api/message/TEST?type=queue&clientId=GETID-1&readTimeout=1"; done&
[1] 6040

#
# send some messages -- output is done by background receiver -- no message loss
#
for ((i=1; i<100; i++)); do curl -d "body=$i." http://localhost:8161/api/message/TEST?type=queue; done
1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16.17.18.19.20.21.22.23.24.25.26.27.28.29.30.31.32.33.34.35.36.37.38.39.40.41.42.43.44.45.46.47.48.49.50.51.52.53.54.55.56.57.58.59.60.61.62.63.64.65.66.67.68.69.70.71.72.73.74.75.76.77.78.79.80.81.82.83.84.85.86.87.88.89.90.91.92.93.94.95.96.97.98.99.

#
# create another subscription with clientId "GETID-2" (no messages read)
#
wget --no-http-keep-alive -q -O - "http://localhost:8161/api/message/TEST?type=queue&clientId=GETID-2&readTimeout=1"

#
# try sending messages again -- expect to see every other message, but lots of message loss
#
for ((i=1; i<100; i++)); do curl -d "body=$i." http://localhost:8161/api/message/TEST?type=queue -so /dev/null; done
1.3.7.11.15.23.27.33.39.45.49.53.57.59.61.65.69.75.83.87.89.91.93.97.

# don't forget to stop background job
kill %1
{code}

Maybe it's just some timing issue which just becomes critical when there are two instead of just one subscription. But still, with a server-side timeout, the server should be able to tell if a message was completely delivered to a client or not, despite the timeout., There are several possible desirable outcomes with REST and JMS.  Unfortunately, they are vastly different in their methodology - REST is based on synchronous request/reply and JMS is based on asynchronous passing of messages to consumers.

Consider load balancing in a scenario mixing REST and JMS clients.  The current model, using prefetch to push messages, provides semantics that are more "fair" to the REST client by allowing messages to cache in the internal consumer for that client, just as they do for JMS clients.

With that said, there is no analogous REST scenario for automatically releasing messages when the connection closes because REST always closes the connection after a single message while JMS holds it open indefinitely.

For more advanced use-cases, perhaps camel would be a good tool to intelligently bridge the REST/JMS gap.

BTW, the messages stored in the consumer for REST clients aren't truly lost.  They will sit indefinitely, but once the broker restarts, they will be returned to the broker.  This is actually bad for many reasons since it really means an idle consumer - very bad for JMS in general and activemq in particular.

Perhaps we need a REST call that will force close a consumer, or a REST call that intentionally performs a one-shot message receive (i.e. automatically closes the consumer after the call).  Honestly, I'd prefer to put my effort elsewhere since, as mentioned above, REST for JMS is not a great fit.

Hope this helps.  Let me know your thoughts., That's a great test case.  There is a race condition when a request takes more than 10ms (the minimum timeout) to complete.  It races with the availability of the message.

I'm working on a fix., Attaching a more complete patch that:

* eliminates a race condition in which the servlet receives a message during a continuation at the same time the continuation times-out leading to a lost message
* eliminates another race condition in which a client may block longer than necessary when a message arrives immediately after the initial receive call fails to return a message
* prevents concurrent use of a consumer by throwing an exception on a request to use a consumer when that consumer is already active in another request.

There is still a possible message-loss scenario that can't be fixed without a rework of the protocol with the client.  Once the servlet receives the message and attempts to send it back to the client, if the client loses the connection to the server, that message is lost.  The only 100% reliable solution to that is to push message acknowledgement down to the client, which opens more potential issues., That patch also adds a URL-encoded parameter, oneShot, that when true, will close the consumer on completion of the request.  Like this:

{noformat}
curl --no-keepalive -o message \
	"http://localhost:8161/api/message/TEST?type=queue&clientId=GETID1&readTimeout=1000&oneShot=true"
{noformat}, Thank you for your explanations. I'm content with the prefetch scenario, as long as every message can be reliably consumed using a known set of subscriptions. With the oneShot parameter, semantics are more intuitive though, as there are no lingering subscriptions.

Back to testing: With the second patch, the "low-timeout multiple receivers" testcase as well as the original testcase work perfectly. Even with prefetch=0 there is no message loss. With two concurrent receivers and no prefetch, messages are not distributed equally, but that is as expected. "oneShot" works, too -- even with multiple aggressively polling receivers.

Great work, thank you very much!
, Patch applied with some minor code cleanup.  , Great!  Thanks Timothy.]