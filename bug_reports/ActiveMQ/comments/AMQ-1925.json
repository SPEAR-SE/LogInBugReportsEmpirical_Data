[Consumer memory dump, Consumer thread-dump, Well the "just don't allow this value to become smaller than zero"-approach does not work. Now I've got consumers whose PrefetchSubscription isn't full but who don't get more than X messages anyway. However the change seems to increase X a notch..., I've looked into it again. My first guess wasn't completly off spot, although the client clear it's buffers (Session#clearMessagesInProgress) it still has a message in progress namely the one of the current transaction. So when this message gets ack'd during the recover (the transaction is commited there) the PrefetchSubscription#acknowlegde gets confused.
It's inAckRange algorithm runs wild because ack.getFirstMessageId==null and the lastMessageId isn't in the dispatch-list (with the "old"-master it'd have been the first entry). So what it does is run to the complete dispatch list and ack's everything. This is a very bad move because it in fact deletes every message in the dispatch list permanently (around 100 messages; also from the DB)!!
It even shows the message "Could not correlate acknowledgment with dispatched message", but only at debug level.

Of course the dispatching gets completey out of sync afterwards, since all the messages in the broker's dispatched-list are now gone... so every single ack afterward clears (aka deletes from the database) the whole dispatched-list (mostly around 100 messages). After some ack's

Note that I only see this happening with transactional consumers, only then I get a transaction-ack-replay that then goes wrong. Also note that it does not happen in the current trunk or the 5.1 release because they fail on AMQ-1710 ("transaction X has not been started")
, TestCase showing the problem with lost messages.
The test testAMQ1925_TXInProgress should fail, the other two run fine.

Note: The test does not always fail, only when it gets interrupted during session.commit. This is the slowest call, so it mostly happens, but there's no guarantee., part-patch for the issue.
Acks are now checked to match the dispatch list, if not an exception is thrown (but unfortunatly only logged, cannot get the clients commit to fail).

Consequences:
- messages get no longer delete arbitary (very good)
- can cause some dup-delivery (since it cannot let commit fail)
- some messages may become "hidden" for the consumers that did failover. New consumers can see the messages.

The patch also contains an updated version of the test-case, Patch applied in SVN revision 693915., I think the change to isFull() to add in isSlave() in PrefetchSubscription[1] is problematic.

In pure master slave, it means that the dispatch list does not get populated in the slave and hence the acks result in errors. 
To see the problem, check out the log4j logging in activemq-core/target/test.log that results from:
   mvn test -Dtest=MasterSlaveTempQueueMemoryTest

For the slave is is fine to let dispatch happen to fill the dispatchList, the actual doDispatch call is gated on isSlave() so the does not actually go to a consumer but is available to the ack. 

What as the reason for the change, is it that the stats or counters are off?

[1] http://svn.apache.org/viewvc/activemq/trunk/activemq-core/src/main/java/org/apache/activemq/broker/region/PrefetchSubscription.java?rev=693915&r1=693914&r2=693915&view=diff, My patch didn't contain the line in question and does not depend on it (in fact I did all the tests with it the line as commited in 692539).
So I suspect it's just a merge problem., ok, did not notice that it was not in your patch, will follow up with rob and revert the change. thanks for the heads up., reverted change to PrefetchSubscription.isFull and updated a test case to catch this in the future. this is full was not part of the original patch, 

r694679.]