[Path file for Unit Test Reproducing the issue., The attached unit test can be used to demonstrate two issues. If you set PRODUCER_COUNT=1 and CONSUMER_COUNT=1, and change the producer message size to 1300 bytes. You can see that this will indeed produce a hang since the consumer's prefetch is 1000 and less than number fit into the queue. 

However, using the test as is unmodified (e.g. 50 producers, and a message size of 1024 doesn't result in a hang, but does result in a serious performance bottleneck. There is some sort of contention happening in the broker. This is the behavior that I was seeing in my original performance runs ... simply a sever slow down. , After some changes committed by Rob and Dejan in 5.4 trunk, the 50_1_1 usecase is better than before. However, setting the pending queue policy to vm results in the old buggy behavior, which is problematic for those thar don't wish to page messages to disk:

{code:title=Modify setup to include vm pending cursor,java}
entry.setPendingQueuePolicy(new VMPendingQueueMessageStoragePolicy());
{code}, I think the safest bet will be to change the client so that he acks more frequently at least in a queue dups ok case., Attaching patch which changes the DUPS_OK queue consumer behavior to match that of AUTO_ACK.  The topic should still behave as before., I added the test case to the build.. slightly modified so that it would exacerbate the DUPS_OK problem to get a quicker failure.
rev: 818496
The fix for the test case went in as rev: 818487]