[Before we can enjoy the improvement,is the optimization acknowledge option for consumers able to rescue the poor performance b/w of loading messages one by one from the store?, Unfortunately not, as memory is released one message at the time even in that case (which triggers new store batch and basically just load one message), allowing a batch load from the store to exceed the memory limit may be the simplest approach. The batch size is already configurable so the excess is controllable. In essence, once we go to the store we always load a full batch if we can so we don't check the usage on each message recovery., Fixed with svn revision 1478823

The solution Gary mentioned has been implemented, basically allowing us to exceed a memory limit a bit for the more optimal store reads., there is only 2% performance improvement in my observation., I have tried to add a testcase to verify the improvement, but I can only see a little effect.
It would be better you can take a look at the testcase and give it a try to see the effect., More concurrence tests indicate load on the database server decreases noticeably than the original design when the memory usage of queues is exhausted.
Seems we can not see the dispatch speed increases while the load of the store server is lower than before., There is a problem with this change. Going past the highWaterMark can have a negative influence on other consumers and destinations leading to the inability to page messages for dispatch.
This is a general problem for a shared limit but it is made worse with this fix.
Consider destinations that has stopped caching at 70% usage, so that
flow control does not kick in and producers can send directly to disk.
If the cache is less than the pageSize this is particularly problematic.
As messages page in, the usage goes past 100% and now producers will be flow controlled, rather than still going directly to disk. That is not what we want.

I am thinking that paging in for dispatch should respect the highWaterMark, so it is not safe to just "always page in a full page", Commit d8cf54b0a9eee4b86db1ffef2cb3dd1171067307 in activemq's branch refs/heads/master from [~gtully]
[ https://git-wip-us.apache.org/repos/asf?p=activemq.git;h=d8cf54b ]

https://issues.apache.org/jira/browse/AMQ-4495 - revisit. Reinstate check for space on pagein, so that highWaterMark is respected and full state is not reached, hense pfc is not triggered in error
, [~gtully],

Two questions/comments,

First, with this new commit, I assume that means there will be reduced performance again? Some of the brokers I've run are on machines with relatively slow disk performance so that is a potential concern.

Second, do you think your new change might reduce potential OOM errors?  I've seen out of memory problems occasionally even though proper usage limits are set and I've always thought that maybe that had something to do with the fact that paging in on dispatch could load more than 100% of the usage into memory., [~cshannon] yes to both.

It was oom that brought me back, then i saw the producer flow control (pfc) use case and was convinced.

The original perf issue is a case where a consumer takes a page and as the acks come back, the page is empty so we go to the store, take in one message (as we again hit our limit (b/c the prefetched messages are not acked yet) and dispatch it. Then repeat.

I think the pref issue can be addressed with some taking account of the prefetch, but it may be tricky.

I am now thinking that we need a high and low water mark. using the low for caching on the producer side, and using the high for page in. That is sort of what we have with the high and full mark at the moment. so maybe there is no need to change much.
however the full trigger is what causes pfc to kick in so it is not isolated.
Thinking now, the pfc check could be conditional on the cursor caching messages. That may separate the concerns some which would help simplify.
Need to investigate that a bit. Thanks for your eyes on this, it is a tricky area, dispatch on memory limits. Not sure there is a perfect answer but I think it can be improved., The second use case here is starvation, simplest when there is a shared memory limit, so just a broker usage limit.
Send to q1, it stops caching at < 70%. Send to q2, it stops caching immediately, same for q3.
Consume from q2, you get to page in some messages, brings you possibly > 70%, consume from q3, you get nothing, till q2 or q1 gets some acks.

This is all expected, but with the usage based on %, and varying message size it is very random., Commit 13ec9949397848c57653845b35e8003f8c490ebd in activemq's branch refs/heads/master from [~gtully]
[ https://git-wip-us.apache.org/repos/asf?p=activemq.git;h=13ec994 ]

Revert "https://issues.apache.org/jira/browse/AMQ-4495 - revisit. Reinstate check for space on pagein, so that highWaterMark is respected and full state is not reached, hense pfc is not triggered in error"

This reverts commit d8cf54b0a9eee4b86db1ffef2cb3dd1171067307.
, I have not found a straight forward way to deal with performance issue so I am reverting. Going to tackle the flow control issue on it own., On review - i have retained the status quo. The performance implications of going to the store for a batch (page) of messages and dropping them due to no space is too high.

Using a destination limit of X, and a cursorHighWaterMark of 70, only if usage is < 0.7*X will we page in from the store. If we page In, it will be a pageSize and we will accept the full page if a full page of messages exists. This may cause usage to increase past X. However we won't go back to page in till usage is again < 0.7*X.
The 0.7 is the cursorHighWaterMark and X is the memoryUsage (via policy entries). PageSize is also configurable.

If per destinations limits are not set, then the global shared usage is available to all destinations which can lead to starvation of consumers due to the inability to page in because usage > 0.7*X due to messages in memory for other destinations. Their consumers may have dropped off for example.

To divide a shared usage between N destinations:
 0.7*X will be used for caching if the cache is enabled (useCache=true). 
Then be prepared for potential increase of pageSize*messageSize usage when we page in from the store for dispatch. This happens when the cache is exhausted and usage drops below 0.7*X. Ideally, the 0.3*X that remains for pageIn is < pageSize*messageSize. 
For N destinations, the fraction available to each destinations needs to account for the above.
This will ensure that the global shared usage is respected so the JVM heap metrics can be sized sensibly.

Mitigation:
lazyDispatch - where we only page in what can be consumed can help limit the pageSize dynamically if consumers are exact about their prefetch limit or use pull consumers. With 3 pull consumers, we will only pageIn 3 messages if lazyDispatch=true, [~cshannon]
{quote}
 I've seen out of memory problems occasionally even though proper usage limits are set 
{quote}

This bug could also be related to what you observed [ENTMQ-1526|https://issues.jboss.org/browse/ENTMQ-1526]., Yes, this is another issue too. The size computation is currently an approximation as you pointed out in that ticket.  Another thing that helped in the queue case with OOM errors was setting the reduceMemoryFootprint flag to true.]