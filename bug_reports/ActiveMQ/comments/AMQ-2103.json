[attaching memory usage diagram showing how the message data can be duplicated., attaching jhat heap analysis showing ActiveMQTextMessage object with duplicate data., attaching jhat heap analysis showing ActiveMQTextMessage->ByteSequence object holding the byte array data., attaching jhat heap analysis showing ActiveMQTextMessage->ByteSequence->byte array object holding the message data., attaching patch that clears ActiveMQTextMessage 'text' field data when marshaling., I also forgot to mention that this probably only affects use cases with something like vm transport where the message stays internal to the same JVM and is not marshaled across the wire., attaching heap usage graph comparing memory before and after applying patch , Patch applied in SVN revision 743258, Rob did you look at all to see if this affects other message types (i.e. ActiveMQObjectMessage, ActiveMQMapMessage)?  I believe some of these these suffer from the same problem., This fix breaks the topic case with multiple consumers and a vm transport producer. There is contention over marshaling so whacking the marshaled state is not possible, results in missing content for some of the consumers. https://issues.apache.org/activemq/browse/AMQ-2966

Think the fix is to have an reduceMemoryFootprint policy option that when enabled for queues, will clear the marshaled state (can work for map and object messages also) when the message is first persisted. This is a natural sync point that is contention free provided the concurrentStoreAndDispatchQueue KahaDB option that tries to optimize out persistence, is not enabled.

, patch reverted and new policy based solution applied in r1021466

The patch, setting the text attribute to null, was unprotected. Currently there are no destructive operations, on a message, allowing it to be shared across topic consumers without synchronization, which allows concurrent dispatch to be fast. This is important to maintain.

A new destination policy for queues, boolean attribute reduceMemoryFootprint has been introduced to do meet this use case.
When true, the marshaled state of a message will be cleared once the message is persisted in the store. This is a natural, contention free sync point.
Note:  The kahaDB concurrentStoreAndForwardQueues option (which introduces concurrency) and the memory persistence adapter (which will not marshall) are not compatible with this new policy.

Trevor, can you validate that this change meet your use case?, This fix looks like it will work but I do have some concerns implementing it as a destination policy. Wouldn't it make more sense to implement this using SoftReferences or a hook into the MarshallAware interface?  In my opinion it makes sense to clear memory at the discretion of the collector when the message has been completely serialized to disk when there is not enough memory. The actual problem I was trying to solve was to prevent "hidden" memory from hanging around that was not tied to the ActiveMQ memory usage policies. I could be missing something here but it seems there are disjoint philosophies for memory management related to overall broker memory usage, fast dispatch cursors and other unaccounted for "live" objects. I'm not sure what to do here but the proposed fix (including my original submission) seems like a kludge at best. Does anyone else have other opinions?, I think there is merit in the original use case, but soft references would not cut it as the messages are retained in memory by the cursors and there is duplication. The key issue with any destruction of a message in memory is that it requires a sync point. A general MarshallAware interface hook would require a sync on message that would have a large impact on scaleability of topics.

I agree, that there needs to be some tie with available vm memory and cursor memory usage, but that sort of scheme would require major surgery atm.

Memory management is something that is central to the amq 6.0 (apollo) architecture. , resolving. 
addressing any further limitations to the memory usage tracking as it pertains to actual jvm usage will be punted to 6.0]