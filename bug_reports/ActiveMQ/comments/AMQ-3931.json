[you have in single transaction of 300,000 text messages with average size of ~10,000 bytes. If they are all in memory at the same time, it would require 300,000 x 10,000 = 3G. You have only 1 g allocated for the broker?
<memoryUsage>                                                                                                    
     <memoryUsage limit="1 gb"/>                                                                                 
</memoryUsage>                    , Do you know if this is a configuration option of ActiveMQ, where it can be told to serialize messages participating in a transaction to disk while more messages for that transaction are coming in?  This would allow it to free up memory for the rest of the incoming messages.  Or is this just a feature/limitation in how ActiveMQ works?, Following links should give shine some light on your issue. I see a lot of queues in your configurations, but not memoryLimit imposed on any queues.

http://activemq.apache.org/javalangoutofmemory.html
http://activemq.apache.org/per-destination-policies.html
http://activemq.apache.org/message-cursors.html, Thanks Fengming, I went through all that documentation and did make some changes.  What we're still seeing is that when producing even 100,000 messages in a single transaction, they all seem to be kept in memory until (presumably) the transaction is committed.  I have verified that producing the same number of messages in multiple transactions, it behaves as expected, and doesn't kill the broker.  It is able to GC periodically and things hum along great.

This seems to all be related to producing that many messages in one transaction., This is working as designed. pending messages in a TX is kept in memory. 

As you have excessive number of messages, make sure your broker have sufficient memory to deal with this requirement., See also this ticket. KahaDB has a special option for overflow to disk for TX
https://issues.apache.org/jira/browse/AMQ-3374]