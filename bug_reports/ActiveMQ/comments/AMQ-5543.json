[Commenting that we just saw the same thing with 5.10.0 and was indeed driven by hitting the max process limit and the inability to create new native threads.   It manifested as an OutOfMemory error (which was confusing during our troubleshooting process) and then was followed up by the behavior you describe of extremely high volume of the "Timer already cancelled" errors., [~jchristner], thanks for confirming the sequence of events that led to the issue.  

I think the broker should be more resilient in the face of running out of threads (which is usually a transient problem that will eventually clear itself up), but I think the priority for making that happen is probably pretty low, so I don't expect this to be implemented anytime soon., Just faced same problem on 5.11.1.

Logs are full of:
XXXX-XX-XX XX:XX:32,416 | WARN  | Transport Connection to: tcp://127.0.0.1:49807 failed: java.io.IOException: Timer already cancelled. | org.apache.activemq.broker.TransportConnection.Transport | ActiveMQ NIO Worker 92939

Before it happened we've seen in logs:

        INFO   | jvm 1    | XXXXXXXXXXXXXXX:21 |  WARN | Transport Connection to: tcp://XXXXXXXXXXXX:35328 failed: java.io.IOException: unable to create new native thread
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:24 | Exception in thread "ActiveMQ InactivityMonitor WriteCheckTimer" java.lang.OutOfMemoryError: unable to create new native thread
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:24 |       at java.lang.Thread.start0(Native Method)
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:24 |       at java.lang.Thread.start(Thread.java:714)
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:24 |       at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:24 |       at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:25 |       at org.apache.activemq.transport.AbstractInactivityMonitor.writeCheck(AbstractInactivityMonitor.java:158)
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:25 |       at org.apache.activemq.transport.AbstractInactivityMonitor$2.run(AbstractInactivityMonitor.java:122)
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:25 |       at org.apache.activemq.thread.SchedulerTimerTask.run(SchedulerTimerTask.java:33)
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:25 |       at java.util.TimerThread.mainLoop(Timer.java:555)
        INFO   | jvm 1    | XXXXXXXXXXXXXXX:25 |       at java.util.TimerThread.run(Timer.java:505)

Then only "Timer already cancelled".

Anyway it is quite strange we run out of native threads as we have set: org.apache.activemq.UseDedicatedTaskRunner=false which I believe should optimise the number of threads.

We are using about 300-700 parallel connections., [~nannou9] What's your current thread limit?  Even with org.apache.activemq.UseDedicatedTaskRunner=false, you're still going to use one thread per connection, so if your OS has the default value of 1024 set, I'm not at all surprised that you're hitting this even with org.apache.activemq.UseDedicatedTaskRunner=false.  Increase the max thread count for the user and you should be fine., Indeed, good spot. VM/OS was not preconfigured with that property.

Thanks Tim!, We've seen this very same issue. Unable to create native thread due to ulimit setting, then broker gets into bad state and sulks for hours, slamming the door in the face of every client that's trying to connect. On the client this is witnessed by sockets being closed from the remote end, which looks like potential external interference at first, but when you look at the broker log it's clear that it's the broker resetting these sockets.

I vote for making the broker more resilient against this type of problem, since hitting a ulimit and thus not being able to create a new pool worker or some such shouldn't throw the entire broker into the abyss. Here's the original OOM reported in the JVM's stdout.log:
{noformat}
INFO   | jvm 1    | 2015/06/08 05:30:26 | WARNING: RMI TCP Accept-0: accept loop for ServerSocket[addr=localhost/127.0.0.1,localport=42882] throws
INFO   | jvm 1    | 2015/06/08 05:30:26 | java.lang.OutOfMemoryError: unable to create new native thread
INFO   | jvm 1    | 2015/06/08 05:30:26 | 	at java.lang.Thread.start0(Native Method)
INFO   | jvm 1    | 2015/06/08 05:30:26 | 	at java.lang.Thread.start(Thread.java:714)
INFO   | jvm 1    | 2015/06/08 05:30:26 | 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
INFO   | jvm 1    | 2015/06/08 05:30:26 | 	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
INFO   | jvm 1    | 2015/06/08 05:30:26 | 	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:414)
INFO   | jvm 1    | 2015/06/08 05:30:26 | 	at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:371)
INFO   | jvm 1    | 2015/06/08 05:30:26 | 	at java.lang.Thread.run(Thread.java:745)
{noformat}
And this next OOM is what starts the "timer already cancelled" messages in the activeMQ log:
{noformat}
INFO   | jvm 1    | 2015/06/08 05:39:34 | Exception in thread "ActiveMQ InactivityMonitor WriteCheckTimer" java.lang.OutOfMemoryError: unable to create new native thread
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at java.lang.Thread.start0(Native Method)
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at java.lang.Thread.start(Thread.java:714)
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at org.apache.activemq.transport.AbstractInactivityMonitor.writeCheck(AbstractInactivityMonitor.java:158)
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at org.apache.activemq.transport.AbstractInactivityMonitor$2.run(AbstractInactivityMonitor.java:122)
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at org.apache.activemq.thread.SchedulerTimerTask.run(SchedulerTimerTask.java:33)
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at java.util.TimerThread.mainLoop(Timer.java:555)
INFO   | jvm 1    | 2015/06/08 05:39:34 | 	at java.util.TimerThread.run(Timer.java:505)
{noformat}

So basically it's the InactivityMonitor getting into a bad state. If that fails to create a new thread we should just disable the monitor entirely - there's certainly no stability to be gained by locking everyone out! At minimum if the broker gets into such a bad state it needs to shut itself down and either restart or relinquish its lock, so another broker can start somewhere else., We've seen this issue in 5.9.1 and had to implement the workaround by increasing the thread limit., In general increasing Thread Limit is a bad idea.
In big deployments most of threads can be taken by connections, when you use CACHE_NONE, which sometimes is default value for AMQ connection cache.
Just try to use CACHE_CONSUMER (non-XA) or CACHE_CONNECTION (XA) altogether with SingleConneectionPool.
So now you can have 10 concurrent consumers with 10 destinations, using max 1 connection instead of max 100 + publishers.
In case of XA environment, please make sure your transaction manager supports CACHE_CONNECTION.

Apache Aries (open source) and Atomikos Extreme Transactions (commercial) both support them very well.
But don't even try to use CACHE_CONNECTION with Atomikos Essentials (open source)- it will fail ugly.

Thanks to above i've noticed huge reduction in amount of connections from about 500 to about 25 and started threads, improving significantly environment stability.
Number of started threads/day dropped from tens of millions a day to tens of thousands.

Everybody who are experiencing problems as in this issue, should try above approach 1st before increasing os limits.
There is nothing good in having so many threads.

Cheers, Are there any risks associated with using the CACHE_CONSUMER with the Single Connection Pool?, AMQ-5251 fixes the issue, Hmm, while that's nice to hear it's very non-obvious that the two issues are even related.]