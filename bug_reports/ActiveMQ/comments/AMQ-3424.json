[Hi,
The directory size is continuously growing in production. Can someone please let me know on this.

Regards
Hari, This was a known issue that was fixed in the 5.5 release (and verified by many reporting it). The only other reason I have found for log files not to be removed is stale durable subscribers.

To isolate the problem you can switch on tracing of the vacuuming process. This should verify whether kahadb "knows" about the logs you can see and if so, whether it is making a conscious decision not to delete them.

Read http://activemq.2283324.n4.nabble.com/KahaDB-storage-size-grows-despite-almost-no-pending-messages-td3573894.html for more information., Thanks James. I'll try this out., I tried using the logger and this is all I see getting logged in kahadb.log.

{code:xml}
 2011-08-02 00:45:24,601 [eckpoint Worker] TRACE MessageDatabase                - Last update: 1:144374, full gc candidates set: [1]
 2011-08-02 00:45:24,601 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after first tx:1:144374, []
 2011-08-02 00:45:24,601 [eckpoint Worker] TRACE MessageDatabase                - gc candidates: []
 2011-08-02 00:45:24,601 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint done.
 2011-08-02 00:45:29,602 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint started.
 2011-08-02 00:45:29,604 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint done.
 2011-08-02 00:45:34,605 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint started.
 2011-08-02 00:45:34,607 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint done.
 2011-08-02 00:45:39,607 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint started.
 2011-08-02 00:45:39,611 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint done.
 2011-08-02 00:45:44,611 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint started.
 2011-08-02 00:45:44,615 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint done.
 2011-08-02 00:45:49,615 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint started.
 2011-08-02 00:45:49,619 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint done.
 2011-08-02 00:45:54,620 [eckpoint Worker] DEBUG MessageDatabase                - Checkpoint started.
 2011-08-02 00:45:54,623 [eckpoint Worker] TRACE MessageDatabase                - Last update: 1:155030, full gc candidates set: [1]
 2011-08-02 00:45:54,623 [eckpoint Worker] TRACE MessageDatabase                - gc candidates after first tx:1:155030, []
 2011-08-02 00:45:54,623 [eckpoint Worker] TRACE MessageDatabase                - gc candidates: []
{code}

So I tried recreating this scenario using test cases. Attached are the test cases of producer and consumer.
Below is the snapshot of the data dir.

{code:xml}
> du -h /activemq_data_kahadb/ ; ls -l /activemq_data_kahadb/
864K    /activemq_data_kahadb/
total 868
-rw-rw-r-- 1 manih manih 9961472 Aug  2 08:26 db-1.log
-rw-rw-r-- 1 manih manih  405504 Aug  2 08:26 db.data
-rw-rw-r-- 1 manih manih  352936 Aug  2 08:26 db.redo
-rw-rw-r-- 1 manih manih       0 Aug  2 08:24 lock

> du -h /activemq_data_kahadb/ ; ls -l /activemq_data_kahadb/
1.1M    /activemq_data_kahadb/
total 1088
-rw-rw-r-- 1 manih manih 9961472 Aug  2 08:28 db-1.log
-rw-rw-r-- 1 manih manih  405504 Aug  2 08:28 db.data
-rw-rw-r-- 1 manih manih  352936 Aug  2 08:28 db.redo
-rw-rw-r-- 1 manih manih       0 Aug  2 08:24 lock

> du -h /activemq_data_kahadb/ ; ls -l /activemq_data_kahadb/
1.3M    /activemq_data_kahadb/
total 1284
-rw-rw-r-- 1 manih manih 9961472 Aug  2 08:29 db-1.log
-rw-rw-r-- 1 manih manih  405504 Aug  2 08:29 db.data
-rw-rw-r-- 1 manih manih  352936 Aug  2 08:29 db.redo
-rw-rw-r-- 1 manih manih       0 Aug  2 08:24 lock
{code}

What I see here is that the file sizes are remaining same while the dir size keeps growing. 

Can you please help me understand where the data is going? 
Please let me know if there is any thing wrong in what am trying to do., Attaching the test cases., The files were not getting deleted as they had not reached the journalMaxFileLength I had specified. I tried the test cases with a smaller value and it worked. 

5.5 is working just fine. Closing this issue.]