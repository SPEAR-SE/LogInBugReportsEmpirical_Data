{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [],
        "created": "2017-09-07T02:28:09.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yangjiandan&avatarId=32648",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yangjiandan&avatarId=32648",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yangjiandan&avatarId=32648",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=yangjiandan&avatarId=32648"
            },
            "displayName": "Jiandan Yang ",
            "key": "yangjiandan",
            "name": "yangjiandan",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=yangjiandan",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2017-09-07T16:46:59.309+0000",
        "customfield_12310222": null,
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "1.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i3jqrz:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Fri Sep 08 01:55:41 UTC 2017",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "In our cluster, when found NodeManager frequently FullGC when decommissioning NodeManager, and we found the biggest object is dataQueue of DataStreamer, it has almost 6w DFSPacket, and every DFSPacket is about 64k, as shown below.\nThe root reason is that the size of dataQueue and ackQueue in DataStreamer has no limit when writer thread is interrupted.\nDataStreamer#waitAndQueuePacket does not wait when writer thread is interrupted. I know NodeManager may stop writing when interruped, but DFSOutputStream also could do something to avoid Infinite growth of dataQueue.\n\n{code:java}\nwhile (!streamerClosed && dataQueue.size() + ackQueue.size() >\n              dfsClient.getConf().getWriteMaxPackets()) {\n            if (firstWait) {\n              Span span = Tracer.getCurrentSpan();\n              if (span != null) {\n                span.addTimelineAnnotation(\"dataQueue.wait\");\n              }\n              firstWait = false;\n            }\n            try {\n              dataQueue.wait();\n            } catch (InterruptedException e) {\n              // If we get interrupted while waiting to queue data, we still need to get rid\n              // of the current packet. This is because we have an invariant that if\n              // currentPacket gets full, it will get queued before the next writeChunk.\n              //\n              // Rather than wait around for space in the queue, we should instead try to\n              // return to the caller as soon as possible, even though we slightly overrun\n              // the MAX_PACKETS length.\n              Thread.currentThread().interrupt();  \n              break;\n            }\n          }\n        } finally {\n          Span span = Tracer.getCurrentSpan();\n          if ((span != null) && (!firstWait)) {\n            span.addTimelineAnnotation(\"end.wait\");\n          }\n        }\n{code}\n\n!mat.jpg|memory_analysis!\n",
        "duedate": null,
        "environment": null,
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yangjiandan&avatarId=32648",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yangjiandan&avatarId=32648",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yangjiandan&avatarId=32648",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=yangjiandan&avatarId=32648"
            },
            "displayName": "Jiandan Yang ",
            "key": "yangjiandan",
            "name": "yangjiandan",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=yangjiandan",
            "timeZone": "Etc/UTC"
        },
        "resolution": null,
        "resolutiondate": null,
        "status": {
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "id": "1",
            "name": "Open",
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "statusCategory": {
                "colorName": "blue-gray",
                "id": 2,
                "key": "new",
                "name": "To Do",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2"
            }
        },
        "subtasks": [],
        "summary": "The size of dataQueue and ackQueue in DataStreamer has no limit when writer thread is interrupted",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2017-09-08T01:55:41.000+0000",
        "versions": [],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-12403/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-12403/watchers",
            "watchCount": 3
        },
        "workratio": -1
    },
    "id": "13100347",
    "key": "HDFS-12403",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13100347"
}