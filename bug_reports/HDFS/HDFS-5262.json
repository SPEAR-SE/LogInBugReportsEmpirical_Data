{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [
            {
                "id": "12312927",
                "name": "datanode",
                "self": "https://issues.apache.org/jira/rest/api/2/component/12312927"
            },
            {
                "id": "12312928",
                "name": "hdfs-client",
                "self": "https://issues.apache.org/jira/rest/api/2/component/12312928"
            }
        ],
        "created": "2013-09-25T21:57:22.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Arun Suresh",
            "key": "asuresh",
            "name": "asuresh",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=asuresh",
            "timeZone": "America/Los_Angeles"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2013-09-26T00:34:38.462+0000",
        "customfield_12310222": null,
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "350414",
        "customfield_12310920": "350707",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i1of3r:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Sep 26 04:10:44 UTC 2013",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "We noticed that when we set the value of 'dfs.client.socket-timeout' to 0, \nand start the HBase regionserver in the same node as the Datanode, we have a situation where the Datanode heap size just blows up in a very short span of time.\n\nA jmap histogram of the live objects in the datanode yields this :\n\n{noformat}\n~/hbase_debug]$ head jmap.histo\n\n num     #instances         #bytes  class name\n----------------------------------------------\n   1:      46054779     1842191160  org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet\n   2:      46054878     1105317072  java.util.LinkedList$Entry\n....\n....\n{noformat}\n\nand again after a couple of seconds :\n\n{noformat}\n~/hbase_debug]$ head jmap2.histo\n\n num     #instances         #bytes  class name\n----------------------------------------------\n   1:      50504594     2020183760  org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet\n   2:      50504693     1212112632  java.util.LinkedList$Entry\n....\n....\n{noformat}\n\nWe also see a very high rate of minor GCs happening and untimately, full GCs start with pause times of around 10 - 15 secs and this keeps increasing..\n\nIt looks like entries are being pushed into a linkedlist very rapidly and thus are not eligible for GC\n\nOn enabling debug logging for the DFS client and hadoop ipc on the HBase regionserver this is what we see :\n{noformat}\n2013-09-24 20:53:10,485 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC Server handler 23 on 60020: has #26 from 192.168.0.67:33790\n2013-09-24 20:53:10,485 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC Server handler 23 on 60020: call #26 executing as NULL principal\n2013-09-24 20:53:10,485 DEBUG org.apache.hadoop.ipc.HBaseServer.trace: Call #26; Served: HRegionInterface#get queueTime=0 processingTime=0 contents=1 Get, 9 bytes\n2013-09-24 20:53:10,486 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC Server Responder: responding to #26 from 192.168.0.67:33790\n2013-09-24 20:53:10,486 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC Server Responder: responding to #26 from 192.168.0.67:33790 Wrote 140 bytes.\n2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:0 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 326\n2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0\n2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0\n2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0\n2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0\n2013-09-24 20:53:10,524 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0\n2013-09-24 20:53:10,524 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0\n....\n....\n....\n{noformat}\n\nThe last line keeps repeating.. and the LOG files run into 100s of MBs really fast..\n\nMy assumption was that the HBase region server creates an hlog file at startup.. which it keeps open by sending a heartbeat (-1 seqno) packet... But we were stumped as to why packets were sent at this alarming rate.\n\nLooking at the DFSOutputstream code, it looks like there is a section inside the DataStreamer class where the 'dfs.client.socket-timeout' is being used :\n\n{code}\n..\n....\n            while ((!streamerClosed && !hasError && dfsClient.clientRunning \n                && dataQueue.size() == 0 && \n                (stage != BlockConstructionStage.DATA_STREAMING || \n                 stage == BlockConstructionStage.DATA_STREAMING && \n                 now - lastPacket < dfsClient.getConf().socketTimeout/2)) || doSleep ) {\n              long timeout = dfsClient.getConf().socketTimeout/2 - (now-lastPacket);\n              timeout = timeout <= 0 ? 1000 : timeout;\n              timeout = (stage == BlockConstructionStage.DATA_STREAMING)?\n                 timeout : 1000;\n              try {\n                dataQueue.wait(timeout);\n              } catch (InterruptedException  e) {\n              }\n              doSleep = false;\n              now = Time.now();\n            }\n...\n..\n{code}\n\nWe see that this code path is never traversed and thus Datastreamer thread keeps sending packets without any delay...\n\nFurther more, on going thru the DataStreamer code, it looks like once the DataStreamer starts sending heartbeat packets, there is no code path that checks to see if there is any valid data in the dataQueue.. except the above piece... \n\nwhich implies that unless the absolute value of 'now - lastPacket' is less than 'dfs.client.socket-timeout', the client would hang...\n\nShouldnt there be a timed 'dataQueue.wait()' in each loop of the DataStreamer irrespective of the value of this parameter ?\n\nKindly do provide comments..\n\n\n\n",
        "duedate": null,
        "environment": "CentOS 6.2",
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Arun Suresh",
            "key": "asuresh",
            "name": "asuresh",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=asuresh",
            "timeZone": "America/Los_Angeles"
        },
        "resolution": null,
        "resolutiondate": null,
        "status": {
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "id": "1",
            "name": "Open",
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "statusCategory": {
                "colorName": "blue-gray",
                "id": 2,
                "key": "new",
                "name": "To Do",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2"
            }
        },
        "subtasks": [],
        "summary": "HDFS Datanode goes out of memory and HBase Regionserver hangs when dfs.client.socket-timeout=0",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2013-09-26T04:10:44.000+0000",
        "versions": [{
            "archived": false,
            "description": "maintenance release on branch-2.0-alpha",
            "id": "12324428",
            "name": "2.0.5-alpha",
            "releaseDate": "2013-06-06",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12324428"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-5262/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-5262/watchers",
            "watchCount": 8
        },
        "workratio": -1
    },
    "id": "12670585",
    "key": "HDFS-5262",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12670585"
}