{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12416913","self":"https://issues.apache.org/jira/rest/api/2/issue/12416913","key":"HDFS-99","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2009-03-23T06:14:12.321+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Nov 17 03:44:52 UTC 2009","customfield_12310420":"17128","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_21311686001_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2009-11-17T03:44:52.527+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-99/watchers","watchCount":3,"isWatching":false},"created":"2009-03-15T11:50:06.526+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12323931","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12323931","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"inwardIssue":{"id":"12419195","key":"HDFS-96","self":"https://issues.apache.org/jira/rest/api/2/issue/12419195","fields":{"summary":"HDFS does not support blocks greater than 2GB","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2009-11-17T03:44:52.467+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"my motivation for trying large dfs.block.size was to enforce locality for large blobs in the multi-gigabyte size range. these are files that must be processed in one step and not broken up into records as with typical map/reduce workflow.\n\ni tried \"-put\" then \"-cat\" for a 1.6 gb file and it worked fine, but\nwhen trying it on a 16.4 gb file (\"bigfile.dat\"), i get the following\nerrors (see below). i got this failure both times i tried it, each\nwith a fresh install of single-node 0.19.1. i set block size to\n32 gb with larger buffer and checksum sizes in config (see below as\nwell)\n\n---\n\n<configuration>\n<property>\n <name>dfs.block.size</name>\n <value>34359738368</value>\n <description>The default block size for new files.</description>\n</property>\n<property>\n <name>io.file.buffer.size</name>\n <value>65536</value>\n <description>The size of buffer for use in sequence files.\n The size of this buffer should probably be a multiple of hardware\n page size (4096 on Intel x86), and it determines how much data is\n buffered during read and write operations.</description>\n</property>\n<property>\n <name>io.bytes.per.checksum</name>\n <value>4096</value>\n <description>The number of bytes per checksum.  Must not be larger than\n io.file.buffer.size.</description>\n</property><property>\n   <name>fs.default.name</name>\n   <value>hdfs://localhost:9000</value>\n </property>\n <property>\n   <name>mapred.job.tracker</name>\n   <value>localhost:9001</value>\n </property>\n <property>\n   <name>dfs.replication</name>\n   <value>1</value>\n</property>\n</configuration>\n\n\n[mra@... hadoop-0.19.1]$ bin/hadoop fs -put /tmp/bigfile.dat /\n[mra@... hadoop-0.19.1]$ bin/hadoop fs -cat /bigfile.dat | md5sum\n09/03/14 15:52:34 WARN hdfs.DFSClient: Exception while reading from\nblk_-4992364814640383286_1013 of /bigfile.dat from 127.0.0.1:50010:\njava.io.IOException: BlockReader: error in packet header(chunkOffset :\n415956992, dataLen : 41284, seqno : 0 (last: -1))\n       at org.apache.hadoop.hdfs.DFSClient$BlockReader.readChunk(DFSClient.java:1186)\n       at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:238)\n       at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:190)\n       at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:159)\n       at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1060)\n       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1615)\n       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1665)\n       at java.io.DataInputStream.read(DataInputStream.java:83)\n       at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:53)\n       at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:85)\n       at org.apache.hadoop.fs.FsShell.printToStdout(FsShell.java:120)\n       at org.apache.hadoop.fs.FsShell.access$100(FsShell.java:49)\n       at org.apache.hadoop.fs.FsShell$1.process(FsShell.java:351)\n       at org.apache.hadoop.fs.FsShell$DelayedExceptionThrowing.globAndProcess(FsShell.java:1872)\n       at org.apache.hadoop.fs.FsShell.cat(FsShell.java:345)\n       at org.apache.hadoop.fs.FsShell.doall(FsShell.java:1519)\n       at org.apache.hadoop.fs.FsShell.run(FsShell.java:1735)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n       at org.apache.hadoop.fs.FsShell.main(FsShell.java:1854)\n\n09/03/14 15:52:34 INFO hdfs.DFSClient: Could not obtain block\nblk_-4992364814640383286_1013 from any node:  java.io.IOException: No\nlive nodes contain current block\n09/03/14 15:52:37 WARN hdfs.DFSClient: Exception while reading from\nblk_-4992364814640383286_1013 of /bigfile.dat from 127.0.0.1:50010:\njava.io.IOException: BlockReader: error in packet header(chunkOffset :\n415956992, dataLen : 41284, seqno : 0 (last: -1))\n       at org.apache.hadoop.hdfs.DFSClient$BlockReader.readChunk(DFSClient.java:1186)\n       at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:238)\n       at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:190)\n       at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:159)\n       at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1060)\n       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1615)\n       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1665)\n       at java.io.DataInputStream.read(DataInputStream.java:83)\n       at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:53)\n       at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:85)\n       at org.apache.hadoop.fs.FsShell.printToStdout(FsShell.java:120)\n       at org.apache.hadoop.fs.FsShell.access$100(FsShell.java:49)\n       at org.apache.hadoop.fs.FsShell$1.process(FsShell.java:351)\n       at org.apache.hadoop.fs.FsShell$DelayedExceptionThrowing.globAndProcess(FsShell.java:1872)\n       at org.apache.hadoop.fs.FsShell.cat(FsShell.java:345)\n       at org.apache.hadoop.fs.FsShell.doall(FsShell.java:1519)\n       at org.apache.hadoop.fs.FsShell.run(FsShell.java:1735)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n       at org.apache.hadoop.fs.FsShell.main(FsShell.java:1854)\n\n09/03/14 15:52:37 INFO hdfs.DFSClient: Could not obtain block\nblk_-4992364814640383286_1013 from any node:  java.io.IOException: No\nlive nodes contain current block\n09/03/14 15:52:40 WARN hdfs.DFSClient: DFS Read: java.io.IOException:\nBlockReader: error in packet header(chunkOffset : 415956992, dataLen :\n41284, seqno : 0 (last: -1))\n       at org.apache.hadoop.hdfs.DFSClient$BlockReader.readChunk(DFSClient.java:1186)\n       at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:238)\n       at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:190)\n       at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:159)\n       at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1060)\n       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1615)\n       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1665)\n       at java.io.DataInputStream.read(DataInputStream.java:83)\n       at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:53)\n       at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:85)\n       at org.apache.hadoop.fs.FsShell.printToStdout(FsShell.java:120)\n       at org.apache.hadoop.fs.FsShell.access$100(FsShell.java:49)\n       at org.apache.hadoop.fs.FsShell$1.process(FsShell.java:351)\n       at org.apache.hadoop.fs.FsShell$DelayedExceptionThrowing.globAndProcess(FsShell.java:1872)\n       at org.apache.hadoop.fs.FsShell.cat(FsShell.java:345)\n       at org.apache.hadoop.fs.FsShell.doall(FsShell.java:1519)\n       at org.apache.hadoop.fs.FsShell.run(FsShell.java:1735)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n       at org.apache.hadoop.fs.FsShell.main(FsShell.java:1854)\n\ncat: BlockReader: error in packet header(chunkOffset : 415956992,\ndataLen : 41284, seqno : 0 (last: -1))\nef8033a70b6691c2b99ad1c74583161a  -\n[mra@... hadoop-0.19.1]$\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"108349","customfield_12312823":null,"summary":"large dfs.block.size doesn't work","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mike.andrews","name":"mike.andrews","key":"mike.andrews","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"mike andrews","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mike.andrews","name":"mike.andrews","key":"mike.andrews","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"mike andrews","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Linux ... 2.6.27.12-170.2.5.fc10.x86_64 #1 SMP Wed Jan 21 01:33:24 EST 2009 x86_64 x86_64 x86_64 GNU/Linux\njava version \"1.6.0_12\"\nJava(TM) SE Runtime Environment (build 1.6.0_12-b04)\nJava HotSpot(TM) 64-Bit Server VM (build 11.2-b01, mixed mode)\n","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12416913/comment/12688220","id":"12688220","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"I did not find this JIRA in my search earlier, so I went ahead and created HADOOP-5552. I will close this JIRA and try to follow up with a fix in HADOOP-5552.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-03-23T06:14:12.321+0000","updated":"2009-03-23T06:14:12.321+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12416913/comment/12778709","id":"12778709","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Closing (dupe of HADOOP-5552).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-11-17T03:44:52.448+0000","updated":"2009-11-17T03:44:52.448+0000"}],"maxResults":2,"total":2,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-99/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0iwfr:"}}