{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12711463","self":"https://issues.apache.org/jira/rest/api/2/issue/12711463","key":"HDFS-6311","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-05-01T00:27:09.222+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Dec 24 08:43:33 UTC 2015","customfield_12310420":"389784","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6311/watchers","watchCount":4,"isWatching":false},"created":"2014-04-30T13:48:09.705+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326143","id":"12326143","description":"2.4.0 release","name":"2.4.0","archived":false,"released":true,"releaseDate":"2014-04-07"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12326696","id":"12326696","description":"2.4.1 bug-fix release","name":"2.4.1","archived":false,"released":true,"releaseDate":"2014-06-30"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12327181","id":"12327181","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xbings","name":"xbings","key":"xbings","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"chenbing","active":true,"timeZone":"Etc/UTC"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-12-24T09:10:51.805+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312916","id":"12312916","name":"test"}],"timeoriginalestimate":null,"description":"I'm testing HDFS 2.4.0 \n\nApache Hadoop HDFS                    : Tests run: 2650, Failures: 2, Errors: 2, Skipped: 99\n\nI have the following error each time I launch my tests (3 tries).\n\nForking command line: /bin/sh -c cd /home/tony/HADOOP/hadoop-2.4.0-src/hadoop-hdfs-project/hadoop-hdfs && /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError -jar /home/tony/HADOOP/hadoop-2.4.0-src/hadoop-hdfs-project/hadoop-hdfs/target/surefire/surefirebooter2355654085353142996.jar /home/tony/HADOOP/hadoop-2.4.0-src/hadoop-hdfs-project/hadoop-hdfs/target/surefire/surefire983005167523288650tmp /home/tony/HADOOP/hadoop-2.4.0-src/hadoop-hdfs-project/hadoop-hdfs/target/surefire/surefire_4328161716955453811297tmp\n\nRunning org.apache.hadoop.hdfs.TestLargeBlock\n\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 16.011 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestLargeBlock\ntestLargeBlockSize(org.apache.hadoop.hdfs.TestLargeBlock)  Time elapsed: 15.549 sec  <<< ERROR!\n\norg.apache.hadoop.ipc.RemoteException: File /tmp/TestLargeBlock/2147484160.dat could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1430)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2684)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:584)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2008)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1410)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy16.addBlock(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at com.sun.proxy.$Proxy16.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:361)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1439)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1261)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"390026","customfield_12312823":null,"summary":"TestLargeBlock#testLargeBlockSize : File /tmp/TestLargeBlock/2147484160.dat could only be replicated to 0 nodes instead of minReplication (=1)","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=trex58","name":"trex58","key":"trex58","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tony Reix","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=trex58","name":"trex58","key":"trex58","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tony Reix","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Virtual Box - Ubuntu 14.04 - x86_64\nVMWorkstation -Centos 6.5-x86_64","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12711463/comment/13986239","id":"13986239","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"The test passes for me and also appears to be passing in Jenkins. You may want to look through the log file.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-05-01T00:27:09.222+0000","updated":"2014-05-01T00:27:09.222+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12711463/comment/14237735","id":"14237735","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=trex58","name":"trex58","key":"trex58","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tony Reix","active":true,"timeZone":"Etc/UTC"},"body":"I'm still having issue with this test, now with Hadoop 2.4.1 .\n\nAs shown below, the test gives different results depending on the environment. Using IBM JVM or Oracle JVM does not seem to be the root cause.\nIt succeedes on Ubuntu/x86_64, but generates (2 different) errors on RHEL7/x86_64 and on RHEL7/PPC64BE and Ubuntu/PPC64LE.\n\nAre the Jenkins tests done with Ubuntu or RHEL7 ? If Ubuntu, that may explain why you have a success. That would be worth to test in a different environment.\n\n\n\n- RHEL7/x86_64 with Oracle JVM 1.7.0_67-b01 :\nRunning org.apache.hadoop.hdfs.TestLargeBlock\n  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.986 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestLargeBlock\n  testLargeBlockSize(org.apache.hadoop.hdfs.TestLargeBlock)  Time elapsed: 5.658 sec  <<< ERROR!\n  org.apache.hadoop.ipc.RemoteException: File /tmp/TestLargeBlock/2147484160.dat could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n  at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1441)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2702)\n  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:584)\n\n- RHEL7/x86_64 with IBM JVM 1.7 :\nRunning org.apache.hadoop.hdfs.TestLargeBlock\n  Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.203 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestLargeBlock\n  testLargeBlockSize(org.apache.hadoop.hdfs.TestLargeBlock)  Time elapsed: 6.611 sec  <<< ERROR!\n  org.apache.hadoop.ipc.RemoteException: File /tmp/TestLargeBlock/2147484160.dat could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.\n  at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1441)\n  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2702)\n  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:584)\n\n- Ubuntu/x86_64 with Oracle JVM 1.7.0_71-b14 :\n Running org.apache.hadoop.hdfs.TestLargeBlock\n  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 124.537 sec - in org.apache.hadoop.hdfs.TestLargeBlock\n\n- Ubuntu/x86_64 with IBM JVM 1.7 :\n  Running org.apache.hadoop.hdfs.TestLargeBlock\n  Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 155.485 sec - in org.apache.hadoop.hdfs.TestLargeBlock\n\n- RHEL7/PPC64 with IBM JVM 1.7 :\nRunning org.apache.hadoop.hdfs.TestLargeBlock\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 55.318 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestLargeBlock\ntestLargeBlockSize(org.apache.hadoop.hdfs.TestLargeBlock)  Time elapsed: 55.024 sec  <<< ERROR!\njava.io.IOException: Premature EOF reading from org.apache.hadoop.net.SocketInputStream@e0e03454\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:260)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:209)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:171)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:173)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)\n        at org.apache.hadoop.hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:683)\n        at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:739)\n        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:796)\n\n- Ubuntu/PPC64LE with  IBM JVM 1.7 :\nRunning org.apache.hadoop.hdfs.TestLargeBlock\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 71.982 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestLargeBlock\ntestLargeBlockSize(org.apache.hadoop.hdfs.TestLargeBlock)  Time elapsed: 71.714 sec  <<< ERROR!\njava.io.IOException: Premature EOF reading from org.apache.hadoop.net.SocketInputStream@50d72ee2\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:260)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:209)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:171)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:173)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)\n        at org.apache.hadoop.hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:683)\n        at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:739)\n        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:796)\n        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)\n        at java.io.DataInputStream.readFully(DataInputStream.java:207)\n        at org.apache.hadoop.hdfs.TestLargeBlock.checkFullFile(TestLargeBlock.java:133)\n        at org.apache.hadoop.hdfs.TestLargeBlock.runTest(TestLargeBlock.java:207)\n        at org.apache.hadoop.hdfs.TestLargeBlock.testLargeBlockSize(TestLargeBlock.java:166)\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=trex58","name":"trex58","key":"trex58","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Tony Reix","active":true,"timeZone":"Etc/UTC"},"created":"2014-12-08T10:32:21.198+0000","updated":"2014-12-08T10:32:21.198+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12711463/comment/15070760","id":"15070760","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xbings","name":"xbings","key":"xbings","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"chenbing","active":true,"timeZone":"Etc/UTC"},"body":"my test is still get Exception ,I use Hadoop-2.6.0 on centos6.5,jdk1.7.0_60 in vmworkstation.Could you had any idea?\n\n\n\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /tmp/logsplit/tmporiginal/normal/127.0.0.1/1450740790794.0 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and no node(s) are excluded in this operation.\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy14.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)\n        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy15.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588)\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xbings","name":"xbings","key":"xbings","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"chenbing","active":true,"timeZone":"Etc/UTC"},"created":"2015-12-24T08:43:33.250+0000","updated":"2015-12-24T08:43:33.250+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6311/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1v51z:"}}