{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12629297","self":"https://issues.apache.org/jira/rest/api/2/issue/12629297","key":"HDFS-4442","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2013-01-25T18:26:31.353+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Dec 05 23:10:32 UTC 2013","customfield_12310420":"309392","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_27149876541_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2013-12-05T23:10:32.787+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-4442/watchers","watchCount":7,"isWatching":false},"created":"2013-01-25T17:32:36.309+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["datanode","hdfs"],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12379568","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12379568","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12616476","key":"HDFS-4201","self":"https://issues.apache.org/jira/rest/api/2/issue/12616476","fields":{"summary":"NPE in BPServiceActor#sendHeartBeat","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-07-28T14:04:53.303+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"}],"timeoriginalestimate":null,"description":"(Note: Some of the message are similar to HDFS-4201)\n\nJust after i created a new HDFS cluster, and this time using Cloudera nightly RPM hadoop-hdfs-datanode-2.0.0+898-1.cdh4.2.0.p0.939.el6.x86_64, HDFS datanodes were unable to initialize or store anything. It stays alive, but keeps logging exceptions every few seconds.\n\nIt was \"Initialization failed for block pool Block pool (...)\" \"org.apache.hadoop.util.DiskChecker$DiskErrorException: Invalid volume failure  config value: 1\" and then repeatedly \"Exception in BPOfferService for Block pool (...)\"\n\nMy config was :\n\n<property>\n <name>dfs.datanode.data.dir</name>\n <value>file:///opt/hadoop/dn1/data</value>\n</property>\n\nAfter a bit of tweaking, it worked once i added a second EBS volume to the node. Yet it does not explain the initial error. A bug ?\n\n<property>\n <name>dfs.datanode.data.dir</name>\n <value>file:///opt/hadoop/dn1/data,file:///opt/hadoop/dn2/data</value>\n</property>\n\n\nOriginal exceptions:\n{code}\n(...)\n2013-01-25 15:04:28,573 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1342054845-10.118.50.25-1359125000145 directory /opt/hadoop/dn1/data/current/BP-1342054845-10.118.50.25-1359125000145/current\n2013-01-25 15:04:28,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1786416716;bpid=BP-1342054845-10.118.50.25-1359125000145;lv=-40;nsInfo=lv=-40;cid=CID-3c2cfe5f-da56-4115-90db-81e06c14bc50;nsid=1786416716;c=0;bpid=BP-1342054845-10.118.50.25-1359125000145\n2013-01-25 15:04:28,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-404982471-10.194.189.193-50010-1359126268221) service to namenode2.somedomain.com/10.2.118.169:8020 beginning handshake with NN\n2013-01-25 15:04:28,605 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-404982471-10.194.189.193-50010-1359126268221) service to namenode1.somedomain.com/10.118.50.25:8020\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Invalid volume failure  config value: 1\n    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:182)\n    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)\n    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)\n    at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:910)\n    at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:872)\n    at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:308)\n    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:218)\n    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)\n    at java.lang.Thread.run(Unknown Source)\n2013-01-25 15:04:28,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-404982471-10.194.189.193-50010-1359126268221) service to namenode2.somedomain.com/10.2.118.169:8020 successfully registered with NN\n2013-01-25 15:04:28,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode namenode2.somedomain.com/10.2.118.169:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n2013-01-25 15:04:28,864 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in BPOfferService for Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-404982471-10.194.189.193-50010-1359126268221) service to namenode2.somedomain.com/10.2.118.169:8020\njava.lang.NullPointerException\n    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:435)\n    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:521)\n    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)\n    at java.lang.Thread.run(Unknown Source)\n2013-01-25 15:04:28,864 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-404982471-10.194.189.193-50010-1359126268221) service to namenode1.somedomain.com/10.118.50.25:8020\n2013-01-25 15:04:33,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode namenode2.somedomain.com/10.2.118.169:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n2013-01-25 15:04:33,864 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in BPOfferService for Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-404982471-10.194.189.193-50010-1359126268221) service to namenode2.somedomain.com/10.2.118.169:8020\njava.lang.NullPointerException\n    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:435)\n    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:521)\n    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)\n    at java.lang.Thread.run(Unknown Source)\n(...)\n{code}\n\nAfter adding the second volume (and deleted the first one too):\n\n{code}\n(...)\n2013-01-25 17:29:18,394 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop/dn1/data/in_use.lock acquired by nodename 22999@datanode1.somedomain.com\n2013-01-25 17:29:18,396 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /opt/hadoop/dn1/data is not formatted\n2013-01-25 17:29:18,396 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...\n2013-01-25 17:29:18,646 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop/dn2/data/in_use.lock acquired by nodename 22999@datanode1.somedomain.com\n2013-01-25 17:29:18,646 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /opt/hadoop/dn2/data is not formatted\n2013-01-25 17:29:18,646 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...\n2013-01-25 17:29:19,509 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2013-01-25 17:29:19,510 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /opt/hadoop/dn1/data/current/BP-1342054845-10.118.50.25-1359125000145 is not formatted.\n2013-01-25 17:29:19,510 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...\n2013-01-25 17:29:19,510 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1342054845-10.118.50.25-1359125000145 directory /opt/hadoop/dn1/data/current/BP-1342054845-10.118.50.25-1359125000145/current\n2013-01-25 17:29:19,519 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2013-01-25 17:29:19,519 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /opt/hadoop/dn2/data/current/BP-1342054845-10.118.50.25-1359125000145 is not formatted.\n2013-01-25 17:29:19,519 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...\n2013-01-25 17:29:19,519 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1342054845-10.118.50.25-1359125000145 directory /opt/hadoop/dn2/data/current/BP-1342054845-10.118.50.25-1359125000145/current\n2013-01-25 17:29:19,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1786416716;bpid=BP-1342054845-10.118.50.25-1359125000145;lv=-40;nsInfo=lv=-40;cid=CID-3c2cfe5f-da56-4115-90db-81e06c14bc50;nsid=1786416716;c=0;bpid=BP-1342054845-10.118.50.25-1359125000145\n2013-01-25 17:29:19,548 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /opt/hadoop/dn1/data/current\n2013-01-25 17:29:19,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /opt/hadoop/dn2/data/current\n2013-01-25 17:29:19,645 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean\n2013-01-25 17:29:19,655 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1359139242655 with interval 21600000\n2013-01-25 17:29:19,656 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1342054845-10.118.50.25-1359125000145\n2013-01-25 17:29:19,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-26684760-10.194.189.193-50010-1359134958661) service to namenode1.somedomain.com/10.118.50.25:8020 beginning handshake with NN\n2013-01-25 17:29:19,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-26684760-10.194.189.193-50010-1359134958661) service to namenode2.somedomain.com/10.2.118.169:8020 beginning handshake with NN\n2013-01-25 17:29:19,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-26684760-10.194.189.193-50010-1359134958661) service to namenode1.somedomain.com/10.118.50.25:8020 successfully registered with NN\n2013-01-25 17:29:19,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode namenode1.somedomain.com/10.118.50.25:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n2013-01-25 17:29:19,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-26684760-10.194.189.193-50010-1359134958661) service to namenode2.somedomain.com/10.2.118.169:8020 successfully registered with NN\n2013-01-25 17:29:19,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode namenode2.somedomain.com/10.2.118.169:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n2013-01-25 17:29:19,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-26684760-10.194.189.193-50010-1359134958661) service to namenode1.somedomain.com/10.118.50.25:8020 trying to claim ACTIVE state with txid=165\n2013-01-25 17:29:19,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1342054845-10.118.50.25-1359125000145 (storage id DS-26684760-10.194.189.193-50010-1359134958661) service to namenode1.somedomain.com/10.118.50.25:8020\n2013-01-25 17:29:19,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockReport of 0 blocks took 0 msec to generate and 39 msecs for RPC and NN processing\n2013-01-25 17:29:19,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: sent block report, processed command:null\n2013-01-25 17:29:19,935 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1342054845-10.118.50.25-1359125000145\n2013-01-25 17:29:19,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: BlockReport of 0 blocks took 1 msec to generate and 50 msecs for RPC and NN processing\n2013-01-25 17:29:19,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@11082823\n2013-01-25 17:29:20,033 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1342054845-10.118.50.25-1359125000145 to blockPoolScannerMap, new size=1\n(...)\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"295561","customfield_12312823":null,"summary":"Initialization failed for block (...) Invalid volume failure  config value: 1","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alexfo","name":"alexfo","key":"alexfo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alexandre Fouché","active":true,"timeZone":"Europe/Paris"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alexfo","name":"alexfo","key":"alexfo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alexandre Fouché","active":true,"timeZone":"Europe/Paris"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Amazon Linux (Centos 6), Cloudera nightly RPMs","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12629297/comment/13562893","id":"13562893","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"[~alexfo] When creating jiras, please keep the description brief and add the details in a subsequent comment.\n\nHave you duplicated the problem on Apache releases? If not, please follow up on CDH mailing lists. Usually a corresponding Apache jira is created, if needed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-01-25T18:26:31.353+0000","updated":"2013-01-25T18:26:31.353+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12629297/comment/13562894","id":"13562894","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"I will close this jira if you are not able to produce this on Apache releases.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-01-25T18:27:57.547+0000","updated":"2013-01-25T18:27:57.547+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12629297/comment/13562920","id":"13562920","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"The error can certainly be improved but what its telling you is that you have 1 disk configured but you also have {{dfs.datanode.failed.volumes.tolerated}} set to 1. Toleration value of disk failures should be strictly less than the total number of configured {{dfs.datanode.data.dir}} values, for a DN cannot run disk-less.\n\nThis isn't a bug, but we'd gladly accept a patch to improve the error message. Would you be interested in submitting one yourself? We have steps outlined for Apache Hadoop contributions at http://wiki.apache.org/hadoop/HowToContribute, and you can ping the dev lists if you have further questions in the process.\n\nLike Suresh has already pointed it out to you, in future, make sure to file JIRAs on Apache JIRA here only if you are sure it is also an Apache Hadoop release issue and not a CDH-specfic issue. For CDH specific issues, you should file a JIRA on their own bug trackers (link: https://issues.cloudera.org/browse/DISTRO) or ping its lists for clarification.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2013-01-25T19:14:15.926+0000","updated":"2013-01-25T19:14:15.926+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12629297/comment/13563066","id":"13563066","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~qwertymaniac] for the pointers to log the jira at the right place.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-01-25T21:38:13.154+0000","updated":"2013-01-25T21:38:13.154+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12629297/comment/13563637","id":"13563637","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"see also http://wiki.apache.org/hadoop/InvalidJiraIssues","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2013-01-26T21:17:36.720+0000","updated":"2013-01-26T21:17:36.720+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12629297/comment/13564098","id":"13564098","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alexfo","name":"alexfo","key":"alexfo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alexandre Fouché","active":true,"timeZone":"Europe/Paris"},"body":"Ok, understood. Thank you for the tolerated volumes thing to know\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alexfo","name":"alexfo","key":"alexfo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alexandre Fouché","active":true,"timeZone":"Europe/Paris"},"created":"2013-01-28T07:31:11.564+0000","updated":"2013-01-28T07:31:11.564+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12629297/comment/13840677","id":"13840677","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"body":"Yes, that's a duplicate of HDFS-4201. Let me close this one and work on HDFS-4201.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jxiang","name":"jxiang","key":"jxiang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jimmy Xiang","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-12-05T23:10:32.815+0000","updated":"2013-12-05T23:10:32.815+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-4442/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1eyrj:"}}