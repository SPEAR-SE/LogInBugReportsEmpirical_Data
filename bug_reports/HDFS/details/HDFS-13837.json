{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13180046","self":"https://issues.apache.org/jira/rest/api/2/issue/13180046","key":"HDFS-13837","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12342772","id":"12342772","description":"","name":"3.2.0","archived":false,"released":false}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2018-08-21T21:28:09.751+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Aug 28 21:23:36 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_82272112_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_614006610","customfield_12312321":null,"resolutiondate":"2018-08-28T20:57:52.537+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-13837/watchers","watchCount":5,"isWatching":false},"created":"2018-08-20T19:33:13.866+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"5.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12342772","id":"12342772","description":"","name":"3.2.0","archived":false,"released":false}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-08-28T21:23:36.208+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12329603","id":"12329603","name":"hdfs"}],"timeoriginalestimate":null,"description":"Stack Trace :\r\n{noformat} \r\njava.lang.AssertionError\r\n at org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClient(TestDistributedFileSystem.java:449)\r\n{noformat}\r\n Â Stdout:\r\n\r\n{noformat}\r\n[truncated]kmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3385)) - Number of blocks being written = 0\r\n 2018-07-31 21:42:46,675 [Reconstruction Queue Initializer] INFO hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3388)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec\r\n 2018-07-31 21:42:46,676 [IPC Server Responder] INFO ipc.Server (Server.java:run(1307)) - IPC Server Responder: starting\r\n 2018-07-31 21:42:46,676 [IPC Server listener on port1] INFO ipc.Server (Server.java:run(1146)) - IPC Server listener on port1: starting\r\n 2018-07-31 21:42:46,678 [main] INFO namenode.NameNode (NameNode.java:startCommonServices(831)) - NameNode RPC up at: localhost/x.x.x.x:port1\r\n 2018-07-31 21:42:46,678 [main] INFO namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1230)) - Starting services required for active state\r\n 2018-07-31 21:42:46,678 [main] INFO namenode.FSDirectory (FSDirectory.java:updateCountForQuota(758)) - Initializing quota with 4 thread(s)\r\n 2018-07-31 21:42:46,679 [main] INFO namenode.FSDirectory (FSDirectory.java:updateCountForQuota(767)) - Quota initialization completed in 0 milliseconds\r\n name space=1\r\n storage space=0\r\n storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0\r\n 2018-07-31 21:42:46,682 [CacheReplicationMonitor(1111752355)] INFO blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds\r\n 2018-07-31 21:42:46,686 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1599)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK][file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2|file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2]\r\n 2018-07-31 21:42:46,687 [main] INFO checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(122)) - Scheduling a check for [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1\r\n 2018-07-31 21:42:46,687 [main] INFO checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(122)) - Scheduling a check for [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2\r\n 2018-07-31 21:42:46,695 [main] INFO impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)\r\n 2018-07-31 21:42:46,695 [main] INFO common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n 2018-07-31 21:42:46,695 [main] INFO datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576\r\n 2018-07-31 21:42:46,696 [main] INFO datanode.DataNode (DataNode.java:<init>(496)) - Configured hostname is x.x.x.x\r\n 2018-07-31 21:42:46,696 [main] INFO common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n 2018-07-31 21:42:46,696 [main] INFO datanode.DataNode (DataNode.java:startDataNode(1385)) - Starting DataNode with maxLockedMemory = 0\r\n 2018-07-31 21:42:46,697 [main] INFO datanode.DataNode (DataNode.java:initDataXceiver(1142)) - Opened streaming server at /x.x.x.x:port2\r\n 2018-07-31 21:42:46,697 [main] INFO datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s\r\n 2018-07-31 21:42:46,697 [main] INFO datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50\r\n 2018-07-31 21:42:46,699 [main] INFO server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n 2018-07-31 21:42:46,699 [main] INFO http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined\r\n 2018-07-31 21:42:46,700 [main] INFO http.HttpServer2 (HttpServer2.java:addGlobalFilter(923)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n 2018-07-31 21:42:46,701 [main] INFO http.HttpServer2 (HttpServer2.java:addFilter(896)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\r\n 2018-07-31 21:42:46,701 [main] INFO http.HttpServer2 (HttpServer2.java:addFilter(906)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n 2018-07-31 21:42:46,701 [main] INFO http.HttpServer2 (HttpServer2.java:addFilter(906)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n 2018-07-31 21:42:46,702 [main] INFO http.HttpServer2 (HttpServer2.java:bindListener(1123)) - Jetty bound to port 35341\r\n 2018-07-31 21:42:46,702 [main] INFO server.Server (Server.java:doStart(346)) - jetty-9.3.20.v20170531\r\n 2018-07-31 21:42:46,704 [main] INFO handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@38b5f25\r\n\r\n{/logs,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}\r\n\r\n2018-07-31 21:42:46,705 [main] INFO handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@67594471\r\n\r\n{/static,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test-classes/webapps/static/,AVAILABLE}\r\n\r\n2018-07-31 21:42:46,711 [main] INFO handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@68fe48d7\r\n\r\n{/,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test-classes/webapps/datanode/,AVAILABLE} \\{/datanode}\r\n 2018-07-31 21:42:46,712 [main] INFO server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@379ce046\\{HTTP/1.1,[http/1.1]}{localhost:35341}\r\n 2018-07-31 21:42:46,712 [main] INFO server.Server (Server.java:doStart(414)) - Started @27029ms\r\n 2018-07-31 21:42:46,715 [main] INFO web.DatanodeHttpServer (DatanodeHttpServer.java:start(239)) - Listening HTTP traffic on /x.x.x.x:port3\r\n 2018-07-31 21:42:46,716 [main] INFO datanode.DataNode (DataNode.java:startDataNode(1412)) - dnUserName = jenkins\r\n 2018-07-31 21:42:46,716 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@10acd6] INFO util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor\r\n 2018-07-31 21:42:46,716 [main] INFO datanode.DataNode (DataNode.java:startDataNode(1413)) - supergroup = supergroup\r\n 2018-07-31 21:42:46,717 [main] INFO ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\r\n 2018-07-31 21:42:46,718 [Socket Reader #1 for port port4|#1 for port port4] INFO ipc.Server (Server.java:run(1067)) - Starting Socket Reader #1 for port port4\r\n 2018-07-31 21:42:46,719 [main] INFO datanode.DataNode (DataNode.java:initIpcServer(1029)) - Opened IPC server at /x.x.x.x:port4\r\n 2018-07-31 21:42:46,721 [main] INFO datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null\r\n 2018-07-31 21:42:46,721 [main] INFO datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(198)) - Starting BPOfferServices for nameservices: <default>\r\n 2018-07-31 21:42:46,722 [Thread-2206] INFO datanode.DataNode (BPServiceActor.java:run(809)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/x.x.x.x:port1 starting to offer service\r\n 2018-07-31 21:42:46,724 [IPC Server Responder] INFO ipc.Server (Server.java:run(1307)) - IPC Server Responder: starting\r\n 2018-07-31 21:42:46,726 [IPC Server listener on port4] INFO ipc.Server (Server.java:run(1146)) - IPC Server listener on port4: starting\r\n 2018-07-31 21:42:46,729 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1599)) - Starting DataNode 1 with dfs.datanode.data.dir: [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3,[DISK][file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4|file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4]\r\n 2018-07-31 21:42:46,730 [Thread-2206] INFO datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/x.x.x.x:port1\r\n 2018-07-31 21:42:46,730 [main] INFO checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(122)) - Scheduling a check for [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3\r\n 2018-07-31 21:42:46,731 [main] INFO checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(122)) - Scheduling a check for [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4\r\n 2018-07-31 21:42:46,732 [Thread-2206] INFO common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(346)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)\r\n 2018-07-31 21:42:46,735 [Thread-2206] INFO common.Storage (Storage.java:tryLock(847)) - Lock on /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 7074@quasar-ztplxb-1.vpc.cloudera.com\r\n 2018-07-31 21:42:46,736 [Thread-2206] INFO common.Storage (DataStorage.java:loadStorageDirectory(273)) - Storage directory with location [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 1027244560. Formatting...\r\n 2018-07-31 21:42:46,736 [Thread-2206] INFO common.Storage (DataStorage.java:createStorageID(150)) - Generated new storageID DS-1d19d47a-b472-463a-a1ed-43be073676f5 for directory /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1\r\n 2018-07-31 21:42:46,740 [main] INFO impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)\r\n 2018-07-31 21:42:46,740 [main] INFO common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n 2018-07-31 21:42:46,740 [main] INFO datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576\r\n 2018-07-31 21:42:46,740 [main] INFO datanode.DataNode (DataNode.java:<init>(496)) - Configured hostname is x.x.x.x\r\n 2018-07-31 21:42:46,741 [main] INFO common.Util (Util.java:isDiskStatsEnabled(395)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n 2018-07-31 21:42:46,741 [main] INFO datanode.DataNode (DataNode.java:startDataNode(1385)) - Starting DataNode with maxLockedMemory = 0\r\n 2018-07-31 21:42:46,741 [main] INFO datanode.DataNode (DataNode.java:initDataXceiver(1142)) - Opened streaming server at /x.x.x.x:port5\r\n 2018-07-31 21:42:46,742 [main] INFO datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s\r\n 2018-07-31 21:42:46,742 [main] INFO datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50\r\n 2018-07-31 21:42:46,744 [main] INFO server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n 2018-07-31 21:42:46,744 [Thread-2206] INFO common.Storage (Storage.java:tryLock(847)) - Lock on /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 7074@quasar-ztplxb-1.vpc.cloudera.com\r\n 2018-07-31 21:42:46,744 [main] INFO http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined\r\n 2018-07-31 21:42:46,744 [Thread-2206] INFO common.Storage (DataStorage.java:loadStorageDirectory(273)) - Storage directory with location [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 1027244560. Formatting...\r\n 2018-07-31 21:42:46,744 [Thread-2206] INFO common.Storage (DataStorage.java:createStorageID(150)) - Generated new storageID DS-c307fe32-1d7b-4c82-9d8f-6c43bb6ebe3f for directory /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2\r\n 2018-07-31 21:42:46,745 [main] INFO http.HttpServer2 (HttpServer2.java:addGlobalFilter(923)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n 2018-07-31 21:42:46,746 [main] INFO http.HttpServer2 (HttpServer2.java:addFilter(896)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\r\n 2018-07-31 21:42:46,746 [main] INFO http.HttpServer2 (HttpServer2.java:addFilter(906)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n 2018-07-31 21:42:46,746 [main] INFO http.HttpServer2 (HttpServer2.java:addFilter(906)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n 2018-07-31 21:42:46,746 [main] INFO http.HttpServer2 (HttpServer2.java:bindListener(1123)) - Jetty bound to port 35966\r\n 2018-07-31 21:42:46,747 [main] INFO server.Server (Server.java:doStart(346)) - jetty-9.3.20.v20170531\r\n 2018-07-31 21:42:46,749 [main] INFO handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2c6aed22\\{/logs,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}\r\n 2018-07-31 21:42:46,750 [main] INFO handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7acfb656\\{/static,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test-classes/webapps/static/,AVAILABLE}\r\n 2018-07-31 21:42:46,755 [main] INFO handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@41ffaeb8\\{/,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test-classes/webapps/datanode/,AVAILABLE}{/datanode}\r\n\r\n2018-07-31 21:42:46,756 [main] INFO server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@43f0c2d1\r\n\r\n{HTTP/1.1,[http/1.1]}\\{localhost:35966}\r\n 2018-07-31 21:42:46,756 [main] INFO server.Server (Server.java:doStart(414)) - Started @27074ms\r\n 2018-07-31 21:42:46,760 [main] INFO web.DatanodeHttpServer (DatanodeHttpServer.java:start(239)) - Listening HTTP traffic on /x.x.x.x:port6\r\n 2018-07-31 21:42:46,761 [main] INFO datanode.DataNode (DataNode.java:startDataNode(1412)) - dnUserName = jenkins\r\n 2018-07-31 21:42:46,761 [main] INFO datanode.DataNode (DataNode.java:startDataNode(1413)) - supergroup = supergroup\r\n 2018-07-31 21:42:46,761 [Thread-2206] INFO common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(252)) - Analyzing storage directories for bpid BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,761 [main] INFO ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\r\n 2018-07-31 21:42:46,761 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4096aa05] INFO util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor\r\n 2018-07-31 21:42:46,761 [Thread-2206] INFO common.Storage (Storage.java:lock(806)) - Locking is disabled for /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,761 [Thread-2206] INFO common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-1309332346-172.26.0.186-1533098566550 is not formatted for BP-1309332346-172.26.0.186-1533098566550. Formatting ...\r\n 2018-07-31 21:42:46,762 [Thread-2206] INFO common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1309332346-172.26.0.186-1533098566550 directory /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1309332346-172.26.0.186-1533098566550/current\r\n 2018-07-31 21:42:46,762 [Socket Reader #1 for port port7|#1 for port port7] INFO ipc.Server (Server.java:run(1067)) - Starting Socket Reader #1 for port port7\r\n 2018-07-31 21:42:46,764 [main] INFO datanode.DataNode (DataNode.java:initIpcServer(1029)) - Opened IPC server at /x.x.x.x:port7\r\n 2018-07-31 21:42:46,766 [main] INFO datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null\r\n 2018-07-31 21:42:46,766 [main] INFO datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(198)) - Starting BPOfferServices for nameservices: <default>\r\n 2018-07-31 21:42:46,767 [Thread-2237] INFO datanode.DataNode (BPServiceActor.java:run(809)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/x.x.x.x:port1 starting to offer service\r\n 2018-07-31 21:42:46,768 [IPC Server Responder] INFO ipc.Server (Server.java:run(1307)) - IPC Server Responder: starting\r\n 2018-07-31 21:42:46,769 [IPC Server listener on port7] INFO ipc.Server (Server.java:run(1146)) - IPC Server listener on port7: starting\r\n 2018-07-31 21:42:46,770 [Thread-2237] INFO datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/x.x.x.x:port1\r\n 2018-07-31 21:42:46,770 [Thread-2237] INFO common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(346)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)\r\n 2018-07-31 21:42:46,772 [main] DEBUG hdfs.DFSClient (DFSClient.java:<init>(311)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0\r\n 2018-07-31 21:42:46,775 [IPC Server handler 2 on port1] INFO FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7816)) - allowed=true ugi=jenkins (auth:SIMPLE) ip=/x.x.x.x cmd=datanodeReport src=null dst=null perm=null proto=rpc\r\n 2018-07-31 21:42:46,776 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2653)) - dnInfo.length != numDataNodes\r\n 2018-07-31 21:42:46,776 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2574)) - Waiting for cluster to become active\r\n 2018-07-31 21:42:46,780 [Thread-2206] INFO common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(252)) - Analyzing storage directories for bpid BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,780 [Thread-2206] INFO common.Storage (Storage.java:lock(806)) - Locking is disabled for /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,780 [Thread-2237] INFO common.Storage (Storage.java:tryLock(847)) - Lock on /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/in_use.lock acquired by nodename 7074@quasar-ztplxb-1.vpc.cloudera.com\r\n 2018-07-31 21:42:46,780 [Thread-2206] INFO common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-1309332346-172.26.0.186-1533098566550 is not formatted for BP-1309332346-172.26.0.186-1533098566550. Formatting ...\r\n 2018-07-31 21:42:46,780 [Thread-2206] INFO common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1309332346-172.26.0.186-1533098566550 directory /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1309332346-172.26.0.186-1533098566550/current\r\n 2018-07-31 21:42:46,780 [Thread-2237] INFO common.Storage (DataStorage.java:loadStorageDirectory(273)) - Storage directory with location [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 is not formatted for namespace 1027244560. Formatting...\r\n 2018-07-31 21:42:46,781 [Thread-2237] INFO common.Storage (DataStorage.java:createStorageID(150)) - Generated new storageID DS-386f4969-f72d-4a3a-baad-ef59d4bc3016 for directory /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3\r\n 2018-07-31 21:42:46,789 [Thread-2206] INFO datanode.DataNode (DataNode.java:initStorage(1693)) - Setting up storage: nsid=1027244560;bpid=BP-1309332346-172.26.0.186-1533098566550;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1027244560;c=1533098566550;bpid=BP-1309332346-172.26.0.186-1533098566550;dnuuid=null\r\n 2018-07-31 21:42:46,792 [Thread-2237] INFO common.Storage (Storage.java:tryLock(847)) - Lock on /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/in_use.lock acquired by nodename 7074@quasar-ztplxb-1.vpc.cloudera.com\r\n 2018-07-31 21:42:46,792 [Thread-2237] INFO common.Storage (DataStorage.java:loadStorageDirectory(273)) - Storage directory with location [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 is not formatted for namespace 1027244560. Formatting...\r\n 2018-07-31 21:42:46,792 [Thread-2237] INFO common.Storage (DataStorage.java:createStorageID(150)) - Generated new storageID DS-cac8e9dc-026d-474d-a76b-57d577c44846 for directory /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4\r\n 2018-07-31 21:42:46,794 [Thread-2206] INFO datanode.DataNode (DataNode.java:checkDatanodeUuid(1517)) - Generated and persisted new Datanode UUID 785a0dd8-bdae-40c1-94f6-a6b639dafb14\r\n 2018-07-31 21:42:46,796 [Thread-2206] INFO impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-1d19d47a-b472-463a-a1ed-43be073676f5\r\n 2018-07-31 21:42:46,796 [Thread-2206] INFO impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(431)) - Added volume - [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK\r\n 2018-07-31 21:42:46,797 [Thread-2206] INFO impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-c307fe32-1d7b-4c82-9d8f-6c43bb6ebe3f\r\n 2018-07-31 21:42:46,797 [Thread-2206] INFO impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(431)) - Added volume - [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK\r\n 2018-07-31 21:42:46,797 [Thread-2206] INFO impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2169)) - Registered FSDatasetState MBean\r\n 2018-07-31 21:42:46,798 [Thread-2206] INFO checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(122)) - Scheduling a check for /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1\r\n 2018-07-31 21:42:46,799 [Thread-2206] INFO checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(219)) - Scheduled health check for volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1\r\n 2018-07-31 21:42:46,799 [Thread-2206] INFO checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(122)) - Scheduling a check for /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2\r\n 2018-07-31 21:42:46,799 [Thread-2206] INFO checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(219)) - Scheduled health check for volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2\r\n 2018-07-31 21:42:46,799 [Thread-2206] INFO impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2655)) - Adding block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,800 [Thread-2251] INFO impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...\r\n 2018-07-31 21:42:46,801 [Thread-2237] INFO common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(252)) - Analyzing storage directories for bpid BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,801 [Thread-2237] INFO common.Storage (Storage.java:lock(806)) - Locking is disabled for /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,801 [Thread-2237] INFO common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 and block pool id BP-1309332346-172.26.0.186-1533098566550 is not formatted for BP-1309332346-172.26.0.186-1533098566550. Formatting ...\r\n 2018-07-31 21:42:46,801 [Thread-2237] INFO common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1309332346-172.26.0.186-1533098566550 directory /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1309332346-172.26.0.186-1533098566550/current\r\n 2018-07-31 21:42:46,804 [Thread-2252] INFO impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...\r\n 2018-07-31 21:42:46,811 [Thread-2237] INFO common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(252)) - Analyzing storage directories for bpid BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,811 [Thread-2237] INFO common.Storage (Storage.java:lock(806)) - Locking is disabled for /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,811 [Thread-2237] INFO common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4 and block pool id BP-1309332346-172.26.0.186-1533098566550 is not formatted for BP-1309332346-172.26.0.186-1533098566550. Formatting ...\r\n 2018-07-31 21:42:46,812 [Thread-2237] INFO common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-1309332346-172.26.0.186-1533098566550 directory /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1309332346-172.26.0.186-1533098566550/current\r\n 2018-07-31 21:42:46,813 [Thread-2252] INFO impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-1309332346-172.26.0.186-1533098566550 on /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 9ms\r\n 2018-07-31 21:42:46,816 [Thread-2237] INFO datanode.DataNode (DataNode.java:initStorage(1693)) - Setting up storage: nsid=1027244560;bpid=BP-1309332346-172.26.0.186-1533098566550;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1027244560;c=1533098566550;bpid=BP-1309332346-172.26.0.186-1533098566550;dnuuid=null\r\n 2018-07-31 21:42:46,817 [Thread-2251] INFO impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-1309332346-172.26.0.186-1533098566550 on /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 17ms\r\n 2018-07-31 21:42:46,817 [Thread-2206] INFO impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-1309332346-172.26.0.186-1533098566550: 18ms\r\n 2018-07-31 21:42:46,817 [Thread-2255] INFO impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...\r\n 2018-07-31 21:42:46,818 [Thread-2255] INFO impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(770)) - Replica Cache file: /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1309332346-172.26.0.186-1533098566550/current/replicas doesn't exist \r\n 2018-07-31 21:42:46,818 [Thread-2256] INFO impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...\r\n 2018-07-31 21:42:46,818 [Thread-2255] INFO impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 0ms\r\n 2018-07-31 21:42:46,818 [Thread-2256] INFO impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(770)) - Replica Cache file: /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1309332346-172.26.0.186-1533098566550/current/replicas doesn't exist \r\n 2018-07-31 21:42:46,818 [Thread-2256] INFO impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 0ms\r\n 2018-07-31 21:42:46,819 [Thread-2206] INFO impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(229)) - Total time to add all replicas to map: 2ms\r\n 2018-07-31 21:42:46,819 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(386)) - Now scanning bpid BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1\r\n 2018-07-31 21:42:46,819 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(386)) - Now scanning bpid BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2\r\n 2018-07-31 21:42:46,819 [Thread-2206] INFO datanode.DirectoryScanner (DirectoryScanner.java:start(281)) - Periodic Directory Tree Verification scan starting at 7/31/18 10:52 PM with interval of 21600000ms\r\n 2018-07-31 21:42:46,819 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO datanode.VolumeScanner (VolumeScanner.java:runLoop(544)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-c307fe32-1d7b-4c82-9d8f-6c43bb6ebe3f): finished scanning block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,819 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO datanode.VolumeScanner (VolumeScanner.java:runLoop(544)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-1d19d47a-b472-463a-a1ed-43be073676f5): finished scanning block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,820 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPServiceActor.java:register(763)) - Block pool BP-1309332346-172.26.0.186-1533098566550 (Datanode Uuid 785a0dd8-bdae-40c1-94f6-a6b639dafb14) service to localhost/x.x.x.x:port1 beginning handshake with NN\r\n 2018-07-31 21:42:46,821 [IPC Server handler 3 on port1] INFO hdfs.StateChange (DatanodeManager.java:registerDatanode(1038)) - BLOCK* registerDatanode: from DatanodeRegistration(x.x.x.x:port2, datanodeUuid=785a0dd8-bdae-40c1-94f6-a6b639dafb14, infoPort=port3, infoSecurePort=0, ipcPort=port4, storageInfo=lv=-57;cid=testClusterID;nsid=1027244560;c=1533098566550) storage 785a0dd8-bdae-40c1-94f6-a6b639dafb14\r\n 2018-07-31 21:42:46,821 [IPC Server handler 3 on port1] INFO net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/x.x.x.x:port2\r\n 2018-07-31 21:42:46,821 [IPC Server handler 3 on port1] INFO blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 785a0dd8-bdae-40c1-94f6-a6b639dafb14 (x.x.x.x:port2).\r\n 2018-07-31 21:42:46,823 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPServiceActor.java:register(782)) - Block pool Block pool BP-1309332346-172.26.0.186-1533098566550 (Datanode Uuid 785a0dd8-bdae-40c1-94f6-a6b639dafb14) service to localhost/x.x.x.x:port1 successfully registered with NN\r\n 2018-07-31 21:42:46,823 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/x.x.x.x:port1 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\r\n 2018-07-31 21:42:46,824 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-1d19d47a-b472-463a-a1ed-43be073676f5): no suitable block pools found to scan. Waiting 1814399995 ms.\r\n 2018-07-31 21:42:46,824 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-c307fe32-1d7b-4c82-9d8f-6c43bb6ebe3f): no suitable block pools found to scan. Waiting 1814399995 ms.\r\n 2018-07-31 21:42:46,825 [IPC Server handler 4 on port1] INFO blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(949)) - Adding new storage ID DS-1d19d47a-b472-463a-a1ed-43be073676f5 for DN x.x.x.x:port2\r\n 2018-07-31 21:42:46,825 [IPC Server handler 4 on port1] INFO blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(949)) - Adding new storage ID DS-c307fe32-1d7b-4c82-9d8f-6c43bb6ebe3f for DN x.x.x.x:port2\r\n 2018-07-31 21:42:46,827 [Block report processor] INFO BlockStateChange (BlockManager.java:processReport(2462)) - BLOCK* processReport 0xa2b93752cc445e4b: Processing first storage report for DS-1d19d47a-b472-463a-a1ed-43be073676f5 from datanode 785a0dd8-bdae-40c1-94f6-a6b639dafb14\r\n 2018-07-31 21:42:46,828 [Block report processor] INFO BlockStateChange (BlockManager.java:processReport(2488)) - BLOCK* processReport 0xa2b93752cc445e4b: from storage DS-1d19d47a-b472-463a-a1ed-43be073676f5 node DatanodeRegistration(x.x.x.x:port2, datanodeUuid=785a0dd8-bdae-40c1-94f6-a6b639dafb14, infoPort=port3, infoSecurePort=0, ipcPort=port4, storageInfo=lv=-57;cid=testClusterID;nsid=1027244560;c=1533098566550), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0\r\n 2018-07-31 21:42:46,828 [Block report processor] INFO BlockStateChange (BlockManager.java:processReport(2462)) - BLOCK* processReport 0xa2b93752cc445e4b: Processing first storage report for DS-c307fe32-1d7b-4c82-9d8f-6c43bb6ebe3f from datanode 785a0dd8-bdae-40c1-94f6-a6b639dafb14\r\n 2018-07-31 21:42:46,828 [Thread-2237] INFO datanode.DataNode (DataNode.java:checkDatanodeUuid(1517)) - Generated and persisted new Datanode UUID 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d\r\n 2018-07-31 21:42:46,828 [Block report processor] INFO BlockStateChange (BlockManager.java:processReport(2488)) - BLOCK* processReport 0xa2b93752cc445e4b: from storage DS-c307fe32-1d7b-4c82-9d8f-6c43bb6ebe3f node DatanodeRegistration(x.x.x.x:port2, datanodeUuid=785a0dd8-bdae-40c1-94f6-a6b639dafb14, infoPort=port3, infoSecurePort=0, ipcPort=port4, storageInfo=lv=-57;cid=testClusterID;nsid=1027244560;c=1533098566550), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0\r\n 2018-07-31 21:42:46,830 [Thread-2237] INFO impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-386f4969-f72d-4a3a-baad-ef59d4bc3016\r\n 2018-07-31 21:42:46,830 [Thread-2237] INFO impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(431)) - Added volume - [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, StorageType: DISK\r\n 2018-07-31 21:42:46,830 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0xa2b93752cc445e4b, containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\r\n 2018-07-31 21:42:46,830 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,831 [Thread-2237] INFO impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-cac8e9dc-026d-474d-a76b-57d577c44846\r\n 2018-07-31 21:42:46,831 [Thread-2237] INFO impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(431)) - Added volume - [DISK]file:/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, StorageType: DISK\r\n 2018-07-31 21:42:46,831 [Thread-2237] INFO impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2169)) - Registered FSDatasetState MBean\r\n 2018-07-31 21:42:46,832 [Thread-2237] INFO checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(122)) - Scheduling a check for /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3\r\n 2018-07-31 21:42:46,833 [Thread-2237] INFO checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(219)) - Scheduled health check for volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3\r\n 2018-07-31 21:42:46,833 [Thread-2237] INFO checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(122)) - Scheduling a check for /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4\r\n 2018-07-31 21:42:46,833 [Thread-2237] INFO checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(219)) - Scheduled health check for volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4\r\n 2018-07-31 21:42:46,833 [Thread-2237] INFO impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2655)) - Adding block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,834 [Thread-2262] INFO impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...\r\n 2018-07-31 21:42:46,834 [Thread-2263] INFO impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...\r\n 2018-07-31 21:42:46,840 [Thread-2263] INFO impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-1309332346-172.26.0.186-1533098566550 on /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 6ms\r\n 2018-07-31 21:42:46,842 [Thread-2262] INFO impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-1309332346-172.26.0.186-1533098566550 on /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 8ms\r\n 2018-07-31 21:42:46,842 [Thread-2237] INFO impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-1309332346-172.26.0.186-1533098566550: 9ms\r\n 2018-07-31 21:42:46,842 [Thread-2266] INFO impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3...\r\n 2018-07-31 21:42:46,843 [Thread-2266] INFO impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(770)) - Replica Cache file: /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1309332346-172.26.0.186-1533098566550/current/replicas doesn't exist \r\n 2018-07-31 21:42:46,843 [Thread-2267] INFO impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4...\r\n 2018-07-31 21:42:46,843 [Thread-2266] INFO impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3: 0ms\r\n 2018-07-31 21:42:46,843 [Thread-2267] INFO impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(770)) - Replica Cache file: /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1309332346-172.26.0.186-1533098566550/current/replicas doesn't exist \r\n 2018-07-31 21:42:46,843 [Thread-2267] INFO impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4: 0ms\r\n 2018-07-31 21:42:46,843 [Thread-2237] INFO impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(229)) - Total time to add all replicas to map: 1ms\r\n 2018-07-31 21:42:46,844 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(386)) - Now scanning bpid BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3\r\n 2018-07-31 21:42:46,844 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(386)) - Now scanning bpid BP-1309332346-172.26.0.186-1533098566550 on volume /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4\r\n 2018-07-31 21:42:46,844 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO datanode.VolumeScanner (VolumeScanner.java:runLoop(544)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-386f4969-f72d-4a3a-baad-ef59d4bc3016): finished scanning block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,844 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO datanode.VolumeScanner (VolumeScanner.java:runLoop(544)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-cac8e9dc-026d-474d-a76b-57d577c44846): finished scanning block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,844 [Thread-2237] INFO datanode.DirectoryScanner (DirectoryScanner.java:start(281)) - Periodic Directory Tree Verification scan starting at 7/31/18 10:08 PM with interval of 21600000ms\r\n 2018-07-31 21:42:46,844 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-386f4969-f72d-4a3a-baad-ef59d4bc3016): no suitable block pools found to scan. Waiting 1814400000 ms.\r\n 2018-07-31 21:42:46,844 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-cac8e9dc-026d-474d-a76b-57d577c44846): no suitable block pools found to scan. Waiting 1814400000 ms.\r\n 2018-07-31 21:42:46,845 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPServiceActor.java:register(763)) - Block pool BP-1309332346-172.26.0.186-1533098566550 (Datanode Uuid 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d) service to localhost/x.x.x.x:port1 beginning handshake with NN\r\n 2018-07-31 21:42:46,846 [IPC Server handler 6 on port1] INFO hdfs.StateChange (DatanodeManager.java:registerDatanode(1038)) - BLOCK* registerDatanode: from DatanodeRegistration(x.x.x.x:port5, datanodeUuid=48eaddaf-0bf7-47e5-bc33-4a640b4cde9d, infoPort=port6, infoSecurePort=0, ipcPort=port7, storageInfo=lv=-57;cid=testClusterID;nsid=1027244560;c=1533098566550) storage 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d\r\n 2018-07-31 21:42:46,846 [IPC Server handler 6 on port1] INFO net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/x.x.x.x:port5\r\n 2018-07-31 21:42:46,846 [IPC Server handler 6 on port1] INFO blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d (x.x.x.x:port5).\r\n 2018-07-31 21:42:46,847 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPServiceActor.java:register(782)) - Block pool Block pool BP-1309332346-172.26.0.186-1533098566550 (Datanode Uuid 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d) service to localhost/x.x.x.x:port1 successfully registered with NN\r\n 2018-07-31 21:42:46,847 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/x.x.x.x:port1 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\r\n 2018-07-31 21:42:46,848 [IPC Server handler 7 on port1] INFO blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(949)) - Adding new storage ID DS-386f4969-f72d-4a3a-baad-ef59d4bc3016 for DN x.x.x.x:port5\r\n 2018-07-31 21:42:46,850 [IPC Server handler 7 on port1] INFO blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(949)) - Adding new storage ID DS-cac8e9dc-026d-474d-a76b-57d577c44846 for DN x.x.x.x:port5\r\n 2018-07-31 21:42:46,852 [Block report processor] INFO BlockStateChange (BlockManager.java:processReport(2462)) - BLOCK* processReport 0xcb3b9727741549e0: Processing first storage report for DS-cac8e9dc-026d-474d-a76b-57d577c44846 from datanode 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d\r\n 2018-07-31 21:42:46,852 [Block report processor] INFO BlockStateChange (BlockManager.java:processReport(2488)) - BLOCK* processReport 0xcb3b9727741549e0: from storage DS-cac8e9dc-026d-474d-a76b-57d577c44846 node DatanodeRegistration(x.x.x.x:port5, datanodeUuid=48eaddaf-0bf7-47e5-bc33-4a640b4cde9d, infoPort=port6, infoSecurePort=0, ipcPort=port7, storageInfo=lv=-57;cid=testClusterID;nsid=1027244560;c=1533098566550), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0\r\n 2018-07-31 21:42:46,852 [Block report processor] INFO BlockStateChange (BlockManager.java:processReport(2462)) - BLOCK* processReport 0xcb3b9727741549e0: Processing first storage report for DS-386f4969-f72d-4a3a-baad-ef59d4bc3016 from datanode 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d\r\n 2018-07-31 21:42:46,852 [Block report processor] INFO BlockStateChange (BlockManager.java:processReport(2488)) - BLOCK* processReport 0xcb3b9727741549e0: from storage DS-386f4969-f72d-4a3a-baad-ef59d4bc3016 node DatanodeRegistration(x.x.x.x:port5, datanodeUuid=48eaddaf-0bf7-47e5-bc33-4a640b4cde9d, infoPort=port6, infoSecurePort=0, ipcPort=port7, storageInfo=lv=-57;cid=testClusterID;nsid=1027244560;c=1533098566550), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0\r\n 2018-07-31 21:42:46,853 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0xcb3b9727741549e0, containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\r\n 2018-07-31 21:42:46,853 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,878 [IPC Server handler 9 on port1] INFO FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7816)) - allowed=true ugi=jenkins (auth:SIMPLE) ip=/x.x.x.x cmd=datanodeReport src=null dst=null perm=null proto=rpc\r\n 2018-07-31 21:42:46,878 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2629)) - Cluster is active\r\n 2018-07-31 21:42:46,880 [main] DEBUG hdfs.DFSClient (DFSClient.java:<init>(311)) - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0\r\n 2018-07-31 21:42:46,881 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1965)) - Shutting down the Mini HDFS Cluster\r\n 2018-07-31 21:42:46,881 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2013)) - Shutting down DataNode 1\r\n 2018-07-31 21:42:46,881 [main] WARN datanode.DirectoryScanner (DirectoryScanner.java:shutdown(340)) - DirectoryScanner: shutdown has been called\r\n 2018-07-31 21:42:46,881 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6aba5d30] INFO datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.\r\n 2018-07-31 21:42:46,882 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)] INFO datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3, DS-386f4969-f72d-4a3a-baad-ef59d4bc3016) exiting.\r\n 2018-07-31 21:42:46,882 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)] INFO datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4, DS-cac8e9dc-026d-474d-a76b-57d577c44846) exiting.\r\n 2018-07-31 21:42:46,887 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@41ffaeb8\\{/,null,UNAVAILABLE}{/datanode}\r\n 2018-07-31 21:42:46,888 [main] INFO server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@43f0c2d1\\{HTTP/1.1,[http/1.1]} \\{localhost:0}\r\n\r\n2018-07-31 21:42:46,888 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7acfb656\r\n\r\n{/static,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test-classes/webapps/static/,UNAVAILABLE}\r\n\r\n2018-07-31 21:42:46,888 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2c6aed22\r\n\r\n{/logs,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}\r\n\r\n2018-07-31 21:42:46,890 [main] INFO ipc.Server (Server.java:stop(3074)) - Stopping server on port7\r\n 2018-07-31 21:42:46,891 [IPC Server listener on port7] INFO ipc.Server (Server.java:run(1178)) - Stopping IPC Server listener on port7\r\n 2018-07-31 21:42:46,892 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] WARN datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted\r\n 2018-07-31 21:42:46,891 [IPC Server Responder] INFO ipc.Server (Server.java:run(1312)) - Stopping IPC Server Responder\r\n 2018-07-31 21:42:46,892 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] WARN datanode.DataNode (BPServiceActor.java:run(852)) - Ending block pool service for: Block pool BP-1309332346-172.26.0.186-1533098566550 (Datanode Uuid 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d) service to localhost/x.x.x.x:port1\r\n 2018-07-31 21:42:46,892 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1309332346-172.26.0.186-1533098566550 (Datanode Uuid 48eaddaf-0bf7-47e5-bc33-4a640b4cde9d)\r\n 2018-07-31 21:42:46,892 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2666)) - Removing block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:46,893 [refreshUsed-/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1309332346-172.26.0.186-1533098566550] WARN fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted\r\n 2018-07-31 21:42:46,893 [refreshUsed-/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1309332346-172.26.0.186-1533098566550] WARN fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted\r\n 2018-07-31 21:42:46,894 [main] INFO impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(191)) - Shutting down all async disk service threads\r\n 2018-07-31 21:42:46,894 [main] INFO impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(199)) - All async disk service threads have been shut down\r\n 2018-07-31 21:42:46,895 [main] INFO impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads\r\n 2018-07-31 21:42:46,895 [main] INFO impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down\r\n 2018-07-31 21:42:46,895 [main] INFO datanode.DataNode (DataNode.java:shutdown(2113)) - Shutdown complete.\r\n 2018-07-31 21:42:46,896 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2013)) - Shutting down DataNode 0\r\n 2018-07-31 21:42:46,896 [main] WARN datanode.DirectoryScanner (DirectoryScanner.java:shutdown(340)) - DirectoryScanner: shutdown has been called\r\n 2018-07-31 21:42:46,896 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@74db12c2] INFO datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.\r\n 2018-07-31 21:42:46,897 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-1d19d47a-b472-463a-a1ed-43be073676f5) exiting.\r\n 2018-07-31 21:42:46,897 [VolumeScannerThread(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-c307fe32-1d7b-4c82-9d8f-6c43bb6ebe3f) exiting.\r\n 2018-07-31 21:42:46,901 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@68fe48d7\r\n\r\n{/,null,UNAVAILABLE}\\{/datanode}\r\n 2018-07-31 21:42:46,902 [main] INFO server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@379ce046\\{HTTP/1.1,[http/1.1]}{localhost:0}\r\n 2018-07-31 21:42:46,902 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@67594471\\{/static,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test-classes/webapps/static/,UNAVAILABLE}\r\n 2018-07-31 21:42:46,902 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@38b5f25\\{/logs,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}\r\n 2018-07-31 21:42:46,904 [main] INFO ipc.Server (Server.java:stop(3074)) - Stopping server on port4\r\n 2018-07-31 21:42:46,905 [IPC Server listener on port4] INFO ipc.Server (Server.java:run(1178)) - Stopping IPC Server listener on port4\r\n 2018-07-31 21:42:46,906 [IPC Server Responder] INFO ipc.Server (Server.java:run(1312)) - Stopping IPC Server Responder\r\n 2018-07-31 21:42:46,906 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] WARN datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted\r\n 2018-07-31 21:42:46,906 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] WARN datanode.DataNode (BPServiceActor.java:run(852)) - Ending block pool service for: Block pool BP-1309332346-172.26.0.186-1533098566550 (Datanode Uuid 785a0dd8-bdae-40c1-94f6-a6b639dafb14) service to localhost/x.x.x.x:port1\r\n 2018-07-31 21:42:47,006 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-1309332346-172.26.0.186-1533098566550 (Datanode Uuid 785a0dd8-bdae-40c1-94f6-a6b639dafb14)\r\n 2018-07-31 21:42:47,007 [BP-1309332346-172.26.0.186-1533098566550 heartbeating to localhost/x.x.x.x:port1] INFO impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2666)) - Removing block pool BP-1309332346-172.26.0.186-1533098566550\r\n 2018-07-31 21:42:47,007 [refreshUsed-/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1309332346-172.26.0.186-1533098566550] WARN fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted\r\n 2018-07-31 21:42:47,007 [refreshUsed-/tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1309332346-172.26.0.186-1533098566550] WARN fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted\r\n 2018-07-31 21:42:47,008 [main] INFO impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(191)) - Shutting down all async disk service threads\r\n 2018-07-31 21:42:47,008 [main] INFO impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(199)) - All async disk service threads have been shut down\r\n 2018-07-31 21:42:47,009 [main] INFO impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads\r\n 2018-07-31 21:42:47,009 [main] INFO impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down\r\n 2018-07-31 21:42:47,009 [main] INFO datanode.DataNode (DataNode.java:shutdown(2113)) - Shutdown complete.\r\n 2018-07-31 21:42:47,010 [main] INFO hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2046)) - Shutting down the namenode\r\n 2018-07-31 21:42:47,010 [main] INFO namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1339)) - Stopping services started for active state\r\n 2018-07-31 21:42:47,010 [main] INFO namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1407)) - Ending log segment 1, 1\r\n 2018-07-31 21:42:47,010 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@263bbfeb] INFO namenode.FSNamesystem (FSNamesystem.java:run(4010)) - NameNodeEditLogRoller was interrupted, exiting\r\n 2018-07-31 21:42:47,011 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@31edeac] INFO namenode.FSNamesystem (FSNamesystem.java:run(4101)) - LazyPersistFileScrubber was interrupted, exiting\r\n 2018-07-31 21:42:47,011 [main] INFO namenode.FSEditLog (FSEditLog.java:printStatistics(775)) - Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 1 1 \r\n 2018-07-31 21:42:47,011 [main] INFO namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000002\r\n 2018-07-31 21:42:47,012 [main] INFO namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000002\r\n 2018-07-31 21:42:47,012 [FSEditLogAsync] INFO namenode.FSEditLog (FSEditLogAsync.java:run(198)) - FSEditLogAsync was interrupted, exiting\r\n 2018-07-31 21:42:47,013 [CacheReplicationMonitor(1111752355)] INFO blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor\r\n 2018-07-31 21:42:47,014 [main] INFO ipc.Server (Server.java:stop(3074)) - Stopping server on port1\r\n 2018-07-31 21:42:47,015 [IPC Server Responder] INFO ipc.Server (Server.java:run(1312)) - Stopping IPC Server Responder\r\n 2018-07-31 21:42:47,015 [IPC Server listener on port1] INFO ipc.Server (Server.java:run(1178)) - Stopping IPC Server listener on port1\r\n 2018-07-31 21:42:47,015 [StorageInfoMonitor] INFO blockmanagement.BlockManager (BlockManager.java:run(4479)) - Stopping thread.\r\n 2018-07-31 21:42:47,017 [RedundancyMonitor] INFO blockmanagement.BlockManager (BlockManager.java:run(4444)) - Stopping RedundancyMonitor.\r\n 2018-07-31 21:42:47,026 [main] INFO namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1339)) - Stopping services started for active state\r\n 2018-07-31 21:42:47,026 [main] INFO namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1435)) - Stopping services started for standby state\r\n 2018-07-31 21:42:47,028 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@1c758545\\{/,null,UNAVAILABLE} \\{/hdfs}\r\n\r\n2018-07-31 21:42:47,029 [main] INFO server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@117bcfdc\r\n\r\n{HTTP/1.1,[http/1.1]} \\{localhost:0}\r\n\r\n2018-07-31 21:42:47,029 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6f4ade6e\r\n\r\n{/static,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test-classes/webapps/static/,UNAVAILABLE}\r\n\r\n2018-07-31 21:42:47,029 [main] INFO handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@514cd540\r\n\r\n{/logs,file:///tmp/tmp.u8GhlLcdks/src/CDH/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}\r\n\r\n2018-07-31 21:42:47,030 [main] INFO impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...\r\n 2018-07-31 21:42:47,031 [main] INFO impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.\r\n 2018-07-31 21:42:47,031 [main] INFO impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.\r\n{noformat}\r\nÂ \r\n\r\n Â ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936496","id":"12936496","filename":"HDFS-13837.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-21T18:24:20.619+0000","size":1236,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12936496/HDFS-13837.001.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12937089","id":"12937089","filename":"HDFS-13837.002.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-24T23:25:13.785+0000","size":1399,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12937089/HDFS-13837.002.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12937343","id":"12937343","filename":"HDFS-13837.003.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-27T22:38:23.484+0000","size":1773,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12937343/HDFS-13837.003.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12937487","id":"12937487","filename":"HDFS-13837.004.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-28T17:58:50.966+0000","size":1800,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12937487/HDFS-13837.004.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12936313","id":"12936313","filename":"TestDistributedFileSystem.testDFSClient_Stderr_log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-20T19:32:32.755+0000","size":65425,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12936313/TestDistributedFileSystem.testDFSClient_Stderr_log"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Enable debug log for LeaseRenewer in TestDistributedFileSystem","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16587833","id":"16587833","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"body":"[~xiaochen], as suggested, I have added the patch for the flaky test. The logging helps in understanding the execution of the Lease Renewer. Please review and suggest if any further details need to be added. Thank you.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-21T18:26:23.115+0000","updated":"2018-08-21T18:26:23.115+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16588022","id":"16588022","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 23s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m 19s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  3s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 19s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  9s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  6s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 12s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  8s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}112m  0s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 45s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}171m 51s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy |\r\n|   | hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations |\r\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |\r\n|   | hadoop.hdfs.TestDFSClientRetries |\r\n|   | hadoop.hdfs.server.balancer.TestBalancerRPCDelay |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:ba1ab08 |\r\n| JIRA Issue | HDFS-13837 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12936496/HDFS-13837.001.patch |\r\n| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux c03174ebcbc7 3.13.0-153-generic #203-Ubuntu SMP Thu Jun 14 08:52:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 9c3fc3e |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_181 |\r\n| findbugs | v3.1.0-RC1 |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/24828/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/24828/testReport/ |\r\n| Max. process+thread count | 3025 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/24828/console |\r\n| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-21T21:28:09.751+0000","updated":"2018-08-21T21:28:09.751+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16588409","id":"16588409","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~shwetayakkali] for creating the issue with details, and providing a patch.\r\n\r\nAs we discussed offline, this case we don't have enough information to investigate the failure, and turning on LeaseRenewer debug logs would help us.\r\n\r\n1 comment on the change though, is could you do the log level change at test class level? There is already a block changing DFSClient's log:\r\n{code}\r\n  static {\r\n    GenericTestUtils.setLogLevel(DFSClient.LOG, Level.ALL);\r\n  }\r\n{code}\r\nThis will make sure the other few tests that calls {{testDFSClient}} will benefit from it as well.\r\n\r\nIf this ends up prove that the test failure are just due to cheap jenkins slave issue, we can do a test rewrite/fix as a follow-on in the future.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-22T05:24:00.231+0000","updated":"2018-08-22T05:24:00.231+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16592311","id":"16592311","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~xiaochen] for the prompt review. Yes, the LeaseRenewer Logs will be helpful when the test fails.\r\nAlso, as per your suggesstion I have made the change at class level so that Lease Renewer logging is present for all the tests in the class. I have added a new patch w.r.t. this change. Please review.\r\nThank you. :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-24T23:27:12.043+0000","updated":"2018-08-24T23:27:12.043+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16592316","id":"16592316","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |\r\n| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HDFS-13837 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| JIRA Issue | HDFS-13837 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12937089/HDFS-13837.002.patch |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/24882/console |\r\n| Powered by | Apache Yetus 0.9.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-24T23:30:37.759+0000","updated":"2018-08-24T23:30:37.759+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16593171","id":"16593171","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the new rev Shweta.\r\n\r\nIt'd be good to use the same API as the existing code, instead of {{org.slf4j.event.Level}}. :)\r\n{code}\r\nGenericTestUtils.setLogLevel(DFSClient.LOG, Level.ALL);\r\n+GenericTestUtils.setLogLevel(LeaseRenewer.LOG, org.slf4j.event.Level.DEBUG);\r\n{code}\r\n\r\nAlso, there is an unnecessary line removal in patch 2 which we should not do, for cleanness.\r\n\r\n+1 pending the above.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-27T03:50:50.525+0000","updated":"2018-08-27T03:50:50.525+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16594305","id":"16594305","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~xiaochen] for the review. I have uploaded a patch based on the suggestions mentioned. \r\nPlease review.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-27T22:38:32.503+0000","updated":"2018-08-27T22:38:32.503+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16594307","id":"16594307","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |\r\n| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  6s{color} | {color:red} HDFS-13837 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| JIRA Issue | HDFS-13837 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12937343/HDFS-13837.003.patch |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/24892/console |\r\n| Powered by | Apache Yetus 0.9.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-27T22:40:44.654+0000","updated":"2018-08-27T22:40:44.654+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16594442","id":"16594442","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~shwetayakkali], thanks for continuing to work on this.\r\n\r\nAs jenkins complains, the patch is does not apply on trunk. Could you generate a patch against latest trunk?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-28T02:59:14.081+0000","updated":"2018-08-28T02:59:14.081+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16595378","id":"16595378","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you [~xiaochen] for helping out with this. I have uploaded the latest patch which is generated against trunk. Please review. :)\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-28T17:58:38.954+0000","updated":"2018-08-28T17:58:38.954+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16595568","id":"16595568","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 16s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 58s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m  1s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  1s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 50s{color} | {color:green} hadoop-hdfs-project_hadoop-hdfs generated 0 new + 529 unchanged - 1 fixed = 529 total (was 530) {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 53s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  2s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}105m 54s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 31s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}158m 41s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |\r\n|   | hadoop.hdfs.web.TestWebHdfsTimeouts |\r\n|   | hadoop.hdfs.TestLeaseRecovery2 |\r\n|   | hadoop.hdfs.server.namenode.TestReconstructStripedBlocks |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:ba1ab08 |\r\n| JIRA Issue | HDFS-13837 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12937487/HDFS-13837.004.patch |\r\n| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux 9557e5401b75 4.4.0-133-generic #159-Ubuntu SMP Fri Aug 10 07:31:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / fd089ca |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_181 |\r\n| findbugs | v3.1.0-RC1 |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/24907/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/24907/testReport/ |\r\n| Max. process+thread count | 3557 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/24907/console |\r\n| Powered by | Apache Yetus 0.9.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-28T20:39:44.775+0000","updated":"2018-08-28T20:39:44.775+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16595589","id":"16595589","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"+1","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-28T20:56:03.024+0000","updated":"2018-08-28T20:56:03.024+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16595593","id":"16595593","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Committed to trunk.\r\n\r\nThank you for the contribution, [~shwetayakkali]!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-28T20:57:52.565+0000","updated":"2018-08-28T20:57:52.565+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16595596","id":"16595596","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you for the commit [~xiaochen]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shwetayakkali","name":"shwetayakkali","key":"shwetayakkali","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=34046","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34046","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34046","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34046"},"displayName":"Shweta","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-08-28T20:58:31.183+0000","updated":"2018-08-28T20:58:31.183+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13180046/comment/16595617","id":"16595617","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #14851 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/14851/])\nHDFS-13837. Enable debug log for LeaseRenewer in (xiao: rev 33f42efc947445b7755da6aad34b5e26b96ad663)\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2018-08-28T21:23:36.208+0000","updated":"2018-08-28T21:23:36.208+0000"}],"maxResults":15,"total":15,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-13837/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3x8gf:"}}