{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12851297","self":"https://issues.apache.org/jira/rest/api/2/issue/12851297","key":"HDFS-8851","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2015-08-05T23:58:06.257+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Aug 06 07:15:54 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_177753875_*|*_4_*:*_1_*:*_2965832_*|*_5_*:*_2_*:*_1351489","customfield_12312321":null,"resolutiondate":"2015-08-06T07:14:04.836+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-8851/watchers","watchCount":6,"isWatching":false},"created":"2015-08-04T04:39:33.669+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327746","id":"12327746","description":"2.5.1 release","name":"2.5.1","archived":false,"released":true,"releaseDate":"2014-09-05"}],"issuelinks":[{"id":"12433379","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12433379","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12838899","key":"HDFS-8636","self":"https://issues.apache.org/jira/rest/api/2/issue/12838899","fields":{"summary":"Tolerate disk errors during block pool initialization","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-08-06T07:15:54.662+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"}],"timeoriginalestimate":null,"description":"Data node can not start due to a bad disk. I found a similar issue HDFS-6245 is reported, but our situation is different.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"datanode fails to start due to a bad disk","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12851297/comment/14653048","id":"14653048","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"body":"<code>\n15/08/04 12:01:24 INFO common.Storage: Analyzing storage directories for bpid BP-454299492-10.84.100.171-1416301904728\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Locking is disabled\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 INFO common.Storage: Restored 0 block files from trash.\n15/08/04 12:01:24 FATAL datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to hadoop001.dx.momo.com/10.84.100.171:8022. Exiting.\njava.io.IOException: Input/output error\n\tat java.io.FileInputStream.readBytes(Native Method)\n\tat java.io.FileInputStream.read(FileInputStream.java:243)\n\tat java.util.Properties$LineReader.readLine(Properties.java:434)\n\tat java.util.Properties.load0(Properties.java:353)\n\tat java.util.Properties.load(Properties.java:341)\n\tat org.apache.hadoop.hdfs.server.common.StorageInfo.readPropertiesFile(StorageInfo.java:247)\n\tat org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:227)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:256)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:155)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:269)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:975)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:946)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:278)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:812)\n\tat java.lang.Thread.run(Thread.java:745)\n15/08/04 12:01:24 WARN datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to hadoop001.dx.momo.com/10.84.100.171:8022\n15/08/04 12:01:24 INFO datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)\n15/08/04 12:01:26 WARN datanode.DataNode: Exiting Datanode\n15/08/04 12:01:26 INFO util.ExitUtil: Exiting with status 0\n15/08/04 12:01:26 INFO datanode.DataNode: SHUTDOWN_MSG:\n<code>","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-08-04T04:40:27.885+0000","updated":"2015-08-04T04:40:27.885+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12851297/comment/14653056","id":"14653056","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"body":"There is a IOException when read VERSION because of the disk is bad, it will causes datanode failed to start. I think we should handle the exception during init storage.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-08-04T04:48:33.449+0000","updated":"2015-08-04T04:48:33.449+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12851297/comment/14653205","id":"14653205","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"body":"data12 is the bad disk\n[hadoop@hadoop070 data12]$ ll\nls: reading directory .: Input/output error\ntotal 0","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-08-04T07:34:15.454+0000","updated":"2015-08-04T07:34:15.454+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12851297/comment/14659223","id":"14659223","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"Is this a dupe of HDFS-8636? I have a half-done patch for that I haven't gotten around to, but you can feel free to take it over if you want.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-05T23:58:06.257+0000","updated":"2015-08-05T23:58:06.257+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12851297/comment/14659431","id":"14659431","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"body":"here is the total log\nSTARTUP_MSG:   java = 1.7.0_71\n************************************************************/\n2015-08-04 10:18:50,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\n2015-08-04 10:18:50,344 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data1/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,344 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data2/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data3/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data4/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data5/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data6/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data7/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data8/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data9/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data10/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,345 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data11/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,346 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data12/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:50,673 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2015-08-04 10:18:50,692 INFO org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Sink ganglia started\n2015-08-04 10:18:50,705 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2015-08-04 10:18:50,705 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started\n2015-08-04 10:18:50,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop070.dx.momo.com\n2015-08-04 10:18:50,707 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0\n2015-08-04 10:18:50,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010\n2015-08-04 10:18:50,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 67108864 bytes/s\n2015-08-04 10:18:50,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 30\n2015-08-04 10:18:50,791 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n2015-08-04 10:18:50,794 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined\n2015-08-04 10:18:50,804 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n2015-08-04 10:18:50,807 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\n2015-08-04 10:18:50,807 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n2015-08-04 10:18:50,807 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n2015-08-04 10:18:50,820 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\n2015-08-04 10:18:50,822 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075\n2015-08-04 10:18:50,822 INFO org.mortbay.log: jetty-6.1.26\n2015-08-04 10:18:50,983 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075\n2015-08-04 10:18:51,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop\n2015-08-04 10:18:51,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup\n2015-08-04 10:18:51,107 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-08-04 10:18:51,120 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020\n2015-08-04 10:18:51,120 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #2 for port 50020\n2015-08-04 10:18:51,120 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #3 for port 50020\n2015-08-04 10:18:51,120 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #4 for port 50020\n2015-08-04 10:18:51,120 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #5 for port 50020\n2015-08-04 10:18:51,120 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #6 for port 50020\n2015-08-04 10:18:51,121 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #7 for port 50020\n2015-08-04 10:18:51,121 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #8 for port 50020\n2015-08-04 10:18:51,121 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #9 for port 50020\n2015-08-04 10:18:51,121 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #10 for port 50020\n2015-08-04 10:18:51,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020\n2015-08-04 10:18:51,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: nameservice1\n2015-08-04 10:18:51,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: nameservice1\n2015-08-04 10:18:51,230 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data1/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data2/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data3/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data4/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data5/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data6/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data7/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data8/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data9/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data10/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data11/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,231 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data12/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop002.dx.momo.com/10.84.100.191:8022 starting to offer service\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data1/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data2/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data3/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data4/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data5/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data6/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data7/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data8/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,234 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data9/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,235 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data10/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,235 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data11/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,235 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data12/dfs/dn should be specified as a URI in configuration files. Please update hdfs configuration.\n2015-08-04 10:18:51,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop001.dx.momo.com/10.84.100.171:8022 starting to offer service\n2015-08-04 10:18:51,241 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting\n2015-08-04 10:18:51,241 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2015-08-04 10:18:51,405 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57\n2015-08-04 10:18:51,408 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data1/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,409 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data2/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,409 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data3/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,410 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data4/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,410 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data5/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,411 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data6/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,411 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data7/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,412 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data8/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,412 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data9/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,413 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data10/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,413 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data11/dfs/dn/in_use.lock acquired by nodename 23928@hadoop070.dx.momo.com\n2015-08-04 10:18:51,417 WARN org.apache.hadoop.hdfs.server.common.Storage: Ignoring storage directory /data12/dfs/dn due to an exception\njava.io.FileNotFoundException: /data12/dfs/dn/in_use.lock (Input/output error)\n\tat java.io.RandomAccessFile.open(Native Method)\n\tat java.io.RandomAccessFile.<init>(RandomAccessFile.java:241)\n\tat org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:697)\n\tat org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:669)\n\tat org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:493)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:186)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:254)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:975)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:946)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:278)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:812)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-08-04 10:18:51,470 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-454299492-10.84.100.171-1416301904728\n2015-08-04 10:18:51,470 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,471 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,471 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,471 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,471 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,472 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:51,474 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to hadoop002.dx.momo.com/10.84.100.191:8022 Input/output error\n2015-08-04 10:18:56,464 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-454299492-10.84.100.171-1416301904728\n2015-08-04 10:18:56,464 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,464 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,464 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,464 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,464 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,465 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,467 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,467 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,467 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,467 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to hadoop001.dx.momo.com/10.84.100.171:8022. Exiting.\njava.io.IOException: Input/output error\n\tat java.io.FileInputStream.readBytes(Native Method)\n\tat java.io.FileInputStream.read(FileInputStream.java:243)\n\tat java.util.Properties$LineReader.readLine(Properties.java:434)\n\tat java.util.Properties.load0(Properties.java:353)\n\tat java.util.Properties.load(Properties.java:341)\n\tat org.apache.hadoop.hdfs.server.common.StorageInfo.readPropertiesFile(StorageInfo.java:247)\n\tat org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:227)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:256)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:155)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:269)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:975)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:946)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:278)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:812)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-08-04 10:18:56,468 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to hadoop001.dx.momo.com/10.84.100.171:8022\n2015-08-04 10:18:56,523 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-454299492-10.84.100.171-1416301904728\n2015-08-04 10:18:56,523 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,523 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,524 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,525 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2015-08-04 10:18:56,526 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to hadoop002.dx.momo.com/10.84.100.191:8022. Exiting.\njava.io.IOException: Input/output error\n\tat java.io.FileInputStream.readBytes(Native Method)\n\tat java.io.FileInputStream.read(FileInputStream.java:243)\n\tat java.util.Properties$LineReader.readLine(Properties.java:434)\n\tat java.util.Properties.load0(Properties.java:353)\n\tat java.util.Properties.load(Properties.java:341)\n\tat org.apache.hadoop.hdfs.server.common.StorageInfo.readPropertiesFile(StorageInfo.java:247)\n\tat org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:227)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:256)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:155)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:269)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:975)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:946)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:278)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:812)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-08-04 10:18:56,526 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to hadoop002.dx.momo.com/10.84.100.191:8022\n2015-08-04 10:18:56,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)\n2015-08-04 10:18:58,527 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode\n2015-08-04 10:18:58,529 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0\n2015-08-04 10:18:58,530 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG:","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-08-06T03:47:10.405+0000","updated":"2015-08-06T03:47:10.405+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12851297/comment/14659519","id":"14659519","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"body":"I think the bad data dir should not be added in storageDirs in the method recoverTransitionRead of Class BlockPoolSliceStorage","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-08-06T05:31:06.924+0000","updated":"2015-08-06T05:31:06.924+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12851297/comment/14659540","id":"14659540","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"Yea, looks like the same error I saw in HDFS-8636 during block pool initialization. I'll dupe it to the other one I mentioned, please re-open if you disagree.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-06T06:02:07.502+0000","updated":"2015-08-06T06:02:07.502+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12851297/comment/14659602","id":"14659602","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"body":"ok, it looks like the same error.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wh831019","name":"wh831019","key":"wh831019","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wang Hao","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-08-06T07:15:54.662+0000","updated":"2015-08-06T07:15:54.662+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-8851/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2ias7:"}}