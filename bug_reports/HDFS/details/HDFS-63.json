{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12417131","self":"https://issues.apache.org/jira/rest/api/2/issue/12417131","key":"HDFS-63","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/2","id":"2","description":"The problem described is an issue which will never be fixed.","name":"Won't Fix"},"customfield_12312322":null,"customfield_12310220":"2009-07-23T08:50:29.058+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jan 25 21:43:47 UTC 2012","customfield_12310420":"17137","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_90174972584_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2012-01-25T21:43:47.650+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-63/watchers","watchCount":4,"isWatching":false},"created":"2009-03-18T05:07:35.167+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2012-01-25T21:43:47.742+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Here is the situation - DFS cluster running Hadoop version 0.19.0. The cluster is running on multiple servers with practically identical hardware. \nEverything works perfectly well, except for one thing - from time to time one of the data nodes (every time it's a different node) starts to consume more and more disk space. The node keeps going and if we don't do anything - it runs out of space completely (ignoring 20GB reserved space settings). \nOnce restarted - it cleans disk rapidly and goes back to approximately the same utilization as the rest of data nodes in the cluster.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"108361","customfield_12312823":null,"summary":"Datanode stops cleaning disk space","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ibolotin","name":"ibolotin","key":"ibolotin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Igor Bolotin","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ibolotin","name":"ibolotin","key":"ibolotin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Igor Bolotin","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Linux","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12417131/comment/12682916","id":"12682916","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ibolotin","name":"ibolotin","key":"ibolotin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Igor Bolotin","active":true,"timeZone":"America/Los_Angeles"},"body":"DF and DU sizes on the datanode match very closely with information reported by dfsadmin command. \nLsof reports some 1000 open files in DFS data directories on the problematic datanode, but total size for open files is only about 10GB.\n\nHere is something interesting - fsck before datanode restart reports very significant number of over-replicated blocks (~10% of blocks are over-replicated):\n\nStatus: HEALTHY\n Total size:    1472758591906 B (Total open files size: 29050588133 B)                                                      \n Total dirs:    58431                                                                                                       \n Total files:   375703 (Files currently being written: 418)                                                                 \n Total blocks (validated):      387205 (avg. block size 3803562 B) (Total open file blocks (not validated): 595)            \n Minimally replicated blocks:   387205 (100.0 %)                                                                            \n Over-replicated blocks:        38782 (10.015883 %)                                                                         \n Under-replicated blocks:       0 (0.0 %)                                                                                   \n Mis-replicated blocks:         0 (0.0 %)                                                                                   \n Default replication factor:    3                                                                                           \n Average block replication:     3.1003888                                                                                   \n Corrupt blocks:                0                                                                                           \n Missing replicas:              0 (0.0 %)                                                                                   \n Number of data-nodes:          7                                                                                           \n Number of racks:               1                                                                                           \n\nAfter datanode restart - over-replicated nodes are practically gone:\n\nStatus: HEALTHY\n Total size:    1310669475298 B (Total open files size: 29535016933 B)\n Total dirs:    59431\n Total files:   377177 (Files currently being written: 387)\n Total blocks (validated):      386661 (avg. block size 3389712 B) (Total open file blocks (not validated): 607)\n Minimally replicated blocks:   386661 (100.0 %)\n Over-replicated blocks:        272 (0.070345856 %)\n Under-replicated blocks:       0 (0.0 %)\n Mis-replicated blocks:         0 (0.0 %)\n Default replication factor:    3\n Average block replication:     3.0007036\n Corrupt blocks:                0\n Missing replicas:              0 (0.0 %)\n Number of data-nodes:          7\n Number of racks:               1\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ibolotin","name":"ibolotin","key":"ibolotin","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Igor Bolotin","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-03-18T05:08:50.750+0000","updated":"2009-03-18T05:08:50.750+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12417131/comment/12734516","id":"12734516","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=skamio","name":"skamio","key":"skamio","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shotaro Kamio","active":true,"timeZone":"Asia/Tokyo"},"body":"I have the same problem on hadoop-0.18.2 + hbase-0.18.1. The one or more nodes in our cluster run out of disk once or twice a week. It gets full more frequently than before. It was less than once a week.\nIs this solved in the latest version?\n\nOur cluster have 5 data nodes. Hbase region server is running on each of them. Usually, read/write (100~1000 requests/sec) comes into hbase via thrift python interface. Heavy hbase map-reduce job runs twice a day.\n\nToday, two nodes got almost full. Other node has 300GB free space. So, I suspect that 300 GB was used for over-replicated blocks.\nAfter dfs was restarted, it deletes many blocks from datanode. Usually, it takes many hours to finish deleting.\n\nThe fsck status when the problem occurs:\n-----\nStatus: HEALTHY\n Total size: 353225642094 B\n Total dirs: 38768\n Total files: 47456\n Total blocks (validated): 51146 (avg. block size 6906222 B)\n Minimally replicated blocks: 51146 (100.0 %)\n Over-replicated blocks: 108 (0.21116021 %)\n Under-replicated blocks: 0 (0.0 %)\n Mis-replicated blocks: 0 (0.0 %)\n Default replication factor: 3\n Average block replication: 3.0021117\n Corrupt blocks: 0\n Missing replicas: 0 (0.0 %)\n Number of data-nodes: 5\n Number of racks: 1\n-----\n\n\nMany errors are reported in datanode log. I don't know if this error is related to this problem.\n\n----\n2009-07-23 16:09:41,594 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.180.10.51:50010, storageID=DS-1052523904-10.180.10.51-50010-1247187249581, infoPort=50075, ipcPort=50020):DataXceiver: java.net.SocketTimeoutException: Read timed out\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.read(SocketInputStream.java:129)\n        at java.net.SocketInputStream.read(SocketInputStream.java:182)\n        at java.io.DataInputStream.readByte(DataInputStream.java:248)\n        at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:324)\n        at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:345)\n        at org.apache.hadoop.io.Text.readString(Text.java:410)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1270)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1076)\n        at java.lang.Thread.run(Thread.java:619)\n\n2009-07-23 16:09:42,889 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.180.10.51:50010, storageID=DS-1052523904-10.180.10.51-50010-1247187249581, infoPort=50075, ipcPort=50020):DataXceiver: java.net.SocketTimeoutException: Read timed out\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.read(SocketInputStream.java:129)\n        at java.net.SocketInputStream.read(SocketInputStream.java:182)\n        at java.io.DataInputStream.readByte(DataInputStream.java:248)\n        at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:324)\n        at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:345)\n        at org.apache.hadoop.io.Text.readString(Text.java:410)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1270)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1076)\n        at java.lang.Thread.run(Thread.java:619)\n-----\n2009-07-23 16:32:40,702 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.180.10.51:50010, storageID=DS-1052523904-10.180.10.51-50010-1247187249581, infoPort=50075, ipcPort=50020):DataXceiver: java.io.EOFException: while trying to read 65557 bytes\n        at org.apache.hadoop.dfs.DataNode$BlockReceiver.readToBuf(DataNode.java:2508)\n        at org.apache.hadoop.dfs.DataNode$BlockReceiver.readNextPacket(DataNode.java:2552)\n        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2616)\n        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2742)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1314)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1076)\n        at java.lang.Thread.run(Thread.java:619)\n\n2009-07-23 16:32:40,706 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.180.10.51:50010, storageID=DS-1052523904-10.180.10.51-50010-1247187249581, infoPort=50075, ipcPort=50020):DataXceiver: java.io.EOFException: while trying to read 65557 bytes\n        at org.apache.hadoop.dfs.DataNode$BlockReceiver.readToBuf(DataNode.java:2508)\n        at org.apache.hadoop.dfs.DataNode$BlockReceiver.readNextPacket(DataNode.java:2552)\n        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2616)\n        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2742)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1314)\n        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1076)\n        at java.lang.Thread.run(Thread.java:619)\n----\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=skamio","name":"skamio","key":"skamio","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Shotaro Kamio","active":true,"timeZone":"Asia/Tokyo"},"created":"2009-07-23T08:50:29.058+0000","updated":"2009-07-23T08:50:29.058+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12417131/comment/12881537","id":"12881537","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"body":"\nis this issue there in 0.20 versions????","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=slukog","name":"slukog","key":"slukog","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Gokul","active":true,"timeZone":"Etc/UTC"},"created":"2010-06-23T04:32:44.690+0000","updated":"2010-06-23T04:32:44.690+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12417131/comment/12970798","id":"12970798","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prateek","name":"prateek","key":"prateek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prateek Sharma","active":true,"timeZone":"Etc/UTC"},"body":"We are experiencing the same issue in Hadoop 0.18.2. Over the time, one of the datanodes gets its disk space clogged, but on subsequent restart of the process, the disk usage evens out. Is the issue still persistent in the upgraded 0.20 versions?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=prateek","name":"prateek","key":"prateek","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Prateek Sharma","active":true,"timeZone":"Etc/UTC"},"created":"2010-12-13T12:56:50.340+0000","updated":"2010-12-13T12:56:50.340+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12417131/comment/13193362","id":"13193362","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"This is a very old bug that has not been reported on 0.20 and later releases. Closing the bug for now. If this is still a problem, please reopen the jira.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-01-25T21:43:47.713+0000","updated":"2012-01-25T21:43:47.713+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-63/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0iwif:"}}