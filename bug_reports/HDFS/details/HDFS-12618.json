{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13108048","self":"https://issues.apache.org/jira/rest/api/2/issue/13108048","key":"HDFS-12618","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-10-17T15:21:14.631+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Mar 15 19:13:18 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-12618/watchers","watchCount":5,"isWatching":false},"created":"2017-10-09T18:38:03.771+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"7.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340638","id":"12340638","name":"3.0.0-alpha3","archived":false,"released":true,"releaseDate":"2017-05-26"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-03-15T19:13:18.156+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312944","id":"12312944","name":"tools"}],"timeoriginalestimate":null,"description":"When snapshot is enabled, if a file is deleted but is contained by a snapshot, *fsck* will not reported blocks for such file, showing different number of *total blocks* than what is exposed in the Web UI. \r\n\r\nThis should be fine, as *fsck* provides *-includeSnapshots* option. The problem is that *-includeSnapshots* option causes *fsck* to count blocks for every occurrence of a file on snapshots, which is wrong because these blocks should be counted only once (for instance, if a 100MB file is present on 3 snapshots, it would still map to one block only in hdfs). This causes fsck to report much more blocks than what actually exist in hdfs and is reported in the Web UI.\r\n\r\nHere's an example:\r\n\r\n1) HDFS has two files of 2 blocks each:\r\n\r\n{noformat}\r\n$ hdfs dfs -ls -R /\r\ndrwxr-xr-x   - root supergroup          0 2017-10-07 21:21 /snap-test\r\n-rw-r--r--   1 root supergroup  209715200 2017-10-07 20:16 /snap-test/file1\r\n-rw-r--r--   1 root supergroup  209715200 2017-10-07 20:17 /snap-test/file2\r\ndrwxr-xr-x   - root supergroup          0 2017-05-13 13:03 /test\r\n{noformat} \r\n\r\n2) There are two snapshots, with the two files present on each of the snapshots:\r\n\r\n{noformat}\r\n$ hdfs dfs -ls -R /snap-test/.snapshot\r\ndrwxr-xr-x   - root supergroup          0 2017-10-07 21:21 /snap-test/.snapshot/snap1\r\n-rw-r--r--   1 root supergroup  209715200 2017-10-07 20:16 /snap-test/.snapshot/snap1/file1\r\n-rw-r--r--   1 root supergroup  209715200 2017-10-07 20:17 /snap-test/.snapshot/snap1/file2\r\ndrwxr-xr-x   - root supergroup          0 2017-10-07 21:21 /snap-test/.snapshot/snap2\r\n-rw-r--r--   1 root supergroup  209715200 2017-10-07 20:16 /snap-test/.snapshot/snap2/file1\r\n-rw-r--r--   1 root supergroup  209715200 2017-10-07 20:17 /snap-test/.snapshot/snap2/file2\r\n{noformat}\r\n\r\n3) *fsck -includeSnapshots* reports 12 blocks in total (4 blocks for the normal file path, plus 4 blocks for each snapshot path):\r\n\r\n{noformat}\r\n$ hdfs fsck / -includeSnapshots\r\nFSCK started by root (auth:SIMPLE) from /127.0.0.1 for path / at Mon Oct 09 15:15:36 BST 2017\r\n\r\nStatus: HEALTHY\r\n Number of data-nodes:\t1\r\n Number of racks:\t\t1\r\n Total dirs:\t\t\t6\r\n Total symlinks:\t\t0\r\n\r\nReplicated Blocks:\r\n Total size:\t1258291200 B\r\n Total files:\t6\r\n Total blocks (validated):\t12 (avg. block size 104857600 B)\r\n Minimally replicated blocks:\t12 (100.0 %)\r\n Over-replicated blocks:\t0 (0.0 %)\r\n Under-replicated blocks:\t0 (0.0 %)\r\n Mis-replicated blocks:\t\t0 (0.0 %)\r\n Default replication factor:\t1\r\n Average block replication:\t1.0\r\n Missing blocks:\t\t0\r\n Corrupt blocks:\t\t0\r\n Missing replicas:\t\t0 (0.0 %)\r\n{noformat}\r\n\r\n4) Web UI shows the correct number (4 blocks only):\r\n{noformat}\r\nSecurity is off.\r\nSafemode is off.\r\n5 files and directories, 4 blocks = 9 total filesystem object(s).\r\n{noformat}\r\n\r\nI would like to work on this solution, will propose an initial solution shortly.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12891116","id":"12891116","filename":"HDFS-121618.initial","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-09T18:46:12.192+0000","size":4525,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12891116/HDFS-121618.initial"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12893245","id":"12893245","filename":"HDFS-12618.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-20T12:24:47.350+0000","size":9229,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12893245/HDFS-12618.001.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12893975","id":"12893975","filename":"HDFS-12618.002.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-25T16:56:11.302+0000","size":11951,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12893975/HDFS-12618.002.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12894634","id":"12894634","filename":"HDFS-12618.003.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-29T15:38:03.402+0000","size":11004,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12894634/HDFS-12618.003.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12899252","id":"12899252","filename":"HDFS-12618.004.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-11-25T16:54:35.977+0000","size":32727,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12899252/HDFS-12618.004.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12904286","id":"12904286","filename":"HDFS-12618.005.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2018-01-02T21:19:06.832+0000","size":33621,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12904286/HDFS-12618.005.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12904662","id":"12904662","filename":"HDFS-12618.006.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2018-01-04T19:23:42.452+0000","size":36821,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12904662/HDFS-12618.006.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"fsck -includeSnapshots reports wrong amount of total blocks","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16197489","id":"16197489","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Initial patch version for review only. Still need to work on tests for the changes. Will be providing another patch later with tests added.\r\n\r\nPlease let me know on any suggestions/improvements you may have.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-09T18:46:43.409+0000","updated":"2017-10-09T18:46:43.409+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16204794","id":"16204794","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"I was going to add some tests to TestFsck, but for some reason when running the tests on trunk (without my changes), am getting below errors for some already existing tests:\r\n\r\n{noformat}\r\nRunning org.apache.hadoop.hdfs.server.namenode.TestFsck\r\nTests run: 32, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 136.329 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestFsck\r\ntestFsckCorruptECFile(org.apache.hadoop.hdfs.server.namenode.TestFsck)  Time elapsed: 3.022 sec  <<< ERROR!\r\njava.lang.IllegalStateException: failed to create a child event loop\r\n\tat sun.nio.ch.KQueueArrayWrapper.init(Native Method)\r\n\tat sun.nio.ch.KQueueArrayWrapper.<init>(KQueueArrayWrapper.java:98)\r\n\tat sun.nio.ch.KQueueSelectorImpl.<init>(KQueueSelectorImpl.java:88)\r\n\tat sun.nio.ch.KQueueSelectorProvider.openSelector(KQueueSelectorProvider.java:42)\r\n\tat io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)\r\n\tat io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)\r\n\tat io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)\r\n\tat io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)\r\n\tat org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:132)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:954)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1402)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:497)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2778)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2681)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1635)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:882)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:494)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:453)\r\n\tat org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckCorruptECFile(TestFsck.java:2304)\r\n\r\ntestBlockIdCKDecommission(org.apache.hadoop.hdfs.server.namenode.TestFsck)  Time elapsed: 2.702 sec  <<< ERROR!\r\njava.lang.IllegalStateException: failed to create a child event loop\r\n\tat sun.nio.ch.IOUtil.makePipe(Native Method)\r\n\tat sun.nio.ch.KQueueSelectorImpl.<init>(KQueueSelectorImpl.java:84)\r\n\tat sun.nio.ch.KQueueSelectorProvider.openSelector(KQueueSelectorProvider.java:42)\r\n\tat io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)\r\n\tat io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)\r\n\tat io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)\r\n\tat io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)\r\n\tat org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:132)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:954)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1402)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:497)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2778)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2681)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1635)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:882)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:494)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:453)\r\n\tat org.apache.hadoop.hdfs.server.namenode.TestFsck.testBlockIdCKDecommission(TestFsck.java:1504)\r\n\r\ntestFsckOpenECFiles(org.apache.hadoop.hdfs.server.namenode.TestFsck)  Time elapsed: 0.428 sec  <<< ERROR!\r\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Too many failed volumes - current valid volumes: 0, volumes configured: 2, volumes failed: 2, volume failures tolerated: 0\r\n\tat org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.check(StorageLocationChecker.java:220)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2771)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2681)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1635)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:882)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:494)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:453)\r\n\tat org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckOpenECFiles(TestFsck.java:689)\r\n\r\ntestBlockIdCK(org.apache.hadoop.hdfs.server.namenode.TestFsck)  Time elapsed: 2.376 sec  <<< ERROR!\r\njava.lang.IllegalStateException: failed to create a child event loop\r\n\tat sun.nio.ch.KQueueArrayWrapper.init(Native Method)\r\n\tat sun.nio.ch.KQueueArrayWrapper.<init>(KQueueArrayWrapper.java:98)\r\n\tat sun.nio.ch.KQueueSelectorImpl.<init>(KQueueSelectorImpl.java:88)\r\n\tat sun.nio.ch.KQueueSelectorProvider.openSelector(KQueueSelectorProvider.java:42)\r\n\tat io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)\r\n\tat io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)\r\n\tat io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)\r\n\tat io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)\r\n\tat io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)\r\n\tat org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:132)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:954)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1402)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:497)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2778)\r\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2681)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1635)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:882)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:494)\r\n\tat org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:453)\r\n\tat org.apache.hadoop.hdfs.server.namenode.TestFsck.testBlockIdCK(TestFsck.java:1449)\r\n\r\n\r\nResults :\r\n\r\nTests in error: \r\n  TestFsck.testFsckCorruptECFile:2304 » IllegalState failed to create a child ev...\r\n  TestFsck.testBlockIdCKDecommission:1504 » IllegalState failed to create a chil...\r\n  TestFsck.testFsckOpenECFiles:689 » DiskError Too many failed volumes - current...\r\n  TestFsck.testBlockIdCK:1449 » IllegalState failed to create a child event loop\r\n{noformat}\r\n\r\nIs this something environmental? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-14T19:36:02.442+0000","updated":"2017-10-14T19:36:02.442+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16204856","id":"16204856","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Definitely environmental, tests are passing on trunk local branch after restarting my machine. Am working on tests for this change.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-14T23:35:59.449+0000","updated":"2017-10-14T23:43:57.636+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16207571","id":"16207571","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Initial patch with additional unit test for the reported block count.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-17T12:30:29.341+0000","updated":"2017-10-17T12:30:29.341+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16207768","id":"16207768","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 13s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 31s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 53s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  6s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 39s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 3 new + 91 unchanged - 4 fixed = 94 total (was 95) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 36s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 22s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 88m 28s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}139m 17s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.qjournal.server.TestJournalNodeSync |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker |  Image:yetus/hadoop:0de40f0 |\r\n| JIRA Issue | HDFS-12618 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12892595/HDFS-121618.001.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux 3f5ab7aa89b6 3.13.0-123-generic #172-Ubuntu SMP Mon Jun 26 18:04:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 31ebccc |\r\n| Default Java | 1.8.0_144 |\r\n| findbugs | v3.1.0-RC1 |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/21726/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/21726/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21726/testReport/ |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21726/console |\r\n| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-10-17T15:21:14.631+0000","updated":"2017-10-17T15:21:14.631+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16212555","id":"16212555","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Attaching a new patch with checkstyle issues fixed. Also had fixed patch name, previously submitted patch had a typo on the jira id.\r\n\r\nLast test failure seems unrelated, as the same is passing locally.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-20T12:26:05.584+0000","updated":"2017-10-20T12:26:05.584+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16212858","id":"16212858","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Findbugs executables are not available. {color} |\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 13s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 34s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 50s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 22s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 31s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 91 unchanged - 4 fixed = 92 total (was 95) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  8m 57s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 82m 22s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 21s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}119m 46s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.web.TestWebHdfsTimeouts |\r\n|   | hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |\r\n|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker |  Image:yetus/hadoop:ca8ddc6 |\r\n| JIRA Issue | HDFS-12618 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12893245/HDFS-12618.001.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux fb6aa782d100 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 1f4cdf1 |\r\n| Default Java | 1.8.0_131 |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/21763/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/21763/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21763/testReport/ |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21763/console |\r\n| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-10-20T16:40:26.665+0000","updated":"2017-10-20T16:40:26.665+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16213269","id":"16213269","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for reporting and working on this [~wchevreuil]!\r\n\r\nI agree this is an issue, but not sure what the best solution would be - this appears to be a difficult problem. Let me research into this too see if any other ideas pop up.\r\n\r\nThe reason webui shows correctly is it's just showing the total number of blocks from the block manager. I'm afraid we don't have information as to 'Total blocks under to a directory', so not useful for fsck.\r\n\r\nSome issues I can see from the current patch:\r\n- {{snapshotSeenBlocks}} could be huge, if the snapshot dir is at the top level, and has a lot of blocks under it. In extreme cases, this may put pressure on NN. At the minimum we should make sure the extra space is allocated only when {{-includeSnapshots}} is set.\r\n- where inodes are involved, file system locks are required. This adds burden to the NN which should be minimized.\r\n- Catching a {{RunTimeException}} and re-throw looks like a bad practice. What are the RTEs that could be thrown that we need to wrap?\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-10-20T21:26:51.569+0000","updated":"2017-10-20T21:26:51.569+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16214955","id":"16214955","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Hi [~xiaochen], thanks for reviewing and sharing your thoughts. Yeah, for Web UI is just a matter of showing the total number of blocks currently mapped on BlockManager. Regarding the issues you had highlighted previously, I have some comments below:\r\n\r\nbq. snapshotSeenBlocks could be huge, if the snapshot dir is at the top level, and has a lot of blocks under it. In extreme cases, this may put pressure on NN. At the minimum we should make sure the extra space is allocated only when -includeSnapshots is set.\r\nThat's indeed something that concerned me while thinking on this. To minimize heap pressure effects, this only adds the 1st block of each snapshot file already checked. It also adds only blocks from files that exist only os snapshots (files that had been already deleted). So, although the *ArrayList* is created on a generic scope, it will only be really populated with blocks in case where \"-includeSnapshots\" is passed. Unfortunately, because *checkDir* is invoked recursively, I needed to initialise the *ArrayList* at first cal to *checkDir*, from *check* method.\r\n\r\nbq. where inodes are involved, file system locks are required. This adds burden to the NN which should be minimized.\r\nI guess this is related to this block of code below, where I'm not acquiring any namesystem lock (apart from the one done on *getBlockLocations*), which could lead to inconsistent report of blocks:\r\n{noformat}\r\n      FSDirectory dir = namenode.getNamesystem().getFSDirectory();\r\n      final INodesInPath iip = dir.getINodesInPath(filePath,\r\n          FSDirectory.DirOp.READ);\r\n      final INodeFile inode = INodeFile.valueOf(iip.getLastINode(), filePath);\r\n      LocatedBlocks blocks = getBlockLocations(filePath, snapshotFileInfo);\r\n      printFileStats(filePath, blocks, snapshotFileInfo);\r\n      if(inode.isWithSnapshot()){\r\n        if(!seenBlocks.contains(inode.getBlocks()[0].getBlockName())) {\r\n          replRes.totalBlocks += inode.getBlocks().length;\r\n          seenBlocks.add(inode.getBlocks()[0].getBlockName());\r\n        }\r\n      }\r\n{noformat}\r\n\r\nI guess that should be rewritten as below:\r\n\r\n{noformat}\r\nFSDirectory dir = namenode.getNamesystem().getFSDirectory();\r\n      final INodesInPath iip = dir.getINodesInPath(filePath,\r\n          FSDirectory.DirOp.READ);\r\n      namenode.getNamesystem().readLock();\r\n      try {\r\n        final INodeFile inode = INodeFile.valueOf(iip.getLastINode(), filePath);\r\n        if (inode.isWithSnapshot()) {\r\n          if (!seenBlocks.contains(inode.getBlocks()[0].getBlockName())) {\r\n            replRes.totalBlocks += inode.getBlocks().length;\r\n            seenBlocks.add(inode.getBlocks()[0].getBlockName());\r\n          }\r\n        }\r\n      }finally {\r\n        namenode.getNamesystem().readUnlock();\r\n      }\r\n      LocatedBlocks blocks = getBlockLocations(filePath, snapshotFileInfo);\r\n      printFileStats(filePath, blocks, snapshotFileInfo);\r\n{noformat}\r\nAgreed that this can cause concurrency issues. Still this code would be executed only for snapshot dirs. I can't see other way to get the block info details required to avoid over counting these blocks.\r\n\r\nbq.Catching a RunTimeException and re-throw looks like a bad practice. What are the RTEs that could be thrown that we need to wrap?\r\nThis was a problem I came with while using java lambda functions. The problem is that I had to declare a *java.util.function.Consumer* param on *checkDir* signature, in order to be able to pass both *checkFilesInSnapshotOnly* and *check* calls as lambda functions, but *java.util.function.Consumer.accept* does not declare any checked exception. That causes any *IOException* thrown by *checkFilesInSnapshotOnly* or *check* methods when executed from a lambda context to be suppressed. Only way I had found to keep any of the *IOException* occurrences to be tracked on the outer method was to wrap it on a RTE, forcibly catch it on the outer method and recover the original *IOException* again by unwrapping it. I still thought using lambdas though would require less refactoring while avoiding code duplication.\r\n\r\nPlease share ony other ideas/concerns you may have.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-23T10:49:32.020+0000","updated":"2017-10-23T10:49:32.020+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16215919","id":"16215919","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the explanation [~wchevreuil]. The arraylist and lambda exception wrapping LGTM. I also looked around this area, and the approach here looks to be the best we can get now. Only caching the 1st block is a smart way. :) Perhaps we could add a warning message to the fsck usage output, to warn the potential impact of -includeSnapshot and -blocks to the NN.\r\n\r\nYou're correct about the locking code, I was talking about that block. Sorry I wasn't explicit.\r\nLooking at that code, there are several improvements I could see:\r\n- {{getINodesInPath}} should be done within the readlock.\r\n- {{inode}} could be gotten from {{iip.getLastINode}}.\r\n- Need to check and handle inode==null\r\n\r\nAlso, could you format the code to follow existing style? We usually put a space to separate parenthesis / brackets. I thought there was an official format file but couldn't find it, just forwarded you the format XML offline.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-10-23T21:56:00.864+0000","updated":"2017-10-23T21:56:00.864+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16219002","id":"16219002","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Thanks for the review and advices, [~xiaochen]! Attaching a new patch version with your recommendations applied. \r\n\r\nAs always, please Let me know on any issues/concerns you may have.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-25T16:28:24.237+0000","updated":"2017-10-25T16:28:24.237+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16219479","id":"16219479","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  8m 41s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 44s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 27s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 59s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 35s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 41s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 40s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  8s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 25s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 25s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 25s{color} | {color:green} the patch passed {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 54s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 115 unchanged - 4 fixed = 116 total (was 119) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 33s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 17s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 42s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  6s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}129m  4s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}200m  9s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\r\n|   | hadoop.hdfs.TestMissingBlocksAlert |\r\n|   | hadoop.hdfs.TestLeaseRecovery2 |\r\n|   | hadoop.hdfs.TestSafeModeWithStripedFileWithRandomECPolicy |\r\n|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |\r\n|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery |\r\n|   | hadoop.hdfs.server.datanode.TestDirectoryScanner |\r\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |\r\n|   | hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker |  Image:yetus/hadoop:5b98639 |\r\n| JIRA Issue | HDFS-12618 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12893975/HDFS-12618.002.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux 08f1c6d304e4 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 5b98639 |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_131 |\r\n| findbugs | v3.1.0-RC1 |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/21823/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/21823/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21823/testReport/ |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21823/console |\r\n| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-10-25T20:37:48.514+0000","updated":"2017-10-25T20:37:48.514+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16219694","id":"16219694","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"I understand the problem, but as soon as you update documentation to warn users with:\r\n{code}\r\n+      + \"\\t2. Option -includeSnapshots should be used with caution. For \"\r\n+      + \"clusters with too many files deleted but available on snapshots \"\r\n+      + \"only, this would require extra NameNode heap to account its blocks \"\r\n+      + \"properly.\";\r\n{code}\r\n\r\nThat should set off alarms.  I assumed it would be a repeat of a reverted content summary patch that put all visited inodes in a set.   It essentially is, but worse.  It's an array list.  I then contemplated whether I cared enough to review because I have no intention of using snapshots.  Then I realized some fatefully day I'll be forced to support them and this will blow up my NN.\r\n\r\nFor that reason alone: *-1*.\r\n\r\nIgnoring my rejection, I'm not even sure the logic for checking just the first block is even correct in light of truncate and append.\r\n\r\nI'm not a lambda expect, but creating a primitive array that requires forced casting of indexed element appears to be an abuse of the construct.  The wrapping and unwrapping of IOEs as suppressed exceptions embedded in RuntimeExceptions in an apparent attempt to thwart checked dependencies is also unnecessary and appears related to the lamba construct.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2017-10-25T22:50:44.375+0000","updated":"2017-10-25T22:50:44.375+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16219732","id":"16219732","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~daryn],\r\n\r\nThanks for the comment. I surely learned the lesson from HDFS-10797, and had exactly the same concerns in my [early comment|https://issues.apache.org/jira/browse/HDFS-12618?focusedCommentId=16213269&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16213269]\r\n\r\nI did not -1 on this one because this only happens when someone runs -includeSnapshots explicitly. Not sure if these snapshot problems can be solved without doing this, please feel free to share any alternatives in mind. For large clusters doing fsck alone on {{/}} are a bad idea. Would it work for you if we put a memory limit on how much each {{fsck -includeSnapshots}}' block map could consume on the NN?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-10-25T23:31:41.932+0000","updated":"2017-10-25T23:31:41.932+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16220202","id":"16220202","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Thanks for the comments [~daryn].\r\n\r\nbq. Ignoring my rejection, I'm not even sure the logic for checking just the first block is even correct in light of truncate and append.\r\nI was not aware of the implications of truncate/append. Given this check is just relevant for files that had been deleted and reside on snapshots only, would it still be a possibility for these files to be truncated/appended?\r\n\r\nbq. I'm not a lambda expect, but creating a primitive array that requires forced casting of indexed element appears to be an abuse of the construct. The wrapping and unwrapping of IOEs as suppressed exceptions embedded in RuntimeExceptions in an apparent attempt to thwart checked dependencies is also unnecessary and appears related to the lamba construct.\r\nThat was an attempt to use built-in *java.util.function.Consumer* interface, that defines only one parameter on its accept method and throws no Exception. Indeed, looking further into lambda API, I guess this can be sorted by creating an own *@FunctionalInterface* that defines the types and checked exceptions needed by *check* and *checkFilesInSnapshotOnly* methods.\r\n\r\n \r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-26T09:25:09.527+0000","updated":"2017-10-26T09:25:09.527+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16220685","id":"16220685","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"bq.  I did not -1 on this one because this only happens when someone runs -includeSnapshots explicitly. Not sure if these snapshot problems can be solved without doing this, please feel free to share any alternatives in mind. \r\nBeen awhile since I (attempted to) analyze snapshots.  I think snapshotted files, even deleted ones, are a {{INodeReference.WithName}} with a parent of {{INodeReference.WithCount}} which maintains a list of all {{INodeReference.WithName}}.  Perhaps we could detect whether the inode is linked into the current namesystem.  If yes, it will be picked up in the namesystem crawl; if no, count it based on all the {{WithName}} refs.  And/or maybe count a reference only if it's the last ref ({{INodeReference.WithCount#getLastWithName}}).  Maybe.\r\n\r\nbq. For large clusters doing fsck alone on / are a bad idea.\r\nWe do this nightly.   Every day.  Every cluster.\r\n\r\nbq. Would it work for you if we put a memory limit on how much each fsck -includeSnapshots' block map could consume on the NN?\r\nI'm not sure how that could work in a user-friendly manner.  I run the fsck, it fails.  I have to run it again on subdirs?  Some fail again.  I have to run it on lower subdirs, then write code to collate all the mini-reports back together in a unified report?\r\n\r\nFsck can run for tens of minutes or hours.  Keeping excessively large state during the operation will cause lots of pressure on old and risk OOM.  It has to stay lightweight (or only as heavy as it already is).\r\n\r\nbq. Given this check is just relevant for files that had been deleted and reside on snapshots only, would it still be a possibility for these files to be truncated/appended?\r\nI'm by no means a truncate/append + snapshot expert.  A renamed file appears as a delete in a snapshot diff.  It can be subsequent modified in later versions that may also be snapshotted and \"deleted\".  Correct snapshot handling has been problematic so we need to ensure it works correctly in all cases w/o causing significant issues.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2017-10-26T16:05:13.187+0000","updated":"2017-10-26T16:05:13.187+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16224047","id":"16224047","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Thanks for the suggestions and relevant comments so far, [~daryn] and [~xiaochen]!\r\n\r\nI had looked further on which type of snapshot info we can access from related inodes, I believe I had found a way to account blocks properly without needing to use ArrayList for tracking already accounted blocks. Am uploading a new patch with this changes, together with enhancements for the lambda construction. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-10-29T15:41:46.917+0000","updated":"2017-10-29T15:41:46.917+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16224145","id":"16224145","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 19m 26s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m  2s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 45s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 15s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 58s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 16s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 45s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 3 new + 115 unchanged - 4 fixed = 118 total (was 119) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 13s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 46s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 56s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}103m 59s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 19s{color} | {color:red} The patch generated 3 ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}181m 14s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.TestRollingUpgrade |\r\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |\r\n| JIRA Issue | HDFS-12618 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12894634/HDFS-12618.003.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux f65133dcb2b9 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 9114d7a |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_131 |\r\n| findbugs | v3.1.0-RC1 |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/21867/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/21867/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21867/testReport/ |\r\n| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/21867/artifact/out/patch-asflicense-problems.txt |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21867/console |\r\n| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-10-29T18:53:39.259+0000","updated":"2017-10-29T18:53:39.259+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16235007","id":"16235007","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the new patch Wellington.\r\n\r\nFrom a quick look this seems to work, nice job. I'd like to see:\r\n- more thorough unit tests covering the description scenario (2 snapshots referring to a deleted file)\r\n- tests covering some combinations of create / delete snapshot, and verify the number is correct\r\n- not an expert on lambda expert, but it seems {{DirTypeCheck}} could be private.\r\n- In general we'd need to acquire the FSDirectory lock as well as the FSNamesystem lock. So need dir.readLock() after the name system readlock, and dir.readUnlock before the fsn unlock.\r\n- Looks like you have applied a formatter to the entire NamenodeFsck.java (instead of just the changed code), which resulted in some unnecessary changes. Let's not make those changes.\r\n\r\nWill provide a more complete review later this week.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-02T00:19:40.386+0000","updated":"2017-11-02T00:19:40.386+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16238893","id":"16238893","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Thanks [~xiaochen], your last suggestions all make sense to me, will start work on these already. Meanwhile, feel free to make any further observations.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-11-04T10:30:04.145+0000","updated":"2017-11-04T10:30:04.145+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16245239","id":"16245239","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Finally managed to review this with more details, together with Daryn's (great) comments. Also helpful to know Yahoo is able to run fsck on / frequently...\r\n\r\nLooking again at the patch and comments, I think Daryn's comment regarding {{INodeReference.With\\[Name|Count\\]}} is not understood and reflected in this patch. It's not a safe assumption that {{iip.getLastINode()}} will be an {{INodeFile}} - for snapshots, that's usually an {{INodeReference.WithName}}.\r\nAdmittedly snapshot is very complicated and requires a lot of efforts (at least for me) to get things right. One way to look into it is probably with some examples, and debug from {{FSDirRenameOp$RenameOperation}} when a rename happens.\r\n\r\nRelated to the above, suggest we have unit test to cover renames (for the above, since delete doesn't trigger the same INode references link as rename does), and multiple snapshots (to cover when multiple snapshots having different but also overlapping blocks, so we test the not-the-last-WithName path).\r\n\r\nThanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-09T05:45:03.260+0000","updated":"2017-11-09T05:45:03.260+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16246394","id":"16246394","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Thanks [~xiaochen]. Last patch was not indeed handling renames. I had done some further analysis on this, here what I found out:\r\n\r\n1) *iip.getLastINode()* always return an instance of *INodeFile* if the given file under snapshot related folder has not been renamed.\r\n2) If a file within a snapshot gets renamed, the snapshot entry returned by *iip.getLastINode()* will be an instance of *INodeReference$WithName*. While counting for blocks, I guess we can simply ignore these ones and not count blocks for *INodeReference$WithName*, as the blocks will either be accounted outside of snapshot check, or by the new file name.\r\n3) If a snapshot is taken after the rename, the snapshot entry for the new file name returned by *iip.getLastINode()* will be an instance of *INodeReference$DstReference*.\r\n4) The main problem is with renamed files that then got deleted, because then related *INodeReference$DstReference* entries should have blocks accounted. That requires for an additional check on the referred inode path, so that if *getINodesInPath().getLastINode()* returns null, that means this *INodeReference$DstReference* is a deleted renamed file and needs to have its blocks counted.\r\n\r\nFor example:\r\n{noformat}\r\n1) Directory content:\r\n/snap-test/file1\r\n/snap-test/file2\r\n\r\n2) Snapshot is taken:\r\n$ hdfs dfs -createSnasphot /snap-test snap1\r\n\r\n3) *iip.getLastINode()* for */snap-test/.snapshot/snap1/file1* entry will be an instance of *INodeFile*\r\n\r\n4) file1 is renamed to /snap-test/file3. Now *iip.getLastINode()* for */snap-test/.snapshot/snap1/file1* entry will be an instance of *INodeReference$WithName*.\r\n\r\n5) Another snapshot is taken:\r\n$ hdfs dfs -createSnasphot /snap-test snap2\r\n\r\n6) *iip.getLastINode()* for */snap-test/.snapshot/snap2/file3* entry will be an instance of *INodeReference$DstReference*.\r\n\r\n7) If /snap-test/file3 is deleted, *iip.getLastINode()* for */snap-test/.snapshot/snap2/file3* entry still returns an instance of *INodeReference$DstReference*.\r\n\r\n{noformat}\r\n\r\nI will work on a new patch following this solutions, and will add more unit tests to cover further scenarios, such as renames and multiple snapshots references. Let me know on your thoughts, if you see any issues with this strategy.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-11-09T19:49:07.696+0000","updated":"2017-11-09T19:49:07.696+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16246526","id":"16246526","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Yup, rename is what makes replaces a child inode to become inodereference. What you described sounds fine to me. Let's please make sure the test also covers the case that different inode references contain different blocks, and verify there is no over/under counting.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-09T21:07:35.122+0000","updated":"2017-11-09T21:07:35.122+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16247872","id":"16247872","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"bq. Let's please make sure the test also covers the case that different inode references contain different blocks, and verify there is no over/under counting.\r\nMy understanding is that this would be the case when a file already present on snapshot(s) has then been appended. Do you guys know of any other condition for such? I'm working on tests and solution to also cover this scenario as well.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-11-10T18:16:12.964+0000","updated":"2017-11-10T18:16:12.964+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16248306","id":"16248306","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Yup, append is definitely a valid scenario. We can also do a truncate. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-11-11T03:50:19.352+0000","updated":"2017-11-11T03:50:19.352+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16251747","id":"16251747","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"I'm having problems with append and truncate. In both cases, *iip.getLastINode()* returns an instance of *INodeFile*. For append, the original block for the file is always kept. Thus, let's say I had a file *file1* originally with 1 block only, and took a snapshot *snap1* for this folder. Then had performed append, so that *file1* has now 2 blocks. This *file1* will have a total of 2 blocks now, block for *snap1* file would be 1st block of the real file, so snap1 file should not have it's block accounted. I thought this would be feasible by performing the following check:\r\n\r\n{noformat}\r\nif (inodeFile.isWithSnapshot() && \r\ninodeFile.getFileWithSnapshotFeature().getDiffs().getLastSnapshotId() == iip.getPathSnapshotId() && \r\ninodeFile.getFileWithSnapshotFeature().isCurrentFileDeleted()) {\r\n            replRes.totalBlocks += inodeFile.getBlocks().length;\r\n          }\r\n{noformat} \r\n\r\nI guess this would work, as we just have to count blocks for files inside snapshots if the original file has been removed, and the file is on the last snapshot. Problem here is that *inodeFile.getFileWithSnapshotFeature().isCurrentFileDeleted()* returns *true* as long as the *DELETE* operation that removed the original file is younger than last fsimage file. Once checkpoint happens and NN is restarted, *inodeFile.getFileWithSnapshotFeature().isCurrentFileDeleted()* then returns *false* for all snapshot files, breaking the logic above. I believe it's possible to set that flag from info available on fsimage during image loading time, but I'm not sure if that's the expected behaviour.\r\n\r\nTruncate problem is harder. It generates new blocks (because it shrinks the file). So now we would need pass the above checks, but within *truncate*, *inodeFile.getFileWithSnapshotFeature().isCurrentFileDeleted()* will never return *true* (cause there were not any deletion). We could obviously make this flag updated by truncate, but it does not seem right.\r\n\r\nThese problems actually make me feel it would be simpler if we could have a way to get all related file paths for a given block from block manager, so that while in snapshot check we could only count blocks for those paths that are actually in snapshots. I don't know if we have any way to do that now, have found *BlockManager.getStoredBlock(Block)*, but returned *BlockInfo* instance only has id for the last inode in the file path, which in this case is the same for both real path and snapshot path. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-11-14T17:09:07.618+0000","updated":"2017-11-14T17:09:07.618+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16265784","id":"16265784","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Here goes another patch attempt. I believe to have found a solution for all the cases. Some explanations below:\r\n\r\n1) For each file in *.snapshot* folder, it first checks if the path resolves to an instance of *INodeFile*. This would be the case for non-renamed files. \r\n1.1) In this case, we need to check if the given file only exists on snapshots, that's possible by calling *inodeFile.isWithSnapshot()*.\r\n1.2) If the file only exists on snapshots, we should then check if it has been deleted from original folder, appended or truncated. \r\n1.3) Files appended or truncated will still have a valid inode outside of snapshot folder, as long as original file has not been deleted yet. To check this condition, we can call *dir.getINodesInPath(inodeFile.getName(),FSDirectory.DirOp.READ).validate();*.\r\nFor appended/truncated cases we then need to compare blocks for file in snaphsot folder with those to original file, counting only blocks from files in snapshot there are not in the original file (outside snapshot).\r\n1.4) If file has been deleted from original folder, it exists only within snapshots. Call for *dir.getINodesInPath(inodeFile.getName(),FSDirectory.DirOp.READ).validate();* will throw an AssertionError in such cases, so in the catch statement we can then verify two additional conditions:\r\n1.4.1) If we checking last snapshot, we can simply count all the blocks for the file. \r\n1.4.2) If this is not last snapshot, we need to compare blocks on this file with the ones on the last snapshot, and count only those blocks that are not on last snapshot.\r\n2) Renamed files will be resolved as either *INodeReference.DstReference* or *INodeReference.WithName)*.\r\n2.1) *INodeReference.DstReference* will be the case where file has been renamed on original folder, then got renamed and snapshoted again. In this case, we only have to count the block if the original file gets deleted. In such scenario, *referenceIip.getLastINode()* returns null, so we can count the blocks.\r\n2.2) Files in snapshot that then got renamed on the original folder will be *INodeReference.WithName*. If the original file gets deleted outside of the snapshot, it then needs to be counted. This can be identified by following condition: *referenceIip.getLastINode() == null && inode.asFile().getParent() == null*.\r\n\r\nCurrent patch is implementing the conditions described above, along with additional 12 unit tests for different variations of possible scenarios.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-11-25T17:20:09.745+0000","updated":"2017-11-25T17:20:09.745+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16269038","id":"16269038","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"bq. Admittedly snapshot is very complicated and requires a lot of efforts (at least for me) to get things right.\r\nUnfortunately it takes a lot of effort for everyone which is why it's so hard to fix issues involving them.\r\n\r\nNever do \"try - lock - finally unlock\".  If the lock attempt throws for any reason, the finally performs an unbalanced unlock.  Always use the pattern \"lock - try - finally unlock\".\r\n\r\nIn general, re-constituting and re-resolving paths is very expensive and should be avoided.  This patch does a lot.\r\n\r\nCatching {{AssertionError}}, or any unchecked exception, and assuming the semantics is a terrible idea.  I see you are relying on {{IIP#validate}} to throw it but it's an implementation detail.  Past experience indicates that method is broken.  When I've tiptoed around snapshots and added logging of {{IIP#toString}} during permission checking, it had many false positives.  Don't use it.\r\n\r\nAbbreviated:\r\n{code}\r\nif(inodeFile.getFileWithSnapshotFeature()!=null && inodeFile.getFileWithSnapshotFeature().getDiffs()!=null) {\r\n  lastSnapshotId = inodeFile.getFileWithSnapshotFeature().getDiffs().getLastSnapshotId();\r\n}\r\n{code}\r\n(I'm not sure the snapshot code is correct).  From a style/performance perspective, you don't want to keep repeatedly calling the same method, notably the feature lookup, versus using a local variable.\r\n\r\nNo need to use {{instanceof}} on the inodes and explicitly cast.  Ie. Rather than {{inode instanceof  INodeFile}}, use {{inode#isFile}} followed by {{inode#asFile}}.  Same for references.\r\n\r\nCatching up from vacation, but I need to find more time to grok the snapshot code.  Like most snapshot code it feels overly complex and brittle.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2017-11-28T16:54:08.655+0000","updated":"2017-11-28T16:54:08.655+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16277482","id":"16277482","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"I agree with the comments and issues pointed on last comment by [~daryn], I believe these are addressable, just some thoughts on the two bullets below: \r\n\r\nbq. In general, re-constituting and re-resolving paths is very expensive and should be avoided. This patch does a lot.\r\nThat comes from the need to check if given file on snapshot is available on other snapshots path and/or outside snapshots as well. I don't think it can be removed completely, but maybe this can be avoided, by reusing values previously resolved already, maybe.\r\n\r\nbq. Catching AssertionError, or any unchecked exception, and assuming the semantics is a terrible idea. I see you are relying on IIP#validate to throw it but it's an implementation detail. Past experience indicates that method is broken. When I've tiptoed around snapshots and added logging of IIP#toString during permission checking, it had many false positives. Don't use it.\r\nA problem that I found is that *FSDirectory.getINodesInPath(filePath, FSDirectory.DirOp.READ)* returns an invalid IIP instance for the cases when the file has been deleted from original folder and exists only on snapshots (I guess IIP is just the last inode with no parent). I can try validate it \"manually\", by looking at it's structure.    ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-12-04T20:55:42.228+0000","updated":"2017-12-04T20:55:42.228+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16279130","id":"16279130","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"This is more of a lament, but HDFS-12811 would make the problem simpler at least for a fast fsck of the whole namespace via the inode map.  Determining if the inode was visited goes away...\r\n\r\nHow is {{FSDirectory.getINodesInPath(filePath, FSDirectory.DirOp.READ)}} broken?  With reserved inode paths?  Otherwise how can you resolve a path (incorrectly) to something that doesn't exist in the live namespace?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2017-12-05T20:08:30.013+0000","updated":"2017-12-05T20:08:30.013+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16281811","id":"16281811","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"bq. How is FSDirectory.getINodesInPath(filePath, FSDirectory.DirOp.READ) broken?\r\nI wouldn't say it's broken, as the \"filePath\" being passed to it in this case does not actually exist. When dealing with appended or truncated files, the original file path may still exist (if the file has never been deleted), but the file version on the snapshot folders may have different blocks. That adds the need to check if the original file path still exists out of snapshots. Thats the reason behind the *dir.getINodesInPath(inodeFile.getName(),FSDirectory.DirOp.READ).validate()*, as *inodeFile.getName()* returns the original file path. So if the file has been deleted, *inodeFile.getName()* actually refers to an invalid path, that's why *dir.getINodesInPath(inodeFile.getName(),FSDirectory.DirOp.READ).validate()* throws the assertion error. An alternative that I thought then was to go through the INodes array from this IIP, comparing it with the *inodeFile.getName()*, since usage of *validate()* is discouraged.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2017-12-07T13:00:29.814+0000","updated":"2017-12-07T13:01:46.382+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16306556","id":"16306556","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Sorry for my month-long delay on reviewing, finally locked myself to the chair and reviewed the latest patch and comments before the end of the year. \r\n\r\nGood to see we're improving, and happy to see the many added test cases. Thanks for the continued work [~wchevreuil], and [~daryn] for the reviews.\r\n\r\nThe problem is pretty hard, but direction looks good. Some comments on patch  4:\r\n- {{validate()}} then catch {{AssertionError}} should be changed, for the reasons Daryn mentioned, plus the fact that assertion could be disabled at run time. See https://docs.oracle.com/javase/8/docs/technotes/guides/language/assert.html#enable-disable . \r\n- I'm not sure the current {{getLastINode()==null}} check is enough for {{INodeReference}}s. What if the block changed in the middle of the snapshots? For example, say file 1 has block 1&2. Then the following happened: snapshot s1, truncate so file has block 1-only, snapshot s2, append so file has block 1&3, snapshot s3. Would we be able to tell the difference when {{fsck -includeSnapshots}} now?\r\n- Because locks are reacquired during fsck, it's theoretically possible that snapshots are created / deleted during the scan. I think current behavior is we're not aware of new snapshots, and skip the deleted snapshots (since {{snapshottableDirs}} is populated before the {{check}} call. Possible to add a fault-injected test to make sure we don't NPE on deleted snapshots?\r\n- Speechlessly {{NamenodeFsck}} also has other block counts like {{numMinReplicatedBlocks}}. Current code only takes care of total blocks, which IMO is the most important. This also seems to be the goal of this jira as suggested by the title and description, so Okay to split that to another jira.\r\n\r\nTrivial ones:\r\n- I see the variable name of {{checkDir}} is changed to {{filePath}}, which is not accurate. Prefer to keep the old name {{path}}.\r\n- {{checkFilesInSnapshotOnly}}: suggest to handle {{inode==null}} in it's own block, so we don't have to worry about that for non {{INodeFile}} code paths. (FYI null is not instanceof anything, so patch 4 code didn't have to check. Need to be careful after changing to {{isFile}}, as (correctly) suggested by Daryn.)\r\n- {{lastSnapshotId = -1}} should use {{Snapshot.NO_SNAPSHOT_ID}} rather than -1.\r\n- {{inodeFile..getFileWithSnapshotFeature().getDiffs()}} cannot never null judging from {{FileWithSnapshotFeature}}, so no need for nullity check\r\n- Please format the code you changed. There are many space inconsistencies around brackets.\r\n- Test should add timeouts. Perhaps better to just use a {{Rule}} on the class, to safeguard cases by default with something like 3 minutes.\r\n- Feels to me the \"HEALTHY\" check in the beginning of each test case is not necessary.\r\n- Could use {{GenericTestUtils.waitFor()}} for the waits.\r\n- Optional - {{TestFsck}} is already 2.4k+ lines long. Maybe better to create a new test class for snapshot blockcount specifically. In that class the name of each test would be shorter and more readable.\r\n\r\nAlso a curiosity question to [~daryn]:\r\nbq. try - lock - finally unlock v.s. lock - try - finally unlock\r\nUnderstood and completely agree with the advice. Curiosity comes from: in current HDFS, it looks like only {{FSN#writeLockInterruptibly}} is possible to throw a checked exception. Of course there could also be unchecked exceptions - is this a coding advice or something you have run into in practice? Care to share the fun details? :)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-12-29T21:35:24.595+0000","updated":"2017-12-29T21:39:03.348+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16308727","id":"16308727","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Attaching an intermediate patch addressing some of the obeservations made by [~daryn] and [~xiaochen]. This had removed call to validate method and catch on assertion error. Also had swicthed to isFile/asFile methods, instead of using instanceof operator. Still need to review tests and additional condition mentioned by [~xiaochen], but feel free to share your initial suggestions on this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2018-01-02T21:19:09.865+0000","updated":"2018-01-02T21:19:09.865+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16308915","id":"16308915","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 30s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 38s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m  8s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 44s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 33s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 2 new + 114 unchanged - 4 fixed = 116 total (was 118) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 53s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 48s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}121m 37s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 24s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}169m 21s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |\r\n|   | hadoop.hdfs.web.TestWebHdfsTimeouts |\r\n|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |\r\n|   | hadoop.hdfs.server.namenode.TestReconstructStripedBlocks |\r\n|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |\r\n| JIRA Issue | HDFS-12618 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12904286/HDFS-12618.005.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux fa80c8c0ef60 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / b4d1133 |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_151 |\r\n| findbugs | v3.1.0-RC1 |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/22532/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22532/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22532/testReport/ |\r\n| Max. process+thread count | 3797 (vs. ulimit of 5000) |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22532/console |\r\n| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-01-03T00:14:10.799+0000","updated":"2018-01-03T00:14:10.799+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16311620","id":"16311620","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"bq. validate() then catch AssertionError should be changed, for the reasons Daryn mentioned, plus the fact that assertion could be disabled at run time. See https://docs.oracle.com/javase/8/docs/technotes/guides/language/assert.html#enable-disable .\r\nLatest submitted patch already has refactorings to replace usage of validate and reliance on AssertionError handling.\r\n\r\nbq. I'm not sure the current getLastINode()==null check is enough for INodeReference}}s. What if the block changed in the middle of the snapshots? For example, say file 1 has block 1&2. Then the following happened: snapshot s1, truncate so file has block 1-only, snapshot s2, append so file has block 1&3, snapshot s3. Would we be able to tell the difference when {{fsck -includeSnapshots now?\r\nI added a unit test for such scenario, it must appear in the next patch to be submitted with additional observations. Current condition seems to be covering this situation, test is passing.\r\n\r\nbq. Because locks are reacquired during fsck, it's theoretically possible that snapshots are created / deleted during the scan. I think current behavior is we're not aware of new snapshots, and skip the deleted snapshots (since snapshottableDirs is populated before the check call. Possible to add a fault-injected test to make sure we don't NPE on deleted snapshots?\r\nI'm not sure I follow this. Is it that we need to make sure snapshottableDir != null? If so, we do have an if on *checkDir* method, line #506.\r\n\r\nbq. Speechlessly NamenodeFsck also has other block counts like numMinReplicatedBlocks. Current code only takes care of total blocks, which IMO is the most important. This also seems to be the goal of this jira as suggested by the title and description, so Okay to split that to another jira.\r\nSo there's another metric currently broken? I may open another jira for that, but would like to first get this sorted.\r\n\r\nbq. I see the variable name of checkDir is changed to filePath, which is not accurate. Prefer to keep the old name path.\r\nThat was changed to fix checkstyle warning.\r\n\r\nbq. checkFilesInSnapshotOnly: suggest to handle inode==null in it's own block, so we don't have to worry about that for non INodeFile code paths. (FYI null is not instanceof anything, so patch 4 code didn't have to check. Need to be careful after changing to isFile, as (correctly) suggested by Daryn.)\r\nLast patch applied suggestions from Daryn to use *isFile* helper method, so now I guess we need to make sure inode is not null.\r\n\r\nbq. lastSnapshotId = -1 should use Snapshot.NO_SNAPSHOT_ID rather than -1.\r\nApplied on last patch.\r\n\r\nbq. inodeFile..getFileWithSnapshotFeature().getDiffs() cannot never null judging from FileWithSnapshotFeature, so no need for nullity check\r\nFixed on last patch.\r\n\r\nbq. Please format the code you changed. There are many space inconsistencies around brackets.\r\nFormatted on last patch.\r\n\r\nbq. Test should add timeouts. Perhaps better to just use a Rule on the class, to safeguard cases by default with something like 3 minutes.\r\nWill be available on next patch.\r\n\r\nbq. Feels to me the \"HEALTHY\" check in the beginning of each test case is not necessary.\r\nWill be available on next patch.\r\n\r\nbq. Could use GenericTestUtils.waitFor() for the waits.\r\nWill be available on next patch.\r\n\r\nbq. Optional - TestFsck is already 2.4k+ lines long. Maybe better to create a new test class for snapshot blockcount specifically. In that class the name of each test would be shorter and more readable.\r\nWill be available on next patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2018-01-04T16:46:21.926+0000","updated":"2018-01-04T16:46:21.926+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16311897","id":"16311897","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"New patch with all last revisions applied. Please let me know on any suggestions/advices.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2018-01-04T19:24:15.411+0000","updated":"2018-01-04T19:24:15.411+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16312131","id":"16312131","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 38s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  1s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 44s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 10s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 17s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  3s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |\r\n| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 54s{color} | {color:red} hadoop-hdfs-project_hadoop-hdfs generated 3 new + 391 unchanged - 3 fixed = 394 total (was 394) {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 37s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 3 new + 112 unchanged - 4 fixed = 115 total (was 116) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 58s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  9s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}125m 34s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 29s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}179m 20s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |\r\n|   | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |\r\n|   | hadoop.hdfs.server.namenode.TestReencryptionWithKMS |\r\n|   | hadoop.hdfs.TestMaintenanceState |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |\r\n| JIRA Issue | HDFS-12618 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12904662/HDFS-12618.006.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux 182b243c1d8b 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / dc735b2 |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_151 |\r\n| findbugs | v3.1.0-RC1 |\r\n| javac | https://builds.apache.org/job/PreCommit-HDFS-Build/22561/artifact/out/diff-compile-javac-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/22561/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22561/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22561/testReport/ |\r\n| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/22561/artifact/out/patch-asflicense-problems.txt |\r\n| Max. process+thread count | 2697 (vs. ulimit of 5000) |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22561/console |\r\n| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-01-04T22:30:03.512+0000","updated":"2018-01-04T22:30:03.512+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16312887","id":"16312887","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Ain't sure on the *javac* WARN, it's complaining about TestFsck class, however there are no changes for it on latest patch, all tests were moved to TestFsckIncludeSnapshots class, TestFsck is not even present in the latest patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2018-01-05T10:27:17.714+0000","updated":"2018-01-05T10:27:17.714+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16400670","id":"16400670","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"body":"Any comments on the last patch proposed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wchevreuil","name":"wchevreuil","key":"wchevreuil","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Wellington Chevreuil","active":true,"timeZone":"Europe/London"},"created":"2018-03-15T16:30:28.151+0000","updated":"2018-03-15T16:30:28.151+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13108048/comment/16400978","id":"16400978","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m  3s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 57s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m  4s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 55s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |\r\n| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 52s{color} | {color:red} hadoop-hdfs-project_hadoop-hdfs generated 3 new + 390 unchanged - 3 fixed = 393 total (was 393) {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 48s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 3 new + 112 unchanged - 4 fixed = 115 total (was 116) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 13s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  4s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 79m 15s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 24s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}132m 16s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |\r\n|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:d4cc50f |\r\n| JIRA Issue | HDFS-12618 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12904662/HDFS-12618.006.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux e4c39aff985d 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 5e013d5 |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_151 |\r\n| findbugs | v3.1.0-RC1 |\r\n| javac | https://builds.apache.org/job/PreCommit-HDFS-Build/23503/artifact/out/diff-compile-javac-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/23503/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/23503/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/23503/testReport/ |\r\n| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/23503/artifact/out/patch-asflicense-problems.txt |\r\n| Max. process+thread count | 3999 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/23503/console |\r\n| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-03-15T19:13:18.156+0000","updated":"2018-03-15T19:13:18.156+0000"}],"maxResults":40,"total":40,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-12618/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3l1if:"}}