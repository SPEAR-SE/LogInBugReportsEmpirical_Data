{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12553088","self":"https://issues.apache.org/jira/rest/api/2/issue/12553088","key":"HDFS-3333","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2012-04-29T20:31:29.279+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Apr 03 02:27:59 UTC 2014","customfield_12310420":"237082","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_60988305592_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-04-03T02:27:59.499+0000","workratio":0,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-3333/watchers","watchCount":4,"isWatching":false},"created":"2012-04-27T05:16:13.941+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":720,"aggregatetimeoriginalestimate":720,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12318885","id":"12318885","description":"0.23.1 - Performance release","name":"0.23.1","archived":false,"released":true,"releaseDate":"2012-02-17"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12320353","id":"12320353","description":"hadoop-2.0.0-alpha release","name":"2.0.0-alpha","archived":false,"released":true,"releaseDate":"2012-05-23"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-04-03T02:27:59.529+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312926","id":"12312926","name":"namenode"}],"timeoriginalestimate":720,"description":"log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).\nlog4j:WARN Please initialize the log4j system properly.\njava.io.IOException: File /user/root/lwr/test31.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1259)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1916)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:472)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:292)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42602)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:428)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:905)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1684)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1205)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1682)\ni:4284\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1159)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:185)\n\tat $Proxy9.addBlock(Unknown Source)\n\tat sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n\tat $Proxy9.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:295)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1097)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:973)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:455)\n\n\n\n\n\n\n\ntestcase:\n\n\nimport java.io.IOException;\nimport java.net.URI;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hdfs.DFSConfigKeys;\nimport org.apache.hadoop.hdfs.DistributedFileSystem;\npublic class Write1 {\n\n\t/**\n\t * @param args\n\t * @throws Exception \n\t */\n\tpublic static void main(String[] args) throws Exception {\n\t\t//System.out.println(\"main\");\n\t\tString hdfsFile=\"/user/root/lwr/test31.txt\";\n    \tbyte writeBuff[] = new byte [1024 * 1024];\n    \tint i=0;\n    \tDistributedFileSystem dfs = new DistributedFileSystem();\n    \tConfiguration conf=new Configuration();\n    \t//conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 512);\n    \t//conf.setLong(DFSConfigKeys.DFS_REPLICATION_KEY, 2);\n        // conf.setInt(\"dfs.replication\", 3);\n        conf.setLong(\"dfs.blocksize\", 512);\n    \tdfs.initialize(URI.create(\"hdfs://10.18.40.154:9000\"), conf);\n    \t//dfs.delete(new Path(hdfsFile));\n\t    //appendFile(dfs,hdfsFile,1024 * 1024,true);\n    \ttry\n    \t{\n    \tFSDataOutputStream out1=dfs.create(new Path(hdfsFile));\n    \t\n   \t    for(i=0;i<100000;i++)\n   \t    {\n    \t out1.write(writeBuff, 0, 512);\n    \t}\n        out1.hsync();\n        out1.close();\n        /*\n\t    FSDataOutputStream out=dfs.append(new Path(hdfsFile),4096);\n\t\tout.write(writeBuff, 0, 512 * 1024);\n\t\tout.hsync();\n\t\tout.close();\n\t\t*/\n    \t}catch (IOException e)\n    \t{    \t   \n    \t   System.out.println(\"i:\" + i);\n    \t   e.printStackTrace();\n\n    \t}\n    \tfinally\n    \t{   \n    \t\t\n    \t\tSystem.out.println(\"i:\" + i);\n    \t    System.out.println(\"end!\");\n    \t \n    \t}\n\n    }\n\t\n\n}\n","customfield_10010":null,"timetracking":{"originalEstimate":"0.2h","remainingEstimate":"0.2h","originalEstimateSeconds":720,"remainingEstimateSeconds":720},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":720,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"74367","customfield_12312823":null,"summary":"java.io.IOException: File /user/root/lwr/test31.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ewenpower","name":"ewenpower","key":"ewenpower","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liaowenrui","active":true,"timeZone":"Asia/Shanghai"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ewenpower","name":"ewenpower","key":"ewenpower","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liaowenrui","active":true,"timeZone":"Asia/Shanghai"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":720,"percent":0},"customfield_12311024":null,"environment":"namenode：1 (IP:10.18.40.154)\ndatanode：3 (IP:10.18.40.154,10.18.40.102,10.18.52.55)\n\nHOST-10-18-40-154:/home/APril20/install/hadoop/namenode/bin # ./hadoop dfsadmin -report\nDEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n\nConfigured Capacity: 129238446080 (120.36 GB)\nPresent Capacity: 51742765056 (48.19 GB)\nDFS Remaining: 49548591104 (46.15 GB)\nDFS Used: 2194173952 (2.04 GB)\nDFS Used%: 4.24%\nUnder replicated blocks: 14831\nBlocks with corrupt replicas: 1\nMissing blocks: 100\n\n-------------------------------------------------\nDatanodes available: 3 (3 total, 0 dead)\n\nLive datanodes:\nName: 10.18.40.102:50010 (10.18.40.102)\nHostname: linux.site\nDecommission Status : Normal\nConfigured Capacity: 22765834240 (21.2 GB)\nDFS Used: 634748928 (605.34 MB)\nNon DFS Used: 1762299904 (1.64 GB)\nDFS Remaining: 20368785408 (18.97 GB)\nDFS Used%: 2.79%\nDFS Remaining%: 89.47%\nLast contact: Fri Apr 27 10:35:57 IST 2012\n\n\nName: 10.18.40.154:50010 (HOST-10-18-40-154)\nHostname: HOST-10-18-40-154\nDecommission Status : Normal\nConfigured Capacity: 23259897856 (21.66 GB)\nDFS Used: 812396544 (774.76 MB)\nNon DFS Used: 8297279488 (7.73 GB)\nDFS Remaining: 14150221824 (13.18 GB)\nDFS Used%: 3.49%\nDFS Remaining%: 60.84%\nLast contact: Fri Apr 27 10:35:58 IST 2012\n\n\nName: 10.18.52.55:50010 (10.18.52.55)\nHostname: HOST-10-18-52-55\nDecommission Status : Normal\nConfigured Capacity: 83212713984 (77.5 GB)\nDFS Used: 747028480 (712.42 MB)\nNon DFS Used: 67436101632 (62.8 GB)\nDFS Remaining: 15029583872 (14 GB)\nDFS Used%: 0.9%\nDFS Remaining%: 18.06%\nLast contact: Fri Apr 27 10:35:58 IST 2012\n\n","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":720,"percent":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12553088/comment/13263351","id":"13263351","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ewenpower","name":"ewenpower","key":"ewenpower","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liaowenrui","active":true,"timeZone":"Asia/Shanghai"},"body":"name node logs:\n\n2012-04-27 10:02:07,453 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* addStoredBlock: blockMap updated: 10.18.52.55:50010 is added to blk_5358037144179192664_118193{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]} size 0\n2012-04-27 10:02:07,454 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not able to place enough replicas, still in need of 2 to reach 3\nFor more information, please enable DEBUG level logging on the org.apache.hadoop.hdfs.server.namenode.FSNamesystem logger.\n2012-04-27 10:02:07,454 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/root/lwr/test31.txt. BP-1941047897-10.18.40.154-1335419775245 blk_-5430945475809539701_118194{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]}\n2012-04-27 10:02:07,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* addStoredBlock: blockMap updated: 10.18.52.55:50010 is added to blk_-5430945475809539701_118194{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]} size 0\n2012-04-27 10:02:07,467 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not able to place enough replicas, still in need of 2 to reach 3\nFor more information, please enable DEBUG level logging on the org.apache.hadoop.hdfs.server.namenode.FSNamesystem logger.\n2012-04-27 10:02:07,467 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/root/lwr/test31.txt. BP-1941047897-10.18.40.154-1335419775245 blk_7179050910888663641_118195{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]}\n2012-04-27 10:02:07,479 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* addStoredBlock: blockMap updated: 10.18.52.55:50010 is added to blk_7179050910888663641_118195{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]} size 0\n2012-04-27 10:02:07,480 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not able to place enough replicas, still in need of 2 to reach 3\nFor more information, please enable DEBUG level logging on the org.apache.hadoop.hdfs.server.namenode.FSNamesystem logger.\n2012-04-27 10:02:07,480 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/root/lwr/test31.txt. BP-1941047897-10.18.40.154-1335419775245 blk_5515331497909593936_118196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]}\n2012-04-27 10:02:07,490 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* addStoredBlock: blockMap updated: 10.18.52.55:50010 is added to blk_5515331497909593936_118196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]} size 0\n2012-04-27 10:02:07,491 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not able to place enough replicas, still in need of 2 to reach 3\nFor more information, please enable DEBUG level logging on the org.apache.hadoop.hdfs.server.namenode.FSNamesystem logger.\n2012-04-27 10:02:07,491 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/root/lwr/test31.txt. BP-1941047897-10.18.40.154-1335419775245 blk_6362697618477922316_118197{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]}\n2012-04-27 10:02:07,503 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* addStoredBlock: blockMap updated: 10.18.52.55:50010 is added to blk_6362697618477922316_118197{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]} size 0\n2012-04-27 10:02:07,503 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not able to place enough replicas, still in need of 2 to reach 3\nFor more information, please enable DEBUG level logging on the org.apache.hadoop.hdfs.server.namenode.FSNamesystem logger.\n2012-04-27 10:02:07,503 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/root/lwr/test31.txt. BP-1941047897-10.18.40.154-1335419775245 blk_454857163161775943_118198{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.18.52.55:50010|RBW]]}\n2012-04-27 10:02:07,591 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Not able to place enough replicas, still in need of 3 to reach 3\nFor more information, please enable DEBUG level logging on the org.apache.hadoop.hdfs.server.namenode.FSNamesystem logger.\n2012-04-27 10:02:07,595 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:l00110880 (auth:SIMPLE) cause:java.io.IOException: File /user/root/lwr/test31.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.\n2012-04-27 10:02:07,596 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.18.47.134:3284: error: java.io.IOException: File /user/root/lwr/test31.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.\njava.io.IOException: File /user/root/lwr/test31.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1259)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1916)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:472)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:292)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42602)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:428)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:905)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1684)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1205)\n                                                                                                                                           17405,1       99%\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ewenpower","name":"ewenpower","key":"ewenpower","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liaowenrui","active":true,"timeZone":"Asia/Shanghai"},"created":"2012-04-27T05:18:39.700+0000","updated":"2012-04-27T05:18:39.700+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12553088/comment/13264597","id":"13264597","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vigith","name":"vigith","key":"vigith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vigith Maurice","active":true,"timeZone":"Etc/UTC"},"body":"Hi,\n\nI had a similar problem, but the reason was because port was not opened between the client host and the datanode. since port was not open, the connection was getting timed out and ended up blacklisting each datanode. (i was using amazon ec2 instances where ports are not open by default.)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vigith","name":"vigith","key":"vigith","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vigith Maurice","active":true,"timeZone":"Etc/UTC"},"created":"2012-04-29T20:31:29.279+0000","updated":"2012-04-29T20:31:29.279+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12553088/comment/13266294","id":"13266294","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ewenpower","name":"ewenpower","key":"ewenpower","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liaowenrui","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi,this issue is diffrent yours,the first,Data is wrriten Nomal, After a while,it throw this exception.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ewenpower","name":"ewenpower","key":"ewenpower","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"liaowenrui","active":true,"timeZone":"Asia/Shanghai"},"created":"2012-05-02T02:03:45.610+0000","updated":"2012-05-02T02:03:45.610+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12553088/comment/13958454","id":"13958454","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"I guess that this is not a problem anymore.  Please feel free to reopen this if I am wrong.  Resolving ...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-04-03T02:27:59.524+0000","updated":"2014-04-03T02:27:59.524+0000"}],"maxResults":4,"total":4,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-3333/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0d3nb:"}}