{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12721254","self":"https://issues.apache.org/jira/rest/api/2/issue/12721254","key":"HDFS-6532","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-03-15T14:25:29.989+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Apr 20 03:22:23 UTC 2018","customfield_12310420":"399450","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6532/watchers","watchCount":10,"isWatching":false},"created":"2014-06-14T16:09:16.793+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"6.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326143","id":"12326143","description":"2.4.0 release","name":"2.4.0","archived":false,"released":true,"releaseDate":"2014-04-07"}],"issuelinks":[{"id":"12477161","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12477161","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12980693","key":"HDFS-10549","self":"https://issues.apache.org/jira/rest/api/2/issue/12980693","fields":{"summary":"Correctly revoke file leases when closing files","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-04-20T03:22:23.679+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"Per https://builds.apache.org/job/Hadoop-Hdfs-trunk/1774/testReport, we had the following failure. Local rerun is successful\n\n{code}\nRegression\n\norg.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt\n\nFailing for the past 1 build (Since Failed#1774 )\nTook 50 sec.\nError Message\n\ntest timed out after 50000 milliseconds\nStacktrace\n\njava.lang.Exception: test timed out after 50000 milliseconds\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:2024)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:2008)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2107)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:98)\n\tat org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt(TestCrcCorruption.java:133)\n{code}\n\nSee relevant exceptions in log\n{code}\n2014-06-14 11:56:15,283 WARN  datanode.DataNode (BlockReceiver.java:verifyChunks(404)) - Checksum error in block BP-1675558312-67.195.138.30-1402746971712:blk_1073741825_1001 from /127.0.0.1:41708\norg.apache.hadoop.fs.ChecksumException: Checksum error: DFSClient_NONMAPREDUCE_-1139495951_8 at 64512 exp: 1379611785 got: -12163112\n\tat org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:353)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:284)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.verifyChunks(BlockReceiver.java:402)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:537)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:734)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:741)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:124)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:234)\n\tat java.lang.Thread.run(Thread.java:662)\n2014-06-14 11:56:15,285 WARN  datanode.DataNode (BlockReceiver.java:run(1207)) - IOException in BlockReceiver.run(): \njava.io.IOException: Shutting down writer and responder due to a checksum error in received data. The error response has been sent upstream.\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstreamUnprotected(BlockReceiver.java:1352)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.sendAckUpstream(BlockReceiver.java:1278)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1199)\n\tat java.lang.Thread.run(Thread.java:662)\n...\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12826166","id":"12826166","filename":"HDFS-6532.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-08-30T11:20:08.564+0000","size":778,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12826166/HDFS-6532.001.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12831614","id":"12831614","filename":"HDFS-6532.002.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-10-04T20:30:39.903+0000","size":811,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12831614/HDFS-6532.002.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12831846","id":"12831846","filename":"jstack","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-10-06T00:07:18.636+0000","size":182771,"mimeType":"text/html","content":"https://issues.apache.org/jira/secure/attachment/12831846/jstack"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12828871","id":"12828871","filename":"PreCommit-HDFS-Build #16770 test - testCorruptionDuringWrt [Jenkins].pdf","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-16T18:04:37.906+0000","size":816065,"mimeType":"application/pdf","content":"https://issues.apache.org/jira/secure/attachment/12828871/PreCommit-HDFS-Build+%2316770+test+-+testCorruptionDuringWrt+%5BJenkins%5D.pdf"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12824779","id":"12824779","filename":"TEST-org.apache.hadoop.hdfs.TestCrcCorruption.xml","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-22T06:44:05.095+0000","size":302477,"mimeType":"text/xml","content":"https://issues.apache.org/jira/secure/attachment/12824779/TEST-org.apache.hadoop.hdfs.TestCrcCorruption.xml"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12831613","id":"12831613","filename":"TEST-org.apache.hadoop.hdfs.TestCrcCorruption-select_timeout.xml","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-10-04T20:30:39.898+0000","size":338193,"mimeType":"text/xml","content":"https://issues.apache.org/jira/secure/attachment/12831613/TEST-org.apache.hadoop.hdfs.TestCrcCorruption-select_timeout.xml"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"399559","customfield_12312823":null,"summary":"Intermittent test failure org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15195362","id":"15195362","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"body":"Still happening.\n\n{noformat}\ntestCorruptionDuringWrt(org.apache.hadoop.hdfs.TestCrcCorruption)  Time elapsed: 50.284 sec  <<< ERROR!\njava.lang.Exception: test timed out after 50000 milliseconds\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:764)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:689)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:770)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:747)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)\n\tat org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt(TestCrcCorruption.java:136)\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"created":"2016-03-15T14:25:29.989+0000","updated":"2016-03-15T14:25:29.989+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15408077","id":"15408077","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"body":"Still sometimes failing in trunk the same way.\n\nWhen it is working, {{close()}} should fail.\n{noformat}\n2016-08-04 11:10:38,293 [Thread-0] INFO  hdfs.DFSClient\n (TestCrcCorruption.java:testCorruptionDuringWrt(140)) - Got expected exception\njava.io.IOException: Failing write. Tried pipeline recovery 5 times without success.\n        at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1128)\n        at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:552)\n{noformat}\n\nIn the failed case, pipeline recovery only happened twice. {{DataStreamer}} usually directly notices a problem or {{ResponseProcessor}} hints it. It looks like the datanode thread tried to terminate, but the connection was not closed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"created":"2016-08-04T16:37:21.059+0000","updated":"2016-08-04T16:37:21.059+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15413411","id":"15413411","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"I looked into the logs info when this test failed, they both showed these stack infos:\n{code}\nBP-1186421078-172.17.0.2-1470312073795:blk_1073741826_1006] WARN  hdfs.DataStreamer (DataStreamer.java:closeResponder(873)) - Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1245)\n\tat java.lang.Thread.join(Thread.java:1319)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:871)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:733)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:729)\n2016-08-04 12:02:02,523 [Thread-0] INFO  hdfs.DFSClient (TestCrcCorruption.java:testCorruptionDuringWrt(140)) - Got expected exception\njava.io.InterruptedIOException: Interrupted while waiting for data to be acknowledged by pipeline\n\tat org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:775)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:697)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:778)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:755)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)\n\tat org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt(TestCrcCorruption.java:136)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\n{code}\n\nBut now I am not sure that why InterruptedException happens intermittently. In addition, I found that if InterruptedException was threw when the program did the {{dataQueue.wait()}}, then it will lead the files not completely closed in {{DFSClient#closeAllFilesBeingWritten}}. This issue was tracked by HDFS-10549. I thinks this two issue was related.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-08-09T11:52:48.699+0000","updated":"2016-08-09T11:52:48.699+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15430141","id":"15430141","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"I further looked at the issue but not root caused yet.\n\nSeems to me the biggest difference between success and failure cases is that, success cases have the following line, and then the retry terminates quickly. No such line in the failure cases, then as Yiqun said, test timeout will cause an interrupt exception thrown from {{close}} after 50 seconds.\n{noformat}\n2016-08-20 08:35:03,052 INFO  datanode.DataNode (DataXceiver.java:writeBlock(835)) - opWriteBlock BP-1047920701-192.168.1.79-1471707299069:blk_1073741826_1008 received exception java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:51075 remote=/127.0.0.1:51111]. 60000 millis timeout left.\n{noformat}\n\nAttaching a full failure log for reference, since this has not failed locally for me at all....\n\nI think there may be some race / corner cases causing said thread(s) not interrupted when checksum error happens, but haven't found any clue about it so far. :(","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-22T06:44:05.167+0000","updated":"2016-08-22T06:44:05.167+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15448770","id":"15448770","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"I looked into this issue again and I might find the root cause.\n\nAs [~kihwal] had mentioned, the failed case will not print the following infos\n{code}\n(TestCrcCorruption.java:testCorruptionDuringWrt(140)) - Got expected exception\njava.io.IOException: Failing write. Tried pipeline recovery 5 times without success.\n{code}\nInstead of that, the failed case will print infos like these:\n{code}\n(TestCrcCorruption.java:testCorruptionDuringWrt(140)) - Got expected exception\njava.io.InterruptedIOException: Interrupted while waiting for data to be acknowledged by pipeline\n\tat org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:775)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:697)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:778)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:755)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)\n{code}\n\nThat means the program has returned before do the recover pipeline operations sometimes. The related codes:\n{code:title=DataStreamer.java|borderStyle=solid}\n  private boolean processDatanodeOrExternalError() throws IOException {\n    if (!errorState.hasDatanodeError() && !shouldHandleExternalError()) {\n      return false;\n    }\n    LOG.debug(\"start process datanode/external error, {}\", this);\n    // If the response has not closed, this method will just return \n    if (response != null) {\n      LOG.info(\"Error Recovery for \" + block +\n          \" waiting for responder to exit. \");\n      return true;\n    }\n    closeStream();\n    ...\n{code}\n\nI looked into the code and I thought there was a bug to cause that, the related codes:\n{code:title=DataStreamer.java|borderStyle=solid}\n  public void run() {\n    long lastPacket = Time.monotonicNow();\n    TraceScope scope = null;\n    while (!streamerClosed && dfsClient.clientRunning) {\n      // if the Responder encountered an error, shutdown Responder\n      if (errorState.hasError() && response != null) {\n        try {\n          response.close();\n          response.join();\n          response = null;\n        } catch (InterruptedException e) {\n          // If interruptedException happens, the response will not be set to null\n          LOG.warn(\"Caught exception\", e);\n        }\n      }\n      // Here need add a finally block to set response as null\n      ...\n{code}\nI think we should move the line {{response = null;}} into {{finally}} block.\n\nFinally attach a patch for this. This test has failed intermitly for a long time, hope my patch can make sense. \nSoftly ping [~xiaochen], [~kihwal] and [~yzhangal] for the comments.\n\nThanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-08-30T11:18:10.434+0000","updated":"2016-08-30T11:24:55.633+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15449965","id":"15449965","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Yiqun for working on this. It does look like we can reuse the {{closeResponder}} method in the loop, but I don't think that's the root cause here.\n\nTaking the failure log in attachment as an example, the test is supposed to end quickly (around 15:41:58) after 5 times failure on checksum error. But somehow it did not, and hangs there until the 50 seconds test timeout is reached. After test timeout, junit interrupts all threads which is what we see in the last 3 messages (around 15:42:43).\n\nI looked into this too, and still think this is some error on triggering / handling the interrupt after the 5th checksum error. Don't have any concrete progress though.\n{noformat}\n2016-08-20 15:41:58,084 INFO  datanode.DataNode (DataXceiver.java:writeBlock(835)) - opWriteBlock BP-1703495320-172.17.0.1-1471707714371:blk_1073741826_1005 received exception java.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing BP-1703495320-172.17.0.1-1471707714371:blk_1073741826_1005 from /127.0.0.1:49059\n2016-08-20 15:41:58,084 ERROR datanode.DataNode (DataXceiver.java:run(273)) - 127.0.0.1:52977:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:49059 dst: /127.0.0.1:52977\njava.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing BP-1703495320-172.17.0.1-1471707714371:blk_1073741826_1005 from /127.0.0.1:49059\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:606)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:896)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:802)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-08-20 15:41:58,258 INFO  BlockStateChange (BlockManager.java:invalidateWorkForOneNode(3667)) - BLOCK* BlockManager: ask 127.0.0.1:51819 to delete [blk_1073741825_1002]\n2016-08-20 15:41:58,258 INFO  BlockStateChange (BlockManager.java:invalidateWorkForOneNode(3667)) - BLOCK* BlockManager: ask 127.0.0.1:39731 to delete [blk_1073741825_1002]\n2016-08-20 15:41:58,258 INFO  BlockStateChange (BlockManager.java:invalidateWorkForOneNode(3667)) - BLOCK* BlockManager: ask 127.0.0.1:52977 to delete [blk_1073741825_1002]\n2016-08-20 15:41:59,235 INFO  BlockStateChange (InvalidateBlocks.java:add(116)) - BLOCK* InvalidateBlocks: add blk_1073741825_1001 to 127.0.0.1:49498\n2016-08-20 15:41:59,238 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:deleteAsync(217)) - Scheduling blk_1073741825_1002 file /tmp/run_tha_test5KJcML/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1703495320-172.17.0.1-1471707714371/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\n2016-08-20 15:41:59,240 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:run(295)) - Deleted BP-1703495320-172.17.0.1-1471707714371 blk_1073741825_1002 file /tmp/run_tha_test5KJcML/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1703495320-172.17.0.1-1471707714371/current/finalized/subdir0/subdir0/blk_1073741825\n2016-08-20 15:41:59,378 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:deleteAsync(217)) - Scheduling blk_1073741825_1002 file /tmp/run_tha_test5KJcML/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1703495320-172.17.0.1-1471707714371/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\n2016-08-20 15:41:59,378 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:run(295)) - Deleted BP-1703495320-172.17.0.1-1471707714371 blk_1073741825_1002 file /tmp/run_tha_test5KJcML/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9/current/BP-1703495320-172.17.0.1-1471707714371/current/finalized/subdir0/subdir0/blk_1073741825\n2016-08-20 15:41:59,698 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:deleteAsync(217)) - Scheduling blk_1073741825_1002 file /tmp/run_tha_test5KJcML/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1703495320-172.17.0.1-1471707714371/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\n2016-08-20 15:41:59,698 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:run(295)) - Deleted BP-1703495320-172.17.0.1-1471707714371 blk_1073741825_1002 file /tmp/run_tha_test5KJcML/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17/current/BP-1703495320-172.17.0.1-1471707714371/current/finalized/subdir0/subdir0/blk_1073741825\n2016-08-20 15:42:01,259 INFO  BlockStateChange (BlockManager.java:invalidateWorkForOneNode(3667)) - BLOCK* BlockManager: ask 127.0.0.1:49498 to delete [blk_1073741825_1001]\n2016-08-20 15:42:02,098 INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(286)) - Volume reference is released.\n2016-08-20 15:42:02,232 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:deleteAsync(217)) - Scheduling blk_1073741825_1001 file /tmp/run_tha_test5KJcML/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1703495320-172.17.0.1-1471707714371/current/rbw/blk_1073741825 for deletion\n2016-08-20 15:42:02,233 INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:run(295)) - Deleted BP-1703495320-172.17.0.1-1471707714371 blk_1073741825_1001 file /tmp/run_tha_test5KJcML/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1703495320-172.17.0.1-1471707714371/current/rbw/blk_1073741825\n2016-08-20 15:42:03,051 INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(286)) - Volume reference is released.\n2016-08-20 15:42:03,056 INFO  impl.FsDatasetImpl (FsVolumeList.java:waitVolumeRemoved(286)) - Volume reference is released.\n2016-08-20 15:42:25,287 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(179)) - Rescanning after 30000 milliseconds\n2016-08-20 15:42:25,288 INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(202)) - Scanned 0 directive(s) and 0 block(s) in 1 millisecond(s).\n2016-08-20 15:42:43,521 WARN  hdfs.DFSClient (DFSOutputStream.java:closeResponder(865)) - Caught exception \njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1281)\n\tat java.lang.Thread.join(Thread.java:1355)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:863)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:831)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:827)\n2016-08-20 15:42:43,522 INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(935)) - Exception for BP-1703495320-172.17.0.1-1471707714371:blk_1073741826_1005\njava.nio.channels.ClosedByInterruptException\n\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:417)\n\tat org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:896)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:802)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-08-20 15:42:43,523 INFO  datanode.DataNode (DataXceiver.java:writeBlock(835)) - opWriteBlock BP-1703495320-172.17.0.1-1471707714371:blk_1073741826_1005 received exception java.nio.channels.ClosedByInterruptException\n2016-08-20 15:42:43,523 ERROR datanode.DataNode (DataXceiver.java:run(273)) - 127.0.0.1:35247:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:44749 dst: /127.0.0.1:35247\njava.nio.channels.ClosedByInterruptException\n\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:417)\n\tat org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:896)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:802)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-08-20 15:42:43,523 INFO  hdfs.DFSClient (TestCrcCorruption.java:testCorruptionDuringWrt(143)) - Got expected exception\njava.io.InterruptedIOException: Interrupted while waiting for data to be acknowledged by pipeline\n\tat org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:2442)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:2420)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2582)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2541)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt(TestCrcCorruption.java:139)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\n2016-08-20 15:42:43,523 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1744)) - Shutting down the Mini HDFS Cluster\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-30T19:54:08.302+0000","updated":"2016-08-30T19:54:08.302+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15450833","id":"15450833","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks [~xiaochen] for the comment.\n{quote}\nIt does look like we can reuse the closeResponder method in the loop\n{quote}\nAgreed. I have filed a new JIRA HDFS-10820 for tracking that. I think we are closing. I'd like to found more clues from failure logs, but it seems the failure log that attached was based on the old codes? Right?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-08-31T02:28:41.724+0000","updated":"2016-08-31T02:28:41.724+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15451120","id":"15451120","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Yep, sadly I'm not able to locally reproduce this at all, either with upstream or cdh.\nThe log I attached is from a CDH code base, where I could use [~andrew.wang]'s [dist_test|http://blog.cloudera.com/blog/2016/05/quality-assurance-at-cloudera-distributed-unit-testing/] to reproduce this. (So far dist_test doesn't work with upstream yet.)\n\nFeel free to attach here if you're able to get a failure log. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-31T04:51:13.692+0000","updated":"2016-08-31T04:51:13.692+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15496954","id":"15496954","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"I haven't managed to look more into this, but attaching a failed precommit log as of today.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-09-16T18:04:37.912+0000","updated":"2016-09-16T18:04:37.912+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15546568","id":"15546568","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"I have looked more into this, and believe to found the proof of what caused the test to fail.\n\nAttaching a failure log with my self-added verbose logging. As you can see, the {{select}} is blocking the thread, and its timeout is 60 seconds. I'm not sure whether the 50-second test timeout is intentional, but after upping it to 180-second, the test failure rate went from 3/100 to 0/1000.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-10-04T20:30:39.922+0000","updated":"2016-10-04T20:30:39.922+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15546854","id":"15546854","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 17s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  9m  5s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 33s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  6s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 16s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 55s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  5s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 26s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 55s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  4s{color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 82m  4s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black}105m 18s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations |\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |\n|   | hadoop.hdfs.server.namenode.TestFileTruncate |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:9560f25 |\n| JIRA Issue | HDFS-6532 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12831614/HDFS-6532.002.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 44c223b96a4a 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 44f48ee |\n| Default Java | 1.8.0_101 |\n| findbugs | v3.0.0 |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/17007/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/17007/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17007/console |\n| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-10-04T22:24:17.315+0000","updated":"2016-10-04T22:24:17.315+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15546919","id":"15546919","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Looked more into this. For failed cases, we see (copied from the 'select-timeout' attachment):\n{noformat}\n2016-10-04 22:10:24,365 INFO  hdfs.DFSOutputStream (DFSOutputStream.java:run(1114)) - ======  \njava.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 28459 millis timeout left.\n        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2247)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:1015)\n{noformat}\n\nAnd for the success cases, we see:\n{noformat}\n2016-10-04 15:13:15,271 INFO  hdfs.DFSOutputStream (DFSOutputStream.java:run(1116)) - ======  \njava.io.IOException: Bad response ERROR for block BP-1283991366-172.16.3.181-1475619192335:blk_1073741825_1001 from datanode DatanodeInfoWithStorage[127.0.0.1:61321,DS-720243dd-55b6-49ef-ae55-4462e20260d5,DISK]\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:1053)\n{noformat}\nand \n{noformat}\n2016-10-04 15:13:16,084 INFO  hdfs.DFSOutputStream (DFSOutputStream.java:run(1116)) - ======  \njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2249)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:1017)\n{noformat}\nI printed the exception from [this line|https://github.com/apache/hadoop/blob/44f48ee96ee6b2a3909911c37bfddb0c963d5ffc/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java#L1149].\n\nSo in the failed cases, the responder is running in [this loop|https://github.com/apache/hadoop/blob/44f48ee96ee6b2a3909911c37bfddb0c963d5ffc/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java#L708], until the following exception is thrown\n{noformat}2016-10-04 22:36:40,403 INFO  datanode.DataNode (BlockReceiver.java:receiveBlock(941)) - Exception for BP-2046749708-172.17.0.1-1475620536833:blk_1073741826_1005\njava.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:42956 remote=/127.0.0.1:56324]\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n        at java.io.DataInputStream.read(DataInputStream.java:149)\n        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:900)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:802)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)\n        at java.lang.Thread.run(Thread.java:745)\n2016-10-04 22:36:40,469 INFO  hdfs.DFSOutputStream (DFSOutputStream.java:run(1116)) - ======  \njava.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2249)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:1017)\n{noformat}\nAfterwards the {{close}} call correctly returns and the test passes. Not sure how we can interrupt early in this case. {{receiveNextPacket}} is just calling {{select}}, and not reached the {{verifyChunks}} yet.\nSince there's no impact on correctness, maybe we should just add the test timeout. [~kihwal], could you share your thoughts on this? Thanks a lot.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-10-04T22:48:58.604+0000","updated":"2016-10-04T23:29:35.316+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15547303","id":"15547303","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks [~xiaochen] for continous work on this. Some comments from me:\n1.{quote}\nmaybe we should just add the test timeout\n{quote}\nI am +0 for increasing the timeout since that I think this seems not the best way.\n2.As the comments said, the socket timeout happens when the test runs failed. Here the socket timeout is normal? Or there is some other exception to trigger this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-10-05T01:29:54.457+0000","updated":"2016-10-05T01:29:54.457+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/15550368","id":"15550368","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the comment, [~linyiqun].\nTook a jstack when right before the test timed out. It has:\n{noformat}\n\"ResponseProcessor for block BP-341806944-172.17.0.1-1475696115790:blk_1073741825_1001\" \n   java.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)\n        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2247)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:1017)\n\n\"PacketResponder: BP-341806944-172.17.0.1-1475696115790:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE\" \n   java.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)\n        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2247)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1303)\n        at java.lang.Thread.run(Thread.java:745)\n\n\"DataXceiver for client DFSClient_NONMAPREDUCE_27347732_8 at /127.0.0.1:36783 [Receiving block BP-341806944-172.17.0.1-1475696115790:blk_1073741825_1001]\" \n   java.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)\n        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n        at java.io.DataInputStream.read(DataInputStream.java:149)\n        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:900)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:802)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\nAttaching the full jstack for information.\n\nHi [~kihwal],\ndo you have any insight to this? IIUC, DFSOutputStream is waiting for ack which is ok, but not sure how the DataXCeiver and BlockReceiver should work. It seems they're all stuck on socket I/O... Is this a normal? I guess not since the usual (~98%) test runs doesn't end up sticking there....\nBut from the stack trace, BlockReceiver's PacketResponder waiting on {{ack.readFields(downstreamIn);}} seems not interruptible, and it can come back after select timeout. So I still think this is acceptable.. Appreciate any feedback. Thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-10-06T00:07:03.521+0000","updated":"2016-10-06T06:05:13.405+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/16445195","id":"16445195","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lars_francke","name":"lars_francke","key":"lars_francke","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lars_francke&avatarId=24833","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lars_francke&avatarId=24833","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lars_francke&avatarId=24833","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lars_francke&avatarId=24833"},"displayName":"Lars Francke","active":true,"timeZone":"Europe/Berlin"},"body":"Hi,\r\n\r\nI know this is old but we're seeing this same error message on a production cluster and are a bit confused by it as well. Do you happen to have any more information or ideas on the root cause?\r\n\r\nThis is from Spark writing to HDFS. And Spark is killing tasks with that same exception (see blow). Looking at the code I also don't know why things would be interrupted there.\r\n\r\nThe DataNode logs look normal to me at the same time (unfortunately for those I don't have the verbatim logs):\r\n\r\n01:12:26 - Receiving Block\r\n01:13:17 - Thread is interrupted\r\n01:13:17 - Terminating\r\n01:13:17 - Premature EOF from inputStream\r\n\r\n \r\n{code:java}\r\n18/04/20 01:12:29 INFO Executor: Executor is trying to kill task 66.0 in stage 231.0 (TID 204526)\r\n18/04/20 01:12:29 INFO DFSClient: Exception in createBlockOutputStream\r\njava.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.194.211.44:52770 remote=/10.194.211.44:1019]. 215000 millis timeout left.\r\nat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\r\nat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\r\nat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\r\nat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\r\nat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\r\nat java.io.FilterInputStream.read(FilterInputStream.java:83)\r\nat java.io.FilterInputStream.read(FilterInputStream.java:83)\r\nat org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2462)\r\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1461)\r\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1380)\r\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:558)\r\n18/04/20 01:12:29 INFO DFSClient: Abandoning BP-1887265555-10.194.210.65-1478836813700:blk_5197463151_4124323282\r\n18/04/20 01:12:29 WARN Client: interrupted waiting to send rpc request to server\r\njava.lang.InterruptedException\r\nat java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404)\r\nat java.util.concurrent.FutureTask.get(FutureTask.java:191)\r\nat org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1094)\r\nat org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\nat org.apache.hadoop.ipc.Client.call(Client.java:1398)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\nat com.sun.proxy.$Proxy12.abandonBlock(Unknown Source)\r\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.abandonBlock(ClientNamenodeProtocolTranslatorPB.java:436)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\r\nat com.sun.proxy.$Proxy13.abandonBlock(Unknown Source)\r\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1384)\r\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:558)\r\n18/04/20 01:12:29 WARN DFSClient: DataStreamer Exception\r\njava.io.IOException: java.lang.InterruptedException\r\nat org.apache.hadoop.ipc.Client.call(Client.java:1463)\r\nat org.apache.hadoop.ipc.Client.call(Client.java:1398)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\nat com.sun.proxy.$Proxy12.abandonBlock(Unknown Source)\r\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.abandonBlock(ClientNamenodeProtocolTranslatorPB.java:436)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\r\nat com.sun.proxy.$Proxy13.abandonBlock(Unknown Source)\r\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1384)\r\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:558)\r\nCaused by: java.lang.InterruptedException\r\nat java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404)\r\nat java.util.concurrent.FutureTask.get(FutureTask.java:191)\r\nat org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1094)\r\nat org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\n... 14 more\r\n\r\n18/04/20 01:12:29 ERROR Executor: Exception in task 66.0 in stage 231.0 (TID 204526)\r\njava.io.InterruptedIOException: Interrupted while waiting for data to be acknowledged by pipeline\r\nat org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:2346)\r\nat org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:2325)\r\nat org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2461)\r\nat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2431)\r\nat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\r\nat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\r\nat org.apache.spark.SparkHadoopWriter.close(SparkHadoopWriter.scala:103)\r\nat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$8.apply$mcV$sp(PairRDDFunctions.scala:1203)\r\nat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1295)\r\nat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1203)\r\nat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1183)\r\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\r\nat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:247)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\nat java.lang.Thread.run(Thread.java:745){code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lars_francke","name":"lars_francke","key":"lars_francke","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lars_francke&avatarId=24833","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lars_francke&avatarId=24833","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lars_francke&avatarId=24833","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lars_francke&avatarId=24833"},"displayName":"Lars Francke","active":true,"timeZone":"Europe/Berlin"},"created":"2018-04-20T02:53:02.679+0000","updated":"2018-04-20T02:53:02.679+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12721254/comment/16445223","id":"16445223","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lars_francke","name":"lars_francke","key":"lars_francke","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lars_francke&avatarId=24833","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lars_francke&avatarId=24833","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lars_francke&avatarId=24833","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lars_francke&avatarId=24833"},"displayName":"Lars Francke","active":true,"timeZone":"Europe/Berlin"},"body":"Okay...I'm sorry you can ignore my \"noise\": Turns out this is Spark Speculative Execution killing tasks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lars_francke","name":"lars_francke","key":"lars_francke","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lars_francke&avatarId=24833","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lars_francke&avatarId=24833","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lars_francke&avatarId=24833","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lars_francke&avatarId=24833"},"displayName":"Lars Francke","active":true,"timeZone":"Europe/Berlin"},"created":"2018-04-20T03:22:23.679+0000","updated":"2018-04-20T03:22:23.679+0000"}],"maxResults":16,"total":16,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6532/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1wrjr:"}}