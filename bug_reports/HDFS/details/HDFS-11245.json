{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13028092","self":"https://issues.apache.org/jira/rest/api/2/issue/13028092","key":"HDFS-11245","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2016-12-14T18:29:04.165+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Dec 15 15:59:52 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_71701955_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2016-12-15T09:23:11.030+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-11245/watchers","watchCount":5,"isWatching":false},"created":"2016-12-14T13:28:11.132+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12335732","id":"12335732","description":"3.0.0-alpha1 release","name":"3.0.0-alpha1","archived":false,"released":true,"releaseDate":"2016-09-03"}],"issuelinks":[{"id":"12489134","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12489134","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12665400","key":"HADOOP-9902","self":"https://issues.apache.org/jira/rest/api/2/issue/12665400","fields":{"summary":"Shell script rewrite","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/2","id":"2","description":"A new feature of the product, which has yet to be developed.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype","name":"New Feature","subtask":false,"avatarId":21141}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-12-15T15:59:52.255+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12329603","id":"12329603","name":"hdfs"}],"timeoriginalestimate":null,"description":"It seems that HDFS on trunk is ignoring {{HADOOP_CONF_DIR}}. On {{branch-2}} I could export {{HADOOP_CONF_DIR}} and use that to store my {{hdfs-site.xml}} and {{log4j.properties}}. But on trunk it appears to ignore the environment variable.\n\nAlso, even if hdfs can find the {{log4j.properties}}, it doesn't seem interested in opening and loading it.\n\nOn Ubuntu 16.10:\n\n{code}\n$ source env.sh\n$ cat env.sh \n#!/bin/bash\nexport JAVA_HOME=/usr/lib/jvm/java-8-oracle\nexport HADOOP_HOME=\"$HOME\"/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT\nexport HADOOP_LOG_DIR=\"$(pwd)/log\"\nPATH=\"$HADOOP_HOME\"/bin:$PATH\nexport HADOOP_CLASSPATH=$(hadoop classpath):\"$HADOOP_HOME\"/share/hadoop/tools/lib/*\nexport HADOOP_USER_CLASSPATH_FIRST=true\n{code}\n\n\nThen I set the HADOOP_CONF_DIR:\n{code}\n$ export HADOOP_CONF_DIR=\"$(pwd)/conf/nn\"\n$ ls $HADOOP_CONF_DIR\nhadoop-env.sh  hdfs-site.xml  log4j.properties\n{code}\n\nNow, we try to run a namenode:\n{code}\n$ hdfs namenode\n2016-12-14 14:04:51,193 ERROR [main] namenode.NameNode: Failed to start namenode.\njava.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:/// has no authority.\n        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:648)\n        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddressCheckLogical(DFSUtilClient.java:677)\n        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:639)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:556)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:687)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:707)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:916)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1633)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1701)\n{code}\n\nThis is weird. We have the {{fs.defaultFS}} set:\n\n{code}\n$ grep -n2 fs.defaultFS $HADOOP_CONF_DIR/hdfs-site.xml\n3-<configuration>\n4-    <property>\n5:        <name>fs.defaultFS</name>\n6-        <value>hdfs://localhost:60010</value>\n7-    </property>\n{code}\n\nSo if isn't finding this config. Where is is looking and finding {{file:///}}?\n{code}\n$ strace -f -eopen,stat hdfs namenode 2>&1 | grep hdfs-site.xml\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml\", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml\", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml\", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml\", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml\", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml\", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml\", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml\", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0\n[pid 16271] open(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", O_RDONLY) = 218\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml\", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml\", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml\", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml\", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml\", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml\", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml\", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml\", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0\n[pid 16271] open(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", O_RDONLY) = 218\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml\", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml\", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml\", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml\", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml\", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml\", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml\", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml\", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0\n[pid 16271] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0\n[pid 16271] open(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", O_RDONLY) = 218\n{code}\n\nSo it's ignoring {{HADOOP_CONF_DIR}}. We can work around it using {{-conf $(pwd)/conf/nn/hdfs-site.xml}}:\n{code}\n$ strace -f -eopen,stat hdfs namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep hdfs-site.xml\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml\", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml\", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml\", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml\", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml\", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml\", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml\", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml\", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0\n[pid 16493] open(\"/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml\", O_RDONLY) = 218\n[pid 16493] stat(\"/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/hdfs-site.xml\", {st_mode=S_IFREG|0644, st_size=2107, ...}) = 0\n[pid 16493] open(\"/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/hdfs-site.xml\", O_RDONLY) = 218\n\n------8<------\n{code}\n\nGreat! However, it's not finding my  log4j.properties for some reason. This is annoying because hdfs isn't printing anything or logging anywhere. Where is it looking?\n{code}\n$ strace -f hdfs namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep log4j.properties\nstat(\"/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/log4j.properties\", {st_mode=S_IFREG|0644, st_size=13641, ...}) = 0\n{code}\n\nIt found it, but it only statted it. It never opened it! So it seems there's at least one bug here where {{log4j.properties}} is being ignored. But shouldn't {{HADOOP_OPTS}} be set and configuring it to print to the console and to my log dir?\n\n{code}\n$ hdfs --debug namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep HADOOP_OPTS\nDEBUG: HADOOP_OPTS accepted -Dhdfs.audit.logger=INFO,NullAppender\nDEBUG: Appending HDFS_NAMENODE_OPTS onto HADOOP_OPTS\nDEBUG: HADOOP_OPTS accepted -Dyarn.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log\nDEBUG: HADOOP_OPTS accepted -Dyarn.log.file=hadoop.log\nDEBUG: HADOOP_OPTS accepted -Dyarn.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT\nDEBUG: HADOOP_OPTS accepted -Dyarn.root.logger=INFO,console\nDEBUG: HADOOP_OPTS accepted -Djava.library.path=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/lib/native\nDEBUG: HADOOP_OPTS accepted -Dhadoop.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log\nDEBUG: HADOOP_OPTS accepted -Dhadoop.log.file=hadoop.log\nDEBUG: HADOOP_OPTS accepted -Dhadoop.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT\nDEBUG: HADOOP_OPTS accepted -Dhadoop.id.str=ehigg90120\nDEBUG: HADOOP_OPTS accepted -Dhadoop.root.logger=INFO,console\nDEBUG: HADOOP_OPTS accepted -Dhadoop.policy.file=hadoop-policy.xml\nDEBUG: HADOOP_OPTS declined -Dhadoop.security.logger=INFO,NullAppender\nDEBUG: Final HADOOP_OPTS: -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dyarn.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT -Dyarn.root.logger=INFO,console -Djava.library.path=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/lib/native -Dhadoop.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT -Dhadoop.id.str=ehigg90120 -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml\n{code}\n\nSo it seems it is being configured and passed to the namenode. It's just not obeying it as far as I can see. \n\nSo maybe there are two possibly related bugs:\n\n1. {{HADOOP_CONF_DIR}} is ignored\n2. The logger is not using {{log4j.properties}} or the command line. I would expect it to use the {{log4j.properties}} in the {{HADOOP_CONF_DIR}}.\n\nI feel like I must be misunderstanding something since this seems like a pretty big issue but I didn't find any open tickets about it or any tickets describing a new way of configuring clusters.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"HDFS ignores HADOOP_CONF_DIR","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ehiggs","name":"ehiggs","key":"ehiggs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ewan Higgs","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ehiggs","name":"ehiggs","key":"ehiggs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ewan Higgs","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Linux","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028092/comment/15749062","id":"15749062","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"body":"Could you check the {{Final CLASSPATH}} in {{--debug}} output? The conf dir should be on the path. Most Hadoop components load resource (conf file and log4j.properties) from classpath.\n\nTrunk has HADOOP-9902 while branch-2 does not.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jzhuge","name":"jzhuge","key":"jzhuge","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jzhuge&avatarId=31264","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jzhuge&avatarId=31264","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jzhuge&avatarId=31264","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jzhuge&avatarId=31264"},"displayName":"John Zhuge","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-14T18:29:04.165+0000","updated":"2016-12-14T18:29:04.165+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028092/comment/15750542","id":"15750542","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"The problem is here:\n\n{code}\nexport HADOOP_CLASSPATH=$(hadoop classpath):\"$HADOOP_HOME\"/share/hadoop/tools/lib/*\nexport HADOOP_USER_CLASSPATH_FIRST=true\n{code}\n\nIIRC, branch-2 didn't actually process the classpath correctly.  HADOOP\\_USER\\_CLASSPATH\\_FIRST was only \"sorta\" true and varied between the different projects.  3.x fixes that and makes it always true.  With that knowledge, we can break this down a bit.\n\nhadoop classpath gives you all of the hadoop jars.  But what you probably don't know is that those jars also contain default versions of log4j and the xml files.  Now by forcing the user classpath first, the default ones override the HADOOP\\_CONF\\_DIR values.\n\nSo for 3.x, do not put hadoop classpath in the path list.  That will always be present. Additionally, there's not much reason to forcefully add all of the tools dir in the classpath.  This now configurable via the HADOOP\\_OPTIONAL\\_TOOLS vars in hadoop-env.sh which will only add the features you actually care about rather than adding a full kitchen sink of stuff.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2016-12-15T06:32:03.295+0000","updated":"2016-12-15T06:32:03.295+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028092/comment/15750815","id":"15750815","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ehiggs","name":"ehiggs","key":"ehiggs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ewan Higgs","active":true,"timeZone":"Etc/UTC"},"body":"[~aw],\nWhen I {{unset HADOOP_USER_CLASSPATH_FIRST}} it all appears to work. I made the rest of the changes you suggest. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ehiggs","name":"ehiggs","key":"ehiggs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ewan Higgs","active":true,"timeZone":"Etc/UTC"},"created":"2016-12-15T08:52:29.423+0000","updated":"2016-12-15T08:52:29.423+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028092/comment/15750884","id":"15750884","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ehiggs","name":"ehiggs","key":"ehiggs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ewan Higgs","active":true,"timeZone":"Etc/UTC"},"body":"HADOOP-9902 had a massive rewrite of scripts which improved the consistency and rationality of all the environment variables. These improvements have changed a few things which end up meaning users/admins need to alter their environment when switching from 2.x to 3.x.\n\nThe issue here was setting {{HADOOP_USER_CLASSPATH_FIRST}} as well as {{HADOOP_CLASSPATH}} when these are no longer needed. Also, getting the tools in the classpath is also done through {{HADOOP_OPTIONAL_TOOLS}} in {{hadoop-env.sh}} instead of {{HADOOP_CLASSPATH}} hacking.\n\nUnsetting these variables seems to fix everything.\n\nPossible follow on: more documentation distilling the changes required when moving from 2.x to 3.x. Especially in the context of a rolling upgrade.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ehiggs","name":"ehiggs","key":"ehiggs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ewan Higgs","active":true,"timeZone":"Etc/UTC"},"created":"2016-12-15T09:23:11.152+0000","updated":"2016-12-15T09:23:11.152+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028092/comment/15751732","id":"15751732","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"FWIW, \n\n{code}\nexport HADOOP_CLASSPATH=$(hadoop classpath):\"$HADOOP_HOME\"/share/hadoop/tools/lib/*\nexport HADOOP_USER_CLASSPATH_FIRST=true\n{code}\n\ndoesn't really work well under branch-2 either. This construction just doubles the amount of jars that the system is going to look through to find what it needs, leading to some very negative performance impacts. In the end, it's effectively the same as\n\n{code}\nexport HADOOP_CLASSPATH=\"$HADOOP_HOME\"/share/hadoop/tools/lib/*\n{code}\n\nbut without the double lookup.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2016-12-15T15:59:52.255+0000","updated":"2016-12-15T15:59:52.255+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-11245/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i37lav:"}}