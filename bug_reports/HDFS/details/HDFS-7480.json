{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12759618","self":"https://issues.apache.org/jira/rest/api/2/issue/12759618","key":"HDFS-7480","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-12-13T18:29:35.413+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jan 06 12:42:55 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-7480/watchers","watchCount":8,"isWatching":false},"created":"2014-12-05T08:13:37.781+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326264","id":"12326264","description":"2.5.0 release","name":"2.5.0","archived":false,"released":true,"releaseDate":"2014-08-11"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-01-06T12:42:55.668+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"A small cluster has 8 servers with 32 G RAM.\nTwo is namenodes (HA-configured), six is Datanodes (8x3 TB disks configured with RAID as one 21 TB drive).\nThe cluster recieves avg 400.000 small files each day. I started archiving (HAR) each day as separate archives. After deleting the orinigal files for one month, the namenodes stared acting up really bad.\nWhen restaring those, both active and passive nodes seems to work OK for some time, but then starts to report a lot of blocks belonging to no files, and the name-node just spins those messages in a massive loop. If the passive node is first, it also influences the active node in susch a way that it's no longer possible to archive new files. If the active node also starts in this loop, it suddenly dies without any error-message.\n\nThe only way I'm able to get rid of the problem, is to start decommission nodes, watching the cluster closely to avoid downtime, and make sure every datanode gets a 'clean' start. After all datanodes has been decommisioned (in turns), and restarted with clean disks, the problem is gone. But if I then delete a lot of files in a short time, the problem starts again...  \nThe main problem (I think), is that the recieving and reporting of those blocks takes so many resources, that the namenodes is too busy to tell the datanodes to delete those blocks.. \n\nIf the active name-node starts on the loop, it does the 'right' thing by telling the datanode to invalidate the block, But the amount of blocks is so massive, that the namenode doesn't do anything else. Just now, I have about 1200-1400 log-entries pr second in the passive node.\n\nupdate :\nJust got the active namenode in the loop - it logs 1000 lines pr second. \n500 'BlockStateChange: BLOCK* processReport: blk_1080796332_7056241 on x.x.x.x:50010 size 1742 does not belong to any file'\nand \n500 ' BlockStateChange: BLOCK* InvalidateBlocks: add blk_1080796332_7056241 to x.x.x.x:50010'","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Namenodes loops on 'block does not belong to any file' after deleting many files","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"CentOS - HDFS-HA (journal), zookeeper","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12759618/comment/14236768","id":"14236768","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"body":"One thought : Could it be that the failover-controller is responsible for killing the namenode before it's finished with the task because it doesn't respond properly ?\n\n\nI had one situation now, where one datanode had only 120.000 blocks without file-referende, and when I restarted this datanode, it connected to the active namenode. This then recieved those block-id's and invalidated them. When finished, I had 120.000 bocks 'pending deletion', but since the name-node kept logging blocks 'does not belong to any file' it took very long time before it managed to tell the datanode to start deleting. When the number of blocks decreased, it took shorter and shorter time between delete-commands. In the end that datanode was cleaned up, but when I tried the samme approach for a datanode with more unattached blocsk, the namenode died before all blocks was marked as invalid. I suspect that it might have beed the failover-controller that actually killed the namenode. And of course; when the namenode died, it lost all information about blocks 'pending deletion' and had to start over when restarted...\n\nFor the moment, I have killed the failover-controller, but it seems that the number of invalid blocks that constantly is bombarding the name-server prevents it from ever getting around to tell the datanode to delete the blocks. (It's taking forever between deletes in the beginning)\n\nThe bug in this case must be that the namenode/datanode-communication repeats the loop of non-attached-blocks, the second bug must be that the name-node get's so busy recieving those messages that it's unresponsive to anything else...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"created":"2014-12-06T12:20:59.152+0000","updated":"2014-12-06T12:20:59.152+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12759618/comment/14245463","id":"14245463","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shv","name":"shv","key":"shv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Konstantin Shvachko","active":true,"timeZone":"America/Los_Angeles"},"body":"Looks similar to HDFS-7503. Is it fixed by it?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shv","name":"shv","key":"shv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Konstantin Shvachko","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-12-13T18:29:35.413+0000","updated":"2014-12-13T18:29:35.413+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12759618/comment/14246605","id":"14246605","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"body":"I will test when 2.6.1 is released..","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"created":"2014-12-15T12:18:32.578+0000","updated":"2014-12-15T12:18:32.578+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12759618/comment/14266042","id":"14266042","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"body":"2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.\nAthe the delete-moment, I don't think that the logging is that problematic.\nThe logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"created":"2015-01-06T12:42:50.453+0000","updated":"2015-01-06T12:42:50.453+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12759618/comment/14266043","id":"14266043","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"body":"2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.\nAthe the delete-moment, I don't think that the logging is that problematic.\nThe logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"created":"2015-01-06T12:42:54.498+0000","updated":"2015-01-06T12:42:54.498+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12759618/comment/14266044","id":"14266044","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"body":"2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.\nAthe the delete-moment, I don't think that the logging is that problematic.\nThe logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"created":"2015-01-06T12:42:55.089+0000","updated":"2015-01-06T12:42:55.089+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12759618/comment/14266045","id":"14266045","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"body":"2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.\nAthe the delete-moment, I don't think that the logging is that problematic.\nThe logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"created":"2015-01-06T12:42:55.542+0000","updated":"2015-01-06T12:42:55.542+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12759618/comment/14266046","id":"14266046","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"body":"2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.\nAthe the delete-moment, I don't think that the logging is that problematic.\nThe logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=frha","name":"frha","key":"frha","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Frode Halvorsen","active":true,"timeZone":"Europe/Oslo"},"created":"2015-01-06T12:42:55.668+0000","updated":"2015-01-06T12:42:55.668+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-7480/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2337b:"}}