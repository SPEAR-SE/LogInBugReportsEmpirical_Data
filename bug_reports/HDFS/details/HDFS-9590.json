{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12923592","self":"https://issues.apache.org/jira/rest/api/2/issue/12923592","key":"HDFS-9590","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/5","id":"5","description":"All attempts at reproducing this issue failed, or not enough information was available to reproduce the issue. Reading the code produces no clues as to why this behavior would occur. If more information appears later, please reopen the issue.","name":"Cannot Reproduce"},"customfield_12312322":null,"customfield_12310220":"2015-12-21T23:18:13.930+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jul 11 18:54:26 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_49061363716_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2017-07-11T18:54:26.916+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-9590/watchers","watchCount":2,"isWatching":false},"created":"2015-12-21T22:45:03.248+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-11T18:54:26.961+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"The code looks to be possible to have race conditions in multiple-threaded runs.\n{code}\n    public void unlock() throws IOException {\n      if (this.lock == null)\n        return;\n      this.lock.release();\n      lock.channel().close();\n      lock = null;\n    }\n{code}\nThis is called in a handful of places, and I don't see any protection. Shall we add some synchronization mechanism? Not sure if I missed any design assumptions here.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12779290","id":"12779290","filename":"HDFS-9590.01.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-12-23T18:48:53.414+0000","size":2002,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12779290/HDFS-9590.01.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"NPE in Storage$StorageDirectory#unlock()","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12923592/comment/15067196","id":"15067196","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"This was observed in the following unit test failure of TestQJMWithFaults#testRecoverAfterDoubleFailures:\nError Message\n{noformat}\nUnable to shut down. Check log for details\n{noformat}\nStacktrace\n{noformat}\njava.io.IOException: Unable to shut down. Check log for details\n\tat org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.shutdown(MiniJournalCluster.java:161)\n\tat org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults.testRecoverAfterDoubleFailures(TestQJMWithFaults.java:181)\n\tat org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults.testRecoverAfterDoubleFailures(TestQJMWithFaults.java:138)\n{noformat}\nStandard Output is pretty long, but the one that Error Message wants us to check is:\n{noformat}\n2015-12-20 18:51:46,825 WARN  qjournal.MiniJournalCluster (MiniJournalCluster.java:shutdown(157)) - Unable to stop journal node org.apache.hadoop.hdfs.qjournal.server.JournalNode@fcb345b\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.unlock(Storage.java:747)\n\tat org.apache.hadoop.hdfs.server.common.Storage.unlockAll(Storage.java:1125)\n\tat org.apache.hadoop.hdfs.qjournal.server.JNStorage.close(JNStorage.java:249)\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.close(Journal.java:227)\n\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNode.stop(JournalNode.java:207)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNode.stopAndJoin(JournalNode.java:232)\n\tat org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.shutdown(MiniJournalCluster.java:154)\n\tat org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults.testRecoverAfterDoubleFailures(TestQJMWithFaults.java:181)\n\tat org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults.testRecoverAfterDoubleFailures(TestQJMWithFaults.java:138)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\n2015-12-20 18:51:46,825 INFO  ipc.Server (Server.java:stop(2485)) - Stopping server on 36031\n{noformat}\n\nWhere Storage.java:747 in the version is {{this.lock.release();}}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-12-21T22:46:13.390+0000","updated":"2015-12-21T22:46:13.390+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12923592/comment/15067239","id":"15067239","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=liuml07","name":"liuml07","key":"liuml07","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=liuml07&avatarId=29203","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=liuml07&avatarId=29203","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=liuml07&avatarId=29203","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=liuml07&avatarId=29203"},"displayName":"Mingliang Liu","active":true,"timeZone":"America/Los_Angeles"},"body":"# Do you mean {{Storage$StorageDirectory#unlock()}}? There is no {{Storage#unlock}} method.\n# Would you kindly elaborate what kind of \"synchronization mechanism\" do you expect to add? I don't know all the design assumption here either, but this code itself is to wrap a lock.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=liuml07","name":"liuml07","key":"liuml07","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=liuml07&avatarId=29203","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=liuml07&avatarId=29203","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=liuml07&avatarId=29203","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=liuml07&avatarId=29203"},"displayName":"Mingliang Liu","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-12-21T23:18:13.930+0000","updated":"2015-12-21T23:18:13.930+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12923592/comment/15067266","id":"15067266","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~liuml07] for the comments. \nbq. Do you mean {{Storage$StorageDirectory#unlock()}}? There is no Storage#unlock method.\nYes, sorry I was unclear. Updated the title.\nbq. Would you kindly elaborate what kind of \"synchronization mechanism\" do you expect to add? I don't know all the design assumption here either, but this code itself is to wrap a lock.\nMy concern is that if 2 threads are calling unlock(), and both passed the null check in the beginning, then if 1 proceed to set {{lock = null}}, the other will throw the NPE in the stack trace above. I'm thinking of adding synchronized to the method, or perhaps more sophisticated protection outside. This NPE seems fundamental, so I'm looking around to see what background stories there are...\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-12-21T23:41:12.243+0000","updated":"2015-12-21T23:41:12.243+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12923592/comment/15067286","id":"15067286","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=liuml07","name":"liuml07","key":"liuml07","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=liuml07&avatarId=29203","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=liuml07&avatarId=29203","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=liuml07&avatarId=29203","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=liuml07&avatarId=29203"},"displayName":"Mingliang Liu","active":true,"timeZone":"America/Los_Angeles"},"body":"{quote}\n2 threads are calling unlock()\n{quote}\nOh, I thought it was an exclusive lock, and thus lock() happens before unlock(). Let me have a look at the code again. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=liuml07","name":"liuml07","key":"liuml07","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=liuml07&avatarId=29203","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=liuml07&avatarId=29203","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=liuml07&avatarId=29203","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=liuml07&avatarId=29203"},"displayName":"Mingliang Liu","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-12-21T23:59:33.352+0000","updated":"2015-12-21T23:59:33.352+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12923592/comment/15067310","id":"15067310","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"I take it back. So the locking itself looks to be exclusive - should only be acquired once. I guess that's why the unlock code doesn't have any protection. I'm still digging into the code to see how the given NPE was thrown...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-12-22T00:13:03.631+0000","updated":"2015-12-22T00:13:03.631+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12923592/comment/15070031","id":"15070031","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"I think this is how the NPE happened. This looks to be a test specific issue.\n\nIn {{TestQJMWithFaults#testRecoverAfterDoubleFailures}}, we're trying to inject JN call failures in all possible permutations. In the test that I saw the NPE (which I will paste in a later comment), the following happened:\n{noformat}\n2015-12-20 18:51:46,820 WARN  namenode.FileJournalManager (FileJournalManager.java:startLogSegment(127)) - Unable to start log segment 7 at /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_inprogress_0000000000000000007: null\n2015-12-20 18:51:46,821 FATAL server.JournalNode (JournalNode.java:reportErrorOnFile(299)) - Error reported on file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_inprogress_0000000000000000007... exiting\njava.lang.Exception\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNode$ErrorReporter.reportErrorOnFile(JournalNode.java:299)\n\tat org.apache.hadoop.hdfs.server.namenode.FileJournalManager.startLogSegment(FileJournalManager.java:130)\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.startLogSegment(Journal.java:559)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.startLogSegment(JournalNodeRpcServer.java:162)\n\tat org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.startLogSegment(QJournalProtocolServerSideTranslatorPB.java:198)\n\tat org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25425)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n{noformat}\nNote that {{reportErrorOnFile}} will end up calling {{Storage$StorageDirectory#unlock}}, in the rpc call.\nMeanwhile, since we also injected at 7, the majority of quorums failed, so {{AsyncLoggerSet#waitForWriteQuorum}} throws out an exception, and {{TestQJMWithFaults}} will shutdown cluster in the {{finally}} block, which also ends up calling {{Storage$StorageDirectory#unlock}}.\n\n\nIt looks to me that this can only happen in the tests, so impact is trivial. I want to revoke my initial thoughts of changing the {{unlock}} method, and think we'd better enhance the {{MiniJournalCluster#shutdown}} to handle this, if we decide to handle this at all. The only issue is that the NPE will cause the test to terminate early and hide the real exception. Attached patch 1 for a brief idea of this, please review and let me know if this is on the right track... Thanks!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-12-23T18:49:30.459+0000","updated":"2015-12-23T18:49:30.459+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12923592/comment/15070032","id":"15070032","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"This is the full log for run (7,1):\n{noformat}\n-------------------------------------------\nBeginning test, failing at (7, 1)\n-------------------------------------------\n\n\n2015-12-20 18:51:46,213 INFO  qjournal.MiniJournalCluster (MiniJournalCluster.java:<init>(95)) - Starting MiniJournalCluster with 3 journal nodes\n2015-12-20 18:51:46,215 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - JournalNode metrics system started (again)\n2015-12-20 18:51:46,216 INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1712)) - Starting Web-server for journal at: http://localhost:0\n2015-12-20 18:51:46,217 INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(282)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n2015-12-20 18:51:46,217 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.journal is not defined\n2015-12-20 18:51:46,217 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(759)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n2015-12-20 18:51:46,218 INFO  http.HttpServer2 (HttpServer2.java:addFilter(737)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context journal\n2015-12-20 18:51:46,218 INFO  http.HttpServer2 (HttpServer2.java:addFilter(744)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n2015-12-20 18:51:46,218 INFO  http.HttpServer2 (HttpServer2.java:addFilter(744)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n2015-12-20 18:51:46,219 INFO  http.HttpServer2 (HttpServer2.java:openListeners(947)) - Jetty bound to port 46849\n2015-12-20 18:51:46,219 INFO  mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26.cloudera.4\n2015-12-20 18:51:46,226 INFO  mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46849\n2015-12-20 18:51:46,226 INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(56)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-12-20 18:51:46,227 INFO  ipc.Server (Server.java:run(616)) - Starting Socket Reader #1 for port 36209\n2015-12-20 18:51:46,229 INFO  ipc.Server (Server.java:run(839)) - IPC Server Responder: starting\n2015-12-20 18:51:46,229 INFO  ipc.Server (Server.java:run(686)) - IPC Server listener on 36209: starting\n2015-12-20 18:51:46,231 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - JournalNode metrics system started (again)\n2015-12-20 18:51:46,232 INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1712)) - Starting Web-server for journal at: http://localhost:0\n2015-12-20 18:51:46,232 INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(282)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n2015-12-20 18:51:46,232 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.journal is not defined\n2015-12-20 18:51:46,233 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(759)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n2015-12-20 18:51:46,233 INFO  http.HttpServer2 (HttpServer2.java:addFilter(737)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context journal\n2015-12-20 18:51:46,233 INFO  http.HttpServer2 (HttpServer2.java:addFilter(744)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n2015-12-20 18:51:46,233 INFO  http.HttpServer2 (HttpServer2.java:addFilter(744)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n2015-12-20 18:51:46,234 INFO  http.HttpServer2 (HttpServer2.java:openListeners(947)) - Jetty bound to port 50567\n2015-12-20 18:51:46,234 INFO  mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26.cloudera.4\n2015-12-20 18:51:46,241 INFO  mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:50567\n2015-12-20 18:51:46,241 INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(56)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-12-20 18:51:46,242 INFO  ipc.Server (Server.java:run(616)) - Starting Socket Reader #1 for port 36031\n2015-12-20 18:51:46,243 INFO  ipc.Server (Server.java:run(839)) - IPC Server Responder: starting\n2015-12-20 18:51:46,243 INFO  ipc.Server (Server.java:run(686)) - IPC Server listener on 36031: starting\n2015-12-20 18:51:46,245 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - JournalNode metrics system started (again)\n2015-12-20 18:51:46,246 INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1712)) - Starting Web-server for journal at: http://localhost:0\n2015-12-20 18:51:46,246 INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(282)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n2015-12-20 18:51:46,247 INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(80)) - Http request log for http.requests.journal is not defined\n2015-12-20 18:51:46,247 INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(759)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n2015-12-20 18:51:46,247 INFO  http.HttpServer2 (HttpServer2.java:addFilter(737)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context journal\n2015-12-20 18:51:46,248 INFO  http.HttpServer2 (HttpServer2.java:addFilter(744)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n2015-12-20 18:51:46,248 INFO  http.HttpServer2 (HttpServer2.java:addFilter(744)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n2015-12-20 18:51:46,248 INFO  http.HttpServer2 (HttpServer2.java:openListeners(947)) - Jetty bound to port 43198\n2015-12-20 18:51:46,248 INFO  mortbay.log (Slf4jLog.java:info(67)) - jetty-6.1.26.cloudera.4\n2015-12-20 18:51:46,255 INFO  mortbay.log (Slf4jLog.java:info(67)) - Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43198\n2015-12-20 18:51:46,256 INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(56)) - Using callQueue class java.util.concurrent.LinkedBlockingQueue\n2015-12-20 18:51:46,256 INFO  ipc.Server (Server.java:run(616)) - Starting Socket Reader #1 for port 44247\n2015-12-20 18:51:46,258 INFO  ipc.Server (Server.java:run(839)) - IPC Server Responder: starting\n2015-12-20 18:51:46,258 INFO  ipc.Server (Server.java:run(686)) - IPC Server listener on 44247: starting\n2015-12-20 18:51:46,280 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(92)) - Initializing journal in directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/waitactive\n2015-12-20 18:51:46,280 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(92)) - Initializing journal in directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/waitactive\n2015-12-20 18:51:46,280 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(92)) - Initializing journal in directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/waitactive\n2015-12-20 18:51:46,280 WARN  common.Storage (Storage.java:analyzeStorage(477)) - Storage directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/waitactive does not exist\n2015-12-20 18:51:46,280 WARN  common.Storage (Storage.java:analyzeStorage(477)) - Storage directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/waitactive does not exist\n2015-12-20 18:51:46,280 WARN  common.Storage (Storage.java:analyzeStorage(477)) - Storage directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/waitactive does not exist\n2015-12-20 18:51:46,596 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #1: [/127.0.0.1:36209] format(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp)\n2015-12-20 18:51:46,597 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #1: [/127.0.0.1:44247] format(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp)\n2015-12-20 18:51:46,597 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #1: [/127.0.0.1:36031] format(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp)\n2015-12-20 18:51:46,598 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(92)) - Initializing journal in directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal\n2015-12-20 18:51:46,598 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(92)) - Initializing journal in directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal\n2015-12-20 18:51:46,599 WARN  common.Storage (Storage.java:analyzeStorage(477)) - Storage directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal does not exist\n2015-12-20 18:51:46,599 WARN  common.Storage (Storage.java:analyzeStorage(477)) - Storage directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal does not exist\n2015-12-20 18:51:46,599 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(92)) - Initializing journal in directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal\n2015-12-20 18:51:46,599 WARN  common.Storage (Storage.java:analyzeStorage(477)) - Storage directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal does not exist\n2015-12-20 18:51:46,600 INFO  server.Journal (Journal.java:format(216)) - Formatting org.apache.hadoop.hdfs.qjournal.server.Journal@7fe9d315 with namespace info: lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp\n2015-12-20 18:51:46,600 INFO  common.Storage (JNStorage.java:format(184)) - Formatting journal Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal with nsid: 12345\n2015-12-20 18:51:46,601 INFO  server.Journal (Journal.java:format(216)) - Formatting org.apache.hadoop.hdfs.qjournal.server.Journal@5021494e with namespace info: lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp\n2015-12-20 18:51:46,601 INFO  common.Storage (JNStorage.java:format(184)) - Formatting journal Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal with nsid: 12345\n2015-12-20 18:51:46,602 INFO  server.Journal (Journal.java:format(216)) - Formatting org.apache.hadoop.hdfs.qjournal.server.Journal@3953c8fb with namespace info: lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp\n2015-12-20 18:51:46,602 INFO  common.Storage (JNStorage.java:format(184)) - Formatting journal Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal with nsid: 12345\n2015-12-20 18:51:46,603 INFO  common.Storage (Storage.java:tryLock(716)) - Lock on /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/in_use.lock acquired by nodename 2194@ec2-beefy-slave-0594.vpc.cloudera.com\n2015-12-20 18:51:46,603 INFO  common.Storage (Storage.java:tryLock(716)) - Lock on /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/in_use.lock acquired by nodename 2194@ec2-beefy-slave-0594.vpc.cloudera.com\n2015-12-20 18:51:46,604 INFO  common.Storage (Storage.java:tryLock(716)) - Lock on /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/in_use.lock acquired by nodename 2194@ec2-beefy-slave-0594.vpc.cloudera.com\n2015-12-20 18:51:46,604 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(435)) - Starting recovery process for unclosed journal segments...\n2015-12-20 18:51:46,604 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #2: [/127.0.0.1:36209] getJournalState(test-journal)\n2015-12-20 18:51:46,604 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #2: [/127.0.0.1:44247] getJournalState(test-journal)\n2015-12-20 18:51:46,604 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #2: [/127.0.0.1:36031] getJournalState(test-journal)\n2015-12-20 18:51:46,605 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #3: [localhost/127.0.0.1:36209] newEpoch(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp, 1)\n2015-12-20 18:51:46,605 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #3: [localhost/127.0.0.1:44247] newEpoch(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp, 1)\n2015-12-20 18:51:46,606 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #3: [localhost/127.0.0.1:36031] newEpoch(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp, 1)\n2015-12-20 18:51:46,606 INFO  server.Journal (Journal.java:updateLastPromisedEpoch(325)) - Updating lastPromisedEpoch from 0 to 1 for client /127.0.0.1\n2015-12-20 18:51:46,606 INFO  server.Journal (Journal.java:updateLastPromisedEpoch(325)) - Updating lastPromisedEpoch from 0 to 1 for client /127.0.0.1\n2015-12-20 18:51:46,606 INFO  server.Journal (Journal.java:updateLastPromisedEpoch(325)) - Updating lastPromisedEpoch from 0 to 1 for client /127.0.0.1\n2015-12-20 18:51:46,607 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(188)) - Scanning storage FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal)\n2015-12-20 18:51:46,607 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(205)) - No files in FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal)\n2015-12-20 18:51:46,607 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(188)) - Scanning storage FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal)\n2015-12-20 18:51:46,607 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(205)) - No files in FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal)\n2015-12-20 18:51:46,607 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(437)) - Successfully started new epoch 1\n2015-12-20 18:51:46,608 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #4: [localhost/127.0.0.1:36209] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@29c63873, 1, -60)\n2015-12-20 18:51:46,608 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #4: [localhost/127.0.0.1:44247] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@14a41b72, 1, -60)\n2015-12-20 18:51:46,608 INFO  server.Journal (Journal.java:startLogSegment(547)) - Updating lastWriterEpoch from 0 to 1 for client /127.0.0.1\n2015-12-20 18:51:46,608 INFO  server.Journal (Journal.java:startLogSegment(547)) - Updating lastWriterEpoch from 0 to 1 for client /127.0.0.1\n2015-12-20 18:51:46,609 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(188)) - Scanning storage FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal)\n2015-12-20 18:51:46,609 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(205)) - No files in FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal)\n2015-12-20 18:51:46,609 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #4: [localhost/127.0.0.1:36031] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@2fa28eb6, 1, -60)\n2015-12-20 18:51:46,610 INFO  server.Journal (Journal.java:startLogSegment(547)) - Updating lastWriterEpoch from 0 to 1 for client /127.0.0.1\n2015-12-20 18:51:46,612 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #5: [localhost/127.0.0.1:36209] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@5ec5a0e2, 1, 1, 3, [B@27e2faeb)\n2015-12-20 18:51:46,612 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #5: [localhost/127.0.0.1:44247] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@26c76ec2, 1, 1, 3, [B@27e2faeb)\n2015-12-20 18:51:46,612 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #5: [localhost/127.0.0.1:36031] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@c5860a, 1, 1, 3, [B@27e2faeb)\n2015-12-20 18:51:46,613 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #6: [localhost/127.0.0.1:36031] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@adc7bb6, 1, 3)\n2015-12-20 18:51:46,613 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #6: [localhost/127.0.0.1:44247] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@79296f6b, 1, 3)\n2015-12-20 18:51:46,613 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #6: [localhost/127.0.0.1:36209] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@113def4c, 1, 3)\n2015-12-20 18:51:46,619 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_inprogress_0000000000000000001 -> /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_0000000000000000001-0000000000000000003\n2015-12-20 18:51:46,624 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_inprogress_0000000000000000001 -> /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_0000000000000000001-0000000000000000003\n2015-12-20 18:51:46,649 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_inprogress_0000000000000000001 -> /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_0000000000000000001-0000000000000000003\n2015-12-20 18:51:46,650 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #7: [localhost/127.0.0.1:44247] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@e3e7226, 4, -60)\n2015-12-20 18:51:46,650 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #7: [localhost/127.0.0.1:36031] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@25b82418, 4, -60)\n2015-12-20 18:51:46,650 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(434)) - Injecting code before IPC #7: [localhost/127.0.0.1:36209] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@5c49b700, 4, -60)\n2015-12-20 18:51:46,653 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #8: [localhost/127.0.0.1:36209] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@582bd643, 4, 4, 3, [B@25fff5ee)\n2015-12-20 18:51:46,653 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #8: [localhost/127.0.0.1:36031] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@476aefbf, 4, 4, 3, [B@25fff5ee)\n2015-12-20 18:51:46,653 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #8: [localhost/127.0.0.1:44247] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@3da2093a, 4, 4, 3, [B@25fff5ee)\n2015-12-20 18:51:46,653 WARN  security.UserGroupInformation (UserGroupInformation.java:doAs(1674)) - PriviledgedActionException as:jenkins (auth:SIMPLE) cause:org.apache.hadoop.hdfs.qjournal.protocol.JournalOutOfSyncException: Can't write, no segment open\n2015-12-20 18:51:46,654 INFO  ipc.Server (Server.java:run(2107)) - IPC Server handler 4 on 36209, call org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol.journal from 127.0.0.1:49480 Call#12141 Retry#0\norg.apache.hadoop.hdfs.qjournal.protocol.JournalOutOfSyncException: Can't write, no segment open\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.checkSync(Journal.java:485)\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.journal(Journal.java:354)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.journal(JournalNodeRpcServer.java:149)\n\tat org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.journal(QJournalProtocolServerSideTranslatorPB.java:158)\n\tat org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25421)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n2015-12-20 18:51:46,654 WARN  client.QuorumJournalManager (IPCLoggerChannel.java:call(388)) - Remote journal 127.0.0.1:36209 failed to write txns 4-6. Will try to write to this JN again after the next log roll.\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.qjournal.protocol.JournalOutOfSyncException): Can't write, no segment open\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.checkSync(Journal.java:485)\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.journal(Journal.java:354)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.journal(JournalNodeRpcServer.java:149)\n\tat org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.journal(QJournalProtocolServerSideTranslatorPB.java:158)\n\tat org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25421)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1466)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1403)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n\tat com.sun.proxy.$Proxy11.journal(Unknown Source)\n\tat org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolTranslatorPB.journal(QJournalProtocolTranslatorPB.java:167)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults$WrapEveryCall.answer(TestQJMWithFaults.java:473)\n\tat org.mockito.internal.MockHandler.handle(MockHandler.java:99)\n\tat org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)\n\tat org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol$$EnhancerByMockitoWithCGLIB$$ea839bd4.journal(<generated>)\n\tat org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:385)\n\tat org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:378)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-12-20 18:51:46,654 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #9: [localhost/127.0.0.1:36031] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@6dd12abe, 4, 6)\n2015-12-20 18:51:46,654 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #9: [localhost/127.0.0.1:44247] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@6dd12abe, 4, 6)\n2015-12-20 18:51:46,654 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #9: [localhost/127.0.0.1:36209] heartbeat(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@6ede3a7a)\n2015-12-20 18:51:46,663 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_inprogress_0000000000000000004 -> /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_0000000000000000004-0000000000000000006\n2015-12-20 18:51:46,667 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_inprogress_0000000000000000004 -> /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_0000000000000000004-0000000000000000006\n2015-12-20 18:51:46,770 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(435)) - Starting recovery process for unclosed journal segments...\n2015-12-20 18:51:46,772 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #1: [/127.0.0.1:36209] getJournalState(test-journal)\n2015-12-20 18:51:46,772 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #1: [/127.0.0.1:36031] getJournalState(test-journal)\n2015-12-20 18:51:46,773 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #1: [/127.0.0.1:44247] getJournalState(test-journal)\n2015-12-20 18:51:46,774 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #2: [localhost/127.0.0.1:36209] newEpoch(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp, 2)\n2015-12-20 18:51:46,774 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #2: [localhost/127.0.0.1:44247] newEpoch(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp, 2)\n2015-12-20 18:51:46,774 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #2: [localhost/127.0.0.1:36031] newEpoch(test-journal, lv=-60;cid=mycluster;nsid=12345;c=0;bpid=my-bp, 2)\n2015-12-20 18:51:46,775 INFO  server.Journal (Journal.java:updateLastPromisedEpoch(325)) - Updating lastPromisedEpoch from 1 to 2 for client /127.0.0.1\n2015-12-20 18:51:46,775 INFO  server.Journal (Journal.java:updateLastPromisedEpoch(325)) - Updating lastPromisedEpoch from 1 to 2 for client /127.0.0.1\n2015-12-20 18:51:46,775 INFO  server.Journal (Journal.java:updateLastPromisedEpoch(325)) - Updating lastPromisedEpoch from 1 to 2 for client /127.0.0.1\n2015-12-20 18:51:46,776 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(188)) - Scanning storage FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal)\n2015-12-20 18:51:46,776 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(188)) - Scanning storage FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal)\n2015-12-20 18:51:46,776 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(194)) - Latest log is EditLogFile(file=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_0000000000000000004-0000000000000000006,first=0000000000000000004,last=0000000000000000006,inProgress=false,hasCorruptHeader=false)\n2015-12-20 18:51:46,776 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(194)) - Latest log is EditLogFile(file=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_0000000000000000004-0000000000000000006,first=0000000000000000004,last=0000000000000000006,inProgress=false,hasCorruptHeader=false)\n2015-12-20 18:51:46,776 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(188)) - Scanning storage FileJournalManager(root=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal)\n2015-12-20 18:51:46,776 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(437)) - Successfully started new epoch 2\n2015-12-20 18:51:46,776 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(263)) - Beginning recovery of unclosed segment starting at txid 4\n2015-12-20 18:51:46,777 INFO  server.Journal (Journal.java:scanStorageForLatestEdits(194)) - Latest log is EditLogFile(file=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_0000000000000000001-0000000000000000003,first=0000000000000000001,last=0000000000000000003,inProgress=false,hasCorruptHeader=false)\n2015-12-20 18:51:46,777 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #3: [localhost/127.0.0.1:44247] prepareRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@606bf823, 4)\n2015-12-20 18:51:46,777 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #3: [localhost/127.0.0.1:36031] prepareRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@506ff349, 4)\n2015-12-20 18:51:46,777 INFO  server.Journal (Journal.java:getSegmentInfo(702)) - getSegmentInfo(4): EditLogFile(file=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_0000000000000000004-0000000000000000006,first=0000000000000000004,last=0000000000000000006,inProgress=false,hasCorruptHeader=false) -> startTxId: 4 endTxId: 6 isInProgress: false\n2015-12-20 18:51:46,777 INFO  server.Journal (Journal.java:prepareRecovery(746)) - Prepared recovery for segment 4: segmentState { startTxId: 4 endTxId: 6 isInProgress: false } lastWriterEpoch: 1 lastCommittedTxId: 6\n2015-12-20 18:51:46,777 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #3: [localhost/127.0.0.1:36209] prepareRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@101d3057, 4)\n2015-12-20 18:51:46,778 INFO  server.Journal (Journal.java:getSegmentInfo(702)) - getSegmentInfo(4): EditLogFile(file=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_0000000000000000004-0000000000000000006,first=0000000000000000004,last=0000000000000000006,inProgress=false,hasCorruptHeader=false) -> startTxId: 4 endTxId: 6 isInProgress: false\n2015-12-20 18:51:46,778 INFO  server.Journal (Journal.java:prepareRecovery(746)) - Prepared recovery for segment 4: segmentState { startTxId: 4 endTxId: 6 isInProgress: false } lastWriterEpoch: 1 lastCommittedTxId: 6\n2015-12-20 18:51:46,778 INFO  server.Journal (Journal.java:prepareRecovery(746)) - Prepared recovery for segment 4: lastWriterEpoch: 1 lastCommittedTxId: 6\n2015-12-20 18:51:46,778 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(272)) - Recovery prepare phase complete. Responses:\n127.0.0.1:36031: segmentState { startTxId: 4 endTxId: 6 isInProgress: false } lastWriterEpoch: 1 lastCommittedTxId: 6\n127.0.0.1:44247: segmentState { startTxId: 4 endTxId: 6 isInProgress: false } lastWriterEpoch: 1 lastCommittedTxId: 6\n2015-12-20 18:51:46,778 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(296)) - Using longest log: 127.0.0.1:36031=segmentState {\n  startTxId: 4\n  endTxId: 6\n  isInProgress: false\n}\nlastWriterEpoch: 1\nlastCommittedTxId: 6\n\n2015-12-20 18:51:46,779 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #4: [localhost/127.0.0.1:36209] acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@7f96717d, startTxId: 4\nendTxId: 6\nisInProgress: false\n, http://localhost:50567/getJournal?jid=test-journal&segmentTxId=4&storageInfo=-60%3A12345%3A0%3Amycluster)\n2015-12-20 18:51:46,779 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #4: [localhost/127.0.0.1:44247] acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@6df506fb, startTxId: 4\nendTxId: 6\nisInProgress: false\n, http://localhost:50567/getJournal?jid=test-journal&segmentTxId=4&storageInfo=-60%3A12345%3A0%3Amycluster)\n2015-12-20 18:51:46,779 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #4: [localhost/127.0.0.1:36031] acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@1b891974, startTxId: 4\nendTxId: 6\nisInProgress: false\n, http://localhost:50567/getJournal?jid=test-journal&segmentTxId=4&storageInfo=-60%3A12345%3A0%3Amycluster)\n2015-12-20 18:51:46,779 INFO  server.Journal (Journal.java:acceptRecovery(792)) - Synchronizing log startTxId: 4 endTxId: 6 isInProgress: false: no current segment in place\n2015-12-20 18:51:46,779 INFO  server.Journal (Journal.java:syncLog(888)) - Synchronizing log startTxId: 4 endTxId: 6 isInProgress: false from http://localhost:50567/getJournal?jid=test-journal&segmentTxId=4&storageInfo=-60%3A12345%3A0%3Amycluster\n2015-12-20 18:51:46,780 INFO  server.Journal (Journal.java:getSegmentInfo(702)) - getSegmentInfo(4): EditLogFile(file=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_0000000000000000004-0000000000000000006,first=0000000000000000004,last=0000000000000000006,inProgress=false,hasCorruptHeader=false) -> startTxId: 4 endTxId: 6 isInProgress: false\n2015-12-20 18:51:46,780 INFO  server.Journal (Journal.java:acceptRecovery(832)) - Skipping download of log startTxId: 4 endTxId: 6 isInProgress: false: already have up-to-date logs\n2015-12-20 18:51:46,780 INFO  server.Journal (Journal.java:getSegmentInfo(702)) - getSegmentInfo(4): EditLogFile(file=/data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_0000000000000000004-0000000000000000006,first=0000000000000000004,last=0000000000000000006,inProgress=false,hasCorruptHeader=false) -> startTxId: 4 endTxId: 6 isInProgress: false\n2015-12-20 18:51:46,780 INFO  server.Journal (Journal.java:acceptRecovery(832)) - Skipping download of log startTxId: 4 endTxId: 6 isInProgress: false: already have up-to-date logs\n2015-12-20 18:51:46,782 INFO  server.Journal (Journal.java:acceptRecovery(865)) - Accepted recovery for segment 4: segmentState { startTxId: 4 endTxId: 6 isInProgress: false } acceptedInEpoch: 2\n2015-12-20 18:51:46,782 INFO  server.Journal (Journal.java:acceptRecovery(865)) - Accepted recovery for segment 4: segmentState { startTxId: 4 endTxId: 6 isInProgress: false } acceptedInEpoch: 2\n2015-12-20 18:51:46,783 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #5: [localhost/127.0.0.1:36031] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@5b7127, 4, 6)\n2015-12-20 18:51:46,783 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #5: [localhost/127.0.0.1:44247] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@7361b79f, 4, 6)\n2015-12-20 18:51:46,784 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #5: [localhost/127.0.0.1:36209] getEditLogManifest(test-journal, 0, false)\n2015-12-20 18:51:46,785 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #6: [localhost/127.0.0.1:36031] getEditLogManifest(test-journal, 0, false)\n2015-12-20 18:51:46,785 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #6: [localhost/127.0.0.1:44247] getEditLogManifest(test-journal, 0, false)\n2015-12-20 18:51:46,786 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #7: [localhost/127.0.0.1:36031] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@62fd6abb, 7, -60)\n2015-12-20 18:51:46,786 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #7: [localhost/127.0.0.1:44247] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@6b6a4dc2, 7, -60)\n2015-12-20 18:51:46,787 INFO  server.Journal (Journal.java:startLogSegment(547)) - Updating lastWriterEpoch from 1 to 2 for client /127.0.0.1\n2015-12-20 18:51:46,787 INFO  server.Journal (Journal.java:startLogSegment(547)) - Updating lastWriterEpoch from 1 to 2 for client /127.0.0.1\n2015-12-20 18:51:46,787 INFO  namenode.TransferFsImage (TransferFsImage.java:receiveFile(546)) - Transfer took 0.01s at 0.00 KB/s\n2015-12-20 18:51:46,790 INFO  server.Journal (Journal.java:acceptRecovery(865)) - Accepted recovery for segment 4: segmentState { startTxId: 4 endTxId: 6 isInProgress: false } acceptedInEpoch: 2\n2015-12-20 18:51:46,790 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #6: [localhost/127.0.0.1:36209] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@3a83f397, 4, 6)\n2015-12-20 18:51:46,791 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #8: [localhost/127.0.0.1:36031] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@1354f527, 7, 7, 3, [B@2eaadd5b)\n2015-12-20 18:51:46,791 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #8: [localhost/127.0.0.1:44247] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@53aa994c, 7, 7, 3, [B@2eaadd5b)\n2015-12-20 18:51:46,791 INFO  server.Journal (Journal.java:finalizeLogSegment(599)) - Validating log segment /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_inprogress_0000000000000000004 about to be finalized\n2015-12-20 18:51:46,791 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_inprogress_0000000000000000004 -> /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_0000000000000000004-0000000000000000006\n2015-12-20 18:51:46,792 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #7: [localhost/127.0.0.1:36209] startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@5f270347, 7, -60)\n2015-12-20 18:51:46,792 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #9: [localhost/127.0.0.1:44247] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@7a485b1e, 7, 9)\n2015-12-20 18:51:46,792 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #9: [localhost/127.0.0.1:36031] finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@3652158, 7, 9)\n2015-12-20 18:51:46,792 INFO  server.Journal (Journal.java:startLogSegment(547)) - Updating lastWriterEpoch from 1 to 2 for client /127.0.0.1\n2015-12-20 18:51:46,809 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_inprogress_0000000000000000007 -> /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal/current/edits_0000000000000000007-0000000000000000009\n2015-12-20 18:51:46,819 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_inprogress_0000000000000000007 -> /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal/current/edits_0000000000000000007-0000000000000000009\n2015-12-20 18:51:46,820 INFO  ipc.Server (Server.java:stop(2485)) - Stopping server on 36209\n2015-12-20 18:51:46,820 WARN  namenode.FileJournalManager (FileJournalManager.java:startLogSegment(127)) - Unable to start log segment 7 at /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_inprogress_0000000000000000007: null\n2015-12-20 18:51:46,821 FATAL server.JournalNode (JournalNode.java:reportErrorOnFile(299)) - Error reported on file /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal/current/edits_inprogress_0000000000000000007... exiting\njava.lang.Exception\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNode$ErrorReporter.reportErrorOnFile(JournalNode.java:299)\n\tat org.apache.hadoop.hdfs.server.namenode.FileJournalManager.startLogSegment(FileJournalManager.java:130)\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.startLogSegment(Journal.java:559)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.startLogSegment(JournalNodeRpcServer.java:162)\n\tat org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.startLogSegment(QJournalProtocolServerSideTranslatorPB.java:198)\n\tat org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25425)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n2015-12-20 18:51:46,821 INFO  ipc.Server (Server.java:run(844)) - Stopping IPC Server Responder\n2015-12-20 18:51:46,821 INFO  ipc.Server (Server.java:run(718)) - Stopping IPC Server listener on 36209\n2015-12-20 18:51:46,821 INFO  ipc.Server (Server.java:stop(2485)) - Stopping server on 36209\n2015-12-20 18:51:46,822 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #8: [localhost/127.0.0.1:36209] journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@2b991bc5, 7, 7, 3, [B@2eaadd5b)\n2015-12-20 18:51:46,822 WARN  client.QuorumJournalManager (IPCLoggerChannel.java:call(388)) - Remote journal 127.0.0.1:36209 failed to write txns 7-9. Will try to write to this JN again after the next log roll.\njava.net.ConnectException: Call From ec2-beefy-slave-0594.vpc.cloudera.com/172.26.1.242 to localhost:36209 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.GeneratedConstructorAccessor31.newInstance(Unknown Source)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1470)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1403)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n\tat com.sun.proxy.$Proxy11.journal(Unknown Source)\n\tat org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolTranslatorPB.journal(QJournalProtocolTranslatorPB.java:167)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults$WrapEveryCall.answer(TestQJMWithFaults.java:473)\n\tat org.mockito.internal.MockHandler.handle(MockHandler.java:99)\n\tat org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)\n\tat org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol$$EnhancerByMockitoWithCGLIB$$ea839bd4.journal(<generated>)\n\tat org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:385)\n\tat org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:378)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:708)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1442)\n\t... 17 more\n2015-12-20 18:51:46,824 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0\n2015-12-20 18:51:46,824 INFO  client.TestQJMWithFaults (TestQJMWithFaults.java:beforeCall(438)) - IPC call #9: [localhost/127.0.0.1:36209] heartbeat(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo@3e467d8f)\n2015-12-20 18:51:46,825 INFO  common.Storage (JNStorage.java:close(248)) - Closing journal storage for Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal\n2015-12-20 18:51:46,825 INFO  common.Storage (JNStorage.java:close(248)) - Closing journal storage for Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/test-journal\n2015-12-20 18:51:46,825 INFO  common.Storage (JNStorage.java:close(248)) - Closing journal storage for Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-0/waitactive\n2015-12-20 18:51:46,825 WARN  qjournal.MiniJournalCluster (MiniJournalCluster.java:shutdown(157)) - Unable to stop journal node org.apache.hadoop.hdfs.qjournal.server.JournalNode@fcb345b\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.unlock(Storage.java:747)\n\tat org.apache.hadoop.hdfs.server.common.Storage.unlockAll(Storage.java:1125)\n\tat org.apache.hadoop.hdfs.qjournal.server.JNStorage.close(JNStorage.java:249)\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.close(Journal.java:227)\n\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNode.stop(JournalNode.java:207)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNode.stopAndJoin(JournalNode.java:232)\n\tat org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.shutdown(MiniJournalCluster.java:154)\n\tat org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults.__CLR4_0_3uct6ut6408(TestQJMWithFaults.java:181)\n\tat org.apache.hadoop.hdfs.qjournal.client.TestQJMWithFaults.testRecoverAfterDoubleFailures(TestQJMWithFaults.java:138)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\n2015-12-20 18:51:46,825 INFO  ipc.Server (Server.java:stop(2485)) - Stopping server on 36031\n2015-12-20 18:51:46,825 WARN  security.UserGroupInformation (UserGroupInformation.java:doAs(1674)) - PriviledgedActionException as:jenkins (auth:SIMPLE) cause:java.nio.channels.ClosedByInterruptException\n2015-12-20 18:51:46,826 INFO  ipc.Server (Server.java:run(2107)) - IPC Server handler 3 on 36209, call org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol.startLogSegment from 127.0.0.1:49483 Call#12169 Retry#0\njava.nio.channels.ClosedByInterruptException\n\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n\tat sun.nio.ch.FileChannelImpl.writeInternal(FileChannelImpl.java:743)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:723)\n\tat org.apache.hadoop.io.IOUtils.writeFully(IOUtils.java:317)\n\tat org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream.preallocate(EditLogFileOutputStream.java:231)\n\tat org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream.flushAndSync(EditLogFileOutputStream.java:203)\n\tat org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:113)\n\tat org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:107)\n\tat org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream.create(EditLogFileOutputStream.java:122)\n\tat org.apache.hadoop.hdfs.server.namenode.FileJournalManager.startLogSegment(FileJournalManager.java:124)\n\tat org.apache.hadoop.hdfs.qjournal.server.Journal.startLogSegment(Journal.java:559)\n\tat org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.startLogSegment(JournalNodeRpcServer.java:162)\n\tat org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.startLogSegment(QJournalProtocolServerSideTranslatorPB.java:198)\n\tat org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25425)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n2015-12-20 18:51:46,826 WARN  ipc.Server (Server.java:processResponse(1039)) - IPC Server handler 3 on 36209, call org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol.startLogSegment from 127.0.0.1:49483 Call#12169 Retry#0: output error\n2015-12-20 18:51:46,826 INFO  ipc.Server (Server.java:run(2152)) - IPC Server handler 3 on 36209 caught an exception\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:265)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:474)\n\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:2621)\n\tat org.apache.hadoop.ipc.Server.access$1900(Server.java:134)\n\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:989)\n\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1054)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2141)\n2015-12-20 18:51:46,826 INFO  ipc.Server (Server.java:run(718)) - Stopping IPC Server listener on 36031\n2015-12-20 18:51:46,827 INFO  ipc.Server (Server.java:run(844)) - Stopping IPC Server Responder\n2015-12-20 18:51:46,827 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0\n2015-12-20 18:51:46,928 INFO  common.Storage (JNStorage.java:close(248)) - Closing journal storage for Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/test-journal\n2015-12-20 18:51:46,928 INFO  common.Storage (JNStorage.java:close(248)) - Closing journal storage for Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-1/waitactive\n2015-12-20 18:51:46,928 INFO  ipc.Server (Server.java:stop(2485)) - Stopping server on 44247\n2015-12-20 18:51:46,929 INFO  ipc.Server (Server.java:run(718)) - Stopping IPC Server listener on 44247\n2015-12-20 18:51:46,929 INFO  ipc.Server (Server.java:run(844)) - Stopping IPC Server Responder\n2015-12-20 18:51:46,930 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:0\n2015-12-20 18:51:46,931 INFO  common.Storage (JNStorage.java:close(248)) - Closing journal storage for Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/test-journal\n2015-12-20 18:51:46,931 INFO  common.Storage (JNStorage.java:close(248)) - Closing journal storage for Storage Directory /data/jenkins/workspace/CDH5-Hadoop-HDFS-2.6.0-Clover/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/journalnode-2/waitactive\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-12-23T18:50:26.038+0000","updated":"2015-12-23T18:50:26.038+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12923592/comment/16082756","id":"16082756","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"body":"Haven't seen this since, closing...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xiaochen","name":"xiaochen","key":"xiaochen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=xiaochen&avatarId=24893","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xiaochen&avatarId=24893","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xiaochen&avatarId=24893","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xiaochen&avatarId=24893"},"displayName":"Xiao Chen","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-11T18:54:26.953+0000","updated":"2017-07-11T18:54:26.953+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-9590/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2q6rb:"}}