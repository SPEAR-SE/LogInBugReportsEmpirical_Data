{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12397103","self":"https://issues.apache.org/jira/rest/api/2/issue/12397103","key":"HDFS-162","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/5","id":"5","description":"All attempts at reproducing this issue failed, or not enough information was available to reproduce the issue. Reading the code produces no clues as to why this behavior would occur. If more information appears later, please reopen the issue.","name":"Cannot Reproduce"},"customfield_12312322":null,"customfield_12310220":"2008-07-22T19:16:07.629+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Dec 28 21:15:46 UTC 2011","customfield_12310420":"16697","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_113014604136_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2011-12-28T21:15:46.009+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-162/watchers","watchCount":12,"isWatching":false},"created":"2008-05-29T20:19:02.131+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2011-12-28T21:15:46.254+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"9/27 update: uploaded the logs, with hopefully all the bits that should be examined. If other things are needed, just let me know. Note that all the paths refer to 0.18.1. This is still an 18.0 installation using the 18.0 core jar, just installed to a non-standard location.\n\n9/26 update: we have successfully reproduced this using Hadoop 0.18 as well. The problem happens on both our own network infrastructure as well as on an Amazon EC2 cluster running CentOS5 images. I'll be attaching the logs Raghu asked for shortly.\n\nA job that used to run correctly on our grid (in 0.15.0) now fails. The failure occurs after the map phase is complete, and about 2/3rds of the way through the reduce phase.   This job is processing a modest amount of input data (approximately 220G)\n\nWhen the error occurs the nodes hosting DataNodes have literally thousands of open socket connections on them.  The DataNode instances are holding large amounts of memory.  Sometimes the DataNodes crash or exit, other times they continue to run.\n\nThe error which gets kicked out from the application perspective is:\n\n08/05/27 11:30:08 INFO mapred.JobClient: map 100% reduce 89%\n08/05/27 11:30:41 INFO mapred.JobClient: map 100% reduce 90%\n08/05/27 11:32:45 INFO mapred.JobClient: map 100% reduce 86%\n08/05/27 11:32:45 INFO mapred.JobClient: Task Id :\n task_200805271056_0001_r_000007_0, Status : FAILED\njava.io.IOException: Could not get block locations. Aborting...\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanode\n Error(DFSClient.java:1832)\nat\n org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1100(DFSClient.java:1487)\nat\n org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1579)\n\nI then discovered that 1 or more DataNode instances on the slave nodes\n are down (we run 1 DataNode instance per machine). The cause for at\n least some of the DataNode failures is a JVM internal error that gets\n raised due to a complete out-of-memory scenario (on a 4G, 4-way machine). \n\nWatching the DataNodes run, I can see them consuming more and more\n memory. For those failures for which there is a JVM traceback, I see (in\n part...NOTE 0.16.4 TRACEBACK):\n#\n# java.lang.OutOfMemoryError: requested 16 bytes for CHeapObj-new. Out\n of swap space?\n#\n# Internal Error (414C4C4F434154494F4E0E494E4C494E450E4850500017),\n pid=4246, tid=2283883408\n#\n# Java VM: Java HotSpot(TM) Server VM (1.6.0_02-b05 mixed mode)\n# If you would like to submit a bug report, please visit:\n# http://java.sun.com/webapps/bugreport/crash.jsp\n#\n--------------- T H R E A D ---------------\nCurrent thread (0x8a942000): JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@3f4f44\" daemon [_thread_in_Java, id=15064]\nStack: [0x881c4000,0x88215000), sp=0x882139e0, free space=318k\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code,\n C=native code)\nV [libjvm.so+0x53b707]\nV [libjvm.so+0x225fe1]\nV [libjvm.so+0x16fdc5]\nV [libjvm.so+0x22aef3]\nJava frames: (J=compiled Java code, j=interpreted, Vv=VM code)\nv blob 0xf4f235a7\nJ java.io.DataInputStream.readInt()I\nj\n org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(Ljava/io/DataOutputStream;Ljava/io/DataInputStream;Ljava/io/DataOutputStream;Ljava/lang/String;Lorg/a\npache/hadoop/dfs/DataNode$Throttler;I)V+126\nj\n org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(Ljava/io/DataInputStream;)V+746\nj org.apache.hadoop.dfs.DataNode$DataXceiver.run()V+174\nj java.lang.Thread.run()V+11\nv ~StubRoutines::call_stub\n--------------- P R O C E S S ---------------\nJava Threads: ( => current thread )\n0x0ae3f400 JavaThread \"process reaper\" daemon [_thread_blocked,\n id=26870]\n0x852e6000 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@e5dce1\" daemon [_thread_in_vm, id=26869]\n0x08a1cc00 JavaThread \"PacketResponder 0 for Block\n blk_-6186975972786687394\" daemon [_thread_blocked, id=26769]\n0x852e5000 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@c40bf8\" daemon [_thread_in_native, id=26768]\n0x0956e000 JavaThread \"PacketResponder 0 for Block\n blk_-2322514873363546651\" daemon [_thread_blocked, id=26767]\n0x852e4400 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@1ca61f9\" daemon [_thread_in_native, id=26766]\n0x09d3a400 JavaThread \"PacketResponder 0 for Block\n blk_8926941945313450801\" daemon [_thread_blocked, id=26764]\n0x852e3c00 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@1e186d9\" daemon [_thread_in_native, id=26763]\n0x0953d000 JavaThread \"PacketResponder 0 for Block\n blk_4785883052769066976\" daemon [_thread_blocked, id=26762]\n0xb13a5c00 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@13d62aa\" daemon [_thread_in_native, id=26761]\n\nThe interesting part here is that if I count the number of JavaThreads\n running org.apache.hadoop.dfs.DataNode I see 4,538 (!) in the\n traceback. The number of threads was surprising.\n\nOther DataNodes just exit without panicking the JVM. In either failure\n mode, the last few lines of the DataNode log file is apparently\n innocuous:\n\n2008-05-27 11:31:47,663 INFO org.apache.hadoop.dfs.DataNode: Datanode 2\n got response for connect ack from downstream datanode with\n firstbadlink as\n2008-05-27 11:31:47,663 INFO org.apache.hadoop.dfs.DataNode: Datanode 2\n forwarding connect ack to upstream firstbadlink is\n2008-05-27 11:31:48,268 INFO org.apache.hadoop.dfs.DataNode: Receiving\n block blk_-2241766430103062484 src: /10.2.14.10:33626 dest:\n /10.2.14.10:50010\n2008-05-27 11:31:48,740 INFO org.apache.hadoop.dfs.DataNode: Receiving\n block blk_313239508245918539 src: /10.2.14.24:37836 dest:\n /10.2.14.24:50010\n2008-05-27 11:31:48,740 INFO org.apache.hadoop.dfs.DataNode: Datanode 0\n forwarding connect ack to upstream firstbadlink is\n2008-05-27 11:31:49,044 INFO org.apache.hadoop.dfs.DataNode: Receiving\n block blk_1684581399908730353 src: /10.2.14.16:51605 dest:\n /10.2.14.16:50010\n2008-05-27 11:31:49,044 INFO org.apache.hadoop.dfs.DataNode: Datanode 0\n forwarding connect ack to upstream firstbadlink is\n2008-05-27 11:31:49,509 INFO org.apache.hadoop.dfs.DataNode: Receiving\n block blk_2493969670086107736 src: /10.2.14.18:47557 dest:\n /10.2.14.18:50010\n2008-05-27 11:31:49,513 INFO org.apache.hadoop.dfs.DataNode: Datanode 1\n got response for connect ack from downstream datanode with\n firstbadlink as\n2008-05-27 11:31:49,513 INFO org.apache.hadoop.dfs.DataNode: Datanode 1\n forwarding connect ack to upstream firstbadlink is\n\nFinally, the task-level output (in userlogs) doesn't reveal much\n either:\n\n2008-05-27 11:38:30,724 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Need 34 map output(s)\n2008-05-27 11:38:30,753 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 done copying\n task_200805271056_0001_m_001976_0 output from worker9.\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1: Got 0 new map-outputs & 0 obsolete\n map-outputs from tasktracker and 0 map-outputs from previous failures\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Got 33 known map output location(s);\n scheduling...\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Scheduled 1 of 33 known outputs (0 slow\n hosts and 32 dup hosts)\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Copying task_200805271056_0001_m_001248_0\n output from worker8.\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Need 33 map output(s)\n2008-05-27 11:38:31,752 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 done copying\n task_200805271056_0001_m_001248_0 output from worker8.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12391078","id":"12391078","filename":"logs.tar.gz","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-27T06:38:22.076+0000","size":2293465,"mimeType":"application/x-gzip","content":"https://issues.apache.org/jira/secure/attachment/12391078/logs.tar.gz"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12391241","id":"12391241","filename":"logsforHadoopTeam.tar.gz","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-30T23:06:30.863+0000","size":275577,"mimeType":"application/x-gzip","content":"https://issues.apache.org/jira/secure/attachment/12391241/logsforHadoopTeam.tar.gz"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"107897","customfield_12312823":null,"summary":"DataNode fails to deliver blocks, holds thousands of open socket connections","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ccgillett","name":"ccgillett","key":"ccgillett","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christopher Gillett","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ccgillett","name":"ccgillett","key":"ccgillett","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christopher Gillett","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Hadoop-0.18.0, 7 node Linux grid (6 DataNodes, 1 master node)\nHadoop-0.18.0, 20 EC2 Linux grid (19 DataNodes, 1 master node)","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12615746","id":"12615746","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Main difference between 0.15 and 0.17 is that 0.17 needs two threads for each block being written and 0.15 needs one. Do you expect thousands of writes to any datanode?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-07-22T19:16:07.629+0000","updated":"2008-07-22T19:16:07.629+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12616069","id":"12616069","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ccgillett","name":"ccgillett","key":"ccgillett","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christopher Gillett","active":true,"timeZone":"Etc/UTC"},"body":"That depends on your definition of \"writes\"...it's essentially a log processing appliation, so it is reading in thousands of lines of log data, and writing back a canonical representation.  So we are writing out several hundred thousand lines of text, distributed across all the nodes in the system (i.e. 24 DataNodes in this case).  \n\nOur nagios monitoring shows thousands of open socket connections at the time that things fall apart, almost like connections are being closed properly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ccgillett","name":"ccgillett","key":"ccgillett","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christopher Gillett","active":true,"timeZone":"Etc/UTC"},"created":"2008-07-23T14:43:56.381+0000","updated":"2008-07-23T14:43:56.381+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12616222","id":"12616222","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"There is no file descriptor or socket leak we are aware of in 0.17.0. Any more detailed information you can provide will help us to see if there is a bug or to check whats happening in your case. Couple of things you could do :\n\n# Run jstack on a datanode when you suspect it is in bad state.\n# Attach datanode log from a node that exhibited this problem.\n# Attach corresponding NameNode log.\n# etc.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-07-23T18:59:44.251+0000","updated":"2008-07-23T18:59:44.251+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12635106","id":"12635106","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"This file is a set of logs that records the 'incident'\n\n2763033 2008-09-27 02:28 hadoop-vmc-datanode-overdrive1-node-worker284.log\n1833871 2008-09-27 02:28 hadoop-vmc-jobtracker-overdrive1-node-worker283.log\n18003026 2008-09-27 02:28 hadoop-vmc-namenode-overdrive1-node-worker283.log\n1548787 2008-09-27 02:28 hadoop-vmc-tasktracker-overdrive1-node-worker284.log\n4187144 2008-09-27 02:28 jstack-logs-overdrive1-node-worker283\n7706321 2008-09-27 02:28 jstack-logs-overdrive1-node-worker284\n87643 2008-09-27 02:28 nohup.out\n482223 2008-09-27 02:28 sockets-logs-overdrive1-node-worker283\n1233900 2008-09-27 02:28 sockets-logs-overdrive1-node-worker284\n\nnohup.out shows that the blowup happens around 1:11 am. Every other log shows some problem around this time.\n\nThe jstack-logs are sampled every 1 minute of the DataNode process for slaves, and for NameNode on the master. The sockets-logs are sampled every 30s for just java sockets.\n\nI have only included 1 slave as a sample. I can throw up the other 5 if needed. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-27T06:38:22.114+0000","updated":"2008-09-27T06:38:22.114+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12635799","id":"12635799","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"has anyone had a chance to take a look at the logs? Is there any other info I could collect that would make analysis easier? I'm going to plunge in and instrument the areas that seem to be affected by this bug in hopes of figuring the problem out. Any hints or advice on what to proceed with?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-30T18:06:39.991+0000","updated":"2008-09-30T18:06:39.991+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12635864","id":"12635864","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"The logs you posted are too much information. Could you please post the following information:\n1. the the log of the task that failed; \n2. The log and the jstack of the problematic datanode around the task failure time.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-30T21:25:00.674+0000","updated":"2008-09-30T21:25:00.674+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12635894","id":"12635894","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"Files in this tar are:\n\nnamenode/:\n-rw-r--r-- 1 mmao visible 4187144 2008-09-27 01:50 jstack-logs-overdrive1-node-worker83\n-rw------- 1 mmao visible    7551 2008-09-30 18:32 nohup.out\n-rw-r--r-- 1 mmao visible    9191 2008-09-30 18:31 overdrive1-node-worker83_1222489161710_job_200809270019_0001_vmc_CommonLogCreate\n-rw-r--r-- 1 mmao visible   34979 2008-09-30 18:30 sockets-logs-overdrive1-node-worker83\n\noverdrive1-node-worker84/:\n-rw-r--r-- 1 mmao visible  29627 2008-09-30 18:27 hadoop-vmc-datanode-overdrive1-node-worker84.log\n-rw-r--r-- 1 mmao visible   4719 2008-09-30 18:19 hadoop-vmc-tasktracker-overdrive1-node-worker84.log\n-rw-r--r-- 1 mmao visible 425741 2008-09-30 18:16 jstack-logs-overdrive1-node-worker84\n-rw-r--r-- 1 mmao visible  78726 2008-09-30 18:17 sockets-logs-overdrive1-node-worker84\n\noverdrive1-node-worker87/:\n-rw-r--r-- 1 mmao visible 19227 2008-09-30 18:24 hadoop-vmc-datanode-overdrive1-node-worker87.log\n-rw-r--r-- 1 mmao visible  7287 2008-09-30 18:11 hadoop-vmc-tasktracker-overdrive1-node-worker87.log\n-rw-r--r-- 1 mmao visible 95278 2008-09-30 18:22 sockets-logs-overdrive1-node-worker87\n\nevery log is edited to be centered right around the start of the phenomenon. 2 slave nodes' logs have been provided, with logs for failed tasks. Note that all six of the slaves experience the same proliferation of sockets and block access failures around the same time; I'm merely leaving those logs out for easy of browsing.\nJstack and socket count logs are provided for both the namenode and the 2 slaves.\n\nFor 84, the failed tasks logged are: _m_001449_1 and and _r_000001_0\nFor 87, the failed task logged is: _m_001444_1\n\nLet me know if you need more.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-09-30T23:06:30.910+0000","updated":"2008-09-30T23:06:30.910+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12636129","id":"12636129","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Good to see you persist through the analysis. It encourages us too.\nLets trace one block \"blk_-4738287144374769594\" :\n\nFrom datanode-84 : \nbq. writeBlock blk_-4738287144374769594_26925 received exception java.io.IOException: Could not read from stream [...]\n\nFrom client log :\n{quote} 2008-09-27 01:07:06,810 INFO org.apache.hadoop.dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Bad connect ack with firstBadLink 10.2.14.89:50010\n2008-09-27 01:07:06,810 INFO org.apache.hadoop.dfs.DFSClient: Abandoning block blk_-4738287144374769594_26925 [...] {quote}\n\nWhat happened here is that, dn-84 could not read response from dn-89. So we need the corresponding log on dn-89 to see why it failed. \n\nAfter we figure out why dn-89 failed, we should think about the issue if the writing should have continued since at least one datanode is ok (dn-84 in this case). Before 0.17 (and may be 0.16) client would just continue to write to dn-84.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-01T19:14:42.773+0000","updated":"2008-10-01T19:14:42.773+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12636137","id":"12636137","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for your speedy reply, Raghu.\n\nVery curiously, I can find no mention of that block from logs for datanode 89. Here're the only occurences of it in the logs:\n\nhadoop-vmc-namenode-overdrive1-node-worker83.log:\n2008-09-27 01:07:06,806 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.allocateBlock: /MENG/import/common_logs/_temporary/_attempt_200809270019_0001_r_000001_0/20071219/12/part-00001. blk_-4738287144374769594_26925\n\nhadoop-vmc-namenode-overdrive1-node-worker83.log:\n2008-09-27 01:07:06,806 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.allocateBlock: /MENG/import/common_logs/_temporary/_attempt_200809270019_0001_r_000001_0/20071219/12/part-00001. blk_-4738287144374769594_26925\n\noverdrive1-node-worker84/userlogs/attempt_200809270019_0001_r_000001_0/syslog:\n2008-09-27 01:07:06,810 INFO org.apache.hadoop.dfs.DFSClient: Abandoning block blk_-4738287144374769594_26925\n\nhadoop-vmc-datanode-overdrive1-node-worker84.log:\n2008-09-27 01:07:06,806 INFO org.apache.hadoop.dfs.DataNode: Receiving block blk_-4738287144374769594_26925 src: /10.2.14.84:59159 dest: /10.2.14.84:50010\n2008-09-27 01:07:06,809 INFO org.apache.hadoop.dfs.DataNode: writeBlock blk_-4738287144374769594_26925 received exception java.io.IOException: Could not read from stream\n\n\nI did double check that the IP ending in .89 does in fact map to datanode 89. Does this mean 89 completely missed/ignored the request to allocate that block?\n\nHere's what happened on 89 shortly before:\n2008-09-27 01:06:43,389 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.2.14.89:50010, storageID=DS-1223952255-10.2.14.89-50010-1222469402047, infoPort=50075, ipcPort=50020):DataXceiver: java.io.IOException: xceiverCount 258 exceeds the limit of concurrent xcievers 256\n\tat org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1030)\n\tat java.lang.Thread.run(Thread.java:619)\n\n2008-09-27 01:06:43,420 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.2.14.89:50010, storageID=DS-1223952255-10.2.14.89-50010-1222469402047, infoPort=50075, ipcPort=50020):DataXceiver: java.io.IOException: xceiverCount 258 exceeds the limit of concurrent xcievers 256\n\tat org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1030)\n\tat java.lang.Thread.run(Thread.java:619)\n(repeated for about 80 times)\n\nThese \"258 exceeds the limit of concurrent xcievers 256\" errors suddenly appear across the datanodes at around 01:07, with the very first appearing on datanode 89 at the time shown above. Counts of the errors across the datanodes:\n84: 80\n85: 0\n86: 554\n87: 84\n88: 50\n89: 83\n\nThe 3 waves of occurrences of these errors seem to coincide with the 3 spikes in socket counts we've observed before the job totally craps out and fails.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-01T19:42:07.781+0000","updated":"2008-10-01T19:56:10.761+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12636150","id":"12636150","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"> These \"258 exceeds the limit of concurrent xcievers 256\" errors suddenly appear across the datanodes at around 01:07, with the very first appearing on datanode 89 at the time shown above. Counts of the errors across the datanodes:\n\nThis is the main problem. I really don't like this limit, its too too low for most users. Please set \"dfs.datanode.max.xcievers\" to something like 2k and run. See HADOOP-3859 and HADOOP-3633 for more info if you need more background.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-01T20:13:53.079+0000","updated":"2008-10-01T20:13:53.079+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12636177","id":"12636177","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"From the description:\n{quote}\nThe interesting part here is that if I count the number of JavaThreads\nrunning org.apache.hadoop.dfs.DataNode I see 4,538 in the\ntraceback. The number of threads was surprising.\n{quote}\n\nSo even after you increase the threads to 2k, you might still hit the issue. You need to find out why so many threads are present. One possibility is that you could have many readers or writers, by mistake.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-01T21:15:50.579+0000","updated":"2008-10-01T21:15:50.579+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12636182","id":"12636182","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"whoops I see the new comments now.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-01T21:25:06.055+0000","updated":"2008-10-01T21:27:13.766+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12636183","id":"12636183","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ccgillett","name":"ccgillett","key":"ccgillett","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christopher Gillett","active":true,"timeZone":"Etc/UTC"},"body":"Raghu:\n\nIntersting about the number of threads...how would lead to a \"too many readers or writers\" condition?\n\nI'm asking only because the code that fails is the same in both 0.15 and 0.18, and while it works in 0.15 it fails in 0.18. \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ccgillett","name":"ccgillett","key":"ccgillett","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christopher Gillett","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-01T21:25:51.573+0000","updated":"2008-10-01T21:25:51.573+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12636186","id":"12636186","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"\nOn 0.15, the situation might not be bad enough. Mostly likely you have many (thousands of) simultaneous writers. 0.18 takes takes 2 threads for each write and 0.15 takes one at the datanode (yes, it should be improved). Another important difference is that client on 0.15 collects one block of data (64MB) on local disk and then writes the block as fast as possible to datanodes. So if your code is writing data slowly, it does not affect the threads on datanodes (ie it is not proportional to number of files open for writing).. but since 0.16, #threads is proportional to #files_bing_written. All these issues should be fixed by  HADOOP-3856.. please voice your support there.\n\nYou could try doing some back-of-the envolop calculations about how many files are being written at a given time: say you have x files being written and you have 'd' datanodes, then each datanode will have {{x*6/d}} threads at each datanode involved in writing (for default replication of 3).\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-01T21:40:22.437+0000","updated":"2008-10-01T21:40:22.437+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12636196","id":"12636196","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"Just for clarity, when you say each write, you mean any time a map or reduce task emits a chunk to be written somewhere, and this write request is handled by a datanode, right?\n\nSo currently, from inspection, we have N = 4500, a few thousand DataNode related threads per node. Let's just say that 3000 of these are outbound write request threads, and 1500 listening threads for the datanode to write with.\n\nWhich number (if any) is dfs.datanode.max.xcievers imposing a cap on?: 4500, 3000, or 1500?\nFrom my best parse of the discussion on 3633, it's a cap on 4500?\n\nWhat is the bottleneck that is first met when raising the dfs.datanode.max.xcievers limit? JVM memory?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-01T21:59:34.967+0000","updated":"2008-10-01T21:59:34.967+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637379","id":"12637379","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"I was able to increase the size of my input data (while maintaining a very large dfs.datanode.max.xcievers) until a new kind of error cropped up. While the number of datanode threads does not exceed the limit, a similar DOS-style crash happens to the grid. This time, some of the nodes' datanode processes die and take all the child threads with them. Is it safe to assume that the JVM is running out of memory trying to open up and maintain so many threads? Do you guys know what is \"beyond the wall,\" as far as how Hadoop will fail if dfs.datanode.max.xcievers is jacked up arbitrarily high?\n\nWe had really hoped that, hack as it might be, that raising dfs.datanode.max.xcievers would be sufficient to cover the upper bound of the data we'd expect to see in a scaled out deployment of our grid, but the data set we used is not really pushing we expect to get. \n\nRegardless of whether our implementation is too naive and should be revised, does our particular problem case lend more weight to the need to do a rewrite as discussed in issue 3856? Or is the general stance to suck it up and work around the limit?\n\nWe greatly appreciate any input you guys have, as this is seriously holding up our project.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-07T05:25:09.815+0000","updated":"2008-10-07T05:25:09.815+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637560","id":"12637560","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"body":"> Is it safe to assume that the JVM is running out of memory trying to open up and maintain so many threads?\n\nYou could be running out of file handles?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cutting","name":"cutting","key":"cutting","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Cutting","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-07T17:37:53.706+0000","updated":"2008-10-07T17:37:53.706+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637584","id":"12637584","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Do you have approximate values for 'x' and 'd' mentioned in my comment above? \n\nWhen you have 3500+ threads you could be running out of different kinds of resources : memory, kernel memory, or ability create any new threads etc.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-07T18:22:35.426+0000","updated":"2008-10-07T18:22:35.426+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637607","id":"12637607","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"> Do you have approximate values for 'x' and 'd' mentioned in my comment above? \nCuriously, d (number of datanodes) doesn't seem to matter.\nI've tested 3 sets of test input data on 2 grids, sized 6 and 64 datanodes.\nFor each input set, _the same_ peak number of DataNode threads are spawned per machine, seemingly independent of the number of datanodes participating.\n\nx (part files appearing in HDFS) is about 4000+ for the largest successful dataset that completes successfully, and is a few thousand more for the larger, unsuccessful run. Is it unreasonable to have this magnitude of files in flight in the course of a map reduce operation?\n\nI should note that the correlation between open files and datanode write threads is expectedly tight. In the two successful test runs, (peak datanode threads, open files) was:\n(178, 144)\n(3900, 4000+)\nAnd that's for both grid sizes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-07T18:56:55.861+0000","updated":"2008-10-07T18:56:55.861+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637611","id":"12637611","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bshi","name":"bshi","key":"bshi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Bo Shi","active":true,"timeZone":"America/Los_Angeles"},"body":"To clarify the last portion of Meng's note, \n\n(178, 144) <-- dataset A\n(3900, 4000+) <-- dataset B (3x size of A)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bshi","name":"bshi","key":"bshi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Bo Shi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-07T19:07:19.569+0000","updated":"2008-10-07T19:07:19.569+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637615","id":"12637615","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"as for the open files, sysctl reports fs.file-max = 1573091, which seems pretty high, and ulimit is unlimited. I'm not aware of other file limit settings that might be involved.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-07T19:19:16.169+0000","updated":"2008-10-07T19:19:16.169+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637659","id":"12637659","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. For each input set, the same peak number of DataNode threads are spawned per machine, seemingly independent of the number of datanodes participating.\n\nThis is surprising. Assuming everything else is same (number of processes writing files and number of files being written), this seems impossible.\n\nHow many nodes are actually writing? Do these processes run on the same machines as datanodes? \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-07T21:30:12.543+0000","updated":"2008-10-07T21:30:12.543+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637666","id":"12637666","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"body":"I have confirmed it at least a couple times. My measurement for number of threads active per DataNode is:\n/usr/java/default/bin/jps | grep DataNode | cut -f1 -d' '  | xargs /usr/java/default/bin/jstack | grep ' daemon ' | wc\nbasically count the number of threads reported by jstack for the DataNode process.\n\nIs this too coarse or inaccurate a measurement?\n\nbq. How many nodes are actually writing?\nNot sure how I can measure how many are writing? Is it possible for a node to open a write operation and write nothing over to the file?\n\nbq.  Do these processes run on the same machines as datanodes? \nI'm not sure to which processes you're referring, but I am doing that command on the datanode machines of each grid. For example, on the 6-node grid, you see about 4600+ threads at the most intensive processing moment in the reduce phase on each of the 6 datanodes. And on the 64-node grid, you also see about 4600 threads at the same moment on _each_ of the datanodes. I think I have the logs that show this but I'm pretty sure I'm not mistaken.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mmao","name":"mmao","key":"mmao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Meng Mao","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-07T21:46:23.420+0000","updated":"2008-10-07T21:46:23.420+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637735","id":"12637735","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ccgillett","name":"ccgillett","key":"ccgillett","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christopher Gillett","active":true,"timeZone":"Etc/UTC"},"body":"Further to this, our configuration is that all slaves run both TaskTracker and DataNode processes, so they are doing double duty.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ccgillett","name":"ccgillett","key":"ccgillett","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christopher Gillett","active":true,"timeZone":"Etc/UTC"},"created":"2008-10-08T01:14:34.902+0000","updated":"2008-10-08T01:14:34.902+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12637791","id":"12637791","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"Better idea on what your maps and reducers do will help. Others here can give more ideas if you give relevant info like number of maps, number of reducers, what maps do approximately.. etc. \n\nIf you have same number of threads with 6 or 64 nodes for the same job, there is certainly something unexpected happening. May be each map is writing a fixed number of files and you have different number of maps on clusters with 6 or 64 nodes (so 'x' is different in each case).\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-10-08T06:22:41.704+0000","updated":"2008-10-08T06:22:41.704+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12697206","id":"12697206","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bshi","name":"bshi","key":"bshi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Bo Shi","active":true,"timeZone":"America/Los_Angeles"},"body":"Sorry for the long silence on this.  We've been heads-down and haven't had a chance to revisit.\n\nWe believe what is happening is that on a moderately sized dataset (>1 billion records) which is processed by a hadoop job that produces  *multiple* output files (using something like MultipleOutputFormat, although we rolled our own functionality in 0.15.0), we encountered this error because we were writing to too many output files [1] causing a large spike in the # of threads and eventually making the job non-responsive.\n\nRecent JIRAs (like HADOOP-3856) related to modifying the data node threads to use asynchronous IO should help in allowing the # of output directories to scale up... but mostly I think this is a case of user-error.\n\nPlease advise on how we should close this out or what new status flag to use for this JIRA.\n\n[1] we were creating daily/hourly subdirectories for each timestamp in a dataset that spanned 5 months, resulting in an absurd number of output directories.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bshi","name":"bshi","key":"bshi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10443","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10443","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10443","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10443"},"displayName":"Bo Shi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-04-08T20:58:55.543+0000","updated":"2009-04-08T20:58:55.543+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/12697390","id":"12697390","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ankur","name":"ankur","key":"ankur","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ankur","active":true,"timeZone":"Etc/UTC"},"body":"We are doing something similar (creating hourly/daily) directories that span 6 - 8 months. We just keep last couple of weeks data inflated and consolidate + compress the older data so that the hourly directories are removed and small number of compressed files remain for a day.\n\nComing to what you are doing, writing too many output-directories and files is anyway not advisable as its a strain on name server. From what you have told, it sounds like your class extending MultipleOutputFormat can be tuned along with the map-red job to reduce the number of output-file.\n\nSee if you can add a prefix/suffix to your key/value to help you accumulate the output in lesser number of files.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ankur","name":"ankur","key":"ankur","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ankur","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-09T04:58:44.318+0000","updated":"2009-04-09T04:59:49.963+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12397103/comment/13176819","id":"13176819","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"This has gone stale. We aren't seeing such a problem with the stable versions today.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2011-12-28T21:15:46.163+0000","updated":"2011-12-28T21:15:46.163+0000"}],"maxResults":28,"total":28,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-162/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0itnb:"}}