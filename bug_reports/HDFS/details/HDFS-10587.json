{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12984537","self":"https://issues.apache.org/jira/rest/api/2/issue/12984537","key":"HDFS-10587","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2016-07-08T21:59:34.651+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jul 21 06:02:52 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_2_*:*_1753779470_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_171262514","customfield_12312321":null,"resolutiondate":"2016-07-21T06:02:52.428+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-10587/watchers","watchCount":17,"isWatching":false},"created":"2016-06-28T23:18:50.948+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12475600","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12475600","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12640242","key":"HDFS-4660","self":"https://issues.apache.org/jira/rest/api/2/issue/12640242","fields":{"summary":"Block corruption can happen during pipeline recovery","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12473546","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12473546","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12731269","key":"HDFS-6804","self":"https://issues.apache.org/jira/rest/api/2/issue/12731269","fields":{"summary":"Add test for race condition between transferring block and appending block causes \"Unexpected checksum mismatch exception\" ","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12472383","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12472383","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12977248","key":"HDFS-10512","self":"https://issues.apache.org/jira/rest/api/2/issue/12977248","fields":{"summary":"VolumeScanner may terminate due to NPE in DataNode.reportBadBlocks","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12475355","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12475355","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12990664","key":"HDFS-10652","self":"https://issues.apache.org/jira/rest/api/2/issue/12990664","fields":{"summary":"Add a unit test for HDFS-4660","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12474947","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12474947","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12989351","key":"HDFS-10625","self":"https://issues.apache.org/jira/rest/api/2/issue/12989351","fields":{"summary":" VolumeScanner to report why a block is found bad","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12475601","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12475601","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12991348","key":"HDFS-10667","self":"https://issues.apache.org/jira/rest/api/2/issue/12991348","fields":{"summary":"Report more accurate info about data corruption location","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12474978","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12474978","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12989605","key":"HDFS-10632","self":"https://issues.apache.org/jira/rest/api/2/issue/12989605","fields":{"summary":"DataXceiver to report the length of the block it's receiving","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/5","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/trivial.svg","name":"Trivial","id":"5"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-08-17T04:15:42.310+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"}],"timeoriginalestimate":null,"description":"We found incorrect offset and length calculation in pipeline recovery may cause block corruption and results in missing blocks under a very unfortunate scenario. \n\n(1) A client established pipeline and started writing data to the pipeline.\n(2) One of the data node in the pipeline restarted, closing the socket, and some written data were unacknowledged.\n(3) Client replaced the failed data node with a new one, initiating block transfer to copy existing data in the block to the new datanode.\n(4) The block is transferred to the new node. Crucially, the entire block, including the unacknowledged data, was transferred.\n(5) The last chunk (512 bytes) was not a full chunk, but the destination still reserved the whole chunk in its buffer, and wrote the entire buffer to disk, therefore some written data is garbage.\n(6) When the transfer was done, the destination data node converted the replica from temporary to rbw, which made its visible length as the length of bytes on disk. That is to say, it thought whatever was transferred was acknowledged. However, the visible length of the replica is different (round up to the next multiple of 512) than the source of transfer. [1]\n(7) Client then truncated the block in the attempt to remove unacknowledged data. However, because the visible length is equivalent of the bytes on disk, it did not truncate unacknowledged data.\n(8) When new data was appended to the destination, it skipped the bytes already on disk. Therefore, whatever was written as garbage was not replaced.\n(9) the volume scanner detected corrupt replica, but due to HDFS-10512, it wouldnâ€™t tell NameNode to mark the replica as corrupt, so the client continued to form a pipeline using the corrupt replica.\n(10) Finally the DN that had the only healthy replica was restarted. NameNode then update the pipeline to only contain the corrupt replica.\n(11) Client continue to write to the corrupt replica, because neither client nor the data node itself knows the replica is corrupt. When the restarted datanodes comes back, their replica are stale, despite they are not corrupt. Therefore, none of the replica is good and up to date.\n\nThe sequence of events was reconstructed based on DataNode/NameNode log and my understanding of code.\nIncidentally, we have observed the same sequence of events on two independent clusters.\n\n[1]\nThe sender has the replica as follows:\n2016-04-15 22:03:05,066 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1556997324_1100153495099, RBW\n  getNumBytes()     = 41381376\n  getBytesOnDisk()  = 41381376\n  getVisibleLength()= 41186444\n  getVolume()       = /hadoop-i/data/current\n  getBlockFile()    = /hadoop-i/data/current/BP-1043567091-10.1.1.1-1343682168507/current/rbw/blk_1556997324\n  bytesAcked=41186444\n  bytesOnDisk=41381376\n\nwhile the receiver has the replica as follows:\n2016-04-15 22:03:05,068 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1556997324_1100153495099, RBW\n  getNumBytes()     = 41186816\n  getBytesOnDisk()  = 41186816\n  getVisibleLength()= 41186816\n  getVolume()       = /hadoop-g/data/current\n  getBlockFile()    = /hadoop-g/data/current/BP-1043567091-10.1.1.1-1343682168507/current/rbw/blk_1556997324\n  bytesAcked=41186816\n  bytesOnDisk=41186816","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12817362","id":"12817362","filename":"HDFS-10587.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-12T07:15:01.811+0000","size":1339,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12817362/HDFS-10587.001.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12818144","id":"12818144","filename":"HDFS-10587-test.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-15T10:38:29.797+0000","size":6300,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12818144/HDFS-10587-test.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Incorrect offset/length calculation in pipeline recovery causes block corruption","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15368564","id":"15368564","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~jojochuang],\n\nThanks a lot for the investigation/findings and the jira!\n\nI did some study and below is what I found:\n\nThe acknowledged length is what has been acknowledged to the client (the writer). The client will continue to write data from there on after a new pipeline is constructed.\n\n{quote}\n(5) The last chunk (512 bytes) was not a full chunk, but the destination still reserved the whole chunk in its buffer, and wrote the entire buffer to disk, therefore some written data is garbage.\n{quote}\n\nTo do the block transfer during the pipeline recovery process, the code that does the padding is in BlockSender's constructor:\n{code}\n      // end is either last byte on disk or the length for which we have a \n      // checksum\n      long end = chunkChecksum != null ? chunkChecksum.getDataLength()\n          : replica.getBytesOnDisk();\n      if (startOffset < 0 || startOffset > end\n          || (length + startOffset) > end) {\n        String msg = \" Offset \" + startOffset + \" and length \" + length\n        + \" don't match block \" + block + \" ( blockLen \" + end + \" )\";\n        LOG.warn(datanode.getDNRegistrationForBP(block.getBlockPoolId()) +\n            \":sendBlock() : \" + msg);\n        throw new IOException(msg);\n      } \n     \n      // Ensure read offset is position at the beginning of chunk\n      offset = startOffset - (startOffset % chunkSize);\n      if (length >= 0) {\n        // Ensure endOffset points to end of chunk.\n        long tmpLen = startOffset + length;\n        if (tmpLen % chunkSize != 0) {\n          tmpLen += (chunkSize - tmpLen % chunkSize); <=====================include data to end of chunk\n        }\n        if (tmpLen < end) {\n          // will use on-disk checksum here since the end is a stable chunk\n          end = tmpLen;\n        } else if (chunkChecksum != null) {\n          // last chunk is changing. flag that we need to use in-memory checksum \n          this.lastChunkChecksum = chunkChecksum;\n        }\n      }\n      endOffset = end;\n{code}\n\n{{endOffset}} is overwriten with {{end}},  which is only overwritten by {{tmpLen}} when {{tmpLen < end}}.\n\nNotice that {{end}} is\n      // end is either last byte on disk or the length for which we have a \n      // checksum\n\nSo in theory, the data sent from BlockSender is still valid data. Thus  the following statement is not true:\n{quote}\n(5) The last chunk (512 bytes) was not a full chunk, but the destination still reserved the whole chunk in its buffer, and wrote the entire buffer to disk, therefore some written data is garbage.\n{quote}\n\nThat said, it's important for the receiving DN to have the accurate visibleLength. That's the key issue here. Because the receiving DN got the wrong visibleLength.\n\nThe way receiver got visible length is:\n{code}\n  @Override // FsDatasetSpi\n  public synchronized ReplicaInPipeline convertTemporaryToRbw(\n      final ExtendedBlock b) throws IOException {\n    final long blockId = b.getBlockId();\n    final long expectedGs = b.getGenerationStamp();\n    final long visible = b.getNumBytes();  <=================================This is how the receiving DN got visible length\n    LOG.info(\"Convert \" + b + \" from Temporary to RBW, visible length=\"\n        + visible);\n{code}\n\nSo I think the solution is for the sender to send only visibleLength worth amount of data, instead of including data toward chunk end.\n\nQuoted below is part of the above quoted code,  \n{code}\n        // Ensure endOffset points to end of chunk.\n        long tmpLen = startOffset + length;\n        if (tmpLen % chunkSize != 0) {\n          tmpLen += (chunkSize - tmpLen % chunkSize); <=====================include data to end of chunk\n        }\n{code}\nThough the comment here says {{// Ensure endOffset points to end of chunk.}}, I don't see why we need to do that. If we ensure the endOffset is equal to startOffset + visibleLength (the {{length}} here is the visible length), then we should solve the incorrect visibleLength issue at the receiver side, thus the corruption issue.\n\nI think this seems to be the solution, unless there is some real subtle reason that we have to {{Ensure endOffset points to end of chunk.}}.\n\nI hope some folks who are more familiar with this code can comment.\n\nThanks.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-08T21:59:34.651+0000","updated":"2016-07-08T22:00:49.379+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15368761","id":"15368761","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I see some subtlety here, if the sending DN does have data up to the chunk end, and the checksum was computed accordingly. Per my proposed solution, now we are sending part of the data that the sender DN has, so we need to recalculate the checksum for this \"truncated: data.\n\nThe problem with existing implementation is, the receiving DN treats the received size as visibleLength, which is not correct here. If we can find a way to pass the visibleLength to the receiving DN, then the sender DN can still send data up to the chunk end.\n\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-08T23:58:05.246+0000","updated":"2016-07-09T02:38:24.821+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15369328","id":"15369328","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I did a quick change\n\n{code}\ndiff --git hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java hadoop-hdfs-project/hadoop-hdfs/src/main/ja\nva/org/apache/hadoop/hdfs/server/datanode/BlockSender.java\nindex 398935d..188768b 100644\n--- hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java\n+++ hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java\n@@ -363,6 +363,7 @@\n       \n       // Ensure read offset is position at the beginning of chunk\n       offset = startOffset - (startOffset % chunkSize);\n+      /*\n       if (length >= 0) {\n         // Ensure endOffset points to end of chunk.\n         long tmpLen = startOffset + length;\n@@ -378,7 +379,8 @@\n         }\n       }\n       endOffset = end;\n-\n+      */\n+      endOffset = length > 0? startOffset + length : end;\n       // seek to the right offsets\n       if (offset > 0 && checksumIn != null) {\n         long checksumSkip = (offset / chunkSize) * checksumSize;\n{code}\nand run all HDFS/common unit tests, they passed fine.\n\nEither we don't have a test to enforce {{// Ensure endOffset points to end of chunk.}} or it's ok not to have this enforcement.\n\nIf we don't need the enforcement, then the solution I would propose is to send {{length}} worth of data (where {{length}} is the visibleLength in this context) in BlockSender, as the quick change above illustrated.\n\nSo I'd suggest that we look more into whether we really need the above mentioned enforcement.\n\nThanks.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-09T22:30:05.113+0000","updated":"2016-07-09T22:30:05.113+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15371294","id":"15371294","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"HI [~jojochuang],\n\nI think it'd be nice to work out a unit test that demonstrates the block corruption, for example, to create a block with visibleLength X, and a replica with data X+delta written to disk,  then use the involved code to copy the replica to a different one, to see if  the corruption happens. If so, then we can see my above proposed change can address the issue. \n\nOf course, we still need to understand better the \"chunk end enforcement\" mentioned in my earlier comment. \n \nWhat do you think?\n\nThanks.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-11T18:00:30.257+0000","updated":"2016-07-11T18:00:30.257+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15371798","id":"15371798","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"HI [~szetszwo] and [~kihwal],\n\nI would like to bring this jira to your attention. Would you please help review the report and the comments I made?\n\nEspecially, I wonder why we have to enforce the size of data sent from BlockSender to the end of a chunk (Please see my comments above for details).\n\nThe problem here is, the receiving DN would treat the size of the sent data as visibleLength, which is wrong.\n\nThanks a lot.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-11T22:30:58.626+0000","updated":"2016-07-11T22:30:58.626+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15372385","id":"15372385","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~kihwal] and [~szetszwo], \n\nUpload a naive patch (001) to illustrate how to possibly only send {{length}} (visibleLength) amount of data, so the receiving DN will not get wrong visibleLength.\n\nThe questions are:\n\n1. when do we have to send more than {{length}} amount of data? Seems that's true for a currently being written replica, which seems a special case, such as when we increase replication factor. \n\n2. Any issue about checksum calculation with this change? \n\nThanks a lot.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-12T07:15:01.835+0000","updated":"2016-07-12T07:15:01.835+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15372553","id":"15372553","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 24s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 26s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 26s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 53s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 48s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 24s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 41 unchanged - 2 fixed = 42 total (was 43) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 54s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 79m 15s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 99m 30s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| FindBugs | module:hadoop-hdfs-project/hadoop-hdfs |\n|  |  Field only ever set to null:null: org.apache.hadoop.hdfs.server.datanode.BlockSender.lastChunkChecksum  In BlockSender.java |\n| Failed junit tests | hadoop.hdfs.TestAppendSnapshotTruncate |\n|   | hadoop.hdfs.TestParallelRead |\n|   | hadoop.hdfs.TestHFlush |\n|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength |\n|   | hadoop.hdfs.server.datanode.TestCachingStrategy |\n|   | hadoop.hdfs.client.impl.TestClientBlockVerification |\n|   | hadoop.hdfs.TestFileConcurrentReader |\n|   | hadoop.hdfs.TestParallelUnixDomainRead |\n|   | hadoop.hdfs.TestFileCreationClient |\n|   | hadoop.fs.contract.hdfs.TestHDFSContractSeek |\n|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart |\n| Timed out junit tests | org.apache.hadoop.hdfs.TestPread |\n|   | org.apache.hadoop.hdfs.TestClientReportBadBlock |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:9560f25 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12817362/HDFS-10587.001.patch |\n| JIRA Issue | HDFS-10587 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 51da0651a7cc 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 819224d |\n| Default Java | 1.8.0_91 |\n| findbugs | v3.0.0 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/16024/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\n| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/16024/artifact/patchprocess/new-findbugs-hadoop-hdfs-project_hadoop-hdfs.html |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/16024/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/16024/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/16024/console |\n| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-07-12T08:59:01.466+0000","updated":"2016-07-12T08:59:01.466+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15373393","id":"15373393","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"The failed tests do indicate some issue with this naive change.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-12T18:23:07.826+0000","updated":"2016-07-12T18:23:07.826+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15373705","id":"15373705","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~jojochuang] for bringing HDFS-4660 to my attention, which seems to have similar symptom.\n\nHDFS-4660 talked about and fixed BlockReceiver side issue. Per the analysis I put above, in the HDFS-10587 case, the issue here is that BlockSender adjusted the data to chunk end, thus making the data larger than the real visibleLength ({{length}}. And the receiver side treats the received size as the new visibleLength, which seems incorrect to me and is the key issue here.\n\nHI [~kihwal], [~vinayrpet] and [~peng.zhang], you guys worked on HDFS-4660, would you please help sharing some insight here for HDFS-10587?\n\nThanks a lot.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-12T21:12:12.350+0000","updated":"2016-07-12T21:12:12.350+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15373875","id":"15373875","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~jojochuang] for adding the log to the jira description.\n\n{code}\nThe sender has the replica as follows:\n2016-04-15 22:03:05,066 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1556997324_1100153495099, RBW\ngetNumBytes() = 41381376\ngetBytesOnDisk() = 41381376\ngetVisibleLength()= 41186444\ngetVolume() = /hadoop-i/data/current\ngetBlockFile() = /hadoop-i/data/current/BP-1043567091-10.1.1.1-1343682168507/current/rbw/blk_1556997324\nbytesAcked=41186444\nbytesOnDisk=41381376\n\nwhile the receiver has the replica as follows:\n2016-04-15 22:03:05,068 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1556997324_1100153495099, RBW\ngetNumBytes() = 41186816\ngetBytesOnDisk() = 41186816\ngetVisibleLength()= 41186816\ngetVolume() = /hadoop-g/data/current\ngetBlockFile() = /hadoop-g/data/current/BP-1043567091-10.1.1.1-1343682168507/current/rbw/blk_1556997324\nbytesAcked=41186816\nbytesOnDisk=41186816\n{code}\n\nThe sender's visibleLength is 41186444, which is not a multiple of chunks (it's 80442 * 512 + 140), then BlockSender \"marks it up\" to 41186816 (or 80443 * 512), because there is enough data on the BlockSender DN's disk.\n\nIt would be ok for The BlockReceiver DN to receive 41186816 data, as long as it can skip the already-received data when receiving more data from the client. But it appears that the BlockReceiver DN is not doing that correctly. If we can fix that behavior, it  would be a good fix for this issue here.\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-12T22:56:03.001+0000","updated":"2016-07-14T06:43:42.075+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15374480","id":"15374480","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"About the visibleLength, I saw \n{code}\nIn ReplicaBeingWritten.java\n  @Override\n  public long getVisibleLength() {\n    return getBytesAcked(); // all acked bytes are visible\n  }\n{code}\nwhich means different replicas may have different visibleLength, because BytesAcked at different DataNodes maybe different.\n\nMy earlier effort was to claim that using different visibleLength at the BlockReceiver than the BlockSender side is wrong. Based on the above code, it might be ok to claim the visibleLength as the received data length at the destination side of blockTransfer (better to get confirmation though).\n\nSo, we need to understand, how the corruption really happened, and where in the block data: Did it happen when we receive this chunk of data, or when we receive new data after reconstructing the pipeline? Because based on my analysis so far, the skipping of the bytes on disk (mentioned in the following statement) is necessary since the data is not garbage (assuming the data at the Sender side is good).\n{quote}\n(8) When new data was appended to the destination, it skipped the bytes already on disk. Therefore, whatever was written as garbage was not replaced.\n{quote}\n\nOne possibility is that the checksum handling there is not correct in a corner situation. \n\nIf we have a testcase to replicate the issue, we need to look at both the source side data and destination side data, to see whether it's real data corruption, or checksum miscalculation. If there is corruption, where exactly the corruption is.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-13T07:02:53.703+0000","updated":"2016-07-13T07:02:53.703+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15375922","id":"15375922","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"The block corruption appears to be corrupted at the very beginning of the chunk right after the block transfer (that copy data up to the previous chunk end).\n\nThe looks similar to HDFS-4660. Unfortunately we don't have the exact block file and checksum file on the source and the destination to compare. Otherwise, it would be easier to tell what might have happened.\n\n\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-13T22:36:36.024+0000","updated":"2016-07-13T22:36:36.024+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376368","id":"15376368","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. (5) The last chunk (512 bytes) was not a full chunk, but the destination still reserved the whole chunk in its buffer, and wrote the entire buffer to disk, therefore some written data is garbage.\nSince the sender have more onDisk data than ack'ed, it can send extra bytes during transfer. That doesnt mean that extra data sent is garbage. Its valid data, but not sent the ack upstream, possibly due to waiting for the ack from downstream.\n\nSo in current case, extra bytes sent to make up to chunk end, should be valid data, along with the checksum available at the sender.\n\nClient, anyway will send the unack'ed packets again. These packets should recalculate the checksum if these are appending data to same chunk.\nIf the onDisk data is at the chunk boundary, that chunk will be skipped and next chunk will be written for the packet.\nIf the packet starting in between the chunk, then it should contain only data to fill up the chunk.\nSo in this case, since the ondisk length is at the chunk boundary, then next packet(which is starting in middle of the chunk) will be skipped.\n\nAnother question is, whats the exact sequence of events? VolumeScanner can scan only completed blocks. Whether this block was closed and re-opened for append?\n\nWhether client reading the data found any corruption later?\n\nIt would be helpful, if more logs are shared for both datanodes.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T05:59:47.358+0000","updated":"2016-07-14T05:59:47.358+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376414","id":"15376414","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~vinayrpet],\n\nThanks a lot for your comment!\n\nI also have found that item (5) in the description is incorrect.  In addition, I observed that the data corruption happened at the next chunk. As my last comment stated, unfortunately we don't have the BlockSender DN's replica data to compare, to see if it's real data corruption, or incorrect checksum. I think it's more likely the checksum calculation issue.\n\nVolumeScanner is just a later protection, when volumeScanner detects the corruption, the replica is already much larger than right after the block transfer.\n\nThanks.\n\n\n\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T06:35:53.759+0000","updated":"2016-07-14T06:35:53.759+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376420","id":"15376420","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq.  In addition, I observed that the data corruption happened at the next chunk.\nCan you give some more details about this. How you found that it happened at next chunk?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T06:42:01.737+0000","updated":"2016-07-14T06:42:01.737+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376427","id":"15376427","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~vinayrpet].\n\nI used \"hdfs debug verify\" on the replica files at the destination. I wish I have the source replica files, but I don't.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T06:47:14.300+0000","updated":"2016-07-14T06:47:14.300+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376452","id":"15376452","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. Another question is, whats the exact sequence of events? VolumeScanner can scan only completed blocks. Whether this block was closed and re-opened for append?\nI meant, whether any {{DFS.append()}} was involved or Just continous write for {{create()}} with pipeline recoveries.?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T07:00:52.152+0000","updated":"2016-07-14T07:00:52.152+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376455","id":"15376455","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~vinayrpet] for the discussion.\nFor a while when I filed this jira, I thought I found the smoking gun, but it turned out it was my misunderstanding of the code. However, other than step (5) and (8), other steps were evident in the log.\n\nFrom what I understand so far, the corruption was detected in the first chunk appended after the pipeline recovery. Incidentally, the corruption initially was only found on the datanode added into pipeline after recovery, and it did not affect other datanodes in the pipeline.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T07:02:55.109+0000","updated":"2016-07-14T07:02:55.109+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376457","id":"15376457","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. From what I understand so far, the corruption was detected in the first chunk appended after the pipeline recovery. Incidentally, the corruption initially was only found on the datanode added into pipeline after recovery, and it did not affect other datanodes in the pipeline.\nYou mean for other replicas \"hdfs debug verify\" didnt show any corruption?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T07:06:20.165+0000","updated":"2016-07-14T07:06:20.165+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376462","id":"15376462","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"I didn't run hdfs debug verify for other replicas. Maybe I jumped to conclusion too soon, but the VolumeScanner on those DNs in the original pipeline never detected checksum error. Those replicas however became stale because the DataNodes were restarted.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T07:13:27.617+0000","updated":"2016-07-14T07:13:27.617+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376464","id":"15376464","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"Out of curiosity, Mechanically, what's the difference between these two? I know for a fact the client is a flume application, so it's mostly append operation.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T07:15:12.475+0000","updated":"2016-07-14T07:15:12.475+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376467","id":"15376467","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~vinayrpet],\n\nThanks for looking into. Some info to share:\n\nThe blockTransfer only transferred data of of size 41186816 in the above example, and the corruption is found to be at the right next chunk starting at 41186816.  \n\nIt's a bit interesting here: it's observed that much ore data is written to this same replica. However, the client keeps getting the following msg and the DFSOutputStream was not created successfully after block transfer (because of the corrupted data, any newly added downstream DN always detect the checksum error and disconnect it from the pipeline, thus the client keeps trying to replace the downstream DN, as reported in HDFS-6937). \n\nQuestion is, If the DFSOutputStream is not created successfully, supposedly the client wouldn't send new data, where is the new data beyond 41186816 from? \n\n{code}\nINFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream\njava.io.IOException: Bad connect ack with firstBadLink as 10.1.1.1:1110\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1472)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1293)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1016)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:560)\n{code}\n\nThanks.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T07:17:29.501+0000","updated":"2016-07-14T07:17:29.501+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376488","id":"15376488","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"From this, I can see that, truncate happens even in Sender's Block, but not for the receivers block.\nNeed to analyze more in this area.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T07:24:34.354+0000","updated":"2016-07-14T07:24:34.354+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376494","id":"15376494","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I don't see truncate on Sender's, Sender actually reports {{getVisibleLength()= 41186444}}. \n\nWould you please elaborate \n{quote}\ntruncate happens even in Sender's Block, but not for the receivers block.\n{quote}\n\nThanks a lot.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T07:33:17.294+0000","updated":"2016-07-14T07:33:17.294+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376563","id":"15376563","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. I don't see truncate on Sender's, Sender actually reports getVisibleLength()= 41186444.\nYes, thats correct. Thats while transferring the block, it displays 41186444 as visibleLength and 41381376 as onDisk. Because of this, during trasnsfer it sends extra bytes to make up the full chunk.\n\nBut once the transfer is complete, during pipeline recovery in {{recoverRbw()}} since the bytesOnDisk > visibleLength, on disk bytes will be truncated to visibleLength. This will happen in almost all Datanodes except the new DN added. So extra bytes sent to new DN will not be overwritter, whereas in old DNs it will be written from packets.\n{code}     // Truncate the potentially corrupt portion.\n      // If the source was client and the last node in the pipeline was lost,\n      // any corrupt data written after the acked length can go unnoticed.\n      if (numBytes > bytesAcked) {\n        final File replicafile = rbw.getBlockFile();\n        truncateBlock(replicafile, rbw.getMetaFile(), numBytes, bytesAcked);\n        rbw.setNumBytes(bytesAcked);\n        rbw.setLastChecksumAndDataLen(bytesAcked, null);\n      }{code}\nBut this step will be skipped in new DN as it has both visible and ondisk bytes as same due to transfer.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T08:28:49.932+0000","updated":"2016-07-14T08:28:49.932+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376576","id":"15376576","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"For End-user both should result in same data. \nbut recovery flows involved are different in append, as pipeline reconstruction happens for the original nodes itself. Why I asked is, before append() block would be in Finalized state, which VolumeScanner would have started scanning.\nIf its only pipeline recoveries with one create(), block will not be Finalized state anytime, and VolumeScanner will not scan the block.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T08:36:58.433+0000","updated":"2016-07-14T08:36:58.433+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376739","id":"15376739","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xupener","name":"xupener","key":"xupener","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"xupeng","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi all :\n\nI encountered the same issue, here is the scenario : \n\n- hbase writing block to pipeline 10.6.134.228 , 10.6.128.215, 10.6.128.208\n-  dn - 10.6.128.208 restarted\n-  pipeline recovery, add new datanode - 10.6.134.229 to pipeline.\n-  client send transfer_block command , and 10.6.134.228 copy the block file to new data node 10.6.134.229  \n-  client writing data\n-   datanode 10.6.128.215 restarted\n-  pipeline recovery, add new datanode - 10.6.129.77 to pipeline.\n-  client send transfer_block command , and 10.6.134.229 copy the block file to new data node 10.6.129.77\n-  129.77 throws \"error.java.io.IOException: Unexpected checksum mismatch\"","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xupener","name":"xupener","key":"xupener","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"xupeng","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-07-14T10:43:32.438+0000","updated":"2016-07-14T10:54:26.427+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376794","id":"15376794","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi [~xupeng], Can you add 229 and 77 logs as well for this block? Including transfer related logs.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T11:38:43.683+0000","updated":"2016-07-14T11:38:43.683+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376897","id":"15376897","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xupener","name":"xupener","key":"xupener","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"xupeng","active":true,"timeZone":"Asia/Shanghai"},"body":"hi [~vinayrpet] :\n\nlogs related are listed below\n\n134.228\n{noformat}\n2016-07-13 11:48:29,528 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer: Transmitted blk_1116167880_42905642 (numBytes=9911790) to /10.6.134.229:5080\n2016-07-13 11:48:29,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving blk_1116167880_42905642 src: /10.6.130.44:26319 dest: /10.6.134.228:5080\n2016-07-13 11:48:29,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover RBW replicablk_1116167880_42905642\n2016-07-13 11:48:29,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1116167880_42905642, RBW\n  getNumBytes()     = 9912487\n  getBytesOnDisk()  = 9912487\n  getVisibleLength()= 9911790\n  getVolume()       = /current\n  getBlockFile()    = /current/current/rbw/blk_1116167880\n  bytesAcked=9911790\n  bytesOnDisk=9912487\n2016-07-13 11:48:29,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: truncateBlock: blockFile=/current/current/rbw/blk_1116167880, metaFile=/current/current/rbw/blk_1116167880_42905642.meta, oldlen=9912487, newlen=9911790\n016-07-13 11:49:01,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving blk_1116167880_42906656 src: /10.6.130.44:26617 dest: /10.6.134.228:5080\n2016-07-13 11:49:01,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover RBW replica blk_1116167880_42906656\n2016-07-13 11:49:01,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1116167880_42906656, RBW\n  getNumBytes()     = 15104963\n  getBytesOnDisk()  = 15104963\n  getVisibleLength()= 15102415\n  getVolume()       = /current\n  getBlockFile()    = /current/current/rbw/blk_1116167880\n  bytesAcked=15102415\n  bytesOnDisk=15104963\n2016-07-13 11:49:01,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: truncateBlock: blockFile=/current/rbw/blk_1116167880, metaFile=/current/rbw/blk_1116167880_42906656.meta, oldlen=15104963, newlen=15102415\n2016-07-13 11:49:01,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 10.6.129.77:5080\n2016-07-13 11:49:01,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 10.6.129.77:5080\n2016-07-13 11:49:01,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: blk_1116167880_42907145, type=HAS_DOWNSTREAM_IN_PIPELINE\njava.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2225)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1179)\n        at java.lang.Thread.run(Thread.java:745)\n2016-07-13 11:49:01,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for blk_1116167880_42907145\njava.io.IOException: Premature EOF from inputStream\n        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:201)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n{noformat}\n\n\n134.229\n{noformat}\n2016-07-13 11:48:29,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving blk_1116167880_42905642 src: /10.6.134.228:24286 dest: /10.6.134.229:5080\n2016-07-13 11:48:29,516 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Convert blk_1116167880_42905642 from Temporary to RBW, visible length=9912320\n2016-07-13 11:48:29,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving blk_1116167880_42905642 src: /10.6.134.228:24321 dest: /10.6.134.229:5080\n2016-07-13 11:48:29,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover RBW replica blk_1116167880_42905642\n2016-07-13 11:48:29,552 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1116167880_42905642, RBW\n  getNumBytes()     = 9912320\n  getBytesOnDisk()  = 9912320\n  getVisibleLength()= 9912320\n  getVolume()       = /current\n  getBlockFile()    = /current/rbw/blk_1116167880\n  bytesAcked=9912320\n  bytesOnDisk=9912320\n\n2016-07-13 11:49:01,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: blk_1116167880_42906656, type=HAS_DOWNSTREAM_IN_PIPELINE\njava.io.IOException: Connection reset by peer\n016-07-13 11:49:01,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: blk_1116167880_42906656, type=HAS_DOWNSTREAM_IN_PIPELINE terminating\n2016-07-13 11:49:01,505 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock blk_1116167880_42906656 received exception java.io.IOException: Premature EOF from inputStream\n2016-07-13 11:49:01,506 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataXceiver error processing WRITE_BLOCK operation  src: /10.6.134.228:24321 dst: /10.6.134.229:5080\njava.io.IOException: Premature EOF from inputStream\n        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:201)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:472)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:789)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:917)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:174)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:80)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n2016-07-13 11:49:01,541 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Not scanning suspicious block blk_1116167880_42906656 on DS-8c209fca-9b34-4a6b-919b-6b4d24a3e13a, because the block scanner is disabled.\n2016-07-13 11:49:01,542 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: :Failed to transfer blk_1116167880_42906656 to 10.6.129.77:5080 got\njava.net.SocketException: Original Exception : java.io.IOException: Connection reset by peer\n        at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)\n        at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428)\n        at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493)\n        at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608)\n        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:576)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:745)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer.run(DataNode.java:2166)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2895)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:980)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:203)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:92)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Connection reset by peer\n        ... 14 more\n2016-07-13 11:49:01,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: blk_1116167880_42906656 src: /10.6.134.228:46441 dest: /10.6.134.229:5080\n2016-07-13 11:49:01,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover RBW replica blk_1116167880_42906656\n2016-07-13 11:49:01,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_1116167880_42906656, RBW\n  getNumBytes()     = 15104963\n  getBytesOnDisk()  = 15104963\n  getVisibleLength()= 15102415\n  getVolume()       = /current\n  getBlockFile()    = /current/rbw/blk_1116167880\n  bytesAcked=15102415\n  bytesOnDisk=15104963\n2016-07-13 11:49:01,567 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: truncateBlock: blockFile=/current/rbw/blk_1116167880, metaFile=current/rbw/blk_1116167880_42906656.meta, oldlen=15104963, newlen=15102415\n2016-07-13 11:49:01,569 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: :Exception transfering block blk_1116167880_42907145 to mirror 10.6.129.77:5080: java.io.EOFException: Premature EOF: no length prefix available\n2016-07-13 11:49:01,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock blk_1116167880_42907145 received exception java.io.EOFException: Premature EOF: no length prefix available\n2016-07-13 11:49:01,569 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataXceiver error processing WRITE_BLOCK operation  src: /10.6.134.228:46441 dst: /10.6.134.229:5080\njava.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2225)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:858)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:174)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:80)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\n129.77\n{noformat}\n2016-07-13 11:49:01,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving blk_1116167880_42906656 src: /10.6.134.229:43844 dest: /10.6.129.77:5080\n2016-07-13 11:49:01,543 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Checksum error in block blk_1116167880_42906656 from /10.6.134.229:43844\norg.apache.hadoop.fs.ChecksumException: Checksum error: DFSClient_NONMAPREDUCE_2019484565_1 at 81920 exp: 1352119728 got: -1012279895\n        at org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(Native Method)\n        at org.apache.hadoop.util.NativeCrc32.verifyChunkedSumsByteArray(NativeCrc32.java:69)\n        at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:347)\n        at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:294)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.verifyChunks(BlockReceiver.java:421)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:558)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:789)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:917)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:174)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:80)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n2016-07-13 11:49:01,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for blk_1116167880_42906656\njava.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing blk_1116167880_42906656 from /10.6.134.229:43844\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:571)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:789)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:917)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:174)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:80)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n2016-07-13 11:49:01,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock blk_1116167880_42906656 received exception java.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing blk_1116167880_42906656 from /10.6.134.229:43844\n2016-07-13 11:49:01,543 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode:  error processing WRITE_BLOCK operation  src: /10.6.134.229:43844 dst: /10.6.129.77:5080\njava.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing blk_1116167880_42906656 from /10.6.134.229:43844\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:571)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:789)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:917)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:174)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:80)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n2016-07-13 11:49:01,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving blk_1116167880_42906656 src: /10.6.134.229:43878 dest: /10.6.129.77:5080\n2016-07-13 11:49:01,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover RBW replica BP-448958278-10.6.130.96-1457941856632:blk_1116167880_42906656\n2016-07-13 11:49:01,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock blk_1116167880_42906656 received exception org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot recover a non-RBW replica ReplicaInPipeline, blk_1116167880_42906656, TEMPORARY\n  getNumBytes()     = 9961472\n  getBytesOnDisk()  = 9830400\n  getVisibleLength()= -1\n  getVolume()       = /current\n  getBlockFile()    = /current/tmp/blk_1116167880\n  bytesAcked=0\n  bytesOnDisk=9830400\n2016-07-13 11:49:01,571 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataXceiver error processing WRITE_BLOCK operation  src: /10.6.134.229:43878 dst: /10.6.129.77:5080\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot recover a non-RBW replica ReplicaInPipeline, blk_1116167880_42906656, TEMPORARY\n  getNumBytes()     = 9961472\n  getBytesOnDisk()  = 9830400\n  getVisibleLength()= -1\n  getVolume()       = /current\n  getBlockFile()    =/current/tmp/blk_1116167880\n  bytesAcked=0\n  bytesOnDisk=9830400\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.recoverRbw(FsDatasetImpl.java:1273)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:190)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:799)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:174)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:80)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xupener","name":"xupener","key":"xupener","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"xupeng","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-07-14T13:24:51.701+0000","updated":"2016-07-14T13:24:51.701+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15376940","id":"15376940","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. org.apache.hadoop.fs.ChecksumException: Checksum error: DFSClient_NONMAPREDUCE_2019484565_1 at 81920 exp: 1352119728 got: -1012279895\nHere it says checksum error at 81920, which is at the very beginning itself. May be 229 disk have some problem, or during transfer to 77 some corruption due to network card would have happened. \nIs not exactly same as current case.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-14T13:49:57.923+0000","updated":"2016-07-14T13:49:57.923+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15377160","id":"15377160","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks a lot [~vinayrpet] and [~xupener]!\n\nAs Vinay pointed, the case Xupeng described looks alike but the corruption position is not like this case. I think HDFS-6937 will help on Xupeng's case.\n\nVinay: About {{recoverRbw}},  since the data the destination DN (the new DN) received is valid data, does not truncating at the new DN hurt? \n\nWe actually allow different visibleLength at different replica, see \nhttps://issues.apache.org/jira/browse/HDFS-10587?focusedCommentId=15374480&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15374480\n\nThough I originally hopeed that the block transfer preserve the visibleLength, so that in the block transfer, the target DN can have the same visibleLength as the source DN.\n\nAssuming it's ok to have the different visibleLength at the new DN, the block transfer seems to have side effect, such that the new chunk after the block transfer at the new DN appears corrupted.\n\nAnother thing is, if the pipeline recovery is failing, see\n\nhttps://issues.apache.org/jira/browse/HDFS-10587?focusedCommentId=15376467&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15376467\n\n why we have more data reaching the new DN (I meant the chunk after block transfer) ?\n\nThanks.\n\n\n \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T15:55:33.445+0000","updated":"2016-07-14T15:55:33.445+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15378483","id":"15378483","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I think this looks why we had new data reaching the new DN after the init block transfer:  after adding the new DN to the pipeline, doing the block transfer to this new DN, the client resumed writing data. Then in the process, corruption is detected again, thus repeating the pipeline recovery process. Even though from client side point of view, it keeps getting the following exception\n\n{code}\nINFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream\njava.io.IOException: Bad connect ack with firstBadLink as 10.1.1.1:1110\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1472)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1293)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1016)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:560)\n{code}\n\nWei-Chiu and I discussed, and we think here is a more complete picture:\n\n* 1. pipeline going on DN1 -> DN2 -> DN3\n* 2. trouble at DN3, it's gone\n* 3. pipeline recovery, new DN DN4 added\n* 4. block transfer from DN1 to DN4, DN4's data is now a multiple of chunks.\n* 5. DataStreamer resumed writing data to DN1 -> DN4 -> DN3 (this is where new data gets in), the first chunk DN4 got is corrupt for some reason that we are searching for\n* 6. DN3 detects corruption, quit; while new data has been written to DN1 and DN4\n* 7. goes back to step 3, new pipeline recovery starts\nDN1 ->DN4 -> DN5\nDN1 -> DN4 -> DN6\n......\n\nAt a corner case, Step 3 could be replaced with \"DN3 restarted\", in which case, another block transfer would happen, and may cause corruption.\n\nSince DN1's visibleLength in step 4 is not a multiple of chunks, this fact might be somehow related to the corruption in step 5.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T22:05:26.515+0000","updated":"2016-07-14T22:05:26.515+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15378546","id":"15378546","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"I see. So for the logs I saw, there's no \"Appending to \" messages. I think the replica was created without being in FINALIZED state.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-14T22:56:14.458+0000","updated":"2016-07-14T22:56:14.458+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15378879","id":"15378879","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. Here it says checksum error at 81920, which is at the very beginning itself. May be 229 disk have some problem, or during transfer to 77 some corruption due to network card would have happened. Is not exactly same as current case.\nI was wrong. [~xupeng] case is also exactly same as this Jira.\nHere is how, \n# 77 is throwing exception while verifying the received packet during transfer from 229(which got the block transfered earlier from 228)\n# While verifying only packet, the position mentioned in the checksum exception, is relative to packet buffer offset, not the block offset. So 81920 is the offset in the exception.\n# Blocks already written to disk in 77 during transfer before checksum exception : 9830400\n# Total : 9830400 + 81920 == 9912320, which is same as bytes received by 229 from 228 when it was added to pipeline.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-15T05:20:40.579+0000","updated":"2016-07-15T05:20:40.579+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15379212","id":"15379212","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"I have written some test to reproduce the flow as mentioned in the description. \nBut could not find the corruption when ran.\nCheck whether this could help any of you.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-15T10:38:29.813+0000","updated":"2016-07-15T10:38:29.813+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15379864","id":"15379864","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you so much [~vinayrpet]! We are looking further!\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-15T18:32:18.379+0000","updated":"2016-07-15T18:32:18.379+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15383685","id":"15383685","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~vinayrpet],\n\nThanks much again for creating the testcase! Upon further investigation, we found that our branch initially had HDFS-4660, then it was reverted because of an issue, which was later identified as HDFS-9220. We had a tool to query jiras in a given branch, and it failed to tell us the jira was reverted, so we thought we already had HDFS-4660, and we kept thinking it's something else, though we saw the symptom here is really same as HDFS-4660. Sorry for the overlook there.\n\nI did an experiment by reverting HDFS-4660 etc, and found your test failed as expected. I created HDFS-10652 for adding this test as a unit test for HDFS-4660. if you don't mind, would you please assign it to yourself, craft it and add comments as needed, and attach new versions there?\n\nThanks a lot!\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-19T07:02:20.330+0000","updated":"2016-07-19T07:03:58.990+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15383690","id":"15383690","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. I did an experiment by reverting HDFS-4660 etc, and found your test failed as expected. \nThat means, this Jira is no longer required investigation right?\nMay be [~xupeng] also can confirm, if his installation had HDFS-4660 then issue might exist. If not, then we can close this Jira as well.\n\nbq.  I created HDFS-10652 for adding this test as a unit test for HDFS-4660. if you don't mind, would you please assign it to yourself, craft it and add comments as needed, and attach new versions there?\nSure. thank you.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-19T07:06:41.069+0000","updated":"2016-07-19T07:06:41.069+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15383692","id":"15383692","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~xupener],\n\nPer [~vinayrpet]'s comment here:\n\nhttps://issues.apache.org/jira/browse/HDFS-10587?focusedCommentId=15378879&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15378879\n\nand my comment above, would you please check whether your release has HDFS-4660, HDFS-9220?\n\nThanks.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-19T07:07:32.202+0000","updated":"2016-07-19T07:07:32.202+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15383695","id":"15383695","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~vinayrpet], you are really fast responding! and our comments collided:-)\n\nlet's wait for [~xupener]'s reply about whether he has HDFS-4660. But can we move ahead with HDFS-10652 asap? \n\nThanks.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-19T07:09:48.531+0000","updated":"2016-07-19T16:04:17.674+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15387045","id":"15387045","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xupener","name":"xupener","key":"xupener","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"xupeng","active":true,"timeZone":"Asia/Shanghai"},"body":"Hi [~yzhangal],  [~vinayrpet]:\n\nSorry for the late replyï¼Œ I checked my installationï¼ˆhadoop-2.6.0-cdh5.4.4ï¼‰ that I don't have HDFS-4660. \n\nSo this means HDFS-4660 can solve these problem discussed above ? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=xupener","name":"xupener","key":"xupener","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"xupeng","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-07-21T02:52:37.160+0000","updated":"2016-07-21T02:52:56.737+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15387083","id":"15387083","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for your reply [~xupener]. \n\nYes, HDFS-4660 + HDFS-9220 would solve these problem, because HDFS-9220 fixed a bug in the HDFS-4660 fix. \n\nI think we should fix the following report\n{quote}\nWhile verifying only packet, the position mentioned in the checksum exception, is relative to packet buffer offset, not the block offset. So 81920 is the offset in the exception.\n{quote}\nto also report the offset in the file, offset in block, and offset in packet.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-21T03:30:07.742+0000","updated":"2016-07-21T03:30:07.742+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12984537/comment/15387220","id":"15387220","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Closing this jira as duplicate of HDFS-4660.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-21T06:02:52.655+0000","updated":"2016-07-21T06:02:52.655+0000"}],"maxResults":43,"total":43,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-10587/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i30a73:"}}