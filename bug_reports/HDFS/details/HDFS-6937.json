{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12736464","self":"https://issues.apache.org/jira/rest/api/2/issue/12736464","key":"HDFS-6937","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-08-25T20:15:38.820+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Aug 02 04:15:32 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6937/watchers","watchCount":18,"isWatching":false},"created":"2014-08-25T06:12:04.617+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326264","id":"12326264","description":"2.5.0 release","name":"2.5.0","archived":false,"released":true,"releaseDate":"2014-08-11"}],"issuelinks":[{"id":"12463048","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12463048","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12731269","key":"HDFS-6804","self":"https://issues.apache.org/jira/rest/api/2/issue/12731269","fields":{"summary":"Add test for race condition between transferring block and appending block causes \"Unexpected checksum mismatch exception\" ","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-08-02T04:15:32.487+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"Given a write pipeline:\n\nDN1 -> DN2 -> DN3\n\nDN3 detected cheksum error and terminate, DN2 truncates its replica to the ACKed size. Then a new pipeline is attempted as\n\nDN1 -> DN2 -> DN4\n\nDN4 detects checksum error again. Later when replaced DN4 with DN5 (and so on), it failed for the same reason. This led to the observation that DN2's data is corrupted. \n\nFound that the software currently truncates DN2's replca to the ACKed size after DN3 terminates. But it doesn't check the correctness of the data already written to disk.\n\nSo intuitively, a solution would be, when downstream DN (DN3 here) found checksum error, propagate this info back to upstream DN (DN2 here), DN2 checks the correctness of the data already written to disk, and truncate the replica to to MIN(correctDataSize, ACKedSize).\n\nFound this issue is similar to what was reported by HDFS-3875, and the truncation at DN2 was actually introduced as part of the HDFS-3875 solution. \n\nFiling this jira for the issue reported here. HDFS-3875 was filed by [~tlipcon]\nand found he proposed something similar there.\n{quote}\nif the tail node in the pipeline detects a checksum error, then it returns a special error code back up the pipeline indicating this (rather than just disconnecting)\nif a non-tail node receives this error code, then it immediately scans its own block on disk (from the beginning up through the last acked length). If it detects a corruption on its local copy, then it should assume that it is the faulty one, rather than the downstream neighbor. If it detects no corruption, then the faulty node is either the downstream mirror or the network link between the two, and the current behavior is reasonable.\n{quote}\n\nThanks.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12806312","id":"12806312","filename":"HDFS-6937.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-05-26T08:05:38.005+0000","size":33349,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12806312/HDFS-6937.001.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12806370","id":"12806370","filename":"HDFS-6937.002.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-05-26T13:46:51.774+0000","size":33214,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12806370/HDFS-6937.002.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Another issue in handling checksum errors in write pipeline","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/14109654","id":"14109654","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cmccabe","name":"cmccabe","key":"cmccabe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cmccabe&avatarId=29060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cmccabe&avatarId=29060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cmccabe&avatarId=29060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cmccabe&avatarId=29060"},"displayName":"Colin P. McCabe","active":true,"timeZone":"America/Los_Angeles"},"body":"I guess the issue here is figuring out which node(s) to kick out of the pipeline when the last datanode detects an error.  We shouldn't always just kick out the last node, since the error might have been introduced by an earlier datanode.  \n\nAt first, you might think that you could simply tell all the nodes in the pipeline to perform a checksum of their un-committed data, and use that to catch the guilty node.  However, one example we've been talking about is a bad network interface that randomly corrupts outgoing packets.  In this case, even if the DN verifies its own buffer and it comes up clean, that DN might still be the problem because of its evil network interface.\n\nThe DFSClient can tell that an error happened between two nodes.  For example, if D1 can verify its checksummed data and D2 cannot, the problem could be either D1 or D2.  But at least we know the problem is not D3.  But the DFSClient can't pinpoint the source of an error immediately.  This can be difficult for us.  We don't get a lot of \"tries\" to find the bad node.  If we kick out too many nodes from the pipeline, we may be left with *only* the bad node in the pipeline.  For example, with a 2-node pipeline, we have a 50% chance of getting it right the first time we kick out a node.\n\nI think ideally we would:\n1. Be able to \"un-kick-out\" nodes.  In the example I gave earlier with the 2-node pipeline, if we discover that kicking out D2 didn't solve the problem, we should be able to get D2 back and kick out D1 instead.  (I haven't looked at the code very closely, so maybe this is already implemented?  But I don't think so)\n\n2. Keep a count of which nodes are associated with problems at the DFSClient, and use this to intelligently kick out nodes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cmccabe","name":"cmccabe","key":"cmccabe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cmccabe&avatarId=29060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cmccabe&avatarId=29060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cmccabe&avatarId=29060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cmccabe&avatarId=29060"},"displayName":"Colin P. McCabe","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-08-25T20:15:38.820+0000","updated":"2014-08-25T20:15:38.820+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/14109808","id":"14109808","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~cmccabe],\n\nThanks for the comments and addition. \n\nAs proposed in the jira report, we need to have a way to communicate the DN3's status to DN2, I think DN3 even can tell DN2 where the checksum error happens, then DN2 can do a checksum checking, if the checksum happens at the same location, that means DN3 is not the culprit, so we should not take out DN3.  If DN2 is good, then DN3 is the culprit and we reconstruct the pipeline without DN3.\n\nIf DN2's data is corrupted, DN2 can do the same by propagating back the checksum error info back to DN1, DN1 then do a checksum checking too and confirm whether itself has good data. If DN1's data is also corrupted, then DN1 is the one to take out. And the client need to reconstruct the pipeline by just throwing away DN1, and keep DN2 and DN3.\n\nIf DN1 is good, then the issue is at DN2.\n\nThis way, we minimize the impact of unnecessarily throwing away good DNs. We need to be aware of that, the corrupted chunks were already acknowledged previously to the client. Client better buffer the data block and be able to rewrite the whole block to the newly constructed pipeline.\n\nWe need to have an infrastructure to communicate back error status to upstream DNs instead of just terminating, so we don't have to incur this run time cost most of the time (assuming checksum error happens infrequently).\n\nI'd prefer working out a functioning solution for the current issue, and have new jira to handle corner cases which were not handled previously.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-08-25T21:51:45.610+0000","updated":"2014-08-25T21:51:45.610+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15018177","id":"15018177","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mikhail.dutikov","name":"mikhail.dutikov","key":"mikhail.dutikov","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mikhail Dutikov","active":true,"timeZone":"Etc/UTC"},"body":"Good day, any update on this? I seem to be running into a similar issue with Hbase WAL based on HDFS 2.6 chd 2.4.8. A pipeline is being reconstructed with many candidate datanodes, but none of the substitutes seems to be receiving the block replica correctly:\n\nChecksum error in block .... from /IP:port\norg.apache.hadoop.fs.ChecksumException: Checksum error: DFSClient_NONMAPREDUCE_1267344484_1 at 2048 exp: -652491368 got: -585724081    [note: checksums are the same on all new candidate nodes]\n\njava.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing .... from /IP:port\n>···at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:562)\n>···at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:780)\n>···at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:783)\n>···at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n>···at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)\n>···at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:243)\n>···at java.lang.Thread.run(Thread.java:745)\n\nThis is repeated until no datanode is left to try, and leads to data loss (?) and terminated Region Servers due to inability to write to WAL. (The cluster has 3 racks and 21 nodes)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mikhail.dutikov","name":"mikhail.dutikov","key":"mikhail.dutikov","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Mikhail Dutikov","active":true,"timeZone":"Etc/UTC"},"created":"2015-11-20T15:46:04.812+0000","updated":"2015-11-20T15:46:04.812+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15301723","id":"15301723","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"I'm proposing a simplified solution here:\n\nInstead of \n{quote}\nSo intuitively, a solution would be, when downstream DN (DN3 here) found checksum error, propagate this info back to upstream DN (DN2 here), DN2 checks the correctness of the data already written to disk, and truncate the replica to to MIN(correctDataSize, ACKedSize).\n{quote}\n\nwhat we can do is, when DN2 find DN3 failed, DN2 simply scan its own replica to check possible corruption, if so, DN2 reports itself as the firstBadLink. \n\nThanks Colin for earlier discussion.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-05-26T08:04:18.058+0000","updated":"2016-05-26T08:04:18.058+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15301733","id":"15301733","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Attached a draft patch that reports\n\n{code}\n2016-05-25 11:35:20,639 [DataStreamer for file /tmp/testClientReportBadBlock/CorruptTwoOutOfThreeReplicas1 block BP-198184347-127.0.0.1-1464201303610:blk_1073741825_1001] WARN  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(1400)) - Error Recovery for BP-198184347-127.0.0.1-1464201303610:blk_1073741825_1001 in pipeline [DatanodeInfoWithStorage[127.0.0.1:54392,DS-d6b01513-ac11-4fdf-99a1-fbb111d0f0c5,DISK], DatanodeInfoWithStorage[127.0.0.1:45555,DS-67174cd1-1f9c-46bc-9dea-8ece7190308d,DISK], DatanodeInfoWithStorage[127.0.0.1:55877,DS-8fe30be4-1244-4059-a567-bc156d49d01a,DISK]]: datanode 0(DatanodeInfoWithStorage[127.0.0.1:54392,DS-d6b01513-ac11-4fdf-99a1-fbb111d0f0c5,DISK]) is bad.\n{code}\nwhere 127.0.0.1:5439 is DN3 the reported example.\n\nWith the fix, we can see\n\n{code}\n2016-05-25 14:15:29,831 [DataXceiver for client DFSClient_NONMAPREDUCE_1623233781_1 at /127.0.0.1:59590 [Receiving block BP-1085730607-127.0.0.1-1464210923866:blk_1073741825_1001]] WARN  datanode.DataNode (DataXceiver.java:determineFirstBadLink(494)) - Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 127.0.0.1:36300, however, the replica on the current Datanode host4:38574 is found to be corrupted, set the firstBadLink to this DataNode.\n{code}\nwhere host4:38574 is DN2 in the example. Thus it's reported bad in Error Recovery message:\n\n{code}\n2016-05-25 14:15:29,833 [DataStreamer for file /tmp/testClientReportBadBlock/CorruptTwoOutOfThreeReplicas1 block BP-1085730607-127.0.0.1-1464210923866:blk_1073741825_1001] WARN  hdfs.DataStreamer (DataStreamer.java:handleBadDatanode(1400)) - Error Recovery for BP-1085730607-127.0.0.1-1464210923866:blk_1073741825_1001 in pipeline [DatanodeInfoWithStorage[127.0.0.1:38574,DS-a743f66a-3379-4a1e-82df-5f6f26815df8,DISK], DatanodeInfoWithStorage[127.0.0.1:38267,DS-32e47236-c29f-435b-995b-f6f4f2a86acc,DISK], DatanodeInfoWithStorage[127.0.0.1:36300,DS-906d823d-0de5-40f1-9409-4c8c6d4edd08,DISK]]: datanode 0(DatanodeInfoWithStorage[127.0.0.1:38574,DS-a743f66a-3379-4a1e-82df-5f6f26815df8,DISK]) is bad.\n{code}\n\nI was able to see the fix constantly succeed in one debug version, and fail when removing the fix. However, before I post the patch now, I observed some intermittency in the unit test env, which is yet to understood. But I'd like to post the patch to let it rolling.\n\nHi [~cmccabe],\n\nWould you please help taking a look? \n\nThanks a lot.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-05-26T08:14:14.181+0000","updated":"2016-05-26T08:14:14.181+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15301992","id":"15301992","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 16s {color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 4 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 47s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 44s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 33s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 51s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 12s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 48s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 12s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 53s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 46s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 46s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 33s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs: patch generated 33 new + 529 unchanged - 4 fixed = 562 total (was 533) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 54s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 11s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 25 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 59s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 2s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 64m 9s {color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 36s {color} | {color:green} Patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 84m 56s {color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.TestDatanodeDeath |\n|   | hadoop.hdfs.server.datanode.TestDiskError |\n|   | hadoop.hdfs.TestDFSClientExcludedNodes |\n|   | hadoop.hdfs.TestPipelineRecovery |\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\n|   | hadoop.hdfs.TestDecommissionWithStriped |\n|   | hadoop.hdfs.TestAsyncDFSRename |\n|   | hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |\n|   | hadoop.hdfs.tools.TestDebugAdmin |\n|   | hadoop.hdfs.TestFileAppend |\n| Timed out junit tests | org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:2c91fd8 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12806312/HDFS-6937.001.patch |\n| JIRA Issue | HDFS-6937 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux cd955b145953 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 77202fa |\n| Default Java | 1.8.0_91 |\n| findbugs | v3.0.0 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/15576/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\n| whitespace | https://builds.apache.org/job/PreCommit-HDFS-Build/15576/artifact/patchprocess/whitespace-eol.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15576/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/15576/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/15576/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/15576/console |\n| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-05-26T12:12:57.226+0000","updated":"2016-05-26T12:12:57.226+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15302078","id":"15302078","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"rev 002 for a better way to get the DNs in a pipeline.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-05-26T13:46:51.780+0000","updated":"2016-05-26T13:46:51.780+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15302230","id":"15302230","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 13s {color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 4 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 5m 59s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 42s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 32s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 49s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 12s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 39s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 4s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 46s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 41s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 41s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 30s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs: patch generated 33 new + 529 unchanged - 4 fixed = 562 total (was 533) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 48s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 9s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 27 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 43s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 2s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 74m 9s {color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 19s {color} | {color:green} Patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 92m 30s {color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.TestPipelineRecovery |\n|   | hadoop.hdfs.TestBlocksScheduledCounter |\n|   | hadoop.hdfs.TestDatanodeDeath |\n|   | hadoop.hdfs.tools.TestDebugAdmin |\n|   | hadoop.hdfs.server.datanode.TestDiskError |\n|   | hadoop.hdfs.TestDFSClientExcludedNodes |\n|   | hadoop.hdfs.TestRollingUpgrade |\n|   | hadoop.hdfs.TestAbandonBlock |\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\n| Timed out junit tests | org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:2c91fd8 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12806370/HDFS-6937.002.patch |\n| JIRA Issue | HDFS-6937 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux fa1b66a08b96 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 77202fa |\n| Default Java | 1.8.0_91 |\n| findbugs | v3.0.0 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/15578/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\n| whitespace | https://builds.apache.org/job/PreCommit-HDFS-Build/15578/artifact/patchprocess/whitespace-eol.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15578/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/15578/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/15578/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/15578/console |\n| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-05-26T15:20:52.619+0000","updated":"2016-05-26T15:20:52.619+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15313007","id":"15313007","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"I am taking over Yongjun's patch because he'll not be able to access Internet for some time.\n\nThis is a great work and I took some time to understand. I think that instead of throwing an IOException to simulate the injection of checksum failure at the last datanode, it should enqueue a ERROR_CHECKSUM to indicate the checksum failure. Without it, the last DN will shutdown the connection, and the second DN in the pipeline will not understand it's checksum failure.\n\n{code:title=BlockReceiver.java#sendAckUpstreamUnprotected}\nif (ack == null) {\n        // A new OOB response is being sent from this node. Regardless of\n        // downstream nodes, reply should contain one reply.\n        replies = new int[] { myHeader };\n      } else if (mirrorError) { // ack read error\n        int h = PipelineAck.combineHeader(datanode.getECN(), Status.SUCCESS);\n        int h1 = PipelineAck.combineHeader(datanode.getECN(), Status.ERROR);\n        replies = new int[] {h, h1};\n      } else {\n        short ackLen = type == PacketResponderType.LAST_IN_PIPELINE ? 0 : ack\n            .getNumOfReplies();\n        replies = new int[ackLen + 1];\n        replies[0] = myHeader;\n        for (int i = 0; i < ackLen; ++i) {\n          replies[i + 1] = ack.getHeaderFlag(i);\n        }\n        // If the mirror has reported that it received a corrupt packet,\n        // do self-destruct to mark myself bad, instead of making the\n        // mirror node bad. The mirror is guaranteed to be good without\n        // corrupt data on disk.\n        if (ackLen > 0 && PipelineAck.getStatusFromHeader(replies[1]) ==\n          Status.ERROR_CHECKSUM) {\n          throw new IOException(\"Shutting down writer and responder \"\n              + \"since the down streams reported the data sent by this \"\n              + \"thread is corrupt\");\n        }\n      }\n{code}\nIn this piece of code, if the next DN shutdown the connection, it is always assumed the local DN is good.\n{code}\n        int h = PipelineAck.combineHeader(datanode.getECN(), Status.SUCCESS);\n        int h1 = PipelineAck.combineHeader(datanode.getECN(), Status.ERROR);\n        replies = new int[] {h, h1};\n{code}\nOn the other hand, if the next DN respond with a ERROR_CHECKSUM, it will thrown an IOException, and this will shutdown the connection with the previous DN in the pipeline. In the end, this will replace the middle datanode:\n\n{code:title=DataStreamer.java#createBlockOutputStream}\n// find the datanode that matches\n        if (firstBadLink.length() != 0) {\n          for (int i = 0; i < nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-02T20:35:34.921+0000","updated":"2016-06-02T20:35:34.921+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15313009","id":"15313009","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"Upload my patch based on Yongjun's version 2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-02T20:36:23.736+0000","updated":"2016-06-02T20:36:23.736+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15313168","id":"15313168","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 11s {color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 4 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 35s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 47s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 32s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 52s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 11s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 45s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 7s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 50s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 44s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 44s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 31s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs: The patch generated 35 new + 529 unchanged - 4 fixed = 564 total (was 533) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 49s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 9s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 23 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 49s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 0s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 63m 51s {color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 31s {color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 83m 32s {color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.TestDatanodeDeath |\n|   | hadoop.hdfs.server.datanode.TestDiskError |\n|   | hadoop.hdfs.TestDFSClientExcludedNodes |\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\n|   | hadoop.hdfs.tools.TestDebugAdmin |\n|   | hadoop.hdfs.TestAbandonBlock |\n|   | hadoop.hdfs.server.datanode.TestDataNodeErasureCodingMetrics |\n| Timed out junit tests | org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:2c91fd8 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12807820/HDFS-6937.003.patch |\n| JIRA Issue | HDFS-6937 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 932d720a33fd 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 97e2449 |\n| Default Java | 1.8.0_91 |\n| findbugs | v3.0.0 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/15637/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\n| whitespace | https://builds.apache.org/job/PreCommit-HDFS-Build/15637/artifact/patchprocess/whitespace-eol.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15637/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/15637/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/15637/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/15637/console |\n| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-06-02T22:02:11.941+0000","updated":"2016-06-02T22:02:11.941+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15353764","id":"15353764","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"So, I think this patch is not valid.\n\nIf there is indeed a checksum error at the middle (second) node of pipeline, the tail node will detect it, sending ERROR_CHECKSUM code back to client and terminate the connection. This should effectively remove the middle node in the pipeline.\n\nIf for some reason the error code is not sent before the connection was terminated, the pipeline recovery will be initiated, transferring the second node's replica to the downstream. Because the replica is corrupt, the destination will terminate the connection. When this happens, the second node will initiate VolumeScanner to check the integrity of local replica, and it should tell NameNode that the replica is bad, and NameNode should have exclude the datanode when client asks to update pipeline. The bug in HDFS-10512 is why it doesn't tell NameNode.\n\nTo sum up, I think this patch is redundant if we can find a better fix for HDFS-10512.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-28T21:50:17.880+0000","updated":"2016-06-28T21:50:17.880+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15355381","id":"15355381","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~jojochuang],\n\nThanks for your continued work here.\n\n{quote}\nIf there is indeed a checksum error at the middle (second) node of pipeline, the tail node will detect it, sending ERROR_CHECKSUM code back to client and terminate the connection. This should effectively remove the middle node in the pipeline.\n{quote}\nAbout the above statement, the initial issue I observed was, when the tail node detects corruption, the implementation will find a replacement DN, and tries to copy from the middle DN to the new DN again. Then checksum error happens again. And this process repeats.\n\nWhy you think \"This should effectively remove the middle node in the pipeline\"?\n\nOn the other hand, I think HDFS-10587 is very relevant here (thanks for the great findings there) and let's dig deeper there. My current thinking is, if HDFS-10587 is fixed, then there is less chance for a replica to get corrupted; however, when the replica on the middle DN does get corrupted, HDFS-6937 would help (waiting for block scanner is not a real solution).\n\nThanks.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-29T14:59:21.385+0000","updated":"2016-06-29T15:00:50.763+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15357681","id":"15357681","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"Remove my patch#3","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-30T19:15:27.543+0000","updated":"2016-06-30T19:15:27.543+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15400633","id":"15400633","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"body":"Thanks for [~yzhangal] and [~jojochuang] thanks for deeper look here...\n\nWe had come across one issue, where write is failed even 7 DN’s are available due to network fault at one datanode which is LAST_IN_PIPELINE.\nScenario : (DN3 has N/W Fault and Min repl=2).\n\nWrite pipeline:\n DN1->DN2->DN3  => DN3 Gives ERROR_CHECKSUM ack. And so DN2 marked as bad\nDN1->DN4-> DN3 => DN3 Gives ERROR_CHECKSUM ack. And so DN4 is marked as bad\n….\nAnd so on ( all the times DN3 is LAST_IN_PIPELINE) ... Continued till no more datanodes to construct the pipeline.\n\nThinking we can handle like below:\n\nInstead of throwing IOException for ERROR_CHECKSUM ack from downstream, If we can send back the pipeline ack and client side we can replace both DN2 and DN3 with new nodes as we can’t decide on which is having network problem.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-07-30T11:06:51.002+0000","updated":"2016-07-30T11:06:51.002+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15400752","id":"15400752","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~brahmareddy],\n\nThanks for reporting the issue you ran into.\n\nIf your problem is really a network issue, then your proposed solution sounds reasonable to me. However, it seems different than what HDFS-6937 intends to solve, and I think we can create a new jira for your issue. Here is why:\n\nHDFS-6937's scenario is that we keep replacing the third node in recovery, and did not detect that the middle node is corrupt. Thus adding a corruption checking for the middle node would solve the issue; In your case, even if we try to check the middle node, it would appear as not corrupt. The problem is that, we don't have a check for network issue (and probably adding a network check may not be feasible here). \n\nOn the other hand, if it's not a network issue, then it could be caused by HDFS-4660, if you don't already have the fix.\n\nHope my explanation makes sense.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-07-30T17:25:25.680+0000","updated":"2016-07-30T17:25:25.680+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12736464/comment/15403313","id":"15403313","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"body":"bq. If your problem is really a network issue, then your proposed solution sounds reasonable to me. However, it seems different than what HDFS-6937 intends to solve, and I think we can create a new jira for your issue. Here is why:\n\nInitially thought of handling with this issue only. Thanks for correction..Raised HDFS-10714 to handle seperately..","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-08-02T04:15:32.487+0000","updated":"2016-08-02T04:15:32.487+0000"}],"maxResults":17,"total":17,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6937/votes","votes":3,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1za93:"}}