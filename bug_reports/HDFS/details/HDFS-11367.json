{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13037706","self":"https://issues.apache.org/jira/rest/api/2/issue/13037706","key":"HDFS-11367","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/5","id":"5","description":"All attempts at reproducing this issue failed, or not enough information was available to reproduce the issue. Reading the code produces no clues as to why this behavior would occur. If more information appears later, please reopen the issue.","name":"Cannot Reproduce"},"customfield_12312322":null,"customfield_12310220":"2017-01-28T15:11:14.463+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Feb 03 02:38:11 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_709602179_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2017-02-02T19:09:02.715+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-11367/watchers","watchCount":3,"isWatching":false},"created":"2017-01-25T14:02:20.565+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326264","id":"12326264","description":"2.5.0 release","name":"2.5.0","archived":false,"released":true,"releaseDate":"2014-08-11"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-02-03T02:44:44.450+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"We have code which creates a file in HDFS and continuously appends lines to the file, then closes the file at the end. This is done by a single dedicated thread.\n\nWe specifically instrumented the code to make sure only one 'client'/thread ever writes to the file because we were seeing \"current leaseholder is trying to recreate file\" errors.\n\nFor some background see this for example: https://community.cloudera.com/t5/Storage-Random-Access-HDFS/How-to-append-files-to-HDFS-with-Java-quot-current-leaseholder/m-p/41369\n\nThis issue is very critical to us as any error terminates a mission critical application in production.\n\nIntermittently, we see the below exception, regardless of what our code is doing which is create the file, keep appending, then close:\n\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): failed to create file /data/records_20170125_1.txt for DFSClient_NONMAPREDUCE_-167421175_1 for client 1XX.2XX.1XX.XXX because current leaseholder is trying to recreate file.\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3075)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2905)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:3189)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:3153)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:612)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.append(AuthorizationProviderProxyClientProtocol.java:125)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:414)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1767)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n \n        at org.apache.hadoop.ipc.Client.call(Client.java:1411)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1364)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy24.append(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:483)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy24.append(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.append(ClientNamenodeProtocolTranslatorPB.java:282)\n        at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1586)\n        at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1626)\n        at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1614)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:313)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:309)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:309)\n        at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)\n        at com.myco.MyAppender.getOutputStream(MyAppender.java:147)","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12849875","id":"12849875","filename":"Appender.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-29T15:26:51.727+0000","size":7496,"mimeType":"text/x-java-source","content":"https://issues.apache.org/jira/secure/attachment/12849875/Appender.java"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"AlreadyBeingCreatedException \"current leaseholder is trying to recreate file\" when trying to append to file","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Red Hat Enterprise Linux Server release 6.8\n","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844089","id":"15844089","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"HDFS supports single writer per file at any given point of time as can be seen in the JIRA https://issues.apache.org/jira/browse/HDFS-8177.\n\nEven if you use a single thread per file and if that thread creates a file and using the returned OutputStream writes data to file then the file will be having an under construction feature and a lease at the time of creating the file which will be cached. If the same thread tries to do an append without closing the OutputStream then the HDFS code finds that the file is underconstruction with a valid lease and doesnot another lease to be created by the same thread. \n\nOnce you get the OutputStream object from your getOutputStream method and once done writing data, close the OutputStream . This will make sure the underconstruction feature of the file and lease is removed. Then calling the  getOutputStream later will result in append and would not result in this kind of error.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-01-28T15:11:14.463+0000","updated":"2017-01-28T15:11:14.463+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844177","id":"15844177","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"This is problematic.\n\nFirst of all, I don't believe HDFS will let us keep opening, appending, and closing. I believe that is what we started with and we kept seeing lease exceptions. Even after closing, HDFS thinks there is a lease on the file, over some kind of a lease period.\n\nThe point of caching a writer in memory per each file was exactly this: to work around the lease exceptions.\n\nSecond problem is performance.  It seems really inefficient to keep reopening, appending, and closing the file on each append.  Imagine adding 10 million records to the file. How 'efficient' would that be?\n\nThe third problem is also relevant here. Which is, what happens if the client crashes and never closes the stream. For example, http://stackoverflow.com/questions/23833318/crashed-hdfs-client-how-to-close-remaining-open-files,.  \n\nSomething is not congealing in this model and needs to be given more thought.\n\nI feel like the design on the HDFS side should be:\n\n1. For each file, keep track of any current appenders; allow for more than one. Implement locking whereby the sequence of appends from potentially multiple clients is regulated by HDFS itself. Each append should be synchronized by a lock, then an unlock.\n2. Get rid of leases. This needs to work the same way an append would work on a local file system's file. As an HDFS API user, why should I care about some internals that prevent me from using the API?\n3. If there needs to be some kind of grace period during which HDFS needs to propagate the append among the nodes in the cluster, block the client performing the append until the append is completely done on the HDFS server side.\n4. Associate a timeout with every client appending to the file. If the client has exceeded the timeout interval and not issued a 'close' call, issue the close internally and consider this client's appends done.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-28T20:01:00.343+0000","updated":"2017-01-28T20:01:00.343+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844179","id":"15844179","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"OK, I'm not sure where I could find better documentation on this design but I'm going by this source: http://www.aosabook.org/en/hdfs.html\n\nRead this passage:\n\n\"The HDFS client that opens a file for writing is granted a lease for the file; no other client can write to the file. The writing client periodically renews the lease by sending a heartbeat to the NameNode. When the file is closed, the lease is revoked. The lease duration is bound by a soft limit and a hard limit. Until the soft limit expires, the writer is certain of exclusive access to the file. If the soft limit expires and the client fails to close the file or renew the lease, another client can preempt the lease. If after the hard limit expires (one hour) and the client has failed to renew the lease, HDFS assumes that the client has quit and will automatically close the file on behalf of the writer, and recover the lease. The writer's lease does not prevent other clients from reading the file; a file may have many concurrent readers.\"\n\nThis is very difficult to work with, from the client's perspective.  How can a client side application structure itself to allow for many appends to a given file to occur safely over an indefinite period of time?\n\nThis cannot be parallelized because HDFS is only allowing one writer.  And as we know, one writer still causes AlreadyBeingCreatedException! Already being created by whom?? There's one thread writing.\n\nAnd if I want multiple clients writing, I have to wait for the soft limit to expire. And how do I know what this soft limit is, and how to cause my other writers to safely wait for it?  And, I can't wait for it.  The application needs to continuously append to the file and not wait around.  I simply don't see how this can be used in any kind of production scenario.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-28T20:08:26.503+0000","updated":"2017-01-28T20:08:26.503+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844352","id":"15844352","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"{quote}\nFirst of all, I don't believe HDFS will let us keep opening, appending, and closing.\n{quote}\nHDFS does allow you to open (create) , append and close. You can use the OutputStream object got from the return of create method and keep writing to the OutputStream from either single or multiple threads but all should refer to the same OutputStream object. By default a write does an append to the file . \n\n{quote}\nSecond problem is performance.\n{quote}\nI agree but HDFS is designed to be more for write once read many times kind of applications. However you can still use HDFS to stream multiple events by using additional components such as Flume, Kafka, Spark streaming depending on your use case. Please read the paragraph starting with line in the below link. :- \"Here we’ll address a different topic: As is well-known, HDFS is subject to the small-files problem\" \n[https://blog.cloudera.com/blog/2016/07/how-to-ingest-email-into-apache-hadoop-in-real-time-for-analysis/]\n\n{quote}\nWhich is, what happens if the client crashes and never closes the stream. \n{quote}\nThe client should ideally close the stream. You can even register a shutdownhook as mentioned in one of the comment in the link to handle graceful shutdown scenarios to close the stream.\n\n{quote}\n How can a client side application structure itself to allow for many appends to a given file to occur safely over an indefinite period of time?\n{quote}\nYes its difficult but the above approach in the link could be used. Also as mentioned earlier if you see too many appends happening continuously to a file its better to cache the OutputStreams for files got during create of file into a sort of a time based LRU cache or a guava cache which would expire your cache of OutputStreams after say every 60 seconds of non usage and also make sure that the close method is called before the OutputStream is evicted from cache. \n\n{quote}\nThis cannot be parallelized because HDFS is only allowing one writer. And as we know, one writer still causes AlreadyBeingCreatedException!\nAnd if I want multiple clients writing, I have to wait for the soft limit to expire. \n{quote}\nAs mentioned earlier the OutputStream returned from create method of FileSystem can be used by either a single thread or multiple threads to do append by calling write on the OutputStream. If the intention is to append to existing file then write method on OutputStream will take care of it. Calling append on FileSystem before closing the OutputStream returned from create can cause AlreadyBeingCreatedException to be thrown. \n\n{quote}\nAnd how do I know what this soft limit is\n{quote}\nI am listing down the comments from hadoop source code which should make things clear about soft and hard limit\n{code}\n/**\n   * For a HDFS client to write to a file, a lease is granted; During the lease\n   * period, no other client can write to the file. The writing client can\n   * periodically renew the lease. When the file is closed, the lease is\n   * revoked. The lease duration is bound by this soft limit and a\n   * {@link HdfsConstants#LEASE_HARDLIMIT_PERIOD hard limit}. Until the\n   * soft limit expires, the writer has sole write access to the file. If the\n   * soft limit expires and the client fails to close the file or renew the\n   * lease, another client can preempt the lease.\n   */\n  public static final long LEASE_SOFTLIMIT_PERIOD = 60 * 1000;\n  /**\n   * For a HDFS client to write to a file, a lease is granted; During the lease\n   * period, no other client can write to the file. The writing client can\n   * periodically renew the lease. When the file is closed, the lease is\n   * revoked. The lease duration is bound by a\n   * {@link HdfsConstants#LEASE_SOFTLIMIT_PERIOD soft limit} and this hard\n   * limit. If after the hard limit expires and the client has failed to renew\n   * the lease, HDFS assumes that the client has quit and will automatically\n   * close the file on behalf of the writer, and recover the lease.\n   */\n  public static final long LEASE_HARDLIMIT_PERIOD = 60 * LEASE_SOFTLIMIT_PERIOD;\n{code}\n\nI hope most of your doubts are covered if not please let us know.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-01-29T10:26:49.638+0000","updated":"2017-01-29T10:26:49.638+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844468","id":"15844468","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"{quote}\nHDFS does allow you to open (create) , append and close. You can use the OutputStream object got from the return of create method and keep writing to the OutputStream from either single or multiple threads but all should refer to the same OutputStream object.\n{quote}\nThat is the setup I was describing. In our case, we have one thread using a dedicated OutputStream. And with that setup, we get intermittent AlreadyBeingCreatedException. This is exactly the subject of the ticket, Manjunath.\n\n{quote}\nBy default a write does an append to the file .\n{quote}\nI don't believe this is correct. FileSystem.create states that files are overwritten by default. So by default a write does a write, not an append.\n\n{quote}\nI agree but HDFS is designed to be more for write once read many times kind of applications.\n{quote}\nThis is not addressing the subject of the ticket that I filed. I am asking for an enhancement or a set of enhancements that would allow applications like ours safely perform potentially many appends, potentially under heavy load. If this is not supported, it should be so documented. If there is a best practice for how to do something like this, this best practice should be published, best if it's published within the HDFS API javadoc as a code sample.\n\n{quote}\nThe client should ideally close the stream.\n{quote}\nWe do close the stream when all the appends are done. This comment is not helping resolve the ticket that I filed.\n\n{quote}\nYou can even register a shutdownhook as mentioned in one of the comment in the link to handle graceful shutdown scenarios to close the stream.\n{quote}\nShutdown hooks are notorious in Java for not always firing properly. Secondly and more importantly, if your application is terminated via a kill -9, you're not going to get the shutdown hook to fire. Not a relevant comment here.\n\n{quote}\nusing additional components such as Flume, Kafka, Spark streaming depending on your use case\n{quote}\nWe're developing a solution that is complimentary to all the technologies you listed. Listing these technologies is not helping resolve the ticket that I filed.\n\n{quote}\nhttps://blog.cloudera.com/blog/2016/07/how-to-ingest-email-into-apache-hadoop-in-real-time-for-analysis/\n{quote}\nA link about Apache James? Completely irrelevant for the solution we're developing.\n\n{quote}\nWhen the file is closed, the lease is revoked.\n{quote}\nI don't feel like that is accurate. If you try to reopen a file and append to it, you will get \"current leaseholder is trying to recreate file\". Sounds like this contract is not being adhered to.\n\nI'm going to try and attach a code snippet which outlines more or less how we do appends. Maybe reviewing the code will reveal the issues.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-29T15:25:59.643+0000","updated":"2017-01-29T15:25:59.643+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844469","id":"15844469","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"Code snippet which outlines how we do the appends.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-29T15:26:51.740+0000","updated":"2017-01-29T15:26:51.740+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844631","id":"15844631","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"{quote}\nAnd with that setup, we get intermittent AlreadyBeingCreatedException.\n{quote}\nI have tried running the code you shared for about a 100,000 appends to an exisiting file a couple of times without any Exceptions  (I did increase the timeout and ran it with 10 threads in executor pool for simplicity) . If you get intermittent AlreadyBeingCreatedException then debug through the logs to see if the scenario which I mentioned which will cause this exception to be thrown is happening which is an existing OutputStream is not closed and an create or append operation is called for a given file\n\n{quote}\nI don't believe this is correct. FileSystem.create states that files are overwritten by default. So by default a write does a write, not an append.\n{quote}\nI have tested this with your code and every time a write is called on OutputStream, it appends to the contents of file and doesnot overwrite it. Logically as well if that didnot happen then the write in your call method will not work.\n\n{quote}\nThis is not addressing the subject of the ticket that I filed. I am asking for an enhancement or a set of enhancements that would allow applications like ours safely perform potentially many appends, potentially under heavy load. If this is not supported, it should be so documented. If there is a best practice for how to do something like this, this best practice should be published, best if it's published within the HDFS API javadoc as a code sample.\n{quote}\nI didnot say it addresses your problem but it was an explaination of the system design and behaviour. As per my understanding goes any API docs wouldnot mention all use cases and list down whats not supported. And as I mentioned in my comments there are several workarounds for performance which was your concern and depends on your use case which would suit best or not\n\nMy response regarding shutdownhook was in response to your comment as to how to address the open file issue. If client crashes yes per my understanding there is nothing which can be done. shutdownhook is one way for handling graceful jvm shutdown scenarios. Whether its notorious or doesnot fit your case I cannot comment\n\n{quote}\nI don't feel like that is accurate. If you try to reopen a file and append to it, you will get \"current leaseholder is trying to recreate file\". Sounds like this contract is not being adhered to.\n{quote}\nTry it out for yourself by adding the below 2 lines within the try block after call to future.get(....)\n{code}\n         writer.close();\n         writerMap.remove(outputFile.toUri().getPath());\n{code}\nThis will result in calling \"Appending to file\" in your getOutputStream method if you passed append as true and works without exception as the lease will be revoked upon call of close. \n\nI want to bring one more thing to your notice which is the excerpt from the link which you shared. Read the line \"After data are written to an HDFS file, HDFS does not provide any guarantee that data are visible to a new reader until the file is closed. \" from the link [http://www.aosabook.org/en/hdfs.html]. I dont see you doing a close on the OutputStream except during shutdown of executor. This would mean data written is not guaranteed to be seen for reads.\n\nI want to put a disclaimer here that all I am trying to do is to explain the behaviour of the current hadoop architecture and why what you stated is not a bug. I do understand that you need an enhancement in the architecture to support your use case in which case this should be an \"Improvement\" rather than a \"Bug\".","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-01-29T20:15:03.987+0000","updated":"2017-01-29T20:15:03.987+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844801","id":"15844801","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"{quote}\n writer.close();\nwriterMap.remove(outputFile.toUri().getPath())\n{quote}\nPerhaps this works. However, I cannot see how an open file-append a line-close file on 1 mln lines in a file would perform. Also I'm not entirely convinced this would never throw this \"lease\" exception or something like it again.\n\nThe code I quoted is not doing anything out of the ordinary, it's just trying to instrument a way to continuously append lines to files. In this sense, having an *intermittent* exception AlreadyBeingCreatedException with the message \"current leaseholder is trying to recreate file\" exhibits itself as a bug.  It's a bug in the sense the word bug is defined by Wikipedia:\n\n{quote}\nA software bug is an error, flaw, failure or fault in a computer program or system that causes it to produce an incorrect or unexpected result, or to behave in unintended ways.\n{quote}\n\nThe results we're seeing are unexpected and possibly unintended due to the intermittent nature of the behavior.  When one line is appended after another, I can't see how that can mean that the current leaseholder is trying to recreate the file. The current lease holder is trying to append a line.  So the expectation is that the line would be appended, not that the caller gets an unexpected exception.\n\nPerhaps the ticket should be reworded as a consequent feature enhancement request. Which is that HDFS should be able to handle the use-case of a client application opening a file and continuously appending segments of content to it one after another.  However, one would expect that such an application of an append would \"just work\" in a file system.  Because, why wouldn't it?  Therefore, I filed this as a bug.\n\nI wonder if one solution would be to try and periodically re-aquire the lease? Maybe that's what's missing. Not sure whether this is the case.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-30T03:23:32.567+0000","updated":"2017-01-30T03:24:36.680+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15844882","id":"15844882","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"{quote}\nHowever, I cannot see how an open file-append a line-close file on 1 mln lines in a file would perform.\n{quote}\nYes closing OutputStream and doing an append on file for large volume of lines will be a performance hit. However as per my previous comment inorder for the readers to see the data close is a necessity . So now it depends on you when you decide to close either on write of each line which will be slow or after write of a bunch of lines for better performance and also to make sure readers are able to see the writes.\n\nAbout the intermittent exception you are getting again am reiterating my earlier comment, please check if you are closing the Outputstream for a file before calling an append on it. Add additional logs and debug to see if this scenario is happening. I tried running your code for 1mln file appends with 100 threads this time with a very extended lineAppendTimeoutMs so that the executor doesnot get timedout. And running this way a couple of times even for 1 mln appends I dont see any exception.\n\nAlso I would like you to try out the below change in your code to see if you are still getting intermittent exception after incorporating the below code:-\n{code}\nprivate Map<String, FSDataOutputStream> fsOutputStreamMap = new HashMap<String, FSDataOutputStream>();\npublic void submitLineAppend(\n                String fileName,\n                String dataType,\n                String line,\n                FileSystem fs,\n                Path outputFile,\n                long lineAppendTimeoutMs) throws IOException {\n\n            BufferedWriter writer = null;\n            FSDataOutputStream opStream = null;\n            synchronized (writerMap) {\n                writer = writerMap.get(outputFile.toUri().getPath());\n                opStream = fsOutputStreamMap.get(outputFile.toUri().getPath());\n                if (writer == null) {\n                    if (fs.mkdirs(outputFile.getParent())) {\n                        log.debug(\"Created dir: {\" + outputFile.getParent() + \"}\");\n                    }\n                    opStream = (FSDataOutputStream)getOutputStream(fs, outputFile, true);\n                    writer = new BufferedWriter(new OutputStreamWriter(opStream));\n                    writerMap.put(outputFile.toUri().getPath(), writer);\n                    fsOutputStreamMap.put(outputFile.toUri().getPath(), opStream);\n                }\n            }\n\n            Future<Void> future = executor.submit(new org.apache.hadoop.hdfs.MyAppender.CallableLineAppender(dataType, writer, line, outputFile));\n\n            try {\n                future.get(lineAppendTimeoutMs, TimeUnit.MILLISECONDS);\n                //writer.close();\n                //writerMap.remove(outputFile.toUri().getPath());\n            } catch (Exception ex) {\n                synchronized (writerMap) {\n                    opStream.close();\n                    fsOutputStreamMap.remove(outputFile.toUri().getPath());\n                    writer.close();\n                    writerMap.remove(outputFile.toUri().getPath());\n                }\n\n                String message = String.format(\"Error while writing line for file %s\", fileName);\n                log.error(message, ex);\n\n                throw new IOException(message, ex);\n            }\n        }\n{code}\n\nThis is to make sure that the scenario which I am referring to which is the hadoop fs OutputStream not being closed properly by single or multi thread and meanwhile an append is called which would result in the lease exception. I am asking you to try as I am not getting the exception. Please try this change in your code and let me know if you still see the intermitten exception","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-01-30T07:10:21.126+0000","updated":"2017-01-30T07:11:25.566+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15845295","id":"15845295","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"We're going in rounds here. Our use-case is for us to be able to continuously do appends, indefinitely, then close the stream when the input has been processed in its entirety.  And we *do* close.  We care a lot less about the readers. We need one read to follow all the writes, which is when the file that was being appended to, gets moved to its next destination.\n\n{quote}\ncheck if you are closing the Outputstream for a file before calling an append on it.\n{quote}\nI am reiterating that we do not call the close before calling append. Nor do we want to. We want to keep the stream open, append a bunch of data, *then* close. I believe we have tested the case of open/append/close, and it was problematic, with exceptions, not just expensive to do.\n\nYour proposed change can be \"tried out\" but this is not what we want to do. You're proposing an open/append/close which is problematic.\n\nI think this ticket needs to be converted to an enhancement request to support the use-case of continuous, multiple appends, with a subsequent close. I'd be OK with this course of action, unless the decision is to not implement the enhancement, for one \"good\" reason or another. I see too many tickets in open source libraries which get voted down for no reason.\n\nAs far as a workaround for us, we're sticking with writing all appends to local copies of the files, then doing a one fell swoop copy of the files from the local file system to HDFS.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-30T14:58:08.218+0000","updated":"2017-01-30T14:59:28.397+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15845752","id":"15845752","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"Yes I agree we are going rounds here and I am not sure if I have to explain better. Let me try to summarize one last time:-\n\nFileSystem(fs) supports open, create and append methods. open is for read purpose so lets not use that word for now. \n\nfs.create and fs.append methods both work on lease for a file and both return OutputStream object on which calling write method allows user to write data to file. If on the same OutputStream object multiple lines either by *single* or *multiple threads* call write method to write a line, the line gets *appended* to the file and not overwritten. \n\nfs.create starts an exclusive lease for the client thread and HDFS caches the lease and expires it and closes the client if the client doesnot write to the file( using the OutputStream object got from calling fs.create) within specified hard limit expiry.\n\nfs.append checks for the lease and if the lease which is cached by the call made earlier to fs.create is still in cache then hadoop throws AlreadyBeingCreatedException. Note that fs.append is not the only way to append lines to a file. As mentioned above calling write on the OutputStream returned by fs.create will also append the lines to file.\n\noutputstream.close method releases the lease and clears it from cache and makes the file contents visible to the readers. without calling this hdfs doesnt guarantee read visibility of the written data to readers.\n\nSo all you need to make sure to not get the AlreadyBeingCreatedException is to make sure that OutputStream from either fs.create or fs.append is used and not both. Your use-case of being able to continuously do appends, indefinitely, then close the stream when the input has been processed in its entirety is supported in HDFS and your code does that and works fine when I run it multiple times with a million appends to a file.\n\nComing to intermittent AlreadyBeingCreatedException I am reiterating my earlier comments I didnot encounter this with million line appends to a file. I suspect you are getting some other exceptions when you are doing a future.get(..) possibly a timeout exception due to lesser timeout value and somehow I feel OutputStream which you had cached using the BufferedWriter wrapper object is not closed properly although you are closing the BufferedWriter. I hope you noticed the difference. I came to this conclusion when I saw through the code of close method of BufferedWriter and I felt there is a window of possibility that the wrapped OutputStream close is not called. This if true will explain the intermittent nature of AlreadyBeingCreatedException.\n\nThe code which I gave you is exactly to try this and it doesnot do a append and close always but only on exceptions which is fine and doesnot impact performance. \n\nApart from this more insight into the debug logs to see when AlreadyBeingCreatedException is thrown will be helpful.\n\nRegarding calling close on the OutputStream the choice is yours but all I wanted is to highlight that without close of Outputstream, readers cannot see the written data.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-01-30T19:37:02.160+0000","updated":"2017-01-30T19:50:29.418+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15846042","id":"15846042","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"Manjunath,\n\nThanks for the explanations.\n\nIt's an interesting theory that a timeout may have happened after which we've closed the stream. And then soon thereafter tried to open and append, maybe, causing the AlreadyBeingCreatedException.\n\nIn theory, our code is structured in such a way that as soon as any error occurs, we \"close shop\" and bubble up the error.  But perhaps another thread is able to squeeze itself in and try appending to a closed file.  I think if this is the case, one workaround can be to mark this particular filename in the writerMap as \"busted\", and cause the code to create a new, unique filename.  In theory, again, we shouldn't have to do that since we're trying to terminate immediately upon getting any error.\n\nUnfortunately, I won't be able to look at the logs in more detail, as they're in our Prod environment which is currently inaccessible.\n\n{quote}\nAs mentioned above calling write on the OutputStream returned by fs.create will also append the lines to file.\n{quote}\nI think this is only true if you call create with overwrite=false.\n\n{quote}\nyour code does that and works fine when I run it multiple times with a million appends to a file.\n{quote}\nThat's good news. Thank you for the thorough code review.\n\nWe have increased the timeout value of the line append. However, we've also by now migrated the code to doing appends to files on the local file system then moving these temp files to HDFS fully.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-30T22:25:24.145+0000","updated":"2017-01-30T22:25:24.145+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15846392","id":"15846392","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"Yes I agree calling fs.create multiple times will cause an overwrite of file if overwrite=false is not supplied but I was referring to calling fs.create once if the file doesnt exist as per your code and then using the OutputStream from the method to keep writing by write method which does append to the file. \n\nFor your use case the Appender code fully supports the append of lines to files and I dont see a reason for intermittent AlreadyBeingCreatedException (apart from the theory I presented above which may not be true)  and I was not able to reproduce as well with the code you shared . I would request you to get access to prod logs and debug more to be able to come to a conclusion on the reason for Exception and also share the code that was responsible for this exception in prod if its different than what you shared. On this note I want to mark this jira as \"Not Reproducible\" if you agree. You can reopen at any time later if you have some findings on what I mentioned.\n\nAlso to the Appender code you can just write a readFromFile method which synchronizes on the writerMap and closes the writer (to make sure writes are visible to readers) and removes it from cache and use the filesystem open method to get InputStream to read the contents. This should work for your scenario of very low reads compared to writes and would be performance issue only if reads are more.\n\n{quote}\nHowever, we've also by now migrated the code to doing appends to files on the local file system then moving these temp files to HDFS fully.\n{quote}\n\nThe code which you shared should work in production without exception but if you already incorporated doing merge in local file system then that would be having a performance edge versus not doing so.\n\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-01-31T04:49:56.345+0000","updated":"2017-01-31T04:57:11.066+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15846888","id":"15846888","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"Thanks, Manjunath. The fact that appending to local files then a write to HDFS is more performant is another important factor here (will verify by testing). My conclusions so far are that:\n\na) There needs to be a best practice / wiki document on best practices of how to properly instrument writing to files and appending to files, using the HDFS API. I am not seeing such a document out there. I would think that the use-case of continuous appends should be elaborated on.\nb) The whole lease based approach is still not entirely clear to me. It needs to be spelled out in more detail. For now, the statement \"When the file is closed, the lease is revoked\" is leaving me unconvinced. The guarantees around the file lease approach need to be spelled out and maybe demonstrated in unit tests.\n\nI think it may be OK to mark this ticket as \"not reproducible\". I hope it serves as a source of info for other folks until the a) and b) points are reconciled on the doc side.\n\nWould you recommend that I file two enhancement requests for documentation of a) and b)?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-01-31T14:17:55.833+0000","updated":"2017-01-31T14:18:59.401+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15850339","id":"15850339","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"Sure Dmitry I think its a good idea to have a best pratice/wiki document for HDFS API. \n\nAs per the below stacktrace, we can see that on say TimeoutException in the submitLineAppend method, when writer.close() is called, the LeaseManager.removeLease is inturn called which removes the cached lease.\n\n{code}\n\"main@1\" prio=5 tid=0x1 nid=NA waiting\n  java.lang.Thread.State: WAITING\n\t  at java.lang.Object.wait(Object.java:-1)\n\t  at java.lang.Object.wait(Object.java:502)\n\t  at org.apache.hadoop.ipc.Client.call(Client.java:1397)\n\t  at org.apache.hadoop.ipc.Client.call(Client.java:1364)\n\t  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\t  at com.sun.proxy.$Proxy15.complete(Unknown Source:-1)\n\t  at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1)\n\t  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t  at java.lang.reflect.Method.invoke(Method.java:497)\n\t  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n\t  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\t  at com.sun.proxy.$Proxy15.complete(Unknown Source:-1)\n\t  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)\n\t  at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2136)\n\t  at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)\n\t  - locked <0x11b6> (a org.apache.hadoop.hdfs.DFSOutputStream)\n\t  at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:74)\n\t  at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:108)\n\t  at sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\t  at sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\t  - locked <0x11b4> (a java.io.OutputStreamWriter)\n\t  at java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\t  at java.io.BufferedWriter.close(BufferedWriter.java:266)\n\t  at org.apache.hadoop.hdfs.MyAppender.submitLineAppend(MyAppender.java:71)\n\t  - locked <0x11a9> (a java.util.HashMap)\n\t  at org.apache.hadoop.hdfs.MyAppender.main(MyAppender.java:220)\n\n\"IPC Server handler 4 on 40660@3266\" daemon prio=5 tid=0x28 nid=NA runnable\n  java.lang.Thread.State: RUNNABLE\n\t  at org.apache.hadoop.hdfs.server.namenode.LeaseManager.removeLease(LeaseManager.java:146)\n\t  - locked <0x10f8> (a org.apache.hadoop.hdfs.server.namenode.LeaseManager)\n\t  at org.apache.hadoop.hdfs.server.namenode.LeaseManager.removeLease(LeaseManager.java:158)\n\t  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.finalizeINodeFileUnderConstruction(FSNamesystem.java:4204)\n\t  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3210)\n\t  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3141)\n\t  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:665)\n\t  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:500)\n\t  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:-1)\n\t  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n\t  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\t  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\t  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\t  at java.security.AccessController.doPrivileged(AccessController.java:-1)\n\t  at javax.security.auth.Subject.doAs(Subject.java:422)\n\t  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n\t  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2009)\n{code} ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-02-02T19:06:40.287+0000","updated":"2017-02-02T19:10:29.649+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15850344","id":"15850344","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"As per the discussion since the exception is not reproducible even for a million appends to same file even after running multiple times, marking this JIRA as \"Cannot Reproduce\" for now. Please re-open in case of any findings.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-02-02T19:08:21.875+0000","updated":"2017-02-02T19:09:35.020+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15850506","id":"15850506","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"body":"{quote}\nI think its a good idea to have a best pratice/wiki document for HDFS API.\n{quote}\n\nHi Manjunath,\n\nHas that been filed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dgoldenberg123","name":"dgoldenberg123","key":"dgoldenberg123","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Goldenberg","active":true,"timeZone":"America/New_York"},"created":"2017-02-02T21:12:14.858+0000","updated":"2017-02-02T21:12:14.858+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13037706/comment/15850960","id":"15850960","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi Dmitry no I didnt. If you dont mind can you please file it. I thought you would be better positioned to put in all the points that are needed to make documentation better. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manju_hadoop","name":"manju_hadoop","key":"manju_hadoop","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manjunath Anand","active":true,"timeZone":"Asia/Kolkata"},"created":"2017-02-03T02:38:11.237+0000","updated":"2017-02-03T02:44:44.441+0000"}],"maxResults":18,"total":18,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-11367/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i396sv:"}}