{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12407296","self":"https://issues.apache.org/jira/rest/api/2/issue/12407296","key":"HDFS-198","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2008-12-22T21:31:50.358+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jan 30 00:09:39 UTC 2014","customfield_12310420":"16653","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_59460149317_*|*_5_*:*_2_*:*_76978187_*|*_4_*:*_1_*:*_105602928067","customfield_12312321":null,"resolutiondate":"2014-01-21T02:28:40.152+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-198/watchers","watchCount":14,"isWatching":false},"created":"2008-10-27T18:14:24.641+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-01-30T00:09:39.279+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12312926","id":"12312926","name":"namenode"}],"timeoriginalestimate":null,"description":"Many long running cpu intensive map tasks failed due to org.apache.hadoop.dfs.LeaseExpiredException.\nSee [a comment below|https://issues.apache.org/jira/browse/HDFS-198?focusedCommentId=12910298&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12910298] for the exceptions from the log:\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"108136","customfield_12312823":null,"summary":"org.apache.hadoop.dfs.LeaseExpiredException during dfs write","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=runping","name":"runping","key":"runping","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Runping Qi","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12658670","id":"12658670","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwensel","name":"cwensel","key":"cwensel","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris K Wensel","active":true,"timeZone":"America/Los_Angeles"},"body":"Has a workaround for this been found?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwensel","name":"cwensel","key":"cwensel","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris K Wensel","active":true,"timeZone":"America/Los_Angeles"},"created":"2008-12-22T21:31:50.358+0000","updated":"2008-12-22T21:31:50.358+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12909932","id":"12909932","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"I believe this issue went stale.  Closing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-09-15T22:56:53.925+0000","updated":"2010-09-15T22:56:53.925+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12910247","id":"12910247","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brondsem","name":"brondsem","key":"brondsem","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dave Brondsema","active":true,"timeZone":"America/New_York"},"body":"I'm getting this error, with a new job we're working on (hive query, with lots of dynamic partitions).  We're using Hadoop 0.18.3-14.cloudera.CH0_3  Do you think a newer version will fix this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brondsem","name":"brondsem","key":"brondsem","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dave Brondsema","active":true,"timeZone":"America/New_York"},"created":"2010-09-16T17:52:07.972+0000","updated":"2010-09-16T17:52:07.972+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12910250","id":"12910250","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Not sure this issue is actually stale.  I think it can still occur if a host running a TT gets heavily loaded, it can take a while between when the task is killed and when it actually dies so a file can disappear out from under from the client. I think this can also be hit if a single key takes so long to sort and merge that the task doesn't check in for over 10 minutes.  ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-09-16T18:07:54.408+0000","updated":"2010-09-16T18:07:54.408+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12910298","id":"12910298","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"If this is still a problem, let's reopen it.  Thanks Dave and Eli for pointing it out.\n\nCopying the stack traces from the original description in order to make it shorter..\n{noformat}\n2008-10-26 11:54:17,282 INFO org.apache.hadoop.dfs.DFSClient: org.apache.hadoop.ipc.RemoteException:\n org.apache.hadoop.dfs.LeaseExpiredException:\n No lease on /xxx/_temporary/_task_200810232126_0001_m_000033_0/part-00033\n File does not exist. [Lease. Holder: 44 46 53 43 6c 69 65 6e 74 5f 74 61 73 6b 5f 32 30 30 38\n 31 30 32 33 32 31 32 36 5f 30 30 30 31 5f 6d 5f 30 30 30 30 33 33 5f 30, heldlocks: 0, pendingcreates: 1]\nat org.apache.hadoop.dfs.FSNamesystem.checkLease(FSNamesystem.java:1194)\nat org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1125)\nat org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:300)\nat sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)\n\nat org.apache.hadoop.ipc.Client.call(Client.java:557)\nat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212)\nat org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\nat org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2335)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2220)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1700(DFSClient.java:1702)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1842)\n\n2008-10-26 11:54:17,282 WARN org.apache.hadoop.dfs.DFSClient: NotReplicatedYetException\n sleeping /xxx/_temporary/_task_200810232126_0001_m_000033_0/part-00033 retries left 2\n2008-10-26 11:54:18,886 INFO org.apache.hadoop.dfs.DFSClient: org.apache.hadoop.ipc.RemoteException:\n org.apache.hadoop.dfs.LeaseExpiredException:\n No lease on /xxx/_temporary/_task_200810232126_0001_m_000033_0/part-00033\n File does not exist. [Lease. Holder: 44 46 53 43 6c 69 65 6e 74 5f 74 61 73 6b 5f 32 30 30 38\n 31 30 32 33 32 31 32 36 5f 30 30 30 31 5f 6d 5f 30 30 30 30 33 33 5f 30, heldlocks: 0, pendingcreates: 1]\nat org.apache.hadoop.dfs.FSNamesystem.checkLease(FSNamesystem.java:1194)\nat org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1125)\nat org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:300)\nat sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)\n\nat org.apache.hadoop.ipc.Client.call(Client.java:557)\nat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212)\nat org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\nat org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2335)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2220)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1700(DFSClient.java:1702)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1842)\n\n2008-10-26 11:54:18,886 WARN org.apache.hadoop.dfs.DFSClient: NotReplicatedYetException\n sleeping /xxx/_temporary/_task_200810232126_0001_m_000033_0/part-00033 retries left 1\n2008-10-26 11:54:22,090 WARN org.apache.hadoop.dfs.DFSClient: DataStreamer Exception:\n org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.dfs.LeaseExpiredException:\n No lease on /xxx/_temporary/_task_200810232126_0001_m_000033_0/part-00033\n File does not exist. [Lease. Holder: 44 46 53 43 6c 69 65 6e 74 5f 74 61 73 6b 5f 32 30 30 38\n 31 30 32 33 32 31 32 36 5f 30 30 30 31 5f 6d 5f 30 30 30 30 33 33 5f 30, heldlocks: 0, pendingcreates: 1]\nat org.apache.hadoop.dfs.FSNamesystem.checkLease(FSNamesystem.java:1194)\nat org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1125)\nat org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:300)\nat sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)\n\nat org.apache.hadoop.ipc.Client.call(Client.java:557)\nat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212)\nat org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\nat org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2335)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2220)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1700(DFSClient.java:1702)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1842)\n\n2008-10-26 11:54:22,090 WARN org.apache.hadoop.dfs.DFSClient: Error Recovery for block null bad datanode[0]\n2008-10-26 11:54:22,219 WARN org.apache.hadoop.mapred.TaskTracker: Error running child\njava.io.IOException: Could not get block locations. Aborting...\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2081)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1300(DFSClient.java:1702)\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1818)\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-09-16T20:19:52.125+0000","updated":"2010-09-16T20:19:52.125+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12915374","id":"12915374","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":" org.apache.hadoop.dfs.LeaseExpiredException occurred in 0.18 might have been caused by this bug:  https://issues.apache.org/jira/browse/HADOOP-6498. The lease renewal thread might hang on lease renew RPC.\n\nEli's description did not convince me. By default, a hard lease limit is 1 hour. 10 minutes delay to renew lease won't cause a lease to expire.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2010-09-27T17:25:26.802+0000","updated":"2010-09-27T17:25:26.802+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12915390","id":"12915390","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"A zombie TT can't result in lease expiration?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-09-27T17:47:54.905+0000","updated":"2010-09-27T17:47:54.905+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12915418","id":"12915418","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"Eli, a zombie TT causes lease expiration. This is expected, right?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2010-09-27T18:36:16.870+0000","updated":"2010-09-27T18:36:16.870+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/12915497","id":"12915497","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Yes, what I meant earlier was that you can still see this issue because loaded TT's that don't respond for over 10 minutes are thought to be dead and therefore their leases expire. Perhaps work similar to HDFS-599 needs to be done on the TT/JT?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-09-27T21:43:11.696+0000","updated":"2010-09-27T21:43:11.696+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/13505228","id":"13505228","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sujesh.chirackkal","name":"sujesh.chirackkal","key":"sujesh.chirackkal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sujesh Chirackkal","active":true,"timeZone":"Asia/Kolkata"},"body":"Getting the same error in CDH 4,Hive 0.8.1 while running query with lot of dynamic partitions. Setting the number of reducers to a larger value helps me to resolve the issue for now.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sujesh.chirackkal","name":"sujesh.chirackkal","key":"sujesh.chirackkal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Sujesh Chirackkal","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-11-28T05:26:33.484+0000","updated":"2012-11-28T05:26:33.484+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/13867013","id":"13867013","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bugcy013","name":"bugcy013","key":"bugcy013","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dhanasekaran Anbalagan","active":true,"timeZone":"Etc/UTC"},"body":"Hi All,\n\ngetting same error. on hive External table, I am using hive-common-0.10.0-cdh4.4.0. \n\nIn my case. we are using sqoop to import data with table. table stored data in rc file format. I am only facing issue with external table. \n\n4/01/08 12:21:40 INFO mapred.JobClient: Task Id : attempt_201312121801_0049_m_000000_0, Status : FAILED\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /dv_data_warehouse/dv_eod_performance_report/_DYN0.337789259996055/trade_date=__HIVE_DEFAULT_PARTITION__/client=__HIVE_DEFAULT_PARTITION__/install=__HIVE_DEFAULT_PARTITION__/_temporary/_attempt_201312121801_0049_m_000000_0/part-m-00000: File is not open for writing. Holder DFSClient_NONMAPREDUCE_-794488327_1 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2452)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2262)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2175)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:501)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:299)\n\tat org.apache.hadoop.hdfs.protocol.pro\nattempt_201312121801_0049_m_000000_0: SLF4J: Class path contains multiple SLF4J bindings.\nattempt_201312121801_0049_m_000000_0: SLF4J: Found binding in [jar:file:/usr/lib/hadoop-0.20-mapreduce/lib/slf4j-simple-1.5.8.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nattempt_201312121801_0049_m_000000_0: SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nattempt_201312121801_0049_m_000000_0: SLF4J: Found binding in [jar:file:/disk1/mapred/local/taskTracker/tech/distcache/-6782344428220505463_-433811577_1927241260/nameservice1/user/tech/.staging/job_201312121801_0049/libjars/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nattempt_201312121801_0049_m_000000_0: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n14/01/08 12:21:55 INFO mapred.JobClient: Task Id : attempt_201312121801_0049_m_000000_1, Status : FAILED\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /dv_data_warehouse/dv_eod_performance_report/_DYN0.337789259996055/trade_date=__HIVE_DEFAULT_PARTITION__/client=__HIVE_DEFAULT_PARTITION__/install=__HIVE_DEFAULT_PARTITION__/_temporary/_attempt_201312121801_0049_m_000000_1/part-m-00000: File is not open for writing. Holder DFSClient_NONMAPREDUCE_-390991563_1 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2452)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2262)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2175)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:501)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:299)\n\tat org.apache.hadoop.hdfs.protocol.pro\nattempt_201312121801_0049_m_000000_1: SLF4J: Class path contains multiple SLF4J bindings.\nattempt_201312121801_0049_m_000000_1: SLF4J: Found binding in [jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nattempt_201312121801_0049_m_000000_1: SLF4J: Found binding in [jar:file:/disk1/mapred/local/taskTracker/tech/distcache/7281954290425601736_-433811577_1927241260/nameservice1/user/tech/.staging/job_201312121801_0049/libjars/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nattempt_201312121801_0049_m_000000_1: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n14/01/08 12:22:12 INFO mapred.JobClient: Task Id : attempt_201312121801_0049_m_000000_2, Status : FAILED\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /dv_data_warehouse/dv_eod_performance_report/_DYN0.337789259996055/trade_date=__HIVE_DEFAULT_PARTITION__/client=__HIVE_DEFAULT_PARTITION__/install=__HIVE_DEFAULT_PARTITION__/_temporary/_attempt_201312121801_0049_m_000000_2/part-m-00000: File is not open for writing. Holder DFSClient_NONMAPREDUCE_1338126902_1 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2452)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2262)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2175)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:501)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:299)\n\tat org.apache.hadoop.hdfs.protocol.pro\nattempt_201312121801_0049_m_000000_2: SLF4J: Class path contains multiple SLF4J bindings.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bugcy013","name":"bugcy013","key":"bugcy013","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dhanasekaran Anbalagan","active":true,"timeZone":"Etc/UTC"},"created":"2014-01-09T20:27:54.674+0000","updated":"2014-01-09T20:27:54.674+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/13877112","id":"13877112","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"This one has gone very stale and we have not seen any properly true reports of lease renewals going amiss during long waiting tasks recently. Marking as 'Not a Problem' (anymore). If there's a proper new report of this behaviour, please lets file a new JIRA with the newer data.\n\n[~bugcy013] - Your problem is pretty different from what OP appears to have reported in an older version. Your problem arises out of MR tasks not utilising an attempt ID based directory (which Hive appears to do sometimes), in which case two different running attempts (out of speculative exec. or otherwise) can cause one of them to run into this error as a result of the file overwrite. Best to investigate further on a mailing list rather than here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-01-21T02:28:40.204+0000","updated":"2014-01-21T02:28:40.204+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12407296/comment/13886051","id":"13886051","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sukhendu","name":"sukhendu","key":"sukhendu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"sukhendu chakraborty","active":true,"timeZone":"Etc/UTC"},"body":"I am seeing the lease not expired error for a partitioned hive tables in CDH 4.5 MR1. I have a similar usecase as Sujesh above, I am using dynamic date partitioning for a year (365 partitions), but have 1B rows (300GB of data for that year). I also want to cluster the data in each partition into 32 buckets.\n\nHere is part  of the error trace:\n3:58:18.531 PM\tERROR\torg.apache.hadoop.hdfs.DFSClient\t\nFailed to close file /tmp/hive-user/hive_2014-01-29_15-33-51_510_4099525102053071439/_task_tmp.-ext-10000/trn_dt=20090531/_tmp.000012_0\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /tmp/hive-user/hive_2014-01-29_15-33-51_510_4099525102053071439/_task_tmp.-ext-10000/trn_dt=20090531/_tmp.000012_0: File does not exist. Holder DFSClient_NONMAPREDUCE_-1745484980_1 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2543)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2535)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2601)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2578)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:556)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:337)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44958)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1752)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1748)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1746)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1238)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)\n\tat $Proxy10.complete(Unknown Source)\n\tat sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)\n\tat $Proxy10.complete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:330)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:1796)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:1783)\n\tat org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:709)\n\tat org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:726)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:561)\n\tat org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2399)\n\tat org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2415)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sukhendu","name":"sukhendu","key":"sukhendu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"sukhendu chakraborty","active":true,"timeZone":"Etc/UTC"},"created":"2014-01-30T00:09:39.279+0000","updated":"2014-01-30T00:09:39.279+0000"}],"maxResults":13,"total":13,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-198/votes","votes":4,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0iv4f:"}}