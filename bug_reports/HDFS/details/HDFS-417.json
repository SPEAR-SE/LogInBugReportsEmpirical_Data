{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12409027","self":"https://issues.apache.org/jira/rest/api/2/issue/12409027","key":"HDFS-417","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/2","id":"2","description":"The problem described is an issue which will never be fixed.","name":"Won't Fix"},"customfield_12312322":null,"customfield_12310220":"2008-11-24T21:53:10.345+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sat May 15 05:28:47 UTC 2010","customfield_12310420":"16123","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_30602196852_*|*_1_*:*_2_*:*_15995824940_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2010-05-15T05:28:47.463+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-417/watchers","watchCount":10,"isWatching":false},"created":"2008-11-21T21:35:05.671+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"23.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12323431","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12323431","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"inwardIssue":{"id":"12414770","key":"HADOOP-5257","self":"https://issues.apache.org/jira/rest/api/2/issue/12414770","fields":{"summary":"Export namenode/datanode functionality through a pluggable RPC layer","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/2","id":"2","description":"A new feature of the product, which has yet to be developed.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype","name":"New Feature","subtask":false,"avatarId":21141}}}},{"id":"12324813","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12324813","type":{"id":"10032","name":"Blocker","inward":"is blocked by","outward":"blocks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10032"},"inwardIssue":{"id":"12422327","key":"HADOOP-5640","self":"https://issues.apache.org/jira/rest/api/2/issue/12422327","fields":{"summary":"Allow ServicePlugins to hook callbacks into key service events","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12324152","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12324152","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12422327","key":"HADOOP-5640","self":"https://issues.apache.org/jira/rest/api/2/issue/12422327","fields":{"summary":"Allow ServicePlugins to hook callbacks into key service events","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2011-05-18T21:48:14.174+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"I have made the following changes to hadoopfs.thrift:\n\n#  Added namespaces for Python, Perl and C++.\n\n# Renamed parameters and struct members to camelCase versions to keep them consistent (in particular FileStatus{blockReplication,blockSize} vs FileStatus.{block_replication,blocksize}).\n\n# Renamed ThriftHadoopFileSystem to FileSystem. From the perspective of a Perl/Python/C++ user, 1) it is already clear that we're using Thrift, and 2) the fact that we're dealing with Hadoop is already explicit in the namespace.  The usage of generated code is more compact and (in my opinion) clearer:\n{quote}\n        *Perl*:\n        use HadoopFS;\n\n        my $client = HadoopFS::FileSystemClient->new(..);\n\n         _instead of:_\n\n        my $client = HadoopFS::ThriftHadoopFileSystemClient->new(..);\n\n        *Python*:\n\n        from hadoopfs import FileSystem\n\n        client = FileSystem.Client(..)\n\n        _instead of_\n\n        from hadoopfs import ThriftHadoopFileSystem\n\n        client = ThriftHadoopFileSystem.Client(..)\n\n        (See also the attached diff [^scripts_hdfs_py.diff] for the\n         new version of 'scripts/hdfs.py').\n\n        *C++*:\n\n        hadoopfs::FileSystemClient client(..);\n\n         _instead of_:\n\n        hadoopfs::ThriftHadoopFileSystemClient client(..);\n{quote}\n\n# Renamed ThriftHandle to FileHandle: As in 3, it is clear that we're dealing with a Thrift object, and its purpose (to act as a handle for file operations) is clearer.\n\n# Renamed ThriftIOException to IOException, to keep it simpler, and consistent with MalformedInputException.\n\n# Added explicit version tags to fields of ThriftHandle/FileHandle, Pathname, MalformedInputException and ThriftIOException/IOException, to improve compatibility of existing clients with future versions of the interface which might add new fields to those objects (like stack traces for the exception types, for instance).\n\nThose changes are reflected in the attachment [^hadoopfs_thrift.diff].\n\nChanges in generated Java, Python, Perl and C++ code are also attached in [^gen.diff]. They were generated by a Thrift checkout from trunk\n([http://svn.apache.org/repos/asf/incubator/thrift/trunk/]) as of revision\n719697, plus the following Perl-related patches:\n\n* [https://issues.apache.org/jira/browse/THRIFT-190]\n* [https://issues.apache.org/jira/browse/THRIFT-193]\n* [https://issues.apache.org/jira/browse/THRIFT-199]\n\nThe Thrift jar file [^libthrift.jar] built from that Thrift checkout is also attached, since it's needed to run the Java Thrift server.\n\nI have also added a new target to src/contrib/thriftfs/build.xml to build the Java bindings needed for org.apache.hadoop.thriftfs.HadoopThriftServer.java (see attachment [^build_xml.diff] and modified HadoopThriftServer.java to make use of the new bindings (see attachment [^HadoopThriftServer_java.diff]).\n\nThe jar file [^lib/hadoopthriftapi.jar] is also included, although it can be regenerated from the stuff under 'gen-java' and the new 'compile-gen' Ant target.\n\nThe whole changeset is also included as [^all.diff].","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394450","id":"12394450","filename":"all.diff","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-21T21:39:35.056+0000","size":1141850,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12394450/all.diff"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394682","id":"12394682","filename":"BlockManager.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-25T19:43:22.178+0000","size":811,"mimeType":"text/x-java","content":"https://issues.apache.org/jira/secure/attachment/12394682/BlockManager.java"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394446","id":"12394446","filename":"build_xml.diff","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-21T21:36:39.258+0000","size":759,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12394446/build_xml.diff"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394683","id":"12394683","filename":"DefaultBlockManager.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-25T19:43:22.222+0000","size":2093,"mimeType":"text/x-java","content":"https://issues.apache.org/jira/secure/attachment/12394683/DefaultBlockManager.java"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394684","id":"12394684","filename":"DFSBlockManager.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-25T19:43:22.225+0000","size":3161,"mimeType":"text/x-java","content":"https://issues.apache.org/jira/secure/attachment/12394684/DFSBlockManager.java"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394445","id":"12394445","filename":"gen.diff","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-21T21:36:17.874+0000","size":1114463,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12394445/gen.diff"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12399953","id":"12399953","filename":"HADOOP-4707.diff","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-10T23:04:07.022+0000","size":3165493,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12399953/HADOOP-4707.diff"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12405310","id":"12405310","filename":"HADOOP-4707.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-13T10:39:07.256+0000","size":2415716,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12405310/HADOOP-4707.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12404718","id":"12404718","filename":"HADOOP-4707.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-06T12:48:59.112+0000","size":2414062,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12404718/HADOOP-4707.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12407392","id":"12407392","filename":"hadoop-4707-31c331.patch.gz","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-06T21:23:01.530+0000","size":262787,"mimeType":"application/x-gzip","content":"https://issues.apache.org/jira/secure/attachment/12407392/hadoop-4707-31c331.patch.gz"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12405811","id":"12405811","filename":"HADOOP-4707-55c046a.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-17T21:41:37.095+0000","size":2469554,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12405811/HADOOP-4707-55c046a.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12406722","id":"12406722","filename":"hadoop-4707-6bc958.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-29T00:56:05.978+0000","size":2439451,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12406722/hadoop-4707-6bc958.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12408978","id":"12408978","filename":"hadoop-4707-867f26.txt.gz","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-26T00:46:30.347+0000","size":158971,"mimeType":"application/x-gzip","content":"https://issues.apache.org/jira/secure/attachment/12408978/hadoop-4707-867f26.txt.gz"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394444","id":"12394444","filename":"hadoopfs_thrift.diff","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-21T21:35:34.256+0000","size":5055,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12394444/hadoopfs_thrift.diff"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394449","id":"12394449","filename":"hadoopthriftapi.jar","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-21T21:38:56.782+0000","size":126893,"mimeType":"application/java-archive","content":"https://issues.apache.org/jira/secure/attachment/12394449/hadoopthriftapi.jar"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394447","id":"12394447","filename":"HadoopThriftServer_java.diff","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-21T21:37:03.260+0000","size":17219,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12394447/HadoopThriftServer_java.diff"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394685","id":"12394685","filename":"HadoopThriftServer.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-25T19:43:38.615+0000","size":20808,"mimeType":"text/x-java","content":"https://issues.apache.org/jira/secure/attachment/12394685/HadoopThriftServer.java"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394611","id":"12394611","filename":"hdfs_py_venky.diff","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-24T23:35:52.906+0000","size":1508,"mimeType":"text/x-diff","content":"https://issues.apache.org/jira/secure/attachment/12394611/hdfs_py_venky.diff"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394600","id":"12394600","filename":"hdfs.py","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=indigoviolet","name":"indigoviolet","key":"indigoviolet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Venky Iyer","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-24T21:57:35.043+0000","size":17057,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12394600/hdfs.py"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12408167","id":"12408167","filename":"libthrift.jar","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-14T19:17:54.329+0000","size":93522,"mimeType":"application/java-archive","content":"https://issues.apache.org/jira/secure/attachment/12408167/libthrift.jar"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12404719","id":"12404719","filename":"libthrift.jar","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-06T12:48:59.360+0000","size":172432,"mimeType":"application/java-archive","content":"https://issues.apache.org/jira/secure/attachment/12404719/libthrift.jar"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12399954","id":"12399954","filename":"libthrift.jar","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-10T23:04:07.113+0000","size":93520,"mimeType":"application/java-archive","content":"https://issues.apache.org/jira/secure/attachment/12399954/libthrift.jar"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12394448","id":"12394448","filename":"libthrift.jar","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-21T21:37:48.438+0000","size":73342,"mimeType":"application/java-archive","content":"https://issues.apache.org/jira/secure/attachment/12394448/libthrift.jar"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"108196","customfield_12312823":null,"summary":"Improvements to Hadoop Thrift bindings","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Tested under Linux x86-64","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12650346","id":"12650346","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=indigoviolet","name":"indigoviolet","key":"indigoviolet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Venky Iyer","active":true,"timeZone":"Etc/UTC"},"body":"These look good to me. I've built a python interface to this, to provide a more pythonic file-object like interface as well as a layer that mimics python's os module. These will need some minor fixes to adjust to the patches here. You're welcome to add these if you feel like it, or I can do so once your patch is committed. See attached:\n\n\nSee https://issues.apache.org/jira/secure/attachment/12394600/hdfs.py","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=indigoviolet","name":"indigoviolet","key":"indigoviolet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Venky Iyer","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-24T21:53:10.345+0000","updated":"2008-11-24T21:58:14.457+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12650348","id":"12650348","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=indigoviolet","name":"indigoviolet","key":"indigoviolet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Venky Iyer","active":true,"timeZone":"Etc/UTC"},"body":"Note that this contains some installation-specific stuff to start up a Thrift hdfs server that I will have to abstract away before public release. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=indigoviolet","name":"indigoviolet","key":"indigoviolet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Venky Iyer","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-24T21:57:35.071+0000","updated":"2008-11-24T21:57:35.071+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12650383","id":"12650383","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"Thanks, Venky. Instead of parsing the log file to get the port, you can pass it as the first argument to HadoopThriftServer (see [^hdfs_py_venky.diff]. A value of '0' will make HadoopThriftServer pick the first one available, which is probably what you want in your hadoopThriftServer Python class. \n\nI don't know how to abstract away the basedir stuff, though - keep me posted, since I'm interested in doing something similar.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-24T23:35:32.817+0000","updated":"2008-11-24T23:35:32.817+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12650700","id":"12650700","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"I have started to implement reading on a per-block basis:\n\n{code}\n    string  readBlock(1:FileHandle handle, 2:BlocLocation block)\n              throws (1: IOException ouch)\n{code}\n\nI have first added two more fields to BlockLocation:\n\n{code}\n    struct BlockLocation {\n      1: list<string> hosts,     \n      2: list<string> names,     \n      3: i64 offset,             \n      4: i64 length,             \n      5: i64 id,           /* bock id */\n      6: i64 genStamp      /* block generation timestamp */\n    }\n{code}\n\nI then provide two implementations of getFileBlockLocations(): One for\nHDFS filesystems (which may fill in the values for the extra fields),\nand another one for generic filesystems (like local ones), which just\nreturn zeroes.\n\nDoes it sound like a reasonable plan?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2008-11-25T19:42:55.088+0000","updated":"2008-11-25T19:42:55.088+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12659923","id":"12659923","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"Thanks Carlos. Your proposed change to BlockLocation sounds good.\n\n>Instead of parsing the log file to get the port, you can pass it as the first argument to HadoopThriftServer (see hdfs_py_venky.diff. A value of '0' will make HadoopThriftServer pick the first one available, which is probably what you want in your hadoopThriftServer Python class.\n\nIn the usual case, we want the HadoopThriftServer to bind to any available port. This allows running multiple HadoopThriftServer on the same machine. Also, the python client needs to know the port that its associated instance of the HadoopThriftServer is using. That's the reason why the python client parses the log file to find the port number. Are you saying that there is another way to achieve the same functionality?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2008-12-30T20:03:54.977+0000","updated":"2008-12-30T20:03:54.977+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12672448","id":"12672448","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"Patch HADOOP-4707.diff removes the need for an extra Thrift server\nprocess by creating Thrift server instances on the namenode and\ndatanode processes. It also allows direct reads and writes to datanode\ninstances.\n\nThe Thrift namenode server instance implements the Thrift service\n'Namenode' defined in src/thrift/hdfs.thrift. It is created by calling\norg.apache.hadoop.hdfs.server.namenode.NameNode.startThriftServer()\nin org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(),\nand acts as a facade to org.apache.hadoop.hdfs.protocol.ClientProtocol.\n\nThe Thrift datanode server instance implements the Thrift service\n'Datanode', defined as well in src/thrift/hdfs.thrift. It is created\nin org.apache.hadoop.hdfs.datanode.DataNode.startDatanode().\n\nIn order to read data from a file, Thrift clients request a list\nof blocks with Namenode.getBlocks(path, offset, length), and then\ncall Datanode.readBlock() on the appropriate datanode servers\nfor each block in the returned list. The Thrift datanode server\ninstance then opens a local socket to the datanode server via\norg.apache.hadoop.hdfs.DFSClient.BlockReader.newBlockReader().\n\nData writes are implemented in a similar way: Thrift\nclients call Namenode.addBlock(path) on the Thrift\nnamenode server, and then call DataNode.writeBlock() on\nas many Thrift Datanode servers as they wish. The Thrift\ndatanode servers write the block to their local storage using\norg.apache.hadoop.hdfs.server.datanode.FSDataset.writeToBlock(),\nand then inform the namenode about the new block using\norg.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.blockReceived(),\nso that full block replication is eventually achieved when the next\nblock report is processed by the namenode.\n\nNamenode.addBlock() must be called on files opened by Namenode.create()\nor Namenode.append(). This last call might return a Block object\nrepresenting the last partial block of an existing file, which is\ncurrently ignored because I don't know how to handle a write to\nthat block (FSDataset.writeToBlock() complains about that block\nbeing valid).\n\nSince Datanode.readBlock() and Datanode.writeBlock() expect data blobs\nof Thrift type 'binary', and that type translates to Java's byte[],\ndata reads and writes are limited to 2**31 -1 bytes (whereas Hadoop\nblocks may be much larger, since their length is measured in longs).\n\nThe following entries in hdfs-default.xml define the locations for\nthe Thrift servers:\n\n    * dfs.thrift.address, set by default to '0.0.0.0:9090'\n\n    * dfs.thrift.datanode.address, set by default to '0.0.0.0:0'.\n\nThe following entries limit the number and lifetime of Thrift server\nthreads:\n\n    * dfs.thrift.threads.min, set by default to 5\n\n    * dfs.thrift.threads.max, set by default to 20,\n\n    * dfs.thrift.timeout, set by default to 60 seconds.\n\nThrift namenode and datanode servers try to obtain the identity of\nthe client by calling org.apache.hadoop.net.NetUtils.getRemoteUser(),\nwhich implements the IDENT protocol defined by RFC 1413. If that call\nfails, the value returned by security.UnixUserGroupInformation.login()\nis used instead.\n\nPerl and Python higher-level APIs for read and write functionality are\nalso included under src/thrift, together with test suites. Perhaps\nit makes more sense to release that code independently from Hadoop,\nso that they might be updated more frequently. The Perl and Python\ntest suites may be run with the following ant target:\n\n    $ ant -Dcompile.thrift=true test-thrift\n\nYou may also need to pass the location of Thrift's Python library\nmodules, since Thrift does not install them under Python's lib\ndirectory by default. For instance, if your Python bindings for\nThrift are installed under /opt/thrift/lib/python2.x/site-packages,\nthen you should do the following:\n\n    $ ant -Dcompile.thrift=true \\\n\t  -Dthrift.pythonpath=/opt/thrift/lib/python2.x/site-packages \\\n\t      test-thrift\n\nPerl and Python tests for chmod() functionality show that write\noperations succeed for read-only files. I guess that's because all\ntests are run as HDFS superuser.\n\nPerl and Python high-level APIs call Namenode.addBlock() on each\ninvocation of Datanode.writeBlock(), so it is possible to create\nfiles with many blocks of smaller size.\n\nI have used Thrift version 20080411-r743112 from Thrift's\nSubversion repository, in order to include the resolution of\nTHRIFT-193 (http://issues.apache.org/jira/browse/THRIFT-193),\nwhich fixes Perl package generation, and THRIFT-249\n(https://issues.apache.org/jira/browse/THRIFT-249), which emits Javadoc\ncomments for Java classes. Also attached is the libthrift.jar from that\nbuild, which is needed at run-time by the Thrift server instances. The\npatch also includes Thrift stubs for all languages supported by Thrift,\nvery much like the stuff under src/contrib/thiftfs.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-10T23:04:07.116+0000","updated":"2009-02-10T23:04:07.116+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12672753","id":"12672753","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"Hi Carlos, This looks awesome! Lots of cool stuff. I have not looked at the details, but here are some of my initial comments:\n\n1. The idea of making the Namenode expose a Thrift interface is a great idea. This removes the need for a separate daemon to map Thrift methods to ClientProcotol methods.\n\n2. Is there a way to \"layer\" the Thrift layer (DatanodeThriftServer and NameNodeThrift server) around the org.apache.hadoop.hdfs package (instead of making them part of org.apache.hadoop.hdfs). Can these be part of org.apache.hadoop.thriftfs and reside in the src/contrib/thriftfs directory? This essentially means that the Thrift server is a \"layer\" then encapsulates the hadoop Namenode/DataNode.  This could be in its own library called libhdfsthrift.jar (or something like that)\n\n3. There could be new source files in the base hdfs package that allows plugging in of multiple protocol stacks (the first one being Thrift). This code could use reflection (and configuration settings) to figure out if the libhdfsthrift.jar is in the classpath, and if so, then use those methods from that jar to initialize the Thrift server. The reason I propose the above is because it does not force every Hadoop install to use Thrift. It keeps the base Namenode/Datanode code clean and elegent. It also allows plugging in other protocol stacks to expose the Namenode/datanode functionality.\n\n4. It is possible that the Thrift implementation might need to use some (currently) package-private methods in the Datanode/Namenode, but we can work on making them public if need be.\n\n5. Allowing the Thrift interface to read file contents is easy. However, writing to blocks is more difficult, especially because the DFSClient.java is a heavy-weight piece of code and participates heavily in ensuring correct recovery from write-pipeline failures, allows \"appending\" to existing files, ensuring all blocks of a file are equal size, etc.etc. Do you have an application that will need to write to HDFS files using this Thrift interface?\n\n6. Your unit tests are nice. It is imperative for us to detect incompatible changes to base HDFS APIs earlier rather than later.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-02-11T20:14:00.603+0000","updated":"2009-02-11T20:14:00.603+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12672780","id":"12672780","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for your feedback, Dhruba!\n\nbq. 2. Is there a way to \"layer\" the Thrift layer (DatanodeThriftServer and NameNodeThrift server) around the org.apache.hadoop.hdfs package (instead of making them part of org.apache.hadoop.hdfs). Can these be part of org.apache.hadoop.thriftfs and reside in the src/contrib/thriftfs directory?\n\nYes, no problem with that. I chose to work in src/hdfs mainly to avoid conflicts with your code under src/contrib/thriftfs.\n\nbq. 3. There could be new source files in the base hdfs package that allows plugging in of multiple protocol stacks (the first one being Thrift). This code could use reflection (and configuration settings) to figure out if the libhdfsthrift.jar is in the classpath, and if so, then use those methods from that jar to initialize the Thrift server. The reason I propose the above is because it does not force every Hadoop install to use Thrift. It keeps the base Namenode/Datanode code clean and elegent. It also allows plugging in other protocol stacks to expose the Namenode/datanode functionality.\n\nYes, that sounds good. Would it make sense to open a new JIRA issue to implement the plugin functionality? Otherwise the attachment list to this issue might end up quite crowded.\n\nbq. 4. It is possible that the Thrift implementation might need to use some (currently) package-private methods in the Datanode/Namenode, but we can work on making them public if need be.\n\nOK, so no real reason for hosting code under org.hadoop.hdfs.server.{datanode,namenode}, then.\n\nbq. 5. Allowing the Thrift interface to read file contents is easy. However, writing to blocks is more difficult, especially because the DFSClient.java is a heavy-weight piece of code and participates heavily in ensuring correct recovery from write-pipeline failures, allows \"appending\" to existing files, ensuring all blocks of a file are equal size, etc.etc. Do you have an application that will need to write to HDFS files using this Thrift interface?\n\nReading is by far more important to me than writing, and I could just do the writes using Java. If the current implementations of append(), create(), addBlock() and writeBlock() were show-stoppers for getting the stuff committed sooner, I can focus first on addressing your points, and then figure out later what to do with them. What would you recommend?\n\nbq. 6. Your unit tests are nice. It is imperative for us to detect incompatible changes to base HDFS APIs earlier rather than later.\n\nBeing able to test the Thrift code (and nothing else) by doing a:\n{code}\n    $ ant -Dtestcase=TestThriftServer test-core\n{code}\n\nwas very helpful during development. Will it be possible to do a similar thing when the Thrift code moves back to src/contrib/thriftfs?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-11T21:11:25.922+0000","updated":"2009-02-11T21:11:25.922+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12672796","id":"12672796","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"> Would it make sense to open a new JIRA issue to implement the plugin functionality? \n\nYes, please open a new JIRA to implement \"export namenode/datanode functionality through a pluggable RPC layer\". The component for this jira will be \"dfs\". You can submit the code for this new JIRA, get it vetted by the HDFS community and then commit to svn. After that is done, then we can work on getting this one (HADOOP-4707) commiited. You might need a separate unit test associated with this new JIRA.\n\n> Reading is by far more important to me than writing, and I could just do the writes using Java. \nI like this staged approach. I know that many would like to write to HDFS using Thrift, but with an eye to gettting this committed sooner, I would rather that we take it in steps.\n\n> was very helpful during development. Will it be possible to do a similar thing when the Thrift code moves back\n\nYes, If you move the TestThriftServer code to src/contrib/thriftfs/test/org/apache/hadoop/thriftfs, that should be all that is required.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-02-11T21:49:07.189+0000","updated":"2009-02-11T21:49:07.189+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12673352","id":"12673352","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"bq. Yes, please open a new JIRA to implement \"export namenode/datanode functionality through a pluggable RPC layer\".\n\nI just filed HADOOP-5257, Dhruba.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-02-13T18:44:31.211+0000","updated":"2009-02-13T18:44:31.211+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12695457","id":"12695457","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=euphoria","name":"euphoria","key":"euphoria","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Greene","active":true,"timeZone":"America/Chicago"},"body":"Now that HADOOP-5257 is in trunk, what does that mean for this issue?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=euphoria","name":"euphoria","key":"euphoria","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Greene","active":true,"timeZone":"America/Chicago"},"created":"2009-04-03T15:35:08.602+0000","updated":"2009-04-03T15:35:08.602+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12695465","id":"12695465","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"bq.Now that HADOOP-5257 is in trunk, what does that mean for this issue?\n\nI'm going to reshape [^HADOOP-4707.diff] into a plugin during the weekend.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-03T15:54:05.323+0000","updated":"2009-04-03T15:54:05.323+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696064","id":"12696064","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"Patch [^HADOOP-4707.patch] provides Thrift interfaces to HDFS namenodes\nand datanodes using HDFS service plugins. Both\nplugins implement the Thrift services\ndefined in {{src/contrib/thriftfs/if/hdfs.thrift}}.\n\nIn order to read data from a file, Thrift clients request a list\nof blocks with {{Namenode.getBlocks(path, offset, length)}}, and then\ncall {{Datanode.readBlock()}} on the appropriate datanode servers\nfor each block in the returned list. The Thrift datanode server\ninstance then opens a local socket to the datanode server via\n{{org.apache.hadoop.hdfs.DFSClient.BlockReader.newBlockReader()}}.\n\nBoth plugins add {{thriftfs-default.xml}} and\n{{thriftfs-site.xml}} as configuration resources. The following\nproperties define the addresses for\nthe Thrift servers:\n\n    * dfs.thrift.address, set by default to '0.0.0.0:9090'\n\n    * dfs.thrift.datanode.address, set by default to '0.0.0.0:0'.\n\nThe following properties limit the number and lifetime of Thrift server\nthreads:\n\n    * dfs.thrift.threads.min, set by default to 5\n\n    * dfs.thrift.threads.max, set by default to 20,\n\n    * dfs.thrift.timeout, set by default to 60 seconds.\n\nSince {{Datanode.readBlock()}} expects data blobs\nof Thrift type 'binary', and that type translates to Java's {{byte[]}},\ndata reads are limited to 2**31 -1 bytes (whereas Hadoop\nblocks may be much larger, since their length is measured in longs).\n\nOn Dhruba's suggestion, I've removed all write-related methods for now.\n\nThrift namenode and datanode servers try to obtain the identity of\nthe client by calling\n{{org.apache.hadoop.thriftfs.PluginBase.getRemoteUser()}},\nwhich implements the IDENT protocol defined by RFC 1413. If that call\nfails, the value returned by\n{{security.UnixUserGroupInformation}}\nis used instead.\n\nI've removed the Perl and Python high-level APIs from this patch in\norder to make it simpler. Those APIs are available at\nhttp://code.pepelabs.net/git/?p=hadoop-thrift.git. Perhaps it's better\nto keep them separate from Hadoop's code base?\n\nI've updated Thrift's [^libthrift.jar] to a recent Subversion checkout.\nIt seems that a Thrift release is imminent, so the final JAR (and the\nJava code it generates) should not be too different from what's included\nhere.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-06T12:48:59.365+0000","updated":"2009-04-06T12:48:59.365+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696666","id":"12696666","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"A few comments on this patch:\n\n1) If the NameNode restarts but the DataNodes stay up, the DataNodes don't re-register their Thrift ports with the NameNode. Calling getDatanodeReport then triggers an NPE in ThriftUtils.toThrift\n\nIt seems to me that we need to add some hooks to DataNode and/or NameNode that allow the plugins to register callbacks on certain events. Specifically for this case, the DataNode needs to re-register its thrift port with the NameNode when it reconnects.\n\nSpecifically, I think the DataNode needs a hook in DataNode.register() that calls through to plugins. Doing this in a generalized way on the HADOOP-5257 plugin interface might be nice - some kind of \"hook point\" pubsub kind of interface. Opinions solicited :)\n\n2) I think the datanode hostnames need to be canonicalized somehow when inserting into the thriftPorts map. On a pseudodistributed cluster, I'm seeing getDatanodeReport fail to find the thriftPort since the DN is registering under the name 127.0.1.1, but then being looked up as 127.0.0.1 for whatever reason. I'll look into a solution for this.\n\n3) Lastly, I think the \"Unknown Thrift port for Datanode\" NPE is unnecessarily strict. I'd prefer for it to return a -1 or a 0 to indicate that the DN thrift server isn't running. This would require some extra checks elsewhere in the code before trying to contact a non-existent thrift server, but it enables getDatanodeReport to work even without the thrift plugin on the DNs.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-07T18:12:10.855+0000","updated":"2009-04-07T18:12:10.855+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696691","id":"12696691","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"> 1) If the NameNode restarts but the DataNodes stay up, the DataNodes don't re-register their Thrift ports with the NameNode.\n\nIs there a way for the Datanode plugin to try re-register its port with the NameNode-plugin when it encounters an error?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-07T18:58:55.714+0000","updated":"2009-04-07T18:58:55.714+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696696","id":"12696696","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":">> 1) If the NameNode restarts but the DataNodes stay up, the DataNodes don't re-register their Thrift ports with the NameNode.\n\n> Is there a way for the Datanode plugin to try re-register its port with the NameNode-plugin when it encounters an error?\n\nThe problem is that the error is encountered on the NameNode. The flow is that the client is asking the NameNode for info, and the NameNode needs to send the client the host:port for the relevant DN. There's no way for the NN plugin to contact the DN plugin to tell it to re-register.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-07T19:08:55.521+0000","updated":"2009-04-07T19:08:55.521+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696715","id":"12696715","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"{quote}\n1) If the NameNode restarts but the DataNodes stay up, the DataNodes don't re-register their Thrift ports with the NameNode. Calling getDatanodeReport then triggers an NPE in ThriftUtils.toThrift\n[..]\nSpecifically, I think the DataNode needs a hook in DataNode.register() that calls through to plugins. Doing this in a generalized way on the HADOOP-5257 plugin interface might be nice - some kind of \"hook point\" pubsub kind of interface. \n{quote}\n\nYep. A simpler option might be to spawn a thread on the datanode which calls {{Namenode.datanodeUp()}} every so often. Any preference?\n\nbq.2) I think the datanode hostnames need to be canonicalized somehow when inserting into the thriftPorts map. On a pseudodistributed cluster, I'm seeing getDatanodeReport fail to find the thriftPort since the DN is registering under the name 127.0.1.1, but then being looked up as 127.0.0.1 for whatever reason. I'll look into a solution for this.\n\nI came across that same issue when writing the test suite. The \"canonical\" name, as far as the {{thriftPorts}} map is concerned, is {{org.apache.hadoop.thriftfs.DatanodePlugin.datanode.DataNode.dnRegistration.getName()}}. On my real 6-node test cluster, that value is the same as the value of  {{org.apache.hadoop.hdfs.protocol.DatanodeInfo.name}} for every {{DatanodeInfo}} instance, so everything works. On a {{MiniDFSCluster}} cluster, however, it is not --- just as you found out in your case, Todd (classloader issues, perharps?).\n\nMy workaround for the test suite was to set the property {{slave.host.name}} to the expected value.\n\nbq.3) Lastly, I think the \"Unknown Thrift port for Datanode\" NPE is unnecessarily strict. I'd prefer for it to return a -1 or a 0 to indicate that the DN thrift server isn't running. This would require some extra checks elsewhere in the code before trying to contact a non-existent thrift server, but it enables getDatanodeReport to work even without the thrift plugin on the DNs.\n\nYep, makes sense. I propose setting the port to -1, and also doing something like this:\n\n{code}\n  public static Block toThrift(LocatedBlock block, String path,\n      Map<String, Integer> thriftPorts) {\n    if (block == null) {\n      return new Block();\n    }\n\n    List<DatanodeInfo> nodes = new ArrayList<DatanodeInfo>();\n    for (org.apache.hadoop.hdfs.protocol.DatanodeInfo n:\n        block.getLocations()) {\n\n        DatanodeInfo node = toThrift(n, thriftPorts);\n        if (node.thriftPort != -1) {\n          nodes.add(toThrift(n, thriftPorts));\n        }\n    }  \n    // [...]\n  }\n{code}\n\nThat way we return to the client the (possibly empty) list of all block locations accessible by the Thrift interface.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-07T19:54:02.263+0000","updated":"2009-04-07T19:54:02.263+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696847","id":"12696847","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"{quote}\nYep. A simpler option might be to spawn a thread on the datanode which calls Namenode.datanodeUp() every so often. Any preference?\n{quote}\n\nThat is definitely simpler but seems ugly to me. I know heartbeats are the norm elsewhere in Hadoop, but I'd prefer not to introduce more.\n\n{quote}\nOn my real 6-node test cluster, that value is the same as the value of org.apache.hadoop.hdfs.protocol.DatanodeInfo.name for every DatanodeInfo instance, so everything works. On a MiniDFSCluster cluster, however, it is not  just as you found out in your case, Todd (classloader issues, perharps?).\n{quote}\n\nI think I've figured out the issue here:\n\nIn ThriftUtils.createNamenodeClient, it uses dfs.thrift.address to connect to the namenode. This same configuration variable is used in NamenodePlugin.getAddress(). So, with the default configuration of 0.0.0.0:9090, the NN plugin binds to all local interfaces, and the datanodes attempt to connect to whatever IP is first in the local wildcard.\n\nIt seems to me that the correct behaviour would be:\n\n - The NamenodePlugin continues to listen on dfs.thrift.address\n   - Having a :0 port on dfs.thrift.address seems inadvisable since there's currently no way for the DatanodePlugin to locate the thrift server in that case.\n - The DatanodePlugin looks at dfs.thrift.address. If it is a \"wildcard\" address (0.0.0.0) it uses only the port portion, and locates the NN host using datanode.getNameNodeAddr()\n - Additionally, it we should inspect the TTransport from the client for the hostname in datanodeUp/datanodeDown rather than taking those as parameters. The hostname registered in the DatanodeRegistration comes from the remote side of the Hadoop RPC socket, so it makes sense to use the same method for getting the hostname on the Thrift side.\n\nI'll work on hacking these up and see where I get.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-08T00:41:14.985+0000","updated":"2009-04-08T00:41:14.985+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696921","id":"12696921","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Did some more hacking around in the code and came to some conclusions:\n\nIn order to guarantee that ThriftUtils.toThrift works on a DatanodeInfo properly, we need to ensure that the host name sent to NamenodePlugin.datanodeUp() is the same name as is in the DatanodeInfo. These DatanodeInfo names come from the DatanodeRegistration that gets sent from DN->NN at DN startup. Unfortunately, with the current setup this is broken for the following reason:\n  - in FSNameSystem.register, the \"name\" field passed in by the DN is ignored in favor of taking the remote host out of the RPC stack. This host is then written back into the DatanodeRegistration which is then returned to the DN.\n  - This means that, when the DN registers itself with the NameNode, its dnRegistration variable is mutated (specifically with regard to the name field)\n  - In the current Plugin architecture, the DN thrift plugin has already started *before* the DN calls 'register\". This means that when the DatanodePlugin calls the NamenodePlugin.datanodeUp function, it's passing a different \"name\" String than will eventually end up registered on the NN.\n\nMy proposed solution is:\n\n1) Add a hook to the Datanode Plugin interface to allow the DN plugin to defer the datanodeUp call until after the DN has registered with the NN. This puts the dnRegistration member in a stable state that matches the DatanodeInfo stored on the NN side. The necessary plugin infrastructure for this is in HADOOP-5640, newly created.\n\n2) This step isn't strictly necessary, but I'd like to modify the datanodeUp and datanodeDown functions to take three parameters (name, storage id, and thrift port) rather than the current two (name and thrift port). This allows those functions to construct DatanodeID objects and then maintain a Map<DatanodeID, Integer> thriftPorts array instead. The advantages here are:\n  - Consistency with data structures elsewhere in the code\n  - Clarity of what the map keys actually are\n  - Some small level of \"security\", in the sense that the Storage ID makes these calls a little harder to spoof.\n\nAlternatively, rather than taking three arguments, we could introduce a DatanodeID thrift struct with name and storageID members.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-08T07:23:27.839+0000","updated":"2009-04-08T07:23:27.839+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696928","id":"12696928","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for your comments, Todd!\n\nbq.1) Add a hook to the Datanode Plugin interface to allow the DN plugin to defer the datanodeUp call until after the DN has registered with the NN. This puts the dnRegistration member in a stable state that matches the DatanodeInfo stored on the NN side. The necessary plugin infrastructure for this is in HADOOP-5640, newly created.\n\nSounds good to me. \n\nbq.2) This step isn't strictly necessary, but I'd like to modify the datanodeUp and datanodeDown functions to take three parameters (name, storage id, and thrift port) rather than the current two (name and thrift port).\n\nIt wouldn't hurt to pass the storageID as well.\n\nbq.Alternatively, rather than taking three arguments, we could introduce a DatanodeID thrift struct with name and storageID members.\n\nI'd rather stick to {{datanodeUp(name, storageID, thiftPort)}}} --- one class less to generate, and smaller Thrift IDL file.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-08T07:51:50.222+0000","updated":"2009-04-08T07:51:50.222+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12696939","id":"12696939","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"{quote}\nbq.Yep. A simpler option might be to spawn a thread on the datanode which calls Namenode.datanodeUp() every so often. Any preference?\n\nThat is definitely simpler but seems ugly to me. I know heartbeats are the norm elsewhere in Hadoop, but I'd prefer not to introduce more.\n{quote}\n\nI agree that HADOOP-5640 is a cleaner path.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-08T08:02:50.976+0000","updated":"2009-04-08T08:02:50.976+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12698369","id":"12698369","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"Minor corrections:\n* Included Todd's suggestion of not throwing a NPE when translating HDFS {{DatanodeInfo}} instances to their Thrift equivalents.\n* Ruby module names must be capitalized.\n* Since JUnit 4 is now fetched by Ivy, moved MiniDFSCluster initialization to {{@BeforeClass}} method. Execution of TestNamenode goes down from 120 to 10. seconds","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-13T10:39:07.793+0000","updated":"2009-04-13T10:39:07.793+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12700338","id":"12700338","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Attached is a patch with some further improvements:\n* datenodeUp/datanodeDown now take a storage ID, and the thriftPorts maps from DatanodeID instances to ints\n* Uses thrift enums now for DatanodeReportType and DatanodeState\n* Reran thrift with newest trunk thrift for all languages (looked like some languages hadn't been re-run recently)\n* Depends on HADOOP-5640 for:\n** Restarting the namenode while DN is still up now properly triggers datanodeUp messages from DNs after they've re-registered at the NN\n* Few misc fixes:\n** DatanodePlugin 'register' member is now volatile since it's accessed by multiple threads without synchronization\n** typo: THRFIT -> THRIFT\n** PluginBase has been refactored to a containment relationship instead of a subclassing relationship, so hence renamed to ThriftPluginServer\n** When connecting to the NN thrift service, if it's configured as 0.0.0.0, use NameNode.getAddress to figure out what the external IP it's actually listening on is.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-17T21:41:37.107+0000","updated":"2009-04-17T21:41:37.107+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12703835","id":"12703835","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for [^HADOOP-4707-55c046a.txt], Todd --- much better than my previous patch.\n\nI just noticed a small improvement to {{DatanodePlugin.ThriftHandler.readBlock}}: If we implement it like this, we might save quite a few memory copies:\n\n{code}\n    public BlockData readBlock(Block block, long offset, int length)\n        throws IOException, TException {\n      LOG.debug(\"readBlock(\" + block.blockId + \",\" + offset + \",\" + length\n\n        // [..]\n\n        // Allocate read buffer on ret directly, so that no extra memory copy is done\n        //  if we read all bytes\n        ret.data = new byte[length];\n        int n = reader.read(ret.data, 0, length);\n        if (n == -1) {\n          throw new EOFException(\"EOF reading \" + length + \" bytes at offset \"\n              + offset + \" from \" + block);\n        }\n        LOG.debug(\"readBlock(\" + block.blockId + \", \" + offset + \", \" + length\n            + \"): Read \" + n + \" bytes\");\n        if (n < length) {\n          byte[] buf = new byte[n];\n          System.arraycopy(ret.data, 0, buf, 0, n);\n          ret.data = buf;\n        }\n        ret.length = n;\n\n        // [..]\n\n      return ret;\n    }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-28T21:38:12.977+0000","updated":"2009-04-28T21:38:12.977+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12703925","id":"12703925","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Nice fix. Here's a diff against my previous patch, and I'll upload a new patch against trunk as well with a couple more fixes.\n\n{code:java}\ndiff --git a/src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/DatanodePlugin.java b/src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/D\nindex 6929a7b..14b2e32 100644\n--- a/src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/DatanodePlugin.java\n+++ b/src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/DatanodePlugin.java\n@@ -100,8 +100,19 @@ public class DatanodePlugin\n         }\n         LOG.debug(\"readBlock(\" + block.blockId + \", \" + offset + \", \" + length\n             + \"): Read \" + n + \" bytes\");\n-        ret.data = new byte[n];\n-        System.arraycopy(buf, 0, ret.data, 0, n);\n+\n+        if (n == length) {\n+            // If we read exactly the same number of bytes that was asked for,\n+            // we can simply return the buffer directly\n+            ret.data = buf;\n+        } else {\n+            assert n < length;\n+            // If we read fewer bytes than they asked for, we need to write\n+            // back a smaller byte array. With the appropriate thrift hook\n+            // we could avoid this copy, too.\n+            ret.data = new byte[n];\n+            System.arraycopy(buf, 0, ret.data, 0, n);\n+        }\n         ret.length = n;\n \n         summer.update(ret.data);\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-29T00:54:18.116+0000","updated":"2009-04-29T00:54:18.116+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12703927","id":"12703927","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"This fixes the test cases, and makes one slightly incompatible change. DatanodeInfo.name was previously just the hostname, but is now the hostname:port (datanodeInfo.getName()) as described in the thrift interface docs. This was required to get tests to pass for me.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-29T00:56:06.014+0000","updated":"2009-04-29T00:56:06.014+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12704320","id":"12704320","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Some more notes after using this API for a bit:\n\n- The Block struct was missing the startOffset from LocatedBlock. This made it somewhat useful for actually reading out of the middle of files. I'll submit a patch for this once I aggregate a few more changes.\n- I think we should think a little bit about authentication. Currently the interface uses identd to determine the unix username of the connecting socket, but this has several problems:\n-- identd doesn't run on a lot of systems\n-- This doesn't really provide any more security than the default hadoop model of \"trust that you are who you say you are\"\n-- This is problematic if you *need* to be able to spoof other user roles. For the example of a web UI, the authentication would happen on that layer, and even though the thrift connections would be coming from \"www-data\" the actual access should assume the role of the authenticated end user for all RPCs\n\nRegarding the authentication issue, I would propose a somewhat major change. We should add a struct called CallContext that looks something like this:\n\n{code}\nstruct CallContext {\n  1:string user\n  2:list<string> groups\n}\n{code}\n\nWe should then add this as a parameter to every RPC, at least in the NameNode service. Since Thrift uses id numbers for parameters, this will safely end up as null for any callers that are using the old interface. We should then add something to ThriftUtils along the lines of:\n\n{code}\nCallContext completeCallContext(CallContext ctx) {\n  if (null == ctx) {\n    ctx = new CallContext();\n    ... // get call context using identd as it does currently;\n  }\n  return ctx\n}\n{code}\n\nWith this code, any existing clients preserve their identd behavior with no issues.\n\nIf people are concerned with the lack of security here, I would make the following points:\n- This is the status quo for security in Hadoop - none of the IPC protocols have any kind of strong authentication\n- The current Thrift interfaces don't check UGI at the datanode level anyway\n- identd is easily hackable in the first place\n- using a generic CallContext struct for all calls would easily allow us to extend the struct later with some kind of authorization token to provide strong authentication when such a system is devised elsewhere in Hadoop","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-29T20:38:31.252+0000","updated":"2009-04-29T20:38:31.252+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12704625","id":"12704625","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"{quote}\n* The Block struct was missing the startOffset from LocatedBlock. This made it somewhat useful for actually reading out of the middle of files. I'll submit a patch for this once I aggregate a few more changes.\n{quote}\n\nYep, sounds very good.\n\n{quote}\nRegarding the authentication issue, I would propose a somewhat major change. We should add a struct called CallContext that looks something like this:\n\nstruct CallContext {\n  1:string user\n  2:list<string> groups\n}\n\nWe should then add this as a parameter to every RPC, at least in the NameNode service.\n{quote}\n\nI would prefer not to follow that route and leave the method signatures untouched, for the following reasons:\n # The identd approach is far from perfect, definitely, but it is more than enough for my particular requirements.\n # The Thrift IDL file, which I see as the main documentation source for the exposed service, becomes cluttered.\n # Although old clients would still be able to call the Thrift services, C++ clients generated from the new IDL source would get the extra argument in the signature, and the user code would be less readable.\n\nIf peope are really concerned about authentication, perhaps it would be better to add some kind of transport-level authentication support for Thrift itself (like TLS- or SSL-enabled Thrift transports, for instance). Then on the server side we could either start a {{org.apache.thrift.transport.TServerSocket}} or a {{org.apache.thrift.transport.TSecureServerSocket}}, depending on some configuration value, and then everyone would be catered for.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-30T13:10:43.916+0000","updated":"2009-04-30T13:10:43.916+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12704663","id":"12704663","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"bq. The identd approach is far from perfect, definitely, but it is more than enough for my particular requirements.\n\nNot to be a pain, but unfortunately it's completely inadequate for my particular requirements ;-) I need to be able to assume the identity of other users in the system.\n\nbq. The Thrift IDL file, which I see as the main documentation source for the exposed service, becomes cluttered.\n\nThis is somewhat true. We can use the new-ish \"thrift -gen html\" to generate html documentation, though, which at least is slightly nicer to look at. I don't think adding a single common parameter to every call really clutters things that much, though, since the additional mental burden is \"O(1)\". Once you understand what the parameter means on one call, you understand it everywhere :)\n\nbq.  Although old clients would still be able to call the Thrift services, C++ clients generated from the new IDL source would get the extra argument in the signature, and the user code would be less readable.\n\nYes, it's slightly less readable, but on the other hand I assume that anyone using this service will be wrapping the thrift client in some kind of fliesystem facade anyway. The change then is confined to a single module. The fact that existing C++ code will need changes doesn't concern me too much since (a) it's a very clear compilation-time error, not something subtle and hard to find, and (b) this hasn't been committed yet, so now is the one time we should feel especially free to break backwards compat!\n\nbq. Then on the server side we could either start a org.apache.thrift.transport.TServerSocket or a org.apache.thrift.transport.TSecureServerSocket, depending on some configuration value\n\nThis doesn't really solve my issue. I don't care about authentication from an access control perspective. I care more about the ability to assume different user roles. One primary use case for this Thrift interface, as I see it, is so that web applications written in non-Java languages can access HDFS more easily. Web applications typically run as some user like www-data or nobody, which is problematic if they really need to be assuming the identity of the end user (tlipcon) who has provided some authentication via a login form, etc.\n\nThe other option I considered was to add a su(...) RPC and use a specialized TTransport on the server side to hold the current UGI, but this breaks a lot of layer encapsulation properties. Namely, the TTransport cannot automatically reconnect to the server, since the UGI will be lost after the reconnect.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-04-30T15:20:09.944+0000","updated":"2009-04-30T15:20:09.944+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12704687","id":"12704687","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"bq.Not to be a pain, but unfortunately it's completely inadequate for my particular requirements I need to be able to assume the identity of other users in the system.\n\nNot a pain at all --- just a different set of requirements ;-)\n\n{quote}\nbq.The Thrift IDL file, which I see as the main documentation source for the exposed service, becomes cluttered.\n\n[..] Yes, it's slightly less readable, but on the other hand I assume that anyone using this service will be wrapping the thrift client in some kind of fliesystem facade anyway.\n{quote}\n\nGood point: I already have wrapper code for Perl, Python and I'm starting with the C++ stuff. I could handle the user parameter in the method wrappers without exposing it on the method wrappers' signature.\n\nNo objections from my side, then.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-04-30T16:27:08.324+0000","updated":"2009-04-30T16:27:08.324+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12706322","id":"12706322","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"body":"Assigning this to Todd (with his agreement), since I won't have much time to work on this for the coming weeks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=carlos.valiente","name":"carlos.valiente","key":"carlos.valiente","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=carlos.valiente&avatarId=20337","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=carlos.valiente&avatarId=20337","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=carlos.valiente&avatarId=20337","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=carlos.valiente&avatarId=20337"},"displayName":"Carlos Valiente","active":true,"timeZone":"Etc/UTC"},"created":"2009-05-06T07:13:26.341+0000","updated":"2009-05-06T07:13:26.341+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12706595","id":"12706595","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Made some more iterations on this. As this is growing somewhat large (especially with the diff including all the generated code), I've also put a branch up on github for those wanting to follow along in a more easily parseable fashion:\n\nhttp://github.com/toddlipcon/hadoop/commits/hadoop-4707\n\nI'll also upload an up to date patch momentarily.\n\nSummary of changes since last upload:\n*      Avoid NPE when getBlocks is called with offset past end of file - instead, return no blocks\n*      Make stat throw FileNotFoundException for bad paths\n*      Add a clazz field to IOException thrift type for the specific subclass.\n*      Add thriftfs to contrib test target\n*      Refactor thrift contrib into more classes to clean things up a bit\n*      HADOOP-4707: Add RequestContext parameter to all RPCs and extract UGI from them\n*      HADOOP-4707: Add a basic DFS Health Report call\n*      HADOOP-4707: Make services inherit from a base service that provides VersionInfo\n*      HADOOP-4707: Expose JVM heap information and a stack trace in base\n*      HADOOP-4707: Add access to Metrics in base service\n\nI'd also like to propose moving this code into contrib/thrift rather than contrib/thriftfs. The motivation for this is that I will soon be working on JobTracker and TaskTracker thrift plugins, and there's a lot of common code I want to share. Putting that code in a module called \"thriftfs\" seems incorrect. Additionally, contrib/thriftfs currently is the home for two entirely separate projects - this, and the earlier \"proxy style\" thrift wrapper of the DFS client. Since these two projects share no code, it makes no sense for them to be in the same contrib dir.\n\nIf no one has any objections, I'll do this move in the next day or two.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-06T21:21:00.727+0000","updated":"2009-05-06T21:21:00.727+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12706632","id":"12706632","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=euphoria","name":"euphoria","key":"euphoria","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Greene","active":true,"timeZone":"America/Chicago"},"body":"Todd: Any chance you could try out my patch for THRIFT-394 and include the C# bindings in the patch for this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=euphoria","name":"euphoria","key":"euphoria","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Michael Greene","active":true,"timeZone":"America/Chicago"},"created":"2009-05-06T22:29:51.646+0000","updated":"2009-05-06T22:29:51.646+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12706635","id":"12706635","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Michael: Carlos and I decided offline that we probably shouldn't be including the language bindings in the patch at all, but rather expect users to generate them themselves. Can you try using thrift -gen csharp if/hdfs.thrift and see how that goes?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-06T22:33:20.363+0000","updated":"2009-05-06T22:33:20.363+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12706686","id":"12706686","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"> \"proxy style\" thrift wrapper of the DFS client.\n\nThis code should be made obselete (and deleted) when this patch goes into trunk. It would be nice to keep the name of the file system related classes (from this patch)to be org.apache.hadoop.thriftfs though.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-07T00:58:50.859+0000","updated":"2009-05-07T00:58:50.859+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12706705","id":"12706705","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"bq.  It would be nice to keep the name of the file system related classes (from this patch)to be org.apache.hadoop.thriftfs though.\n\nSure thing. You're fine with renaming the directory to src/contrib/thrift though?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-07T03:33:17.610+0000","updated":"2009-05-07T03:33:17.610+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12706706","id":"12706706","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"> You're fine with renaming the directory to src/contrib/thrift though?\n\nSure. \n\nThe only caveat is that the convention is to match the package name with the directory name. So, if you have the package name is org.apache.hadoop.thriftfs, then it might be convenient to keep the directory name same as what we got now.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-07T03:40:21.352+0000","updated":"2009-05-07T03:40:21.352+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12709537","id":"12709537","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"bq. The only caveat is that the convention is to match the package name with the directory name. So, if you have the package name is org.apache.hadoop.thriftfs, then it might be convenient to keep the directory name same as what we got now.\n\nHow about a package structure like:\n- org.apache.hadoop.thrift - base service, utility classes\n- org.apache.hadoop.thrift.hdfs  - NameNodePlugin, DataNodePlugin, common DFS-related \"convert to thrift\" functions\n- org.apache.hadoop.thrift.mapred - for JobTracker/TaskTracker thrift interfaces when they come along\n\nI think this makes the most sense to me at this point. Renaming contrib/thriftfs to contrib/thrift is a pain, but I would hate to see a mapred dir inside a \"thriftfs\" package or contrib project.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-14T19:12:01.668+0000","updated":"2009-05-14T19:12:01.668+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12709539","id":"12709539","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"I'd like to stop work on this JIRA with the most recent patch (31c331) and perform the above package refactoring in a new JIRA.\n\nFor those that would like a more granular review, the commits are available on github:\n\nhttp://github.com/toddlipcon/hadoop/tree/hadoop-4707\n\nThe diff includes all of these commits compared against HADOOP-5640, which is required by this issue. I'll also upload the newest libthrift.jar that I've been testing with (slightly newer than Carlos's). This needs to be put in src/contrib/thrift/lib/. md5sum should be fc6596d8c9b8964ac60cd80c277b9f92","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-14T19:17:08.617+0000","updated":"2009-05-14T19:17:08.617+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12709543","id":"12709543","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"How about a package structure that is like this:\n\norg.apache.hadoop.thriftfs\norg.apache.hadoop.thriftmp","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-14T19:24:37.572+0000","updated":"2009-05-14T19:24:37.572+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12709564","id":"12709564","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"{quote}\nHow about a package structure that is like this:\n\norg.apache.hadoop.thriftfs\norg.apache.hadoop.thriftmp\n{quote}\n\nWhere does the common code go? In the most recent patch, the NN and DN services extend a common HadoopService which exposes common elements like JVM stats, VersionInfo, and Metrics. I'd certainly want to share this code between mr and fs. There's also some common code for starting up a Thrift server, handling UserGroupInformation, etc.\n\nI just opened HADOOP-5840 to continue this discussion.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-14T20:27:36.694+0000","updated":"2009-05-14T20:27:36.694+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12712826","id":"12712826","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"The old patch fell out of date because of the new AccessToken work in HADOOP-4359. The attached patch adds an AccessToken struct to the Thrift side.\n\nI also took the opportunity to remove all of the generated code aside from gen-java as we decided above.\n\nAs before, this patch is relative to and blocked by HADOOP-5640 (plugin infrastructure with hooks on important actions)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-26T00:46:30.396+0000","updated":"2009-05-26T00:46:30.396+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12712827","id":"12712827","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Marking as patch available, though Hudson is likely to fail building it since it requires the new libthrift.jar, and the patch is gzipped since it was large. Tests pass for me. Since the only modification to non-contrib is the addition of DatanodeID.getLongString(), and this is currently blocking HADOOP-5840 (thrift contrib reorg) and through that, HADOOP-5703 (jobtracker/mapred thrift), I'd really appreciate if someone could take a look at this (and HADOOP-5640 which it depends on) ASAP.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-05-26T00:50:13.014+0000","updated":"2009-05-26T00:50:13.014+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12713661","id":"12713661","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12408978/hadoop-4707-867f26.txt.gz\n  against trunk revision 779106.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    -1 tests included.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no tests are needed for this patch.\n\n    -1 patch.  The patch command could not apply the patch.\n\nConsole output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/412/console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2009-05-27T17:58:57.701+0000","updated":"2009-05-27T17:58:57.701+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12716845","id":"12716845","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"Thsi patch allows the hdfs-thrift server to run inside the NN. This is a good thing. However, some administrators might like to keep running the hdfs-thrift server separately from the namenode (as it currently stands in trunk)... the reason being that the NN is already a big memory hog and allowing the hdfs-thrift server to share the same address space could introduce new resource bottlenecks in the NN that could impact the whole cluster stability. Do you agree?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-06-06T07:18:59.746+0000","updated":"2009-06-06T07:18:59.746+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12716901","id":"12716901","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Hey Dhruba,\n\nIt's true that this introduces code that runs in the NN. However, the design of this new code maps one-for-one onto the communication design of HDFS itself - no data goes through the NN thrift server. The additional load on the NN is simply the metadata RPCs that the clients are sending, and of course the sockets serving the Thrift connections. Given that the existing (external) Thrift service just fowards the metadata RPCs to the NameNode, there's no additional load there. The external server probably does demultiplex them down to fewer RPC connections, and Thrift is probably less CPU-efficient than the hand-coded Writable-based RPCs that Hadoop uses internally.\n\nAs you mentioned, and correlated with my experience, the NN resource that is in short supply is memory more often than CPU. Given that, I don't think the additional load from the Thrift SerDe is going to end up being significant.\n\nIf we expect that there will be a high number of concurrent clients to the NN Thrift service, switching out the TThreadPoolServer for something based on the new-ish TNonBlockingServer would decrease memory consumption by reducing the number of threads. I can look into that if you like.\n\nHowever, I do agree that some ops people will have philosophical objections to putting more code in the NameNode. This makes sense, and might be a good reason to keep the external Thrift gateway alive. However, I haven't touched its code, and don't want to commit to maintain it implicitly by adding new code next to it :)\n\nThanks\n-Todd","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-06-06T18:28:21.740+0000","updated":"2009-06-06T18:28:21.740+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12716998","id":"12716998","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"@Todd, thanks for the info.\n\nI am kinda suspicious about the memory requirements for the Thrift server. It is possible that I might like to to run it separately from the NN for starters. For example, our NN is always kinds maxed out on memory and it would be a definite no-no to make the thridt server run inside it. On the other hand, some admins might be ok runnign these two pieces two together.\n\nis it possible to enhance this patch so that the thrift-hdfs server can be run separately from the NN?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-06-06T23:40:17.431+0000","updated":"2009-06-06T23:40:17.431+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12726134","id":"12726134","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"is it possible to enhance this patch so that the thrift-hdfs server can be run separately from the NN?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-07-01T17:13:20.132+0000","updated":"2009-07-01T17:13:20.132+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12726141","id":"12726141","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Woops, sorry, didn't realize I hadn't updated the JIRA a couple weeks ago.\n\nWith the current feature set it's possible to do so, since the NN plugin only accesses the NameNode by making calls on it that are available over RPC. Doing so would probably be a day's work, so I'd like to do it as a separate JIRA later on. Additionally, I'm worried that making this architecture change will tie our hands in the future if we want to make any calls from the NN into the Plugin using hooks like in HADOOP-5640.\n\nI think we had talked offline and decided that, as long as the old Thrift server remains in tact, your use case won't be disturbed for now. Is that OK?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2009-07-01T17:27:43.718+0000","updated":"2009-07-01T17:27:43.718+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12867798","id":"12867798","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"I am canceling this patch because it is sadly out of date and we do not yet have consensus on what to change. Please resurrect this discussion if you so desire. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2010-05-15T05:26:49.878+0000","updated":"2010-05-15T05:26:49.878+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12409027/comment/12867800","id":"12867800","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Yes, I agree - it seems the demand in the community isn't high for this at the moment, and Avro is coming down the pipe anyway. Resolving wontfix.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2010-05-15T05:28:47.343+0000","updated":"2010-05-15T05:28:47.343+0000"}],"maxResults":51,"total":51,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-417/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0ivhr:"}}