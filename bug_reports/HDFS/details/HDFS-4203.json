{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12616487","self":"https://issues.apache.org/jira/rest/api/2/issue/12616487","key":"HDFS-4203","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2012-11-16T22:32:11.180+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Nov 16 22:32:11 UTC 2012","customfield_12310420":"258353","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-4203/watchers","watchCount":11,"isWatching":false},"created":"2012-11-16T21:57:03.334+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12317959","id":"12317959","description":"1.1.0 release","name":"1.1.0","archived":false,"released":true,"releaseDate":"2012-10-13"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2012-11-16T22:32:20.854+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312926","id":"12312926","name":"namenode"}],"timeoriginalestimate":null,"description":"After calling recoverFileLease, an append to a file gets stuck retying this:\n\n{code}\n2012-11-16 13:06:14,298 DEBUG [IPC Server handler 2 on 53224] namenode.PendingReplicationBlocks(92): Removing pending replication for blockblk_-3222397051272483489_1006\n2012-11-16 13:06:43,881 WARN  [DataStreamer for file /hbase/hlog/hlog.dat.2 block blk_-3222397051272483489_1003] hdfs.DFSClient$DFSOutputStream(3216): Error Recovery for block blk_-3222397051272483489_1003 bad datanode[0] 127.0.0.1:53228\n2012-11-16 13:06:43,881 WARN  [DataStreamer for file /hbase/hlog/hlog.dat.2 block blk_-3222397051272483489_1003] hdfs.DFSClient$DFSOutputStream(3267): Error Recovery for block blk_-3222397051272483489_1003 in pipeline 127.0.0.1:53228, 127.0.0.1:53231: bad datanode 127.0.0.1:53228\n2012-11-16 13:06:43,884 INFO  [IPC Server handler 1 on 53233] datanode.DataNode(2123): Client calls recoverBlock(block=blk_-3222397051272483489_1003, targets=[127.0.0.1:53231])\n2012-11-16 13:06:43,884 DEBUG [IPC Server handler 1 on 53233] datanode.FSDataset(2143): Interrupting active writer threads for block blk_-3222397051272483489_1006\n2012-11-16 13:06:43,884 DEBUG [IPC Server handler 1 on 53233] datanode.FSDataset(2159): getBlockMetaDataInfo successful block=blk_-3222397051272483489_1006 length 120559 genstamp 1006\n2012-11-16 13:06:43,884 DEBUG [IPC Server handler 1 on 53233] datanode.DataNode(2039): block=blk_-3222397051272483489_1003, (length=120559), syncList=[BlockRecord(info=BlockRecoveryInfo(block=blk_-3222397051272483489_1006 wasRecoveredOnStartup=false) node=127.0.0.1:53231)], closeFile=false\n2012-11-16 13:06:43,885 INFO  [IPC Server handler 2 on 53224] namenode.FSNamesystem(5468): blk_-3222397051272483489_1003 has out of date GS 1003 found 1006, may already be committed\n2012-11-16 13:06:43,885 ERROR [IPC Server handler 2 on 53224] security.UserGroupInformation(1139): PriviledgedActionException as:stack cause:java.io.IOException: blk_-3222397051272483489_1003 has out of date GS 1003 found 1006, may already be committed\n2012-11-16 13:06:43,885 ERROR [IPC Server handler 1 on 53233] security.UserGroupInformation(1139): PriviledgedActionException as:blk_-3222397051272483489_1003 cause:org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3222397051272483489_1003 has out of date GS 1003 found 1006, may already be committed\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5469)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:781)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)\n\n2012-11-16 13:06:43,886 WARN  [DataStreamer for file /hbase/hlog/hlog.dat.2 block blk_-3222397051272483489_1003] hdfs.DFSClient$DFSOutputStream(3292): Failed recovery attempt #1 from primary datanode 127.0.0.1:53231\norg.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3222397051272483489_1003 has out of date GS 1003 found 1006, may already be committed\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5469)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:781)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1092)\n\tat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)\n\tat $Proxy12.nextGenerationStamp(Unknown Source)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2059)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2027)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2107)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1092)\n\tat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)\n\tat $Proxy17.recoverBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3290)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2300(DFSClient.java:2754)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2958)\n{code}\n\nEventually the DFSClient fails with:\n\n{code}\n2012-11-16 13:06:53,945 WARN  [ZombieLastLogWriterRegionServer] hdfs.DFSClient$DFSOutputStream(3918): Error while syncing\njava.io.IOException: Error Recovery for block blk_-3222397051272483489_1003 failed  because recovery from primary datanode 127.0.0.1:53228 failed 6 times.  Pipeline was 127.0.0.1:53228. Aborting...\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3326)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2300(DFSClient.java:2754)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2958)\n\n{code}\n\nIt is good that it eventually comes back so this issue does not mean it is the end of the world.  It seems uninterruptible up in the client while it is doing the above retrying.  It would just be nice if the DN gave up quicker when lease is gone.\n\nLooking in code, it seems that in DataNode#syncBlock, we are doing an explicit call against the NN to getNewGenerationStamp and it is this call that is failing.  Should we be checking we have the file lease first?\n\nI see this issue in 1.1 and 1.0.  It looks like it is also in trunk.  I've not tried it.\n\nIf someone made suggestion on what should do here, I can try making a patch.  Thanks.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"119286","customfield_12312823":null,"summary":"After recoverFileLease, datanode gets stuck complaining block '...has out of data GS ....may already be committed'","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stack","name":"stack","key":"stack","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"stack","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12616487/comment/13499204","id":"13499204","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"A few options here, I think:\n\n1) When we call recoverBlock for a client, we pass the client lease holder name to the DN. Then when the DN asks for a new generation stamp, it passes along the lease holder to the NN, who can reject if it's not the current lease holder. This would likely be difficult to do in branch-1 without breaking compatibility, but could be done in branch-2 given we have protobufs.\n\n2) In the client, if the error message we get back from attempting recovery is that the generation stamp is out of date, then no number of retries is going to help, so we can immediately stop retrying at that point.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-11-16T22:32:11.180+0000","updated":"2012-11-16T22:32:11.180+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-4203/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0krl3:"}}