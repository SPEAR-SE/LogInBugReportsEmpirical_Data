{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12737769","self":"https://issues.apache.org/jira/rest/api/2/issue/12737769","key":"HDFS-6974","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-09-02T20:19:15.446+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri May 29 08:04:34 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6974/watchers","watchCount":5,"isWatching":false},"created":"2014-08-29T10:36:18.960+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327181","id":"12327181","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"issuelinks":[{"id":"12397625","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12397625","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12743886","key":"HADOOP-11127","self":"https://issues.apache.org/jira/rest/api/2/issue/12743886","fields":{"summary":"Improve versioning and compatibility support in native library for downstream hadoop-common users.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}},{"id":"12395495","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12395495","type":{"id":"12310050","name":"Regression","inward":"is broken by","outward":"breaks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310050"},"outwardIssue":{"id":"12737767","key":"SLIDER-377","self":"https://issues.apache.org/jira/rest/api/2/issue/12737767","fields":{"summary":"slider MiniHDFSCluster tests failing on windows+branch2","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/7","id":"7","description":"The sub-task of the issue","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype","name":"Sub-task","subtask":true,"avatarId":21146}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-05-29T08:04:34.932+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312916","id":"12312916","name":"test"}],"timeoriginalestimate":null,"description":"SLIDER-377 shows the trace of a MiniHDFSCluster test failing on native library calls ... the root cause appears to be the 2.4.1 hadoop lib on the path doesn't have all the methods needed by branch-2.\n\nWhen this situation arises, MiniHDFS cluster fails to work.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"MiniHDFScluster breaks if there is an out of date hadoop.lib on the lib path ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Windows with a version of Hadoop (HDP2.1) installed somewhere via an MSI","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737769/comment/14115099","id":"14115099","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"Core stack\n{code}\n4-192.168.1.138-1409307851902:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(243)) - 127.0.0.1:52841:DataXceiver\n error processing WRITE_BLOCK operation  src: /127.0.0.1:52858 dst: /127.0.0.1:52841                                                 \njava.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(II[BI[BIILjava/lang/String;JZ)V \n        at org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(Native Method)                                       \n        at org.apache.hadoop.util.NativeCrc32.verifyChunkedSumsByteArray(NativeCrc32.java:67)                                        \n        at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:344)                                              \n        at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:292)                                              \n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.verifyChunks(BlockReceiver.java:416)                                 \n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:551)                                \n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:771)                                 \n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:718)                                       \n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:126)                                     \n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:72)                                         \n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:225)                                              \n        at java.lang.Thread.run(Thread.java:745)                                                                                     \n2014-08-29 11:24:15,800 [ResponseProcessor for block BP-126326924-192.168.1.138-1409307851902:blk_1073741825_1001] WARN  hdfs.DFSClie\nnt (DFSOutputStream.java:run(880)) - DFSOutputStream ResponseProcessor exception  for block BP-126326924-192.168.1.138-1409307851902:\nblk_1073741825_1001                                                                                                                  \njava.io.EOFException: Premature EOF: no length prefix available                                                                      \n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2081)                                               \n        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)                                 \n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:79\n{code}\n\nI can work around this in my tests by simply disabling JNI load, but it could be a problem that re-occurs elsewhere. The native lib is loading (so the flag is set), but the operations downstream are failing in more obscure ways.\n\nWhat to do?\n# nothing. This JIRA and stack trace can be used to identify the problem and fix (get rid of that lib/change the {{\"java.library.path\"}})\n# do some post-load tests ... are all the methods there? But if not: what to do?\n# include some version numbering on the library, so the first version of {{hadoop.dll}}, or {{hadoop.lib}} isn't blindly picked up. \n\nI'd argue for action 3, numbering the native lib with the Hadoop version built.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2014-08-29T10:41:13.024+0000","updated":"2014-08-29T10:41:13.024+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737769/comment/14115110","id":"14115110","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"There's another extension too: have a {{getVersion()}} call that returns version info (build info etc), which can be used to help in diags. I'd add that, but still look for hadoop-2.6.lib so that you could  have >1 lib on the path","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2014-08-29T10:44:09.917+0000","updated":"2014-08-29T10:44:09.917+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737769/comment/14118668","id":"14118668","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cmccabe","name":"cmccabe","key":"cmccabe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cmccabe&avatarId=29060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cmccabe&avatarId=29060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cmccabe&avatarId=29060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cmccabe&avatarId=29060"},"displayName":"Colin P. McCabe","active":true,"timeZone":"America/Los_Angeles"},"body":"Is this really any different than needing to set {{HADOOP_CLASSPATH}} correctly?  We don't handle mixing old jars into the classpath, so why should we handle mixing old {{hadoop.dll}} files into the path?  It seems inconsistent.  But maybe I'm missing something that makes this case different.\n\nbq. There's another extension too: have a getVersion() call that returns version info (build info etc), which can be used to help in diags. I'd add that, but still look for hadoop-2.6.lib so that you could have >1 lib on the path\n\nWe don't make any guarantees that the libhadoop supplied with 2.6 will work with Hadoop 2.6.1.  libhadoop doesn't have a fixed or standardized API; it's just \"the C half\" of random bits of Hadoop code.\n\nThink if you were making changes to the JNI code and redeploying.  You need to redeploy with the correct, new JNI code, not the old stuff.  This again, the same as with jar files... you wouldn't mix jar files from Hadoop 2.6 and Hadoop 2.6.1 in the same directory.  So I would argue for your solution #1.\n\nWe could perhaps give a better error message here.  We might be able to inject the git hash into the library, and error out if it didn't match the git hash in the jar files.  But then that means that partial rebuilds of the source tree no longer work, so maybe not.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cmccabe","name":"cmccabe","key":"cmccabe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cmccabe&avatarId=29060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cmccabe&avatarId=29060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cmccabe&avatarId=29060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cmccabe&avatarId=29060"},"displayName":"Colin P. McCabe","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-09-02T20:19:15.446+0000","updated":"2014-09-02T20:19:15.446+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737769/comment/14564384","id":"14564384","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Skamandros","name":"Skamandros","key":"skamandros","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=skamandros&avatarId=29555","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=skamandros&avatarId=29555","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=skamandros&avatarId=29555","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=skamandros&avatarId=29555"},"displayName":"Michael Schmeißer","active":true,"timeZone":"Europe/Berlin"},"body":"The existence of this ticket helped to track down the problem, but if it is feasible, a more meaningfull error message would help here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Skamandros","name":"Skamandros","key":"skamandros","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=skamandros&avatarId=29555","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=skamandros&avatarId=29555","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=skamandros&avatarId=29555","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=skamandros&avatarId=29555"},"displayName":"Michael Schmeißer","active":true,"timeZone":"Europe/Berlin"},"created":"2015-05-29T08:04:34.932+0000","updated":"2015-05-29T08:04:34.932+0000"}],"maxResults":4,"total":4,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6974/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1zhbr:"}}