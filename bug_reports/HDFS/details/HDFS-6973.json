{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12737750","self":"https://issues.apache.org/jira/rest/api/2/issue/12737750","key":"HDFS-6973","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-08-29T14:56:27.612+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Nov 17 06:40:36 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6973/watchers","watchCount":19,"isWatching":false},"created":"2014-08-29T08:58:14.203+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326143","id":"12326143","description":"2.4.0 release","name":"2.4.0","archived":false,"released":true,"releaseDate":"2014-04-07"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-11-17T06:40:36.978+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"HBase as HDFS Client dose not close a dead connection with the datanode.\nThis resulting in over 30K+ CLOSE_WAIT and at some point HBase can not connect to the datanode because too many mapped sockets from one host to another on the same port:50010. \nAfter I restart all RSs, the count of CLOSE_WAIT will increase always.\n$ netstat -an|grep CLOSE_WAIT|wc -l\n2545\nnetstat -nap|grep CLOSE_WAIT|grep 6569|wc -l\n2545\nps -ef|grep 6569\nhbase 6569 6556 21 Aug25 ? 09:52:33 /opt/jdk1.6.0_25/bin/java -Dproc_regionserver -XX:OnOutOfMemoryError=kill -9 %p -Xmx1000m -XX:+UseConcMarkSweepGC\nI aslo have reviewed these issues:\n[HDFS-5697]\n[HDFS-5671]\n[HDFS-1836]\n[HBASE-9393]\nI found in HBase 0.98/Hadoop 2.4.0 source codes of these patchs have been added.\nBut I donot understand why HBase 0.98/Hadoop 2.4.0 also have this isssue. Please check. Thanks a lot.\nThese codes have been added into BlockReaderFactory.getRemoteBlockReaderFromTcp(). Another bug maybe lead my problem,\n{code:title=BlockReaderFactory.java|borderStyle=solid}\n// Some comments here\n  private BlockReader getRemoteBlockReaderFromTcp() throws IOException {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(this + \": trying to create a remote block reader from a \" +\n          \"TCP socket\");\n    }\n    BlockReader blockReader = null;\n    while (true) {\n      BlockReaderPeer curPeer = null;\n      Peer peer = null;\n      try {\n        curPeer = nextTcpPeer();\n        if (curPeer == null) break;\n        if (curPeer.fromCache) remainingCacheTries--;\n        peer = curPeer.peer;\n        blockReader = getRemoteBlockReader(peer);\n        return blockReader;\n      } catch (IOException ioe) {\n        if (isSecurityException(ioe)) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(this + \": got security exception while constructing \" +\n                \"a remote block reader from \" + peer, ioe);\n          }\n          throw ioe;\n        }\n        if ((curPeer != null) && curPeer.fromCache) {\n          // Handle an I/O error we got when using a cached peer.  These are\n          // considered less serious, because the underlying socket may be\n          // stale.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Closed potentially stale remote peer \" + peer, ioe);\n          }\n        } else {\n          // Handle an I/O error we got when using a newly created peer.\n          LOG.warn(\"I/O error constructing remote block reader.\", ioe);\n          throw ioe;\n        }\n      } finally {\n        if (blockReader == null) {\n          IOUtils.cleanup(LOG, peer);\n        }\n      }\n    }\n    return null;\n  }\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"DFSClient does not closing a closed socket resulting in thousand of CLOSE_WAIT sockets","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevenxu","name":"stevenxu","key":"stevenxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"steven xu","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevenxu","name":"stevenxu","key":"stevenxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"steven xu","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"RHEL 6.3 -HDP 2.1 -6 RegionServers/Datanode -18T per node -3108Regions","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/14115304","id":"14115304","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~stevenxu], thanks for finding the reporting the issue. HBASE-9393 reported same issue as you reported, however, this jira is still unresolved. I saw [~cmccabe]  did some anaylsis and gave suggestion there (Colin Patrick McCabe added a comment - 11/Oct/13 19:25), which makes sense to me. I wonder if you can try what he recommended there? Thanks.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-08-29T14:56:27.612+0000","updated":"2014-08-29T14:56:27.612+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/14609767","id":"14609767","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"body":"I have the same problem,too many tcp status wait_close to datanode 50010. According to issue HBASE-9393 ，set dfs.client.socketcache.capacity to 0 and dfs.datanode.socket.reuse.keepalive to 0, but not take effect, How did you solve this problem?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=lvchuanwen","name":"lvchuanwen","key":"lvchuanwen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"lvchuanwen","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-07-01T08:52:45.380+0000","updated":"2015-07-01T08:52:45.380+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/15072108","id":"15072108","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zdtjkl","name":"zdtjkl","key":"zdtjkl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zdtjkl","active":true,"timeZone":"Etc/UTC"},"body":"forbid IPv6 function in  all nodes, I  meet the problem，you try it","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zdtjkl","name":"zdtjkl","key":"zdtjkl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"zdtjkl","active":true,"timeZone":"Etc/UTC"},"created":"2015-12-27T09:54:16.117+0000","updated":"2015-12-27T09:54:16.117+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/15108213","id":"15108213","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=max_datapath","name":"max_datapath","key":"max_datapath","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Max Schmidt","active":true,"timeZone":"Europe/Berlin"},"body":"I can relate to that. We're using a org.apache.hadoop.fs.FSDataInputStream for reading multiple files continuously 2 times an hour from a 2.7.1 cluster. \n\nI've added \"-Djava.net.preferIPv4Stack=true\" and \"-Djava.net.preferIPv6Addresses=false\" but it only changed that the sockets are now ipv4 instead of ipv6.\n\nAfter 12 hours usage, 1.4K open sockets:\n\njava    10486 root 2233u  IPv4           28226850      0t0      TCP 10.134.160.9:55927->10.134.160.28:50010 (CLOSE_WAIT)\njava    10486 root 2237u  IPv4           28223758      0t0      TCP 10.134.160.9:37363->10.134.160.17:50010 (CLOSE_WAIT)\njava    10486 root 2240u  IPv4           28223759      0t0      TCP 10.134.160.9:48976->10.134.160.41:50010 (CLOSE_WAIT)\njava    10486 root 2248u  IPv4           28222398      0t0      TCP 10.134.160.9:55976->10.134.160.28:50010 (CLOSE_WAIT)\njava    10486 root 2274u  IPv4           28222403      0t0      TCP 10.134.160.9:53185->10.134.160.35:50010 (CLOSE_WAIT)\njava    10486 root 2283u  IPv4           28211085      0t0      TCP 10.134.160.9:56009->10.134.160.28:50010 (CLOSE_WAIT)\n\n10.134.160.9 ip of the host with the driver programm, dst-ips are the hadoop-nodes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=max_datapath","name":"max_datapath","key":"max_datapath","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Max Schmidt","active":true,"timeZone":"Europe/Berlin"},"created":"2016-01-20T08:35:49.997+0000","updated":"2016-01-20T08:35:49.997+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/15110342","id":"15110342","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=max_datapath","name":"max_datapath","key":"max_datapath","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Max Schmidt","active":true,"timeZone":"Europe/Berlin"},"body":"Sorry here, but i found out that our driver programm forgot to close the FSDataInputStream at some place, which fixed the behaviour above.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=max_datapath","name":"max_datapath","key":"max_datapath","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Max Schmidt","active":true,"timeZone":"Europe/Berlin"},"created":"2016-01-21T09:42:02.374+0000","updated":"2016-01-21T09:42:02.374+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/15110999","id":"15110999","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"HBASE-9393, is due to unclosed streams maintained in hbase for later reads.\nnumber of CLOSE_WAITs is same as number of streams kept open. \nWhen the stream is re-used for reading, corresponding CLOSE_WAIT will get closed and read will happen by opening new connection.\n\nSo IMO, this is not a problem. As already suggested in HBASE-9393, if want to keep the stream open, without keeping the socket open, FSDataInputStrean#unbuffer() can be called after reading to close the block readers.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2016-01-21T17:54:08.919+0000","updated":"2016-01-21T17:54:08.919+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/15657103","id":"15657103","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gfeng","name":"gfeng","key":"gfeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gfeng","active":true,"timeZone":"Etc/UTC"},"body":"Disable IPv6 cannot kill the problem. The number of opened socket in IPv4 keep increase.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=gfeng","name":"gfeng","key":"gfeng","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"gfeng","active":true,"timeZone":"Etc/UTC"},"created":"2016-11-11T13:42:35.631+0000","updated":"2016-11-11T13:42:35.631+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/16201706","id":"16201706","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yaolong+zhu","name":"yaolong zhu","key":"yaolong zhu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yaolong zhu","active":true,"timeZone":"Etc/UTC"},"body":"I also meet a similar issue. We use parquet file in a 2.7.1 hdfs cluster to store result from spark. In my part, I need to read the parquet file by java. But I found the lots of CLOSW_WAIT connection to 50010 port.\r\n\r\ntcp        1      0 192.168.1.83:50437      192.168.1.83:50010      CLOSE_WAIT  46462/java          \r\ntcp        1      0 192.168.1.83:50539      192.168.1.83:50010      CLOSE_WAIT  46462/java          \r\ntcp        1      0 192.168.1.83:50520      192.168.1.83:50010      CLOSE_WAIT  46462/java          \r\ntcp        1      0 192.168.1.83:50540      192.168.1.83:50010      CLOSE_WAIT  46462/java          \r\ntcp        1      0 192.168.1.83:50513      192.168.1.83:50010      CLOSE_WAIT  46462/java          \r\ntcp        1      0 192.168.1.83:50440      192.168.1.83:50010      CLOSE_WAIT  46462/java          \r\ntcp        1      0 192.168.1.83:50530      192.168.1.83:50010      CLOSE_WAIT  46462/java\r\n\r\nAnd in my cod I have closed the parquet reader. Here is my code\r\n\r\n        List<Map<String, Object>> result = new ArrayList<>();\r\n        Path path = new Path(filePath);\r\n        GroupReadSupport readSupport = new GroupReadSupport();\r\n        ParquetReader.Builder<Group> builder = ParquetReader.builder(readSupport, path).withConf(configuration);\r\n        ParquetReader<Group> reader = builder.build();\r\n        Group record = null;\r\n\r\n        while ((record = reader.read()) != null) {\r\n            List<Type> columnDescriptors =  record.getType().getFields();\r\n\r\n            Map<String, Object> recordMap = genRecordMap(record, columnDescriptors);\r\n\r\n            result.add(recordMap);\r\n        }\r\n\r\n        reader.close();\r\n        reader = null;\r\n\r\n        return result;\r\n\r\nAny one have ideas on how to fix it?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yaolong+zhu","name":"yaolong zhu","key":"yaolong zhu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yaolong zhu","active":true,"timeZone":"Etc/UTC"},"created":"2017-10-12T09:37:42.743+0000","updated":"2017-10-12T09:37:42.743+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/16251718","id":"16251718","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=robreeves","name":"robreeves","key":"robreeves","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rob Reeves","active":true,"timeZone":"Etc/UTC"},"body":"Yaolong Zhu (JIRA won't let me tag you for some reason), I just ran into the same issue when using ParquetReader. For us it happened after we upgraded parquet-hadoop and parquet-column to version 1.9.0. When we downgraded back to 1.8.1 the issue went away. I haven't had time to search for known issues or report a new issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=robreeves","name":"robreeves","key":"robreeves","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rob Reeves","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-14T16:54:07.128+0000","updated":"2017-11-14T16:54:07.128+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12737750/comment/16256555","id":"16256555","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yaolong+zhu","name":"yaolong zhu","key":"yaolong zhu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yaolong zhu","active":true,"timeZone":"Etc/UTC"},"body":"[~robreeves] Hi Rob, I found the root cause of this issue which lies in the close method of ParquetFileReader.\r\n\r\n@Override\r\n  public void close() throws IOException {\r\n    try {\r\n      if (f != null) {\r\n        f.close();\r\n      }\r\n    } finally {\r\n      if (codecFactory != null) {\r\n        codecFactory.release();\r\n      }\r\n    }\r\n  }\r\n\r\nThe f.close() is actually calling the close() method of InputStream which is an empty method rather than H2SeekableInputStream or H1SeekableInputStream. So I update this close method to \r\n@Override\r\n  public void close() throws IOException {\r\n    try {\r\n      if (f != null) {\r\n        if(f instanceof H2SeekableInputStream) {\r\n          ((H2SeekableInputStream)f).close();\r\n        } else if(f instanceof H1SeekableInputStream) {\r\n          ((H1SeekableInputStream)f).close();\r\n        } else {\r\n          f.close();\r\n        }\r\n\r\n      }\r\n\r\n\r\n    } finally {\r\n      if (codecFactory != null) {\r\n        codecFactory.release();\r\n      }\r\n    }\r\n  }\r\n\r\nAnd the problem is solved. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yaolong+zhu","name":"yaolong zhu","key":"yaolong zhu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yaolong zhu","active":true,"timeZone":"Etc/UTC"},"created":"2017-11-17T06:40:36.978+0000","updated":"2017-11-17T06:40:36.978+0000"}],"maxResults":10,"total":10,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6973/votes","votes":3,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1zh7z:"}}