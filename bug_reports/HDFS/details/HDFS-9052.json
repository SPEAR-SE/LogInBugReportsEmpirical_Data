{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12863206","self":"https://issues.apache.org/jira/rest/api/2/issue/12863206","key":"HDFS-9052","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2015-09-10T21:55:54.784+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Sep 16 18:21:24 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_507834926_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-09-16T18:23:56.679+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-9052/watchers","watchCount":2,"isWatching":false},"created":"2015-09-10T21:20:01.790+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12440106","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12440106","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12735884","key":"HDFS-6908","self":"https://issues.apache.org/jira/rest/api/2/issue/12735884","fields":{"summary":"incorrect snapshot directory diff generated by snapshot deletion","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-09-16T18:23:56.708+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"CDH 5.0.5 upgraded from CDH 5.0.0 (Hadoop 2.3)\n\nUpon deleting a snapshot, we run into the following assertion error. The scenario is as follows:\n1. We have a program that deletes snapshots in reverse chronological order.\n2. The program deletes a couple of hundred snapshots successfully but runs into the following exception:\njava.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]\n3. There seems to be an issue with that snapshot, which causes a file, which normally gets overwritten in every snapshot to be added to the SnapshotDiff delete queue twice.\n4. Once the deleteSnapshot is run on the problematic snapshot, if the Namenode is restarted, it cannot be started again until the transaction is removed from the EditLog.\n5. Sometimes the bad snapshot can be deleted but the prior snapshot seems to \"inherit\" the same issue.\n6. The error below is from Namenode starting when the DELETE_SNAPSHOT transaction is replayed from the EditLog.\n\n2015-09-01 22:59:59,140 INFO  [IPC Server handler 0 on 8022] BlockStateChange (BlockManager.java:logAddStoredBlock(2342)) - BLOCK* addStoredBlock: blockMap updated: 10.52.209.77:1004 is added to blk_1080833995_7093259{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-16de62e5-f6e2-4ea7-aad9-f8567bded7d7:NORMAL|FINALIZED]]} size 0\n2015-09-01 22:59:59,140 INFO  [IPC Server handler 0 on 8022] BlockStateChange (BlockManager.java:logAddStoredBlock(2342)) - BLOCK* addStoredBlock: blockMap updated: 10.52.209.77:1004 is added to blk_1080833996_7093260{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-1def2b07-d87f-49dd-b14f-ef230342088d:NORMAL|FINALIZED]]} size 0\n2015-09-01 22:59:59,141 ERROR [IPC Server handler 0 on 8022] namenode.FSEditLogLoader (FSEditLogLoader.java:loadEditRecords(232)) - Encountered exception on operation DeleteSnapshotOp [snapshotRoot=/data/tenants/pdx-svt.baseline84/wddata, snapshotName=s2015022614_maintainer_soft_del, RpcClientId=7942c957-a7cf-44c1-880d-6eea690e1b19, RpcCallId=1]\n2015-09-01 22:59:59,141 ERROR [IPC Server handler 0 on 8022] namenode.FSEditLogLoader (FSEditLogLoader.java:loadEditRecords(232)) - Encountered exception on operation DeleteSnapshotOp [snapshotRoot=/data/tenants/pdx-svt.baseline84/wddata, snapshotName=s2015022614_maintainer_soft_del, RpcClientId=7942c957-a7cf-44c1-880d-6eea690e1b19, RpcCallId=1]\njava.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]\n        at org.apache.hadoop.hdfs.util.Diff.insert(Diff.java:193)\n        at org.apache.hadoop.hdfs.util.Diff.delete(Diff.java:239)\n        at org.apache.hadoop.hdfs.util.Diff.combinePosterior(Diff.java:462)\n        at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.initChildren(DirectoryWithSnapshotFeature.java:293)\n        at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.iterator(DirectoryWithSnapshotFeature.java:303)\n        at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDeletedINode(DirectoryWithSnapshotFeature.java:531)\n        at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDirectory(DirectoryWithSnapshotFeature.java:823)\n        at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtree(INodeDirectory.java:714)\n        at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtreeRecursively(INodeDirectory.java:684)\n        at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDirectory(DirectoryWithSnapshotFeature.java:830)\n        at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtree(INodeDirectory.java:714)\n        at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.removeSnapshot(INodeDirectorySnapshottable.java:341)\n        at org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.deleteSnapshot(SnapshotManager.java:238)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:667)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:224)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:133)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:802)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:783)","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"deleteSnapshot runs into AssertionError","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14739684","id":"14739684","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the report, Alex.\n\nI'm thinking whether the exception may be caused by HDFS-6908, which has been fixed in release 2.6. It is possible that a stale INode is left in the deleted list due to HDFS-6908, which caused this conflicts (there happened to be another deleted INode with the same local name in the previous deleted list).\n\nBecause 2.3 is a released version, I suggest you to try the latest version (2.7.1) and see whether the same issue can still be reproduced. But the corruption may have to be manually fixed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-10T21:55:54.784+0000","updated":"2015-09-10T21:55:54.784+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14739709","id":"14739709","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"body":"I saw [https://issues.apache.org/jira/browse/HDFS-6908] but since the exception was different, NullPointerException, I opened this Jira.\nI'm not sure how this corruption can be manually fixed since it stems from the FsImage. Do you have a workaround in mind or were you thinking of a patch? Also, maybe you can explain something that I struggle to understand since it's not documented anywhere: how come when a snapshot is deleted, its Diffs are merged with the PRIOR snapshot, and not the subsequent one? I appreciate your help!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-10T22:09:28.539+0000","updated":"2015-09-10T22:09:28.539+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14739734","id":"14739734","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"A possible scenario is like this:\n1. We deleted a file \"foo\" but it belongs to snapshot s1 so it is in s1's deleted list\n2. Under the same directory, we did all the steps listed in the description in HDFS-6908, and the file created in step 3 is also named \"foo\". And we suppose the snapshots created in step 2&4 are named s2 and s3.\n3. Because of the bug reported in HDFS-6908, we may have the later \"foo\" left in s2's deleted list. Then when you try to delete s2 you will hit the above exception.\n\nThe bug fixed in HDFS-6908 is that an INode which should be cleared is wrongly left in the deleted list. In HDFS-6908, because we changed the fsimage format in release 2.4, in the fsimage we only record INode ID in deleted list, and use the id to lookup the inode map. Since the real INode has been cleared from the INode Map, the lookup will hit NPE. You will not see NPE when loading fsimage in 2.3. And this conflict happens only when you have files with the same name (\"foo\" in the above example).\n\nBut the above example is just one possible scenario. It's still possible that the issue is caused by some other bug. To bypass the issue, you may need to apply a temporary patch to ignore the INode in the later snapshot's delete list.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-10T22:26:22.000+0000","updated":"2015-09-10T22:27:12.241+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14740108","id":"14740108","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"body":"I've confirmed that the CDH 5.0.0/5.0.5 code actually contains the Hadoop 2.4 change you're referring to:\n[Grepcode FSImageFormatPBSnapshot.java|http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hadoop/hadoop-hdfs/2.3.0-cdh5.0.5/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java/]\n\nLet me elaborate a little on my scenario. We have a log file in HDFS (useraction.log.crypto), which we overwrite, and then do a snapshot on a parent folder. So the snapshots look as follows in the XML translation of the FsImage (abridged):\n{code}\nSNAPSHOT ID=5063; NAME=s2015022606\n<diff><inodeid>3432626</inodeid>\t\t\t\t// useractionlogs folder\n  <dirdiff><snapshotId>5063</snapshotId><isSnapshotroot>false</isSnapshotroot><childrenSize>3</childrenSize><name>useractionlogs</name>\n    <created><name>useraction.log.crypto</name></created>\n    <deleted><inode>3564976</inode></deleted>\n  </dirdiff>\n  ... \n</diff>\n<diff><inodeid>3564976</inodeid>\n  <filediff><snapshotId>5063</snapshotId><size>1388</size><name>useraction.log.crypto</name></filediff>\n</diff>\n...\n\nSNAPSHOT ID=5065; NAME=s2015022607\n<diff><inodeid>3432626</inodeid>\t\t\t\t// useractionlogs folder\n  <dirdiff><snapshotId>5065</snapshotId><isSnapshotroot>false</isSnapshotroot><childrenSize>3</childrenSize><name>useractionlogs</name>\n    <created><name>useraction.log.crypto</name></created>\n    <deleted><inode>3565860</inode></deleted>\n  </dirdiff>\n  ...\n</diff>\n<diff><inodeid>3565860</inodeid>\n  <filediff><snapshotId>5065</snapshotId><size>1388</size><name>useraction.log.crypto</name></filediff>\n</diff>\n...\n{code}\n\nAs you see, each snapshot deletes and re-creates the _useraction.log.crypto_ file. It seems that there are 2 ways for me to run into the AssertionError, and I don't think the fix for [HDFS-6908|https://issues.apache.org/jira/browse/HDFS-6908] addresses Case 2.\n\n*Case 1 - Do a snapshot diff using the hdfs client*\n{code}\n[root@node1075]# hdfs snapshotDiff /data/tenants/pdx-svt.baseline84/wddata s2015022606 s2015022607\nsnapshotDiff: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]\n{code}\n\nHere's the _tail_ from the Namenode logs:\n{code}\n2015-09-11 01:44:59,558 WARN  [IPC Server handler 89 on 8020] ipc.Server (Server.java:run(2002)) - IPC Server handler 89 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getSnapshotDiffReport from 10.52.209.75:60609 Call#0 Retry#1: error: java.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]\njava.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]\n\tat org.apache.hadoop.hdfs.util.Diff.insert(Diff.java:193)\n\tat org.apache.hadoop.hdfs.util.Diff.delete(Diff.java:239)\n\tat org.apache.hadoop.hdfs.util.Diff.combinePosterior(Diff.java:462)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.initChildren(DirectoryWithSnapshotFeature.java:293)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.iterator(DirectoryWithSnapshotFeature.java:303)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.computeDiffRecursively(INodeDirectorySnapshottable.java:441)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.computeDiffRecursively(INodeDirectorySnapshottable.java:446)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.computeDiffRecursively(INodeDirectorySnapshottable.java:446)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.computeDiff(INodeDirectorySnapshottable.java:390)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.diff(SnapshotManager.java:378)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getSnapshotDiffReport(FSNamesystem.java:7123)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getSnapshotDiffReport(NameNodeRpcServer.java:1261)\n...\n{code}\n\nIt seems that the fix for the other Jira is likely to address this issue. However, the next case doesn't seem to follow the same exception stack.\n\n*Case 2 - Deleting a snapshot*\n{code}\n[root@node1075]# hdfs dfs -deleteSnapshot /data/tenants/pdx-svt.baseline84/wddata s2015022607_maintainer_soft_del\ndeleteSnapshot: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]\n{code}\n\nHere's the exception from the Namenode logs this time:\n{code}\n2015-09-11 03:25:16,403 WARN  [IPC Server handler 10 on 8020] ipc.Server (Server.java:run(2002)) - IPC Server handler 10 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.deleteSnapshot from 10.52.209.75:61662 Call#1 Retry#0: error: java.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]\njava.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]\n\tat org.apache.hadoop.hdfs.util.Diff.insert(Diff.java:193)\n\tat org.apache.hadoop.hdfs.util.Diff.delete(Diff.java:239)\n\tat org.apache.hadoop.hdfs.util.Diff.combinePosterior(Diff.java:462)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.initChildren(DirectoryWithSnapshotFeature.java:293)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.iterator(DirectoryWithSnapshotFeature.java:303)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDeletedINode(DirectoryWithSnapshotFeature.java:531)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDirectory(DirectoryWithSnapshotFeature.java:823)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtree(INodeDirectory.java:714)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtreeRecursively(INodeDirectory.java:684)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDirectory(DirectoryWithSnapshotFeature.java:830)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtree(INodeDirectory.java:714)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.removeSnapshot(INodeDirectorySnapshottable.java:341)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.deleteSnapshot(SnapshotManager.java:238)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteSnapshot(FSNamesystem.java:7173)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.deleteSnapshot(NameNodeRpcServer.java:1222)\n{code}\n\nAs you see, the method where the fix is, _computeDiffBetweenSnapshots_, is never called. I'd assume that some kind of check is needed in the ELSE clause of the following method in _Diff.java_ before _insert_ is called:\n{code}\npublic UndoInfo<E> delete(final E element) {\n    final int c = search(created, element.getKey());\n    E previous = null;\n    Integer d = null;\n    if (c >= 0) {\n      // remove a newly created element\n      previous = created.remove(c);\n    } else {\n      // not in c-list, it must be in previous\n      d = search(deleted, element.getKey());\n      insert(ListType.DELETED, element, d);\n    }\n    return new UndoInfo<E>(c, previous, d);\n  }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-11T03:34:59.779+0000","updated":"2015-09-11T03:34:59.779+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14746329","id":"14746329","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"body":"[~jingzhao], please let me know if you have any additional comments on this since we're trying to figure out how to work around this problem in our production clusters.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-15T21:50:37.540+0000","updated":"2015-09-15T21:50:37.540+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14746534","id":"14746534","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Alex, so the issue here is not about {{computeDiffBetweenSnapshots}} or deleting a snapshot. These are just possible cases that can expose the corrupted snapshot diff list. Let me try to provide more context information about snapshot diff list. In our current snapshot implementation, we record newly created files in create list and deleted files in the delete list. So let's suppose we take a snapshot s1, and then delete the file \"useraction.log.crypto\", since the file exists before creating snapshot s1, we have:\n{noformat}\ns1: deleted list: [INodeFile_1(useraction.log.crypto)]\n{noformat}\nNow we take another snapshot s2, and then create the new log file with the same name. s2's diff list looks like:\n{noformat}\ns2: created list: [INodeFile_2(useraction.log.crypto)]\n{noformat}\nWe then take snapshot s3, and delete the log file. Now we have:\n{noformat}\ns1: created list:[], deleted list: [INodeFile_1(useraction.log.crypto)]\ns2: created list: [INodeFile_2(useraction.log.crypto)], deleted list: []\ns3: created list: [], deleted list: [INodeFile_2(useraction.log.crypto)]\n{noformat}\nLet's say we now delete s3. The diff lists of s2 and s3 should be combined and because INodeFile_2(useraction.log.crypto) is created after taking s2, the correct diff lists should look like:\n{noformat}\ns1: created list: [], deleted list: [INodeFile_1(useraction.log.crypto)]\ns2: created list: [], deleted list: []\n{noformat}\nBut before HDFS-6908 we have a bug which caused INodeFile_2(useraction.log.crypto) still stayed in s2's deleted list. Then we have:\n{noformat}\ns1: deleted list: [INodeFile_1(useraction.log.crypto)]\ns2: deleted list: [INodeFile_2(useraction.log.crypto)]\n{noformat}\nNow we have a corrupted diff list state. No matter we compute snapshot diff between s1 and the current state, or delete the snapshot s2, in case that we have to combine s1 and s2, we will get the AssertionError.\n\nBecause the corruption has been persisted in your fsimage, to fix the issue you may have to use a patched jar to remove the INodeFile_2(useraction.log.crypto) from s2's deleted list when loading the fsimage.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-16T00:02:01.945+0000","updated":"2015-09-16T00:02:01.945+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14746697","id":"14746697","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"body":"Thank you for the detailed explanation, Jing. I had not seen the following change in _cleanDirectory_ method in [HDFS-6908|https://issues.apache.org/jira/browse/HDFS-6908], which threw me off:\n{code}\n+      counts.add(currentINode.cleanSubtreeRecursively(snapshot, prior,\n+          collectedBlocks, removedINodes, priorDeleted, countDiffChange));\n+\n       // check priorDiff again since it may be created during the diff deletion\n       if (prior != Snapshot.NO_SNAPSHOT_ID) {\n         DirectoryDiff priorDiff = this.getDiffs().getDiffById(prior);\n{code}\n\nI will follow your suggestion to fix the fsimage. Should I link this Jira to [HDFS-6908|https://issues.apache.org/jira/browse/HDFS-6908] and resolve it?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-16T02:32:00.450+0000","updated":"2015-09-16T02:32:00.450+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14790779","id":"14790779","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Yeah, we can resolve this jira as duplicated with HDFS-6908.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-16T17:38:30.983+0000","updated":"2015-09-16T17:38:30.983+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12863206/comment/14790858","id":"14790858","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"body":"Sounds good. Thanks again, Jing, appreciate your help!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=axenol","name":"axenol","key":"axenol","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=axenol&avatarId=29291","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=axenol&avatarId=29291","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=axenol&avatarId=29291","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=axenol&avatarId=29291"},"displayName":"Alex Ivanov","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-09-16T18:21:24.776+0000","updated":"2015-09-16T18:21:24.776+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-9052/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2k0tj:"}}