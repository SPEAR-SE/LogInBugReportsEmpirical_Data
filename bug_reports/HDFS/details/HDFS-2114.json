{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12512233","self":"https://issues.apache.org/jira/rest/api/2/issue/12512233","key":"HDFS-2114","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12315571","id":"12315571","description":"","name":"0.23.0","archived":false,"released":true,"releaseDate":"2011-11-11"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2011-06-29T17:58:49.885+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jul 20 23:54:34 UTC 2011","customfield_12310420":"15144","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_1840103531_*|*_1_*:*_1_*:*_1390431_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2011-07-20T23:37:46.781+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-2114/watchers","watchCount":4,"isWatching":false},"created":"2011-06-29T16:06:12.849+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"6.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2011-11-15T00:53:08.667+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"If a decommissioned node is removed from the decommissioned list, namenode does not delete the excess replicas it created while the node was decommissioned.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12484655","id":"12484655","filename":"HDFS-2114.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-29T16:08:17.609+0000","size":7160,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12484655/HDFS-2114.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12484659","id":"12484659","filename":"HDFS-2114-2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-29T16:28:03.187+0000","size":6863,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12484659/HDFS-2114-2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12485596","id":"12485596","filename":"HDFS-2114-3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-07T14:06:36.480+0000","size":6981,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12485596/HDFS-2114-3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12486518","id":"12486518","filename":"HDFS-2114-4.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-14T22:04:35.141+0000","size":8846,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12486518/HDFS-2114-4.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12486877","id":"12486877","filename":"HDFS-2114-5.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-18T15:03:54.680+0000","size":8846,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12486877/HDFS-2114-5.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12487155","id":"12487155","filename":"HDFS-2114-6.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-20T14:30:08.453+0000","size":11009,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12487155/HDFS-2114-6.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"53052","customfield_12312823":null,"summary":"re-commission of a decommissioned node does not delete excess replica","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13057329","id":"13057329","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"patch based on trunk-latest","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-29T16:28:03.208+0000","updated":"2011-06-29T16:28:03.208+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13057330","id":"13057330","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"When stop decommission is called, it checks to make sure that the excess replicas are identified and put in the right queue for deletion.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-29T16:29:23.271+0000","updated":"2011-06-29T16:29:23.271+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13057370","id":"13057370","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12484659/HDFS-2114-2.patch\n  against trunk revision 1140939.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/867//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/867//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/867//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2011-06-29T17:58:49.885+0000","updated":"2011-06-29T17:58:49.885+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13058143","id":"13058143","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"body":"Is processOverReplicatedBlocksOnReCommission() called in the context of the writeLock?  Does it need to be?  Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-01T00:00:19.898+0000","updated":"2011-07-01T00:00:19.898+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13058572","id":"13058572","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"Yes, it is called in the context of writeLock(). I believe it needs to be since \nchooseExcessReplicates() is called which requires the writeLock to be held.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-01T14:27:40.047+0000","updated":"2011-07-01T14:27:40.047+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13061331","id":"13061331","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"added javadoc and increased test timeout.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-07T14:08:55.587+0000","updated":"2011-07-07T14:08:55.587+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13061409","id":"13061409","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12485596/HDFS-2114-3.patch\n  against trunk revision 1143147.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.hdfs.TestHDFSTrash\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/893//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/893//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/893//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2011-07-07T16:15:13.038+0000","updated":"2011-07-07T16:15:13.038+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13064102","id":"13064102","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"Can someone please review this patch?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-12T20:04:36.975+0000","updated":"2011-07-12T20:04:36.975+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13064409","id":"13064409","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi John, \nTestDecommission.java:\n\n1. I know you didn't write TestDecommission.checkFile(), but your addition in this patch of the {{isNodeDown}} flag raises a number of questions:\n* Aren't (isNodeDown && nodes[j].getName().equals(downnode)) and (nodes[j].getName().equals(downnode)) always the same?  So is the first use of {{isNodeDown}} even necessary?\n* Can there be any cases where (nodes[j].getName().equals(downnode)) and (nodes[j].isDecommissioned()) are different? So can't the following two blocks be merged?  And the location of the \"is decommissioned\" log message further confuses this issue.\n{code}\n        if (isNodeDown && nodes[j].getName().equals(downnode)) {\n          hasdown++;\n          LOG.info(\"Block \" + blk.getBlock() + \" replica \" + nodes[j].getName()\n              + \" is decommissioned.\");\n        }\n        if (nodes[j].isDecommissioned()) {\n          if (firstDecomNodeIndex == -1) {\n            firstDecomNodeIndex = j;\n          }\n          continue;\n        }\n{code}\n* What is the purpose of the assertion\n{code}\n        if(isNodeDown)\n          assertEquals(\"Decom node is not at the end\", firstDecomNodeIndex, -1);\n{code}\nAnd why does it even work, since the node to decommission is chosen at random?\n* And in the same block, why is it important to condition it on {{isNodeDown}}, since (!isNodeDown) implies there shouldn't be any decommissioned nodes?  So the second use of {{isNodeDown}} also seems unnecessary.\n\n2. Regarding the timeout:  In TestDecommission, you appropriately set BLOCKREPORT_INTERVAL_MSEC down to 1 sec to match the HEARTBEAT_INTERVAL.  You may also want to consider DFS_NAMENODE_REPLICATION_INTERVAL_KEY (default 3 sec).  Adjusting this value to 1 might allow testRecommission() to run in 5 sec instead of 10.\n\n3. I wish there were a clear way to share the duplicated code between testDecommission and testRecommission, but I couldn't see an obviously better choice.  Can you think of a way to improve this?\n\nFSNamesystem and BlockManager:\n\nThe implementation looks okay to me.\n\nThe failure of unit test TestTrash.testTrashEmptier does not seem to be related to this change.  It is probably related to HDFS-7326, although the symptoms reported are slightly different.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-13T07:43:59.900+0000","updated":"2011-07-13T07:43:59.900+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13064963","id":"13064963","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"Mat, Thanks a lot for reviewing this patch. I will post a newer patch tomorrow with changes to checkFile(). I have tried to integrate your comments.\n\n> 1. I know you didn't write TestDecommission.checkFile(), but your addition in \n> this patch of the {{isNodeDown}} flag raises a number of questions:\n> * Aren't (isNodeDown && nodes[j].getName().equals(downnode)) and \n> (nodes[j].getName().equals(downnode)) always the same?  So is the first use of \n> {{isNodeDown}} even necessary?\n\nThe first use of {isNodeDown} is necessary because {downnode} could be null in cases when we are checking for \"Recommission\".\n\n> * Can there be any cases where (nodes[j].getName().equals(downnode)) and \n> (nodes[j].isDecommissioned()) are different? So can't the following two blocks \n> be merged?  And the location of the \"is decommissioned\" log message further \n> confuses this issue.\n> {code}\n>         if (isNodeDown && nodes[j].getName().equals(downnode)) {\n>           hasdown++;\n>           LOG.info(\"Block \" + blk.getBlock() + \" replica \" + \n> nodes[j].getName()\n>               + \" is decommissioned.\");\n>         }\n>         if (nodes[j].isDecommissioned()) {\n>           if (firstDecomNodeIndex == -1) {\n>             firstDecomNodeIndex = j;\n>           }\n>           continue;\n>         }\n> {code}\nI think you are right. I will change the code to assert if they are not the same.\n\n> * What is the purpose of the assertion\n> {code}\n>         if(isNodeDown)\n>           assertEquals(\"Decom node is not at the end\", firstDecomNodeIndex, \n> -1);\n> {code}\n\nMy understanding is that this assert ensures that the current blk has only ONE replica that is in decommissioned state.\n\n> And why does it even work, since the node to decommission is chosen at random?\n> * And in the same block, why is it important to condition it on \n> {{isNodeDown}}, since (!isNodeDown) implies there shouldn't be any \n> decommissioned nodes?  So the second use of {{isNodeDown}} also seems \n> unnecessary.\nBecause, we don't care for this in cases where we there is no node that is down.\n\n> \n> 2. Regarding the timeout:  In TestDecommission, you appropriately set \n> BLOCKREPORT_INTERVAL_MSEC down to 1 sec to match the HEARTBEAT_INTERVAL.  You \n> may also want to consider DFS_NAMENODE_REPLICATION_INTERVAL_KEY (default 3 \n> sec).  Adjusting this value to 1 might allow testRecommission() to run in 5 \n> sec instead of 10.\n> \nWill try this.\n\n> 3. I wish there were a clear way to share the duplicated code between \n> testDecommission and testRecommission, but I couldn't see an obviously better \n> choice.  Can you think of a way to improve this?\nok","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-13T23:52:17.037+0000","updated":"2011-07-13T23:52:17.037+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13064978","id":"13064978","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. The first use of {isNodeDown} is necessary because {downnode} could be null in cases when we are checking for \"Recommission\".\n\nYes, but in this case nodes[j].getName().equals(downnode) will return false without any problem.\n\nbq. My understanding is that this assert ensures that the current blk has only ONE replica that is in decommissioned state.\n\nBut it also seems to ensure that the only replica that is down is the LAST one in the array of nodes for the block,\ndue to the way this block works:\n{code}\n      int firstDecomNodeIndex = -1;\n      DatanodeInfo[] nodes = blk.getLocations();\n      for (int j = 0; j < nodes.length; j++) {     // for each replica of blk\n        ...\n        if (nodes[j].isDecommissioned()) {\n          if (firstDecomNodeIndex == -1) {\n            firstDecomNodeIndex = j;\n          }\n          continue;\n        }\n        assertEquals(\"Decom node is not at the end\", firstDecomNodeIndex, -1);\n      }\n{code}\nYet I don't see any code (eg in LocatedBlock.getLocations()) where downed nodes are moved to the end of that list. Maybe somewhere else?\n\n{quote}\n> * And in the same block, why is it important to condition it on \n> isNodeDown, since (!isNodeDown) implies there shouldn't be any \n> decommissioned nodes? So the second use of isNodeDown also seems \n> unnecessary.\nBecause, we don't care for this in cases where we there is no node that is down.\n{quote}\nRight, but in this case the assertEquals(\"Decom node is not at the end\", firstDecomNodeIndex, -1) will always pass, because no nodes were decommissioned.\n\nIn summary, the use of the \"isNodeDown\" flag is only serving to delineate a human-interesting state, but has no impact on the outcome of the code.  I would remove it, since the checkFile() method works correctly without this flag when downnode == null or downnode != null.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-14T00:24:21.939+0000","updated":"2011-07-14T00:24:21.939+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13065560","id":"13065560","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"\n> Matt Foley commented on HDFS-2114:\n> ----------------------------------\n> \n> bq. The first use of {isNodeDown} is necessary because {downnode} could be \n> null in cases when we are checking for \"Recommission\".\n> \n> Yes, but in this case nodes[j].getName().equals(downnode) will return false \n> without any problem.\n\nAlso, I forgot to mention the check ensures that hasdown is not incremented. But, I agree with you that the code does not look as good as it should. I have attached a modified patch with some more changes.\n\n> \n> bq. My understanding is that this assert ensures that the current blk has only \n> ONE replica that is in decommissioned state.\n> \n> But it also seems to ensure that the only replica that is down is the LAST one \n> in the array of nodes for the block,\n> due to the way this block works:\n> {code}\n>       int firstDecomNodeIndex = -1;\n>       DatanodeInfo[] nodes = blk.getLocations();\n>       for (int j = 0; j < nodes.length; j++) {     // for each replica of blk\n>         ...\n>         if (nodes[j].isDecommissioned()) {\n>           if (firstDecomNodeIndex == -1) {\n>             firstDecomNodeIndex = j;\n>           }\n>           continue;\n>         }\n>         assertEquals(\"Decom node is not at the end\", firstDecomNodeIndex, -1);\n>       }\n> {code}\n> Yet I don't see any code (eg in LocatedBlock.getLocations()) where downed \n> nodes are moved to the end of that list. Maybe somewhere else?\n\nYes I think it checks to ensure that the nodes are moved to the end of that list. The code is in FSNamesystem.java. The last 5 lines in the following code is what does it.\n\n{code}\n  /**\n\n   * Get block locations within the specified range.\n\n   * @see ClientProtocol#getBlockLocations(String, long, long)\n\n   */\n\n  LocatedBlocks getBlockLocations(String clientMachine, String src,\n\n      long offset, long length) throws AccessControlException,\n\n      FileNotFoundException, UnresolvedLinkException, IOException {\n\n    LocatedBlocks blocks = getBlockLocations(src, offset, length, true, true);\n\n    if (blocks != null) {\n\n      //sort the blocks\n\n      DatanodeDescriptor client = host2DataNodeMap.getDatanodeByHost(\n\n          clientMachine);\n\n      for (LocatedBlock b : blocks.getLocatedBlocks()) {\n\n        clusterMap.pseudoSortByDistance(client, b.getLocations());\n\n\n\n        // Move decommissioned datanodes to the bottom\n\n        Arrays.sort(b.getLocations(), DFSUtil.DECOM_COMPARATOR);\n\n      }\n\n    }\n\n    return blocks;\n\n  }\n\n{code}\n\n> \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-14T21:54:21.457+0000","updated":"2011-07-14T21:54:21.457+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13065563","id":"13065563","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"modifying checkFile based on Matt's comments.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-14T22:04:35.185+0000","updated":"2011-07-14T22:04:35.185+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13067070","id":"13067070","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"attaching a patch with new name so that hudson picks it up.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-18T15:03:54.701+0000","updated":"2011-07-18T15:03:54.701+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13067117","id":"13067117","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12486877/HDFS-2114-5.patch\n  against trunk revision 1147762.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.hdfs.TestHDFSTrash\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/960//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/960//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/960//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2011-07-18T16:26:41.742+0000","updated":"2011-07-18T16:26:41.742+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13067866","id":"13067866","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"body":"\nHi John, big improvement.  Comments:\n\n1. Thanks for finding the place where decommissioned nodes are sorted to the end of the list.  I feel much better now :-)\n\n2. I'll go along with use of the isNodeDown flag on the grounds that if a node name were ever null (which should be impossible) it might match a null value of \"downnode\".\n\n3. This is a nit, but could you please change local constant NAMENODE_REPLICATION_INTERVAL_KEY to NAMENODE_REPLICATION_INTERVAL ?\n\n4. checkFile():  You've definitely improved the inner loop a lot.  However, on second reading I am concerned about the use of checkFile() in testRecommission(). I believe the use in testDecommission() is intended to be instantaneous, reading the state of the system before replica deletion has time to be done, while the use in testRecommission() is intended to be after it reaches steady state, after the system has readjusted the number of replicas.  The use of exceptions to notify problem states doesn't work too well in the latter case, as you had to work around.  Also I'm concerned about race conditions while trying to get a particular reading from checkFile() during potential changes in replication state.  My suggested fix is too long for a comment, so I've emailed you a code fragment.  Thanks.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-19T17:59:41.005+0000","updated":"2011-07-19T17:59:41.005+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13068403","id":"13068403","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks a lot Matt for the comments and the code snippet. Attaching a new patch with the suggested changes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=johnvijoe","name":"johnvijoe","key":"johnvijoe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John George","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-20T14:30:08.521+0000","updated":"2011-07-20T14:30:08.521+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13068444","id":"13068444","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12487155/HDFS-2114-6.patch\n  against trunk revision 1148348.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 3 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed core unit tests.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/981//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/981//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/981//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2011-07-20T15:45:01.584+0000","updated":"2011-07-20T15:45:01.584+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13068718","id":"13068718","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"body":"+1. Committed to trunk.  Thanks, John!\n\nNote: There were garbage characters in the JavaDoc comments for method TestDecommission.checkFile().  (Might be my fault since I emailed the text to you.)  I just removed them before committing.\n\nNote that TestDecommission takes 190 seconds to run on a quad-core Mac.  Should consider ways to decrease this time.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mattf","name":"mattf","key":"mattf","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Matt Foley","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-07-20T23:37:46.790+0000","updated":"2011-07-20T23:37:46.790+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12512233/comment/13068729","id":"13068729","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Hdfs-trunk-Commit #797 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/797/])\n    HDFS-2114. re-commission of a decommissioned node does not delete excess replicas. Contributed by John George.\n\nmattf : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148981\nFiles : \n* /hadoop/common/trunk/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/trunk/hdfs/CHANGES.txt\n* /hadoop/common/trunk/hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestDecommission.java\n* /hadoop/common/trunk/hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2011-07-20T23:54:34.342+0000","updated":"2011-07-20T23:54:34.342+0000"}],"maxResults":20,"total":20,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-2114/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i09g5z:"}}