{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12530471","self":"https://issues.apache.org/jira/rest/api/2/issue/12530471","key":"HDFS-2542","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2011-11-06T18:56:28.815+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu May 11 15:39:17 UTC 2017","customfield_12310420":"216209","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-2542/watchers","watchCount":32,"isWatching":false},"created":"2011-11-06T04:26:26.582+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12345320","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12345320","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12512240","key":"HDFS-2115","self":"https://issues.apache.org/jira/rest/api/2/issue/12512240","fields":{"summary":"Transparent compression in HDFS","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/2","id":"2","description":"A new feature of the product, which has yet to be developed.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype","name":"New Feature","subtask":false,"avatarId":21141}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-05-11T15:39:17.638+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"As HDFS-2115, we want to provide a mechanism to improve storage usage in hdfs by compression. Different from HDFS-2115, this issue focus on compress storage. Some idea like below:\n\nTo do:\n1. compress cold data.\n   Cold data: After writing (or last read), data has not touched by anyone for a long time.\n   Hot data: After writing, many client will read it , maybe it'll delele soon.\n   \n   Because hot data compression is not cost-effective,  we only compress cold data. \n   In some cases, some data in file can be access in high frequency,  but in the same file, some data may be cold data. \nTo distinguish them, we compress in block level.\n\n2. compress data which has high compress ratio.\n   To specify high/low compress ratio, we should try to compress data, if compress ratio is too low, we'll never compress them.\n\n2. forward compatibility.\n    After compression, data format in datanode has changed. Old client will not access them. To solve this issue, we provide a mechanism which decompress on datanode.\n\n3. support random access and append.\n   As HDFS-2115, random access can be support by index. We separate data before compress by fixed-length (we call these fixed-length data as \"chunk\"), every chunk has its index.\nWhen random access, we can seek to the nearest index, and read this chunk for precise position.   \n\n4. async compress to avoid compression slow down running job.\n   In practice, we found the cluster CPU usage is not uniform. Some clusters are idle at night, and others are idle at afternoon. We should make compress \ntask running in full speed when cluster idle, and in low speed when cluster busy.\n\nWill do:\n1. client specific codec and support  compress transmission.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12512759","id":"12512759","filename":"tranparent compress storage.docx","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"created":"2012-02-01T15:06:58.534+0000","size":416972,"mimeType":"application/vnd.openxmlformats-officedocument.wordprocessingml.document","content":"https://issues.apache.org/jira/secure/attachment/12512759/tranparent+compress+storage.docx"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"113922","customfield_12312823":null,"summary":"Transparent compression storage in HDFS","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13144909","id":"13144909","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"body":"We has implement a prototype which cover 4-TODO before, we use quicklz as compress codec.\n\nDuring compressing, we get some statistics like below:\ndfs.block.compress.chunk.size   1M\ndfs.block.compress.ratio.min    1.2  \nblock.compressor.thread.num     1\n\ncompress block 132744, before size 8008031402428, after size 2910562972347. compress ratio 2.75136\ntotal block 300060, before size 14813811957857, after size 9716343527776. compress ratio 1.52462.\ncompress block/total block : 0.44239 \ncompress save space : 4.63612 T\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"created":"2011-11-06T04:33:41.693+0000","updated":"2011-11-06T04:33:41.693+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13145073","id":"13145073","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tbroberg","name":"tbroberg","key":"tbroberg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tbroberg&avatarId=22860","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tbroberg&avatarId=22860","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tbroberg&avatarId=22860","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tbroberg&avatarId=22860"},"displayName":"Tim Broberg","active":true,"timeZone":"America/Los_Angeles"},"body":"Efficiency of compression seems like something that varies depending on the codec and available hardware. Do we need some kind of metric to guide the tool in deciding when to compress? ...or just provide appropriate controls to the user with reasonable defaults?\n\nIN any event, I don't think it is a given that compression of hot data will always be inefficient in all codecs for all hardware for all users at all times.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tbroberg","name":"tbroberg","key":"tbroberg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tbroberg&avatarId=22860","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tbroberg&avatarId=22860","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tbroberg&avatarId=22860","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tbroberg&avatarId=22860"},"displayName":"Tim Broberg","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-11-06T18:56:28.815+0000","updated":"2011-11-06T18:56:28.815+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13145553","id":"13145553","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"body":"I think the detection of hot vs. cold data is something important that should be added in with or without transparent compression.  It opens up a number of possibilities for trying to improve storage efficiencies.  In addition to compression we could keep hot data in a cache of some sort SSD, Ram Disk, etc.  We could also migrate cold data to spinning disks, or even to something else even slower and cheaper per GB if it is really never accessed.  I am sure others will have ideas as well about what to do with hot vs cold data.  It could be tied into some of the work for fsync and fadvise behind the scenes so that we try to keep hot data in the disk cache and only run fadvise to flush out the cache if it is not really hot.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"created":"2011-11-07T15:23:38.879+0000","updated":"2011-11-07T15:23:38.879+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13147141","id":"13147141","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"body":"To Tim:\n   Absolutely, efficiency of compression depend on codec and data to be compress. In the first step, we can use a specify codec on prototype. In the future, we can use right codec for different data in an self-adaption way, but I have no idea in implement it in an effective way yet.\n   In our prototype,we decide \"when to compress\" in two ways. \n   1. data xceiver number and number of compressing task. \n      When a datanode has a high data xceiver number, it always means it've to serve for many client request(include balance/block replication).At this time, I think, compression is not a very urgent task, so it can be slow down, and release resource for computing task.\n   2. We make compression as a single process, and make it running as idle CPU class. In this way, when some CPU-intensive job coming, compression task can release CPU slice to job, and when our cluster idle, compression can work in full speed.\n\n >> IN any event, I don't think it is a given that compression of hot data will always be inefficient in all codecs for all hardware for all users at all times.\n\n  It's right, compression before upload can save bandwidth and reduce transmission cost, but it'll slow down running job. It's a trade off. In our cluster, CPU utilization isn't mean in every time, so use idle time to make compression is valuable. To reduce transmission cost, we'll support compression on client as while.\n\nTo Robert:\n   Absolutely, detection of hot/cold data is really an important thing.To distinguish them, we add atime in block level.\"Atime\" will be updated only when any DFSClient read it, and block replication,block scanner or re-balance should not modify it.This value will be store in disk, to avoid atime loss when datanode restart.\n   Back to cold/hot data topic, we can make many improvements for different application. For example, if we use hdfs as an image storage, and hot image can be accessed for thousands of time in a second, we can use SSD to reduce latency, and use sata disk for cold data for cost-effective. \n   Currently, in our hadoop cluster, low latency is not a very important thing, so for cost-effective, we have not made any improvements for hot data.But for cold data, I think , compression + RaidNode + cheaper  disk is a feasible way to limit storage cost. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"created":"2011-11-09T16:34:53.454+0000","updated":"2011-11-09T16:34:53.454+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13147152","id":"13147152","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"body":"To Jinglong:\nI agree completely with you, I just wanted to be sure that any final solution provided a generic solution.  Something that can cleanly separate out classification of hot vs. cold vs really cold data from any extra processing that might happen when data goes from one classification to another.  Access time is a great start, but I can imagine a lot of potential innovation and experimentation in this area.  I can also see lots of different groups wanting to do something when the classification changes.  Like you said, compress the data, possibly move it to a different disk, possibly apply RAID to it.  What ever we do it should be something that is also pluggable. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"created":"2011-11-09T16:55:46.337+0000","updated":"2011-11-09T16:55:46.337+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13147448","id":"13147448","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"body":"I agree with you.\nTo classify cold/hot data and store them in lower cost is a generic issue, and it's always related to application characteristic, so I think we should make strategy pluggable, and provider a default implement.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"created":"2011-11-10T02:16:15.508+0000","updated":"2011-11-10T02:16:15.508+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13147833","id":"13147833","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"HDFS-2115 had lot smaller scope than the problem being solved here.\n\nWhile the description of the jira starts off the discussion, there are lot of details to be covered. Some of the questions I am left with is:\n# Post compression, the block files have completely different length. The length tracked at NN for the blocks is no longer valid.\n# What is the state of the file during compression?\n# How do you deal with data that was deemed cold, that could become hot at a later point?\n# How does Datanode block scanner and directory scanner, internal datanode data structures that track block length, Append interact with this feature?\n\nGiven that, based on the approach taken, this could result in changes to some core parts of HDFS, please write a design document. Alternatively should we look at an external tool that can do this analysis and compress the files, based on HDFS-2115 mechanism proposed by Todd, to minimize the impact to HDFS core code?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-11-10T17:15:11.688+0000","updated":"2011-11-10T17:15:11.688+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13147891","id":"13147891","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harip","name":"harip","key":"harip","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hari Mankude","active":true,"timeZone":"Etc/UTC"},"body":"Adding to Suresh's comments, one of the key goals of compression is space reclamation. Given the hdfs has rigid notions of block sizes, compression could leave the filesystem with varied hdfs block sizes and NN has to be aware of the varied block sizes. NN needs to be able to reclaim the storage. \n\nThe other problem is that when data becomes hot again sometime in the future, filesystem needs to have space to store uncompressed version of the block.\n\nData deduplication is another approach that can be combined with compression to reduce the storage footprint.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harip","name":"harip","key":"harip","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hari Mankude","active":true,"timeZone":"Etc/UTC"},"created":"2011-11-10T18:28:33.723+0000","updated":"2011-11-10T18:28:33.723+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13147928","id":"13147928","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=apurtell","name":"apurtell","key":"apurtell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=apurtell&avatarId=20553","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=apurtell&avatarId=20553","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=apurtell&avatarId=20553","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=apurtell&avatarId=20553"},"displayName":"Andrew Purtell","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Data deduplication is another approach that can be combined with compression to reduce the storage footprint.\n\nDedup seems a strategy contrary to the basic rationale of HDFS providing reliable storage. Instead of one missing block corrupting one file, it may impact many, perhaps hundreds, thousands.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=apurtell","name":"apurtell","key":"apurtell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=apurtell&avatarId=20553","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=apurtell&avatarId=20553","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=apurtell&avatarId=20553","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=apurtell&avatarId=20553"},"displayName":"Andrew Purtell","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-11-10T19:06:51.910+0000","updated":"2011-11-10T19:06:51.910+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13147992","id":"13147992","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harip","name":"harip","key":"harip","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hari Mankude","active":true,"timeZone":"Etc/UTC"},"body":"Dedup blocks would be stored in a hdfs filesystem with 3 replicas. In fact, if the deduped block is a \"hot\" block with lots of references, replica count can be increased for those blocks as a policy setting.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harip","name":"harip","key":"harip","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hari Mankude","active":true,"timeZone":"Etc/UTC"},"created":"2011-11-10T20:37:10.927+0000","updated":"2011-11-10T20:37:10.927+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13148000","id":"13148000","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=apurtell","name":"apurtell","key":"apurtell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=apurtell&avatarId=20553","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=apurtell&avatarId=20553","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=apurtell&avatarId=20553","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=apurtell&avatarId=20553"},"displayName":"Andrew Purtell","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Dedup blocks would be stored in a hdfs filesystem with 3 replicas. \n\nThis was implied even so in my comment, obviously.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=apurtell","name":"apurtell","key":"apurtell","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=apurtell&avatarId=20553","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=apurtell&avatarId=20553","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=apurtell&avatarId=20553","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=apurtell&avatarId=20553"},"displayName":"Andrew Purtell","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-11-10T20:51:51.510+0000","updated":"2011-11-10T20:51:51.510+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/13197893","id":"13197893","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"body":"Sorry for so late design document and my poor English.And patch will be commit latter. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=baggioss","name":"baggioss","key":"baggioss","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"jinglong.liujl","active":true,"timeZone":"Etc/UTC"},"created":"2012-02-01T15:06:58.641+0000","updated":"2012-02-01T15:06:58.641+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12530471/comment/16006648","id":"16006648","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harisekhon","name":"harisekhon","key":"harisekhon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hari Sekhon","active":true,"timeZone":"Etc/UTC"},"body":"I recall looking for this feature 2-3 years ago while at a large financial and it's just come up again with another large financial client I'm working for right now.\n\nI see I actually already upvoted this jira the last time I looked at it but there has been no movement on this in years.\n\nHaving transparent compression on a directory tree would be a very useful feature.\n\nIs there any chance of this being implemented?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=harisekhon","name":"harisekhon","key":"harisekhon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hari Sekhon","active":true,"timeZone":"Etc/UTC"},"created":"2017-05-11T15:39:17.638+0000","updated":"2017-05-11T15:39:17.638+0000"}],"maxResults":13,"total":13,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-2542/votes","votes":5,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0jutb:"}}