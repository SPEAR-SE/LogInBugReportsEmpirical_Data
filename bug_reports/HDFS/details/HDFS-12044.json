{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13082673","self":"https://issues.apache.org/jira/rest/api/2/issue/13082673","key":"HDFS-12044","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12335737","id":"12335737","description":"3.0.0-beta1 release","name":"3.0.0-beta1","archived":false,"released":true,"releaseDate":"2017-10-03"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2017-06-27T23:48:25.011+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Jul 28 18:15:12 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_77718762_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_2662842599","customfield_12312321":null,"resolutiondate":"2017-07-28T17:53:16.966+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-12044/watchers","watchCount":6,"isWatching":false},"created":"2017-06-27T00:37:15.667+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":["hdfs-ec-3.0-must-do"],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"6.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12340638","id":"12340638","name":"3.0.0-alpha3","archived":false,"released":true,"releaseDate":"2017-05-26"}],"issuelinks":[{"id":"12510537","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12510537","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13090836","key":"HDFS-12215","self":"https://issues.apache.org/jira/rest/api/2/issue/13090836","fields":{"summary":"DataNode#transferBlock does not create its daemon in the xceiver thread group","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12510448","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12510448","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"13090601","key":"HDFS-12208","self":"https://issues.apache.org/jira/rest/api/2/issue/13090601","fields":{"summary":"NN should consider DataNode#xmitInProgress when placing new block","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-28T18:15:12.029+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12327961","id":"12327961","name":"erasure-coding"}],"timeoriginalestimate":null,"description":"{{ErasureCodingWorker#stripedReconstructionPool}} is with {{corePoolSize=2}} and {{maxPoolSize=8}} as default. And it rejects more tasks if the queue is full.\n\nWhen {{BlockManager#maxReplicationStream}} is larger than {{ErasureCodingWorker#stripedReconstructionPool#corePoolSize/maxPoolSize}}, for example, {{maxReplicationStream=20}} and {{corePoolSize=2 , maxPoolSize=8}}.  Meanwhile, NN sends up to {{maxTransfer}} reconstruction tasks to DN for each heartbeat, and it is calculated in {{FSNamesystem}}:\n\n{code}\nfinal int maxTransfer = blockManager.getMaxReplicationStreams() - xmitsInProgress;\n{code}\n\nHowever, at any giving time, {{{ErasureCodingWorker#stripedReconstructionPool}} takes 2 {{xmitInProcess}}. So for each heartbeat in 3s, NN will send about {{20-2 = 18}} reconstruction tasks to the DN, and DN throw away most of them if there were 8 tasks in the queue already. So NN needs to take longer to re-consider these blocks were under-replicated to schedule new tasks.\n\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12335737","id":"12335737","description":"3.0.0-beta1 release","name":"3.0.0-beta1","archived":false,"released":true,"releaseDate":"2017-10-03"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12874764","id":"12874764","filename":"HDFS-12044.00.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-27T21:58:39.767+0000","size":1147,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12874764/HDFS-12044.00.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12874933","id":"12874933","filename":"HDFS-12044.01.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-28T18:28:52.966+0000","size":5545,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12874933/HDFS-12044.01.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12874969","id":"12874969","filename":"HDFS-12044.02.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-28T22:03:11.830+0000","size":7994,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12874969/HDFS-12044.02.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12875339","id":"12875339","filename":"HDFS-12044.03.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-30T23:25:51.212+0000","size":14346,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12875339/HDFS-12044.03.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12878902","id":"12878902","filename":"HDFS-12044.04.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-25T20:47:59.562+0000","size":17647,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12878902/HDFS-12044.04.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12879251","id":"12879251","filename":"HDFS-12044.05.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-27T22:07:35.737+0000","size":17704,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12879251/HDFS-12044.05.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Mismatch between BlockManager#maxReplicationStreams and ErasureCodingWorker.stripedReconstructionPool pool size causes slow and bursty recovery","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16065561","id":"16065561","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"There are two potential approaches to this.\n\n* Allow {{ErasureCodingWorker}} to accept un-bounded number of re-construct tasks. The reconstruction worker for regular replicated files accept unbounded number of reconstruction tasks as well. \n* Make {{DFS_NAMENODE_REPLICATION_MAX_STREAMS_KEY}} and {{DFS_DN_EC_RECONSTRUCTION_STRIPED_BLK_THREADS_KEY}} be the same, i.e., sharing the same key and value.\n\nAttach a patch for the first approach which is simpler. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-27T22:12:08.227+0000","updated":"2017-06-27T22:12:08.227+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16065673","id":"16065673","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 13s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 42s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 30s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 34s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 28s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 36s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 66m 23s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 16s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 90m  4s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.web.TestWebHdfsTimeouts |\n|   | hadoop.hdfs.server.namenode.TestNamenodeCapacityReport |\n| Timed out junit tests | org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:14b5c93 |\n| JIRA Issue | HDFS-12044 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12874764/HDFS-12044.00.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux a8178b113a85 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 63ce159 |\n| Default Java | 1.8.0_131 |\n| findbugs | v3.1.0-RC1 |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/20064/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/20064/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20064/console |\n| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-27T23:48:25.011+0000","updated":"2017-06-27T23:48:25.011+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16067011","id":"16067011","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Add unit test to approach 1. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-28T18:30:22.133+0000","updated":"2017-06-28T18:30:22.133+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16067218","id":"16067218","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Eddy, thanks for working on this,\n\nIIUC the intent of this patch is to remove limiting on the DN side, since the NN already has limits. However, I don't follow all the changes. The max threads is changed to Integer.MAX_VALUE, but I think it still makes sense to limit the degree of parallelism. Can we increase the queue size? Is the NN sending duplicate tasks for items already in the queue?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-28T21:03:41.904+0000","updated":"2017-06-28T21:03:41.904+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16067304","id":"16067304","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi, [~andrew.wang]\n\nNN limits the speed of putting such tasks to DN's queue at speed of {{maxReplicationStreams - xmitsInProcess}}, so that when DN has active reconstruction tasks taking {{xmitsInProcess}}, NN will adaptively slow down the enqueue process and will not increase the queue length infinitely.  I think this is how the non-EC  block recovery works today without an {{Executor}}. \n\nI agree that it would provide more guarantee if we explicitly limit the parallelism from the Executor, so I changed patch {{02}} to use an unbounded queue in the Executor to hold submitted re-construction tasks in {{02}} patch.  Do you think it is sufficient, [~andrew.wang]\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-28T21:52:30.737+0000","updated":"2017-06-28T21:52:30.737+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16067315","id":"16067315","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |\n| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HDFS-12044 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| JIRA Issue | HDFS-12044 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12874965/HDFS-12044.02.patch |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20083/console |\n| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-28T21:58:34.951+0000","updated":"2017-06-28T21:58:34.951+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16067331","id":"16067331","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Rebase and re-upload patch 02","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-28T22:03:45.366+0000","updated":"2017-06-28T22:03:45.366+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16067487","id":"16067487","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 33s{color} | {color:blue} Maven dependency ordering for branch {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 44s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 46s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 57s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 49s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 15s{color} | {color:green} trunk passed {color} |\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  9s{color} | {color:blue} Maven dependency ordering for patch {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 45s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 54s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 54s{color} | {color:green} the patch passed {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 51s{color} | {color:orange} hadoop-hdfs-project: The patch generated 1 new + 29 unchanged - 0 fixed = 30 total (was 29) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 50s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 59s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 24s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 74m 14s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 21s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black}116m 28s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.server.namenode.ha.TestEditLogsDuringFailover |\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:14b5c93 |\n| JIRA Issue | HDFS-12044 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12874969/HDFS-12044.02.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 0a8940a1337a 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 990aa34 |\n| Default Java | 1.8.0_131 |\n| findbugs | v3.1.0-RC1 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/20084/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/20084/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/20084/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20084/console |\n| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-06-29T00:04:49.732+0000","updated":"2017-06-29T00:04:49.732+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16069077","id":"16069077","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the explanation Eddy, very helpful. I refreshed myself on the block reconstruction process. A review for myself and other watchers:\n\n* BlockManager calculates reconstruction work and places tasks in the DatanodeDescriptor's queue, and also in PendingReconstruction work.\n* handleHeartbeat polls the DD queue and gives tasks to the DN every heartbeat, based on {{maxReplicationStreams - xmitsInProgress}}\n* PendingReconstruction will retry timedout tasks after 5 minutes\n\nThe core of the current issue is that the DN is refusing work from the NN because it exceeds numReconstructionThreads, and the NN keeps assigning more work because it thinks the DN has additional xmit capacity.\n\nI think the basic fix here is to increment xmitsInProgress even for queued reconstruction work. This way {{maxReplicationStreams - xmitsInProgress}} will eventually be <= 0 and the NN will stop giving more work. Otherwise the queue will not converge.\n\nI also wonder about the relative weights of EC and replicated reconstruction. 20 EC reconstruction tasks is a different amount of work than 20 re-replication tasks. We should be counting each block reader in StripedReconstructor as its own xmit, i.e. an RS(10,4) recovery task would count as 10 xmits. I looked at this in HDFS-11023 and thought it was accounted for properly, but looking again that's not true.\n\nMore generally, I think HDFS-11023 is still worth revisiting. The NN throttles are coarse and only operate on the heartbeat interval. The DN would ideally have byte-based throttles the same as the balancer settings, to be more user-friendly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-29T22:10:44.735+0000","updated":"2017-06-29T22:10:44.735+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16070860","id":"16070860","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the review, Andrew.\n\nAs you suggested, in the latest patch, {{xmitsInProgress}} increases for queued tasks also, to throttle the speed of NN sending tasks to DN.  Also for EC recon task, it increases the xmit with a \"weight\", that is currently calculated as {{len(sources) + len(targets)}} to represent the # of network connections. \n\n I feel that the way of this weight calculation would not need to be calculate, as long as it presents the relative cost of recovery task (i.e., more connections *_usuallly_* mean more I/O (block size is the same) and more CPU (because more data)). ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-06-30T23:31:11.546+0000","updated":"2017-06-30T23:31:11.546+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16070905","id":"16070905","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Eddy, looks real good, few comments:\n\n* Can the LinkedBlockingDeque be a LinkedBlockingQueue? I don't think it needs the Deque functionality.\n* SRInfo#getWeight adds together the # sources and # targets. I think this will overestimate. Note in StripedReader#readMinimumSources we only read from minRequiredSources. We also don't read and write at the same time, so it'd be better to take max(minSources, targets).\n* In ECWorker, xmits is incremented after submitting the task. Is there a possible small race here? We could increment first to reserve capacity, then try/catch to decrement if the submit fails.\n* Comment could be enhanced slightly, maybe:\n{noformat}\n          // See HDFS-12044. We increase xmitsInProgress even if the task is only\n          // enqueued, so that\n          //   1) NN will not send more tasks than DN can execute and\n          //   2) DN will not throw away reconstruction tasks, and instead keeps an\n          //      unbounded number of tasks in the executor's task queue.\n{noformat}\n\nI also had another question about accounting. The NN accounts for DN xciever load when doing block placement, but the xmit count is not factored in. The source and target DNs will each use an xceiver to send or receive the block, but the DN running the reconstruction task doesn't (AFAICT). Should we twiddle the xceiver count (or use an xceiver?) to influence BPP?\n\nAside, I noticed what looks like an existing bug, that DataNode#transferBlock does not create its Daemon in the xceiver thread group (which is how we currently count the # of xceivers). BlockRecoveryWorker#recoverBlocks is an example of something not in DataTransferProtocol that still counts against this thread group.\n\nUnit tests:\n* Could you add a unit test with two node failures, for some additional coverage? IIUC a single reconstruction task will recover all the missing blocks for an EC group, would be good to validate.\n* Also would be good to do some reconstruct tasks and validate at the end that the xmitsInProgress for all DNs go back to zero at the end.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-01T00:38:04.595+0000","updated":"2017-07-01T00:38:04.595+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16071095","id":"16071095","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 23s{color} | {color:blue} Maven dependency ordering for branch {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m 22s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 29s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 34s{color} | {color:green} trunk passed {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 30s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs-client in trunk has 2 extant Findbugs warnings. {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 42s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 10 extant Findbugs warnings. {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 58s{color} | {color:green} trunk passed {color} |\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  7s{color} | {color:blue} Maven dependency ordering for patch {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 21s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 26s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 26s{color} | {color:green} the patch passed {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 39s{color} | {color:orange} hadoop-hdfs-project: The patch generated 1 new + 184 unchanged - 0 fixed = 185 total (was 184) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 26s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 22s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 16s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 81m 37s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black}115m 51s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure080 |\n|   | hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy |\n|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure070 |\n|   | hadoop.hdfs.server.namenode.TestDecommissioningStatus |\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\n|   | hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |\n|   | hadoop.hdfs.web.TestWebHdfsTimeouts |\n|   | hadoop.hdfs.TestFileChecksum |\n| Timed out junit tests | org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:14b5c93 |\n| JIRA Issue | HDFS-12044 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12875339/HDFS-12044.03.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 6c831fdeb141 4.4.0-43-generic #63-Ubuntu SMP Wed Oct 12 13:48:03 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 147df30 |\n| Default Java | 1.8.0_131 |\n| findbugs | v3.1.0-RC1 |\n| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/20123/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-client-warnings.html |\n| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/20123/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/20123/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/20123/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/20123/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20123/console |\n| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-01T08:04:16.291+0000","updated":"2017-07-01T08:04:16.291+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16100755","id":"16100755","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the detailed suggestions, [~andrew.wang]. Updated the patch accordingly.\n\nbq. The NN accounts for DN xciever load when doing block placement, but the xmit count is not factored in. \n\nI will file another JIRA.\n\nbq.  that DataNode#transferBlock does not create its Daemon in the xceiver thread group (which is how we currently count the # of xceivers).  \n\nWill file JIRA for this as well.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-25T20:49:38.864+0000","updated":"2017-07-25T20:49:38.864+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16103631","id":"16103631","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"LGTM +1, I just retriggered the precommit build since I don't see a run on the 04 patch.\n\nCould you link the follow-on JIRAs to this one? I might have missed them. Thanks Eddy!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-27T18:13:15.747+0000","updated":"2017-07-27T18:13:15.747+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16103787","id":"16103787","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |\n|| || || || {color:brown} Prechecks {color} ||\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n|| || || || {color:brown} trunk Compile Tests {color} ||\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  8s{color} | {color:blue} Maven dependency ordering for branch {color} |\n| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  4m 26s{color} | {color:red} root in trunk failed. {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 26s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 28s{color} | {color:green} trunk passed {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 26s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs-client in trunk has 2 extant Findbugs warnings. {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 42s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 10 extant Findbugs warnings. {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |\n|| || || || {color:brown} Patch Compile Tests {color} ||\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  7s{color} | {color:blue} Maven dependency ordering for patch {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 24s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 27s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 27s{color} | {color:green} the patch passed {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 42s{color} | {color:orange} hadoop-hdfs-project: The patch generated 2 new + 186 unchanged - 0 fixed = 188 total (was 186) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 29s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 23s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |\n|| || || || {color:brown} Other Tests {color} ||\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 12s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 68m 45s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 93m 47s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure150 |\n|   | hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy |\n|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure010 |\n|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure080 |\n|   | hadoop.hdfs.TestFileChecksum |\n|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |\n|   | hadoop.hdfs.TestCrcCorruption |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:14b5c93 |\n| JIRA Issue | HDFS-12044 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12878902/HDFS-12044.04.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux db232ee595db 3.13.0-117-generic #164-Ubuntu SMP Fri Apr 7 11:05:26 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / f66fd11 |\n| Default Java | 1.8.0_131 |\n| mvninstall | https://builds.apache.org/job/PreCommit-HDFS-Build/20438/artifact/patchprocess/branch-mvninstall-root.txt |\n| findbugs | v3.1.0-RC1 |\n| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/20438/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-client-warnings.html |\n| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/20438/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/20438/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/20438/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/20438/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20438/console |\n| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-27T19:46:22.660+0000","updated":"2017-07-27T19:46:22.660+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16103857","id":"16103857","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"The test failures seem to be relevant. Looking into it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-27T20:33:42.590+0000","updated":"2017-07-27T20:33:42.590+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16103994","id":"16103994","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Fix {{TestFileChecksum}}. The rest of tests pass on my machine.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-27T22:08:09.684+0000","updated":"2017-07-27T22:08:09.684+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16104153","id":"16104153","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |\n|| || || || {color:brown} Prechecks {color} ||\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n|| || || || {color:brown} trunk Compile Tests {color} ||\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  8s{color} | {color:blue} Maven dependency ordering for branch {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 13m  9s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 26s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 27s{color} | {color:green} trunk passed {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 22s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs-client in trunk has 2 extant Findbugs warnings. {color} |\n| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 39s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 10 extant Findbugs warnings. {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  1s{color} | {color:green} trunk passed {color} |\n|| || || || {color:brown} Patch Compile Tests {color} ||\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m  8s{color} | {color:blue} Maven dependency ordering for patch {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 20s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 23s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m 23s{color} | {color:green} the patch passed {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 40s{color} | {color:orange} hadoop-hdfs-project: The patch generated 1 new + 186 unchanged - 0 fixed = 187 total (was 186) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 23s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m 14s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |\n|| || || || {color:brown} Other Tests {color} ||\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  9s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 64m 59s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 98m  8s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\n|   | hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:14b5c93 |\n| JIRA Issue | HDFS-12044 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12879251/HDFS-12044.05.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 56ce65acd785 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / e3c7300 |\n| Default Java | 1.8.0_131 |\n| findbugs | v3.1.0-RC1 |\n| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/20445/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-client-warnings.html |\n| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/20445/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/20445/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/20445/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/20445/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20445/console |\n| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-28T00:06:49.916+0000","updated":"2017-07-28T00:06:49.916+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16104227","id":"16104227","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 thanks Eddy, that checkstyle error looks extant.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-28T00:49:33.131+0000","updated":"2017-07-28T00:49:33.131+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16104252","id":"16104252","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"body":"BTW did we JIRA this follow-on?\n\nbq. that DataNode#transferBlock does not create its Daemon in the xceiver thread group (which is how we currently count the # of xceivers).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andrew.wang","name":"andrew.wang","key":"andrew.wang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=andrew.wang&avatarId=19230","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andrew.wang&avatarId=19230","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andrew.wang&avatarId=19230","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andrew.wang&avatarId=19230"},"displayName":"Andrew Wang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-28T00:57:59.410+0000","updated":"2017-07-28T00:57:59.410+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16105369","id":"16105369","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the reviews, [~andrew.wang]\n\nCommitted to trunk.\n\nAlso filed HDFS-12215 and HDFS-12208 as follow on.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-07-28T17:53:17.012+0000","updated":"2017-07-28T17:53:17.012+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13082673/comment/16105422","id":"16105422","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #12068 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/12068/])\nHDFS-12044. Mismatch between BlockManager.maxReplicationStreams and (lei: rev 77791e4c36ddc9305306c83806bf486d4d32575d)\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReader.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReconstructStripedFile.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructionInfo.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-28T18:15:12.029+0000","updated":"2017-07-28T18:15:12.029+0000"}],"maxResults":22,"total":22,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-12044/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3grdr:"}}