{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12927969","self":"https://issues.apache.org/jira/rest/api/2/issue/12927969","key":"HDFS-9617","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2016-01-06T13:30:51.062+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jan 12 08:21:52 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_35107178_*|*_4_*:*_1_*:*_1098883_*|*_5_*:*_2_*:*_468648047","customfield_12312321":null,"resolutiondate":"2016-01-12T05:55:54.403+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-9617/watchers","watchCount":2,"isWatching":false},"created":"2016-01-06T09:41:40.333+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"3.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-01-12T20:59:13.175+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /Tmp2/43.bmp.tmp (inode 2913263): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_2084151715_1, pendingcreates: 250]\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3358)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3160)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3042)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)\n\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1653)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1411)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1364)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy14.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy15.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)\n\n\nmy java client(JVM -Xmx=2G) :\njmap TOP15：\nnum     #instances         #bytes  class name\n----------------------------------------------\n   1:         48072     2053976792  [B\n   2:         45852        5987568  <constMethodKlass>\n   3:         45852        5878944  <methodKlass>\n   4:          3363        4193112  <constantPoolKlass>\n   5:          3363        2548168  <instanceKlassKlass>\n   6:          2733        2299008  <constantPoolCacheKlass>\n   7:           533        2191696  [Ljava.nio.ByteBuffer;\n   8:         24733        2026600  [C\n   9:         31287        2002368  org.apache.hadoop.hdfs.DFSOutputStream$Packet\n  10:         31972         767328  java.util.LinkedList$Node\n  11:         22845         548280  java.lang.String\n  12:         20372         488928  java.util.concurrent.atomic.AtomicLong\n  13:          3700         452984  java.lang.Class\n  14:           981         439576  <methodDataKlass>\n  15:          5583         376344  [S","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12781171","id":"12781171","filename":"HadoopLoader.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T09:43:07.841+0000","size":585,"mimeType":"text/x-java-source","content":"https://issues.apache.org/jira/secure/attachment/12781171/HadoopLoader.java"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12781172","id":"12781172","filename":"LoadThread.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T09:43:07.847+0000","size":1640,"mimeType":"text/x-java-source","content":"https://issues.apache.org/jira/secure/attachment/12781172/LoadThread.java"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12781173","id":"12781173","filename":"UploadProcess.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T09:43:07.851+0000","size":1007,"mimeType":"text/x-java-source","content":"https://issues.apache.org/jira/secure/attachment/12781173/UploadProcess.java"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"my java client use muti-thread to put a same file to a same hdfs uri, after no lease error，then client OutOfMemoryError","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15085526","id":"15085526","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"body":"Thanks for reporting this.\nbq. my java client use muti-thread to put a same file to a same hdfs uri\nI'm a little confused. How you did this or what's your code like? Would you elaborate this a little bit? It may help to understand why it happened. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"created":"2016-01-06T13:30:51.062+0000","updated":"2016-01-06T13:30:51.062+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15086069","id":"15086069","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=liuml07","name":"liuml07","key":"liuml07","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=liuml07&avatarId=29203","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=liuml07&avatarId=29203","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=liuml07&avatarId=29203","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=liuml07&avatarId=29203"},"displayName":"Mingliang Liu","active":true,"timeZone":"America/Los_Angeles"},"body":"If this is not yet confirmed a bug or a feature request, please send email to [mailto:user@hadoop.apache.org]. People there are willing to help you with your problems.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=liuml07","name":"liuml07","key":"liuml07","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=liuml07&avatarId=29203","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=liuml07&avatarId=29203","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=liuml07&avatarId=29203","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=liuml07&avatarId=29203"},"displayName":"Mingliang Liu","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-01-06T18:59:39.138+0000","updated":"2016-01-06T18:59:39.138+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15086124","id":"15086124","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"body":"bq. my java client use muti-thread to put a same file to a same hdfs uri\nUnless each thread creates a separate instance of UserGroupInformation for its file system, they will all look like one writer to the namenode, causing all sorts of problems.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kihwal","name":"kihwal","key":"kihwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=kihwal&avatarId=34594","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kihwal&avatarId=34594","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kihwal&avatarId=34594","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kihwal&avatarId=34594"},"displayName":"Kihwal Lee","active":true,"timeZone":"America/Chicago"},"created":"2016-01-06T19:26:30.752+0000","updated":"2016-01-06T19:26:30.752+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15088904","id":"15088904","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"body":"Thank you for your reply. \n\nmy client code :\n【main class】\npublic class HadoopLoader {\n  public HadoopLoader() {\n  }\n\n  public static void main(String[] args) {\n    HadoopLoader hadoopLoader = new HadoopLoader();\n\n    //上传数据\n    hadoopLoader.upload();\n  }\n\n  private void upload() {\n    new UploadProcess().upload();\n  }\n====================================================================================================\npublic class UploadProcess {\n  private ExecutorService executorService;\n  private Map<String, Boolean> processingFileMap = new ConcurrentHashMap<String, Boolean>();\n\n  public void upload() {\n    executorService = Executors.newFixedThreadPool(HadoopLoader.CONFIG_PROPERTIES.getHandleNum());\n\n    for (int i = 0; i < 1000; i++) {\n      processLoad(\"/home/ztb/testdata/43.bmp\", \"hdfs://10.43.156.157:9000/ztbtest\");\n    }\n  }\n\n  private void processLoad(String filePathName, String hdfsFilePathName) {\n    LoadThread loadThread = new LoadThread(filePathName, hdfsFilePathName);\n    executorService.execute(loadThread);\n  }\n\n}\n\n===================================================================================================\npublic class LoadThread implements Runnable {\n  private static final org.apache.commons.logging.Log LOG = LogFactory.getLog(LoadThread.class);\n\n  String filePathName;         //数据文件完整名称（路径名+文件名）\n  String hdfsFilePathName;     //数据文件完整名称（路径名+文件名）\n\n  public LoadThread(String filePathName, String hdfsFilePathName) {\n    this.filePathName = filePathName;\n    this.hdfsFilePathName = hdfsFilePathName;\n  }\n\n  public void writeToHdfs(String filePathName, String hdfsFilePathName) throws IOException {\n    LOG.info(\"Start to upload \" + filePathName + \" to \" + hdfsFilePathName);\n    Configuration conf = new Configuration();\n    FileSystem fs = FileSystem.get(URI.create(hdfsFilePathName), conf);\n    InputStream in = null;\n    OutputStream out = null;\n    Path hdfsFilePath;\n    try {\n      in = new BufferedInputStream(new FileInputStream(filePathName));\n      hdfsFilePath = new Path(hdfsFilePathName);\n      out = fs.create(hdfsFilePath);\n      IOUtils.copyBytes(in, out, conf);\n    } finally {\n      if (in != null) {\n        in.close();\n      }\n      if (out != null) {\n        out.close();\n      }\n    }\n    LOG.info(\"Finish uploading \" + filePathName + \" to \" + hdfsFilePathName);\n  }\n\n  @Override\n  public void run() {\n    try {\n      writeToHdfs(filePathName, hdfsFilePathName);\n    } catch (IOException e) {\n      LOG.error(e.getMessage(), e);\n    }\n  }\n\n}\n\n\n\n\ni get java_pid8820.hprof when i set -XX:+HeapDumpOnOutOfMemoryError\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T08:46:29.135+0000","updated":"2016-01-08T08:46:29.135+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15088923","id":"15088923","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"body":"I do not use \"FileSystem get(final URI uri, final Configuration conf, String user)\" to get the file system.\n\nBy the way, this is my another question, why when i use \"FileSystem get(final URI uri, final Configuration conf, String user)\" with the same user string , the file system i got are different(DFSClient\n  is a new instance for each time), the FS cache is useless ? should i must close the fs everytime if i use \"FileSystem get(final URI uri, final Configuration conf, String user)\" to get a fs even with the same user?\n\n\n\nThanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T09:03:18.357+0000","updated":"2016-01-08T09:03:18.357+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15088936","id":"15088936","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"body":"Looking forward to your reply, I am very grateful for your help","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-08T09:09:20.109+0000","updated":"2016-01-08T09:09:20.109+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15089226","id":"15089226","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"body":"Looking at your attached codes, you're crazily trying to use *10000* threads to write to the same HDFS file, which is surely not to work. What's behavior and output would you expect? As Kihwal said, this can cause all sorts of problems. I thought you need to be clear about what you want to achieve, then ask your questions in the user mailing list about how to do it, as MIngliang suggested.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"created":"2016-01-08T14:01:47.714+0000","updated":"2016-01-08T14:01:47.714+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15089245","id":"15089245","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"body":"Looks like you want to implement a files loading tool that uploads files to HDFS cluster, if so, you may take a look at the work in HDFS-8968, where a benchmark tool does the similar things to benchmark write throughput, using multiple concurrent writers in threads, but surely writing to different HDFS files.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"created":"2016-01-08T14:14:30.402+0000","updated":"2016-01-08T14:14:30.402+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15093039","id":"15093039","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"body":"Yes i got you, this is a abnormal test scenario, but whatever it should not cause the client dumped with OOM, right? I just want to know why this scenario cause the client OOM, maybe some underlying streams of hdfs have not bean closed?  \n\nThank you very much.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-12T01:02:32.386+0000","updated":"2016-01-12T01:02:32.386+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15093331","id":"15093331","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"body":"Why you reopened this issue? Please note JIRA is not a place to answer your questions. As suggested above, please move to user/dev mailing list. Please close it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"created":"2016-01-12T05:43:10.400+0000","updated":"2016-01-12T05:43:10.400+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15093341","id":"15093341","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"body":"bq. whatever it should not cause the client dumped with OOM\nYou opened 10000 threads to upload file, and then ask why it OOMed? This sounds really interesting. You please do yourself a favor investigating it and might not expect others to have the time for this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"created":"2016-01-12T05:48:22.102+0000","updated":"2016-01-12T05:48:22.102+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15093498","id":"15093498","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"body":"Sorry i did not make it clear. \n\nWhen the upload uri is different everytime with 10000 threads, everything is ok.\n \n\nTest codes like this:\n  \n    for (int i = 0; i < 10000; i++) {\n      //this case is ok\n      String uri = \"hdfs://10.43.156.157:9000/ztbtest/\" + i;\n\n      //this case OOMed \n      //String uri= \"hdfs://10.43.156.157:9000/ztbtest\";\n\n      processLoad(\"/home/ztb/testdata/43.bmp\", uri);\n    }\n\n\n  ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zuo.tingbing9","name":"zuo.tingbing9","key":"zuo.tingbing9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zuo.tingbing9&avatarId=34768","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zuo.tingbing9&avatarId=34768","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zuo.tingbing9&avatarId=34768","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zuo.tingbing9&avatarId=34768"},"displayName":"zuotingbing","active":true,"timeZone":"Etc/UTC"},"created":"2016-01-12T08:02:09.706+0000","updated":"2016-01-12T08:02:09.706+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12927969/comment/15093521","id":"15093521","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"body":"I knew what you did, as I said above, you were using so many threads writing to the same HDFS file, which is not supported yet, if it's ever going to be supported. Every time there is only ONE writer to own the file lease and can write to the file. You stacktrace told it clearly: LeaseExpiredException, No lease. You just made the NN and your client crazy, causing all sorts of problems, which I don't think you would need the time to investigate.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=drankye","name":"drankye","key":"drankye","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kai Zheng","active":true,"timeZone":"Asia/Chongqing"},"created":"2016-01-12T08:21:52.667+0000","updated":"2016-01-12T08:21:52.667+0000"}],"maxResults":13,"total":13,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-9617/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2qxan:"}}