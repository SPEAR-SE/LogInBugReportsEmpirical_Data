{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12859390","self":"https://issues.apache.org/jira/rest/api/2/issue/12859390","key":"HDFS-8960","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-08-26T21:35:06.552+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jan 27 17:05:07 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-8960/watchers","watchCount":22,"isWatching":false},"created":"2015-08-26T18:04:55.960+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"3.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12331979","id":"12331979","description":"2.7.1 release","name":"2.7.1","archived":false,"released":true,"releaseDate":"2015-07-06"}],"issuelinks":[{"id":"12436190","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12436190","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12859403","key":"HBASE-14317","self":"https://issues.apache.org/jira/rest/api/2/issue/12859403","fields":{"summary":"Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-01-27T17:05:07.972+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"Since we upgraded to 2.7.1 we regularly see single-drive failures cause widespread problems at the HBase level (with the default 3x replication target).\n\nHere's an example.  This HBase RegionServer is r12s16 (172.24.32.16) and is writing its WAL to [172.24.32.16:10110, 172.24.32.8:10110, 172.24.32.13:10110] as can be seen by the following occasional messages:\n\n{code}\n2015-08-23 06:28:40,272 INFO  [sync.3] wal.FSHLog: Slow sync cost: 123 ms, current pipeline: [172.24.32.16:10110, 172.24.32.8:10110, 172.24.32.13:10110]\n{code}\n\nA bit later, the second node in the pipeline above is going to experience an HDD failure.\n\n{code}\n2015-08-23 07:21:58,720 WARN  [DataStreamer for file /hbase/WALs/r12s16.sjc.aristanetworks.com,9104,1439917659071/r12s16.sjc.aristanetworks.com%2C9104%2C1439917659071.default.1440314434998 block BP-1466258523-172.24.32.1-1437768622582:blk_1073817519_77099] hdfs.DFSClient: Error Recovery for block BP-1466258523-172.24.32.1-1437768622582:blk_1073817519_77099 in pipeline 172.24.32.16:10110, 172.24.32.13:10110, 172.24.32.8:10110: bad datanode 172.24.32.8:10110\n{code}\n\nAnd then HBase will go like \"omg I can't write to my WAL, let me commit suicide\".\n\n{code}\n2015-08-23 07:22:26,060 FATAL [regionserver/r12s16.sjc.aristanetworks.com/172.24.32.16:9104.append-pool1-t1] wal.FSHLog: Could not append. Requesting close of wal\njava.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[172.24.32.16:10110, 172.24.32.13:10110], original=[172.24.32.16:10110, 172.24.32.13:10110]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:969)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1035)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1184)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:933)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:487)\n{code}\n\nWhereas this should be mostly a non-event as the DFS client should just drop the bad replica from the write pipeline.\n\nThis is a small cluster but has 16 DNs so the failed DN in the pipeline should be easily replaced.  I didn't set {{dfs.client.block.write.replace-datanode-on-failure.policy}} (so it's still {{DEFAULT}}) and didn't set {{dfs.client.block.write.replace-datanode-on-failure.enable}} (so it's still {{true}}).\n\nI don't see anything noteworthy in the NN log around the time of the failure, it just seems like the DFS client gave up or threw an exception back to HBase that it wasn't throwing before or something else, and that made this single drive failure lethal.\n\nWe've occasionally be \"unlucky\" enough to have a single-drive failure cause multiple RegionServers to commit suicide because they had their WALs on that drive.\n\nWe upgraded from 2.7.0 about a month ago, and I'm not sure whether we were seeing this with 2.7 or not â€“ prior to that we were running in a quite different environment, but this is a fairly new deployment.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12752687","id":"12752687","filename":"blk_1073817519_77099.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-27T07:36:39.512+0000","size":3392,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12752687/blk_1073817519_77099.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12752688","id":"12752688","filename":"r12s13-datanode.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-27T07:36:39.515+0000","size":82841,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12752688/r12s13-datanode.log"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12752689","id":"12752689","filename":"r12s16-datanode.log","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-27T07:36:39.518+0000","size":124697,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12752689/r12s16-datanode.log"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"DFS client says \"no more good datanodes being available to try\" on a single drive failure","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"openjdk version \"1.8.0_45-internal\"\nOpenJDK Runtime Environment (build 1.8.0_45-internal-b14)\nOpenJDK 64-Bit Server VM (build 25.45-b02, mixed mode)","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14715564","id":"14715564","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"HI [~tsuna],\n\nThanks for reporting the issue. While the failure reason is yet to be found out with different DNs, would you please try to set config {{dfs.client.block.write.replace-datanode-on-failure.best-effort}} to true in hdfs-default.xml, restarting Hbase RS?\n\nFor some more details, see http://blog.cloudera.com/blog/2015/03/understanding-hdfs-recovery-processes-part-2/\n\nThanks.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-26T21:35:06.552+0000","updated":"2015-08-26T21:35:06.552+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14715584","id":"14715584","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"body":"I'm not sure that's a good idea.  Why would I want to let the RS succeed if it didn't manage to write the three replicas?  My understanding from reading that post is that if the block can be written to only one replica, then the write would be considered successful.  Now my data is at risk because if the drive on which that block is goes bad, I have data loss, and data loss in the WAL would be pretty bad.\n\nWhy doesn't HDFS simply recover from the bad DN in the pipeline?  The blog post confirms my understanding of how this recovery should happen, yet here it seems that a single bad drive screws things up to the point that the pipeline doesn't recover at all.  Why?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-26T21:53:22.313+0000","updated":"2015-08-26T21:53:22.313+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14715622","id":"14715622","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Yes, agree that we need to look further why a single drive failure cause all to fail. Any big GC observed on different DNs? thanks.\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-26T22:12:12.571+0000","updated":"2015-08-26T22:12:12.571+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14715635","id":"14715635","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"body":"I looked at the GC logs and don't see anything unusual (both on the RS/DN/NN sides).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-26T22:20:05.056+0000","updated":"2015-08-26T22:20:05.056+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14716191","id":"14716191","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"would you please collect the logs and jstack of some of the involved DNs (claimed \"bad\" during the pipeline recovering) when the issue is happening? \n\nwonder what kind of errors/warns are reported there.\n\nthanks.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-27T07:00:38.021+0000","updated":"2015-08-27T07:00:38.021+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14716193","id":"14716193","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"is the single drive that failed on the node where RS is running? or is it on a middle node of the three node pipeline? Maybe the recovery process somehow used the replica on the failed drive as the source to copy to a different node attempted thus always fail (like HDFS-6937)?\n\nThanks.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-27T07:06:39.941+0000","updated":"2015-08-27T07:06:39.941+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14716215","id":"14716215","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"body":"It's kinda hard for me to collect jstack of the involved DNs \"when the issue is happening\".  I'm going to attach the log of the DNs from the two surviving nodes, for the 1h time period where the problem occurred.  The logs contain some complaints that mostly relate to r12s8's struggle and subsequent death.  I'm also attaching the output of {{grep blk_1073817519_77099}} from the NN log, which shows that the blog above was allocated and then more than 3 days later it underwent \"recovery\" as I finally killed the stuck RS.  The RS was stuck forever waiting on HDFS (see HBASE-14317 for that other issue).\n\nThe drive that failed was not on the node where the RS was running.  The RS was on r12s16 (172.24.32.16) while the drive failed on r12s8 (172.24.32.8).  As shown in my initial report, this one is the middle node of the three node pipeline.\n\nWhile checksum errors are possible here, the symptom observed was that the drive simply stopped accepting any I/O and was subsequently forcefully unmounted by the kernel.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-27T07:36:15.936+0000","updated":"2015-08-27T07:36:15.936+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14717137","id":"14717137","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks, would you please provide the DN log on r12s8 too?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-27T17:40:36.425+0000","updated":"2015-08-27T17:40:36.425+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14717242","id":"14717242","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"body":"No, these were lost in the HDD failure.\n\nDo you see any sign of pipeline recovery even trying to happen here?  I'm kinda confused because none of the logs I've looked at show any indication of it happening.  Am I missing something?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tsuna","name":"tsuna","key":"tsuna","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890"},"displayName":"Benoit Sigoure","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-27T18:29:06.958+0000","updated":"2015-08-27T18:29:06.958+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14718146","id":"14718146","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"body":"Yes, it's trying to do pipeline recovery, see below:\n\n{code}\n[yzhang@localhost Downloads]$ grep -B 3 blk_1073817519  r12s16-datanode.log | grep firstbadlink\n15/08/23 07:21:49 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.5:10110\n15/08/23 07:21:49 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.5:10110\n15/08/23 07:21:52 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.1:10110\n15/08/23 07:21:52 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.1:10110\n15/08/23 07:21:55 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.6:10110\n15/08/23 07:21:55 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.6:10110\n15/08/23 07:21:58 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.8:10110\n15/08/23 07:21:58 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.8:10110\n15/08/23 07:22:01 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.14:10110\n15/08/23 07:22:01 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.14:10110\n15/08/23 07:22:04 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.2:10110\n15/08/23 07:22:04 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.2:10110\n15/08/23 07:22:07 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.9:10110\n15/08/23 07:22:07 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.9:10110\n15/08/23 07:22:10 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.3:10110\n15/08/23 07:22:10 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.3:10110\n15/08/23 07:22:13 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.7:10110\n15/08/23 07:22:13 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.7:10110\n15/08/23 07:22:16 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.10:10110\n15/08/23 07:22:16 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.10:10110\n15/08/23 07:22:19 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.12:10110\n15/08/23 07:22:19 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.12:10110\n15/08/23 07:22:23 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.11:10110\n15/08/23 07:22:23 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.11:10110\n15/08/23 07:22:26 INFO datanode.DataNode: Datanode 2 got response for connect ack  from downstream datanode with firstbadlink as 172.24.32.15:10110\n15/08/23 07:22:26 INFO datanode.DataNode: Datanode 2 forwarding connect ack to upstream firstbadlink is 172.24.32.15:10110\n{code}\n\nIt happened you loaded r12s13 which is not one of the node in the grepped message (per your report, r12s13 is the last node in the initial pipeline), and r12s16 is the source node.\n\nWould you please upload a few more DN logs?\n\nThanks.\n\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yzhangal","name":"yzhangal","key":"yzhangal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Yongjun Zhang","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-08-28T07:27:05.292+0000","updated":"2015-08-28T07:27:05.292+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14730359","id":"14730359","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in HBase-TRUNK #6778 (See [https://builds.apache.org/job/HBase-TRUNK/6778/])\nHBASE-14317 Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL (stack: rev 661faf6fe0833726d7ce7ad44a829eba3f8e3e45)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControlBasic.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControl.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DamagedWALException.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFailedAppendAndSync.java\nHBASE-14317 Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL; addendum (stack: rev 54717a6314ef6673f7607091e5f77321c202d49f)\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-04T05:57:29.033+0000","updated":"2015-09-04T05:57:29.033+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14730699","id":"14730699","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in HBase-TRUNK #6780 (See [https://builds.apache.org/job/HBase-TRUNK/6780/])\nHBASE-14317 Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL; addendum2 -- found a fix testing the branch-1 patch (stack: rev ec4d719f1927576d3de321c2e380e4c4acd099db)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-04T12:13:09.634+0000","updated":"2015-09-04T12:13:09.634+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14733333","id":"14733333","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in HBase-1.3 #152 (See [https://builds.apache.org/job/HBase-1.3/152/])\nHBASE-14317 Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL (stack: rev bbafb47f7271449d46b46569ca9f0cb227b44c6e)\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DamagedWALException.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFailedAppendAndSync.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControl.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControlBasic.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-07T07:26:25.465+0000","updated":"2015-09-07T07:26:25.465+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14733356","id":"14733356","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in HBase-1.2 #154 (See [https://builds.apache.org/job/HBase-1.2/154/])\nHBASE-14317 Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL (stack: rev 990e3698a7ca7e95894150a2905ba4271eb371e9)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DamagedWALException.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControl.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFailedAppendAndSync.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControlBasic.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-07T07:54:38.682+0000","updated":"2015-09-07T07:54:38.682+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14733402","id":"14733402","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in HBase-1.2-IT #130 (See [https://builds.apache.org/job/HBase-1.2-IT/130/])\nHBASE-14317 Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL (stack: rev 990e3698a7ca7e95894150a2905ba4271eb371e9)\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFailedAppendAndSync.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControl.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DamagedWALException.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControlBasic.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-07T08:40:29.120+0000","updated":"2015-09-07T08:40:29.120+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/14733444","id":"14733444","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in HBase-1.3-IT #136 (See [https://builds.apache.org/job/HBase-1.3-IT/136/])\nHBASE-14317 Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL (stack: rev bbafb47f7271449d46b46569ca9f0cb227b44c6e)\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControl.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/DamagedWALException.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALKey.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConcurrencyControlBasic.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestFailedAppendAndSync.java\n* hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestWALLockup.java\n* hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-09-07T09:13:16.159+0000","updated":"2015-09-07T09:13:16.159+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12859390/comment/15119793","id":"15119793","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Tagar","name":"Tagar","key":"tagar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10448","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10448","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10448","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10448"},"displayName":"Ruslan Dautkhanov","active":true,"timeZone":"America/Denver"},"body":"We seems getting the same problem on a Hive job too:\n\n{quote}\nError: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.20.32.60:1004,DS-1cc9c7cd-f1f9-4cad-b6e2-c9821d644033,DISK]], original=[DatanodeInfoWithStorage[10.20.32.60:1004,DS-1cc9c7cd-f1f9-4cad-b6e2-c9821d644033,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration. at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265) at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.20.32.60:1004,DS-1cc9c7cd-f1f9-4cad-b6e2-c9821d644033,DISK]], original=[DatanodeInfoWithStorage[10.20.32.60:1004,DS-1cc9c7cd-f1f9-4cad-b6e2-c9821d644033,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration. at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:729) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:1047) at org.apache.hadoop.hive.ql.exec.GroupByOperator.flushHashTable(GroupByOperator.java:1015) at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:833) at \n{quote}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=Tagar","name":"Tagar","key":"tagar","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10448","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10448","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10448","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10448"},"displayName":"Ruslan Dautkhanov","active":true,"timeZone":"America/Denver"},"created":"2016-01-27T17:05:07.972+0000","updated":"2016-01-27T17:05:07.972+0000"}],"maxResults":17,"total":17,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-8960/votes","votes":3,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2jevb:"}}