{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12465099","self":"https://issues.apache.org/jira/rest/api/2/issue/12465099","key":"HDFS-1169","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/2","id":"2","description":"The problem described is an issue which will never be fixed.","name":"Won't Fix"},"customfield_12312322":null,"customfield_12310220":"2010-08-26T22:43:31.700+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed May 25 13:46:53 UTC 2011","customfield_12310420":"16033","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_31320145459_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2011-05-18T21:46:25.144+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-1169/watchers","watchCount":3,"isWatching":false},"created":"2010-05-21T09:43:59.807+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12314204","id":"12314204","description":"","name":"0.20.2","archived":false,"released":true,"releaseDate":"2010-02-16"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2011-05-25T13:46:53.634+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Trying to access binary data stored in HDFS (in my case, TypedByte files generated by Dumbo) via thrift talking to org.apache.hadoop.thriftfs.HadoopThriftServer, the data I get back is mangled. For example, when I read a file which contains the value 0xa2, it's coming back as 0xef 0xbf 0xbd, also known as the Unicode replacement character.\n\nI think this is because the read method in HadoopThriftServer.java is trying to convert the data read from HDFS into UTF-8 via the String() constructor. \n\nThis essentially makes the HDFS thrift API useless for me :-(.\n\nNot being an expert on Thrift, but would it be possible to modify the API so that it uses the binary type listed on http://wiki.apache.org/thrift/ThriftTypes?","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12453185","id":"12453185","filename":"hadoopfs.thrift","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"created":"2010-08-26T23:05:04.988+0000","size":4090,"mimeType":"application/octet-stream","content":"https://issues.apache.org/jira/secure/attachment/12453185/hadoopfs.thrift"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12453186","id":"12453186","filename":"HadoopThriftServer.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"created":"2010-08-26T23:10:29.710+0000","size":20750,"mimeType":"text/java","content":"https://issues.apache.org/jira/secure/attachment/12453186/HadoopThriftServer.java"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12480414","id":"12480414","filename":"thriftfs.jar","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=balboah","name":"balboah","key":"balboah","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Johnny Boy","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-25T13:44:47.616+0000","size":23474,"mimeType":"application/java-archive","content":"https://issues.apache.org/jira/secure/attachment/12480414/thriftfs.jar"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12480412","id":"12480412","filename":"thriftfs.jar","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=balboah","name":"balboah","key":"balboah","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Johnny Boy","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-25T13:43:27.704+0000","size":23474,"mimeType":"application/java-archive","content":"https://issues.apache.org/jira/secure/attachment/12480412/thriftfs.jar"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"113361","customfield_12312823":null,"summary":"Can't read binary data off HDFS via thrift API","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=forsberg","name":"forsberg","key":"forsberg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Erik Forsberg","active":true,"timeZone":"Europe/Berlin"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=forsberg","name":"forsberg","key":"forsberg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Erik Forsberg","active":true,"timeZone":"Europe/Berlin"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/12903118","id":"12903118","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"body":"I think I know enough to make this change and do some unit testing, but I need a little java guidance (on building everything).\n\nMainly, I need help on compiling the hadoopthriftapi.jar file from the gen-java files.\n\nI actually really need this for my own uses.\n\nMy first take outlined below starts with just to converting the read/write methods to use binary (vs adding new methods). This way I don't have to worry about making sure the correct read/write methods are called in the initial version.\n\nI re-generated the thrift java files with a new thrift interface the reads/writes in binary.\n\n- note that binary data is converted to UTF-8 on write as well as read, so if you just update the thrift client to write binary, the server will add unicode escape characters before it's even saved to hdfs.\n\nThe code in:\n\nhadoop-0.20.2/src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/HadoopThriftServer.java\n\nis straightforward as well.\n\nHowever! this implements the interface defined in:\n\norg.apache.hadoop.thriftfs.api.ThriftHadoopFileSystem.Iface\n\nAnd even though I update the source in:\n\nhadoop-0.20.2/src/contrib/thriftfs/gen-java\n\nI get an error about overriding the read/write methods incorrectly, so it appears to be pulling the definition of the\n\norg.apache.hadoop.thriftfs.api.ThriftHadoopFileSystem.Iface\n\nfrom hadoopthriftapi.jar (which makes sense).\n\nHowever, I don't know how to rebuild hadoopthriftapi.jar.\n\nI'll attach the thrift file and the HadoopThriftServer.java file a little later, but I just wanted to get this comment up - maybe someone can give me simple instructions on how to build hadoopthriftapi.jar from the gen-java files?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"created":"2010-08-26T22:43:31.700+0000","updated":"2010-08-26T22:43:31.700+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/12903134","id":"12903134","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"body":"Hadoop thrift IDL file with binary read/write and some fixes to generate c# code correctly.\n\nThe charp part was just renaming a variable called \"out\" (reserved keyword) and renaming a field called pathname (which got capitalized to Pathname in accessor, and then conflicted with the class name).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"created":"2010-08-26T23:05:05.068+0000","updated":"2010-08-26T23:05:05.068+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/12903136","id":"12903136","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"body":"Modified HadoopThriftServer with binary read/write.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"created":"2010-08-26T23:10:29.807+0000","updated":"2010-08-26T23:10:29.807+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/12903177","id":"12903177","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"body":"Completely hackalicious solution:\n\nBe forewarned, if you know how to rebuild: \n\nhadoopthriftapi.jar \n\nFrom the gen-java files generated by the thrift IDL file above, you're WAY better off, and please let me know.\nOtherwise go to:\n\n/hadoop-0.20.2/src/contrib/thriftfs$\n\nOpen the file:\n\n/hadoop-0.20.2/src/contrib/thriftfs$ gvim src/java/org/apache/hadoop/thriftfs/HadoopThriftServer.java\n\nimport commons-encoder:\n\nimport org.apache.commons.codec.binary.Base64;\nimport org.apache.commons.codec.DecoderException;\nimport org.apache.commons.codec.EncoderException;\n\nNote that hadoop &  the hadoop thrift api's depend on commons-encoder-1.3 , not 1.4.\nThis is unforntunate, because 1.3 has a pretty brain-dead interface.\n\nModify the send and receive functions to treat the string arguments (and return value) as base64 encoded binary:\n\n    /**\n     * write to a file\n     */\n    public boolean write(ThriftHandle tout, String encodedData) throws ThriftIOException {\n      try {\n        now = now();\n        HadoopThriftHandler.LOG.debug(\"write: \" + tout.id);\n        FSDataOutputStream out = (FSDataOutputStream)lookup(tout.id);\n        Base64 base64 = new Base64();\n        byte[] tmp = null;\n        tmp = (byte[])base64.decode( (byte[]) encodedData.getBytes(\"UTF-8\") );\n            \n        out.write(tmp, 0, tmp.length);\n        HadoopThriftHandler.LOG.debug(\"wrote: \" + tout.id);\n        return true;\n      } catch (IOException e) {\n        throw new ThriftIOException(e.getMessage());\n      }\n    }\n\n    /**\n     * read from a file\n     */\n    public String read(ThriftHandle tout, long offset,\n                       int length) throws ThriftIOException {\n      try {\n        now = now();\n        HadoopThriftHandler.LOG.debug(\"read: \" + tout.id +\n                                     \" offset: \" + offset +\n                                     \" length: \" + length);\n        FSDataInputStream in = (FSDataInputStream)lookup(tout.id);\n        if (in.getPos() != offset) {\n          in.seek(offset);\n        }\n        byte[] tmp = new byte[length];\n        int numbytes = in.read(offset, tmp, 0, length);\n        HadoopThriftHandler.LOG.debug(\"read done: \" + tout.id);\n        try\n        {\n            Base64 base64 = new Base64();\n            return new String( (byte[])base64.encode( (Object)tmp ), \"UTF-8\");\n        }\n        catch( EncoderException e )\n        {\n            e.printStackTrace();\n            System.exit(0);\n            return \"\";\n        }\n      } catch (IOException e) {\n        throw new ThriftIOException(e.getMessage());\n      }\n    }\n\nCompile:\n\n/hadoop-0.20.2/src/contrib/thriftfs$ ant\n\nCopy the jar file:\n\nhadoop-0.20.2/build/contrib/thriftfs/hadoop-0.20.2-thriftfs.jar\n\nto your namenode (or wherever you run your hadoop thrift server from), and drop it in:\n\nhadoop-0.20.2/contrib/thriftfs/hadoop-0.20.2-thriftfs.jar\n\n(no build dir).\n\nthen start your thrift server as normal:\n\n/hadoop/src/contrib/thriftfs/scripts$ ./start_thrift_server.sh 50050\n\n\nNow, in all your thrift clients, you have to base64 encode any data before sending it, and decode after receiving.\n\nBut you can finally get binary data on hdfs. Albeit at a high price in ugliness & performance (coz I'm assuming your storing large files on hdfs...)\n\nI've only tested this on one 224 Kb file, but I move everything in 8K chunks client side, so it should work on large files (it'll just be horrifically slow).\n\nAgain, if anyone figures out how to rebuild: \n\nhadoopthriftapi.jar \n\nFrom the gen-java files, please enlighten!\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"created":"2010-08-27T01:38:30.393+0000","updated":"2010-08-27T01:38:30.393+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/12903178","id":"12903178","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"body":"oy. didn't format the code, sorry:\n\n{noformat} \n    /**\n     * write to a file\n     */\n    public boolean write(ThriftHandle tout, String encodedData) throws ThriftIOException {\n      try {\n        now = now();\n        HadoopThriftHandler.LOG.debug(\"write: \" + tout.id);\n        FSDataOutputStream out = (FSDataOutputStream)lookup(tout.id);\n        Base64 base64 = new Base64();\n        byte[] tmp = null;\n        tmp = (byte[])base64.decode( (byte[]) encodedData.getBytes(\"UTF-8\") );\n            \n        out.write(tmp, 0, tmp.length);\n        HadoopThriftHandler.LOG.debug(\"wrote: \" + tout.id);\n        return true;\n      } catch (IOException e) {\n        throw new ThriftIOException(e.getMessage());\n      }\n    }\n\n    /**\n     * read from a file\n     */\n    public String read(ThriftHandle tout, long offset,\n                       int length) throws ThriftIOException {\n      try {\n        now = now();\n        HadoopThriftHandler.LOG.debug(\"read: \" + tout.id +\n                                     \" offset: \" + offset +\n                                     \" length: \" + length);\n        FSDataInputStream in = (FSDataInputStream)lookup(tout.id);\n        if (in.getPos() != offset) {\n          in.seek(offset);\n        }\n        byte[] tmp = new byte[length];\n        int numbytes = in.read(offset, tmp, 0, length);\n        HadoopThriftHandler.LOG.debug(\"read done: \" + tout.id);\n        try\n        {\n            Base64 base64 = new Base64();\n            return new String( (byte[])base64.encode( (Object)tmp ), \"UTF-8\");\n        }\n        catch( EncoderException e )\n        {\n            e.printStackTrace();\n            System.exit(0);\n            return \"\";\n        }\n      } catch (IOException e) {\n        throw new ThriftIOException(e.getMessage());\n      }\n    }\n\n{noformat} ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stu","name":"stu","key":"stu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Stuart Smith","active":true,"timeZone":"Etc/UTC"},"created":"2010-08-27T01:40:51.741+0000","updated":"2010-08-27T01:40:51.741+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/13035710","id":"13035710","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nidaley","name":"nidaley","key":"nidaley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nigel Daley","active":true,"timeZone":"Etc/UTC"},"body":"thriftfs contrib removed","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nidaley","name":"nidaley","key":"nidaley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nigel Daley","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-18T21:46:25.162+0000","updated":"2011-05-18T21:46:25.162+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/13039121","id":"13039121","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=balboah","name":"balboah","key":"balboah","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Johnny Boy","active":true,"timeZone":"Etc/UTC"},"body":"HadoopThriftServer with changed encoding for reading and writing","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=balboah","name":"balboah","key":"balboah","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Johnny Boy","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-25T13:43:27.751+0000","updated":"2011-05-25T13:43:27.751+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/13039123","id":"13039123","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=balboah","name":"balboah","key":"balboah","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Johnny Boy","active":true,"timeZone":"Etc/UTC"},"body":"Modified version of HadoopThriftServer","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=balboah","name":"balboah","key":"balboah","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Johnny Boy","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-25T13:44:47.652+0000","updated":"2011-05-25T13:44:47.652+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12465099/comment/13039124","id":"13039124","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=balboah","name":"balboah","key":"balboah","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Johnny Boy","active":true,"timeZone":"Etc/UTC"},"body":"Bleh double post and I don't know how to remove it :)\nThe comment should be that it was enough to change utf-8 into latin1 for me to get the binary data to work, see attached file above","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=balboah","name":"balboah","key":"balboah","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Johnny Boy","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-25T13:46:53.601+0000","updated":"2011-05-25T13:46:53.601+0000"}],"maxResults":9,"total":9,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-1169/votes","votes":1,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0jrcn:"}}