{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13152093","self":"https://issues.apache.org/jira/rest/api/2/issue/13152093","key":"HDFS-13441","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2018-04-13T01:30:57.367+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Apr 19 02:24:09 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-13441/watchers","watchCount":6,"isWatching":false},"created":"2018-04-12T21:30:25.344+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"3.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12331979","id":"12331979","description":"2.7.1 release","name":"2.7.1","archived":false,"released":true,"releaseDate":"2015-07-06"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-04-19T02:24:09.482+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12312926","id":"12312926","name":"namenode"}],"timeoriginalestimate":null,"description":"After NameNode failover, lots of application failed due to some DataNodes can't re-compute password from block token.\r\n{code:java}\r\n2018-04-11 20:10:52,448 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: hdc3-lvs01-400-1701-048.stratus.lvs.ebay.com:50010:DataXceiver error processing unknown operation  src: /10.142.74.116:57404 dst: /10.142.77.45:50010\r\njavax.security.sasl.SaslException: DIGEST-MD5: IO error acquiring password [Caused by org.apache.hadoop.security.token.SecretManager$InvalidToken: Can't re-compute password for block_token_identifier (expiryDate=1523538652448, keyId=1762737944, userId=hadoop, blockPoolId=BP-36315570-10.103.108.13-1423055488042, blockId=12142862700, access modes=[WRITE]), since the required block key (keyID=1762737944) doesn't exist.]\r\n        at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:598)\r\n        at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:244)\r\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant.evaluateChallengeOrResponse(SaslParticipant.java:115)\r\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.doSaslHandshake(SaslDataTransferServer.java:376)\r\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.getSaslStreams(SaslDataTransferServer.java:300)\r\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.receive(SaslDataTransferServer.java:127)\r\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:194)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.hadoop.security.token.SecretManager$InvalidToken: Can't re-compute password for block_token_identifier (expiryDate=1523538652448, keyId=1762737944, userId=hadoop, blockPoolId=BP-36315570-10.103.108.13-1423055488042, blockId=12142862700, access modes=[WRITE]), since the required block key (keyID=1762737944) doesn't exist.\r\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.retrievePassword(BlockTokenSecretManager.java:382)\r\n        at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.retrievePassword(BlockPoolTokenSecretManager.java:79)\r\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.buildServerPassword(SaslDataTransferServer.java:318)\r\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer.access$100(SaslDataTransferServer.java:73)\r\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer$2.apply(SaslDataTransferServer.java:297)\r\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer$SaslServerCallbackHandler.handle(SaslDataTransferServer.java:241)\r\n        at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:589)\r\n        ... 7 more\r\n\r\n{code}\r\n \r\n\r\nIn the DataNode log, we didn't see DataNode update block keys around 2018-04-11 09:55:00 and around 2018-04-11 19:55:00.\r\n{code:java}\r\n2018-04-10 14:51:36,424 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys\r\n2018-04-10 23:55:38,420 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys\r\n2018-04-11 00:51:34,792 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys\r\n2018-04-11 10:51:39,403 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys\r\n2018-04-11 20:51:44,422 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys\r\n2018-04-12 02:54:47,855 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys\r\n2018-04-12 05:55:44,456 INFO org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Setting block keys\r\n{code}\r\nThe reason is there is SocketTimeOutException when sending heartbeat to StandbyNameNode\r\n{code:java}\r\n2018-04-11 09:55:34,699 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService\r\njava.net.SocketTimeoutException: Call From hdc3-lvs01-400-1701-048.stratus.lvs.ebay.com/10.142.77.45 to ares-nn.vip.ebay.com:8030 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.142.77.45:48803 remote=ares-nn.vip.ebay.com/10.103.108.200:8030]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\r\n        at sun.reflect.GeneratedConstructorAccessor32.newInstance(Unknown Source)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)\r\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1430)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n        at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)\r\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:155)\r\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:427)\r\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:543)\r\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:725)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n2018-04-11 19:56:13,308 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService\r\njava.net.SocketTimeoutException: Call From hdc3-lvs01-400-1701-048.stratus.lvs.ebay.com/10.142.77.45 to ares-nn.vip.ebay.com:8030 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.142.77.45:50381 remote=ares-nn.vip.ebay.com/10.103.108.200:8030]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\r\n        at sun.reflect.GeneratedConstructorAccessor32.newInstance(Unknown Source)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)\r\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1430)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\r\n        at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)\r\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:155)\r\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:427)\r\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:543)\r\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:725)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.142.77.45:50381 remote=ares-nn.vip.ebay.com/10.103.108.200:8030]\r\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)\r\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\r\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\r\n        at java.io.FilterInputStream.read(FilterInputStream.java:133)\r\n        at java.io.FilterInputStream.read(FilterInputStream.java:133)\r\n        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:523)\r\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1087)\r\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:982)\r\n\r\n{code}\r\nIn the Standby NameNode, we can see it dropped the HeartbeatResponses which contain new BlockKey.\r\n{code:java}\r\n2018-04-11 09:53:34,293 INFO org.apache.hadoop.ipc.Server: IPC Server handler 25 on 8030: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.sendHeartbeat from 10.142.77.45:54371 Call#13364496 Retry#0\r\n2018-04-11 09:56:42,436 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8030: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.sendHeartbeat from 10.142.77.45:48803 Call#13364569 Retry#0\r\n\r\n2018-04-11 19:54:13,092 INFO org.apache.hadoop.ipc.Server: IPC Server handler 43 on 8030: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.sendHeartbeat from 10.142.77.45:52870 Call#13385487 Retry#0\r\n2018-04-11 19:57:21,774 INFO org.apache.hadoop.ipc.Server: IPC Server handler 50 on 8030: skipped org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.sendHeartbeat from 10.142.77.45:35918 Call#13385562 Retry#0\r\n\r\n\r\n{code}\r\n \r\n\r\n \r\n\r\n ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12919272","id":"12919272","filename":"HDFS-13441.002.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-16T19:11:13.390+0000","size":5458,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12919272/HDFS-13441.002.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12919706","id":"12919706","filename":"HDFS-13441.003.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-18T23:34:27.178+0000","size":757,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12919706/HDFS-13441.003.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12918851","id":"12918851","filename":"HDFS-13441.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-13T01:15:34.642+0000","size":1863,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12918851/HDFS-13441.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"DataNode missed BlockKey update from NameNode due to HeartbeatResponse was dropped","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16436646","id":"16436646","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"There are two ways to fix this bug: one is making sure NameNode's send new BlockKey to DataNodes successfully; another one is once DataNode can't find BlockKey, re-register the Datanode to NameNodes to make sure DataNode get the newest BlockKeys.\r\n\r\nThe first way is much more complex and need change more code than second way.\r\n\r\nThe attached patch is re-register DataNode to NameNodes.\r\n\r\nNot tested, just for ideas.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-13T01:23:26.530+0000","updated":"2018-04-13T01:23:26.530+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16436649","id":"16436649","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~zhaoyunjiong] thanks for reporting the issue.\r\n\r\nBlockKey is usually synchronized aggressively – IIRC every 2.5 hours, and BlockKey's life time is much longer than that (can't recall right away) so it's surprising to me a single missing heartbeat would cause the error you mentioned. There's probably something deeper you need to dig into.\r\n\r\nThat said, with regard to your patch,\r\n\r\n# please attach a test case to demonstrate the fix will work in this scenario\r\n# it is not a good idea to match exception's message string. As you see in BlockTokenSecretManager, there are two sources where exception message starts with \"Can't re-compute\" and message can change.\r\n\r\n P.S. Take a look at HDFS-11741 and HDFS-10609 for reference to craft your test case.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-13T01:30:57.367+0000","updated":"2018-04-13T01:31:37.585+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16436752","id":"16436752","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"body":"hi [~zhaoyunjiong],[~jojochuang]\r\nwe meet the similar problem in our production cluster, and when dig deeply, we found that the root case is about reregister. HDFS-12749 is following this case. FYI.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-04-13T03:38:05.472+0000","updated":"2018-04-13T03:38:05.472+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16436754","id":"16436754","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"body":"the detailed analysis ref: https://issues.apache.org/jira/browse/HDFS-12749?focusedCommentId=16364280&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16364280","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-04-13T03:42:10.159+0000","updated":"2018-04-13T03:42:10.159+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16436799","id":"16436799","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 25s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 23m 21s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 30s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 39s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 46s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 46s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}111m 44s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 25s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}166m 36s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.web.TestWebHdfsTimeouts |\r\n|   | hadoop.hdfs.TestEncryptionZonesWithKMS |\r\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |\r\n| JIRA Issue | HDFS-13441 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12918851/HDFS-13441.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux 174e9aa21171 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 53b3e59 |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_162 |\r\n| findbugs | v3.1.0-RC1 |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/23915/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/23915/testReport/ |\r\n| Max. process+thread count | 3401 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/23915/console |\r\n| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-04-13T04:45:16.616+0000","updated":"2018-04-13T04:45:16.616+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16437740","id":"16437740","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"{quote} \r\n\r\nBlockKey is usually synchronized aggressively – IIRC every 2.5 hours, and BlockKey's life time is much longer than that (can't recall right away) so it's surprising to me a single missing heartbeat would cause the error you mentioned. There's probably something deeper you need to dig into.\r\n{quote}\r\nDataNode gets block keys from NameNode happens at two places.\r\n\r\nOne is when DataNode registers to NameNode.\r\n\r\nAnother one is via heartbeat, by default happens every 600 minutes.\r\n\r\nBy default,\r\n{quote}<property>\r\n <name>dfs.block.access.key.update.interval</name>\r\n <value>600</value>\r\n <description>\r\n Interval in minutes at which namenode updates its access keys.\r\n </description>\r\n </property>\r\n{quote}\r\n{quote}<property>\r\n <name>dfs.block.access.token.lifetime</name>\r\n <value>600</value>\r\n <description>The lifetime of access tokens in minutes.</description>\r\n </property>\r\n{quote}\r\n[~jojochuang] , double checked the code and log, the DataNode must be missing *two* heartbeats from Standby NameNode which contains new block key just before Standby NameNode become active.\r\n\r\nWe have more than 2000 Datanodes in this cluster, if more than three Datanodes doesn't have the current Block key, worst case is users will not able to read some blocks for 10 at most hours.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-13T18:47:19.379+0000","updated":"2018-04-13T21:50:03.707+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16438448","id":"16438448","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"body":"[~zhaoyunjiong], If I understand correctly, this issue was caused by StandbyNameNode restart.\r\nNameNode's load is very high when restart in a large cluster, especially when NameNode starts to process block report, if there are some DataNode to reregister in this period, it may be timeout very likely, since BPServiceActor#register can not catch IOException which wrap over SocketTimeoutException. In one word, NameNode correctly processed the registration, but the DataNode timeout before receiving the response. so #updateBlockKeysWhenStartup could not be invoke.\r\n{code:java}\r\n  void register(NamespaceInfo nsInfo) throws IOException {\r\n    // The handshake() phase loaded the block pool storage\r\n    // off disk - so update the bpRegistration object from that info\r\n    DatanodeRegistration newBpRegistration = bpos.createRegistration();\r\n\r\n    LOG.info(this + \" beginning handshake with NN\");\r\n\r\n    while (shouldRun()) {\r\n      try {\r\n        // Use returned registration from namenode with updated fields\r\n        newBpRegistration = bpNamenode.registerDatanode(newBpRegistration);\r\n        newBpRegistration.setNamespaceInfo(nsInfo);\r\n        bpRegistration = newBpRegistration;\r\n        break;\r\n      } catch(EOFException e) {  // namenode might have just restarted\r\n        LOG.info(\"Problem connecting to server: \" + nnAddr + \" :\"\r\n            + e.getLocalizedMessage());\r\n        sleepAndLogInterrupts(1000, \"connecting to server\");\r\n      } catch(SocketTimeoutException e) {  // namenode is busy\r\n        LOG.info(\"Problem connecting to server: \" + nnAddr);\r\n        sleepAndLogInterrupts(1000, \"connecting to server\");\r\n      }\r\n    }\r\n    \r\n    LOG.info(\"Block pool \" + this + \" successfully registered with NN\");\r\n    bpos.registrationSucceeded(this, bpRegistration);\r\n\r\n    // random short delay - helps scatter the BR from all DNs\r\n    scheduler.scheduleBlockReport(dnConf.initialBlockReportDelay);\r\n    updateBlockKeysWhenStartup();\r\n  }\r\n{code}\r\n\r\nIn this case, the following Read/Write from client to this DataNode would be certain to thrown {{SaslException}}. HDFS-12749 is trying to resolve this matter once for all. FYI.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-04-14T18:01:26.204+0000","updated":"2018-04-14T18:09:24.356+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16438531","id":"16438531","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"[~hexiaoqiao] , this issue is different, it is not about DN register to NN, it is about lost heartbeat responses from Standby NameNode could end with some DataNodes missed the block key after Standby NameNode become active.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-14T22:41:44.554+0000","updated":"2018-04-14T22:41:44.554+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16438968","id":"16438968","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"body":"[~zhaoyunjiong] it is my misunderstand above\r\nOne minor suggestion for patch-v1, if catch IOException in {{DataXceiver}} and re-register DataNode to NameNode, it also could be failure, so I thinks this solution can reduce the possibility of failure only but not solute it completely.\r\nIs there possibility that change the mode of getting BlockKey from *Push* by NameNode to *Pull* periodically by DataNode, just like scheduler {{BlockReport}} of DataNode.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-04-16T04:08:33.409+0000","updated":"2018-04-16T04:08:33.409+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16439897","id":"16439897","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"[~hexiaoqiao] , Let DataNode pull Block Key from NameNode is one choice, since it need change protocol, if I didn't understand wrong, it will need go to Hadoop 4.\r\n\r\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-16T19:17:23.002+0000","updated":"2018-04-16T19:17:23.002+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16439905","id":"16439905","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Upload HDFS-13441.002.patch, which contain unit test.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-16T19:21:45.158+0000","updated":"2018-04-16T19:21:45.158+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16440080","id":"16440080","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 26m 30s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  5s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 40s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  2s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  6s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |\r\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  1m  0s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 215 unchanged - 0 fixed = 216 total (was 215) {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 47s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  8s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 87m 52s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 25s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}152m 17s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.TestDFSInotifyEventInputStreamKerberized |\r\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |\r\n| JIRA Issue | HDFS-13441 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12919272/HDFS-13441.002.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux 25203e55ceaa 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 2d0662c |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_162 |\r\n| findbugs | v3.1.0-RC1 |\r\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/23957/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/23957/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/23957/testReport/ |\r\n| Max. process+thread count | 3752 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/23957/console |\r\n| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-04-16T21:53:05.852+0000","updated":"2018-04-16T21:53:05.852+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16440104","id":"16440104","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Unit test failure is not related to this patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-16T22:23:33.971+0000","updated":"2018-04-16T22:23:33.971+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16440798","id":"16440798","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"body":"{quote}Let DataNode pull Block Key from NameNode is one choice, since it need change protocol, if I didn't understand wrong, it will need go to Hadoop 4.{quote}\r\n[~zhaoyunjiong] \r\nThe interface about get BlockKeys is ready now, reference: NamenodeProtocol#getBlockKeys.\r\nif DataNode get Block Keys from NameNode using this interface, then it is more convenient to process kind of exceptions. if possible, is it necessary to create new issue to push this feature?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hexiaoqiao","name":"hexiaoqiao","key":"hexiaoqiao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hexiaoqiao&avatarId=26980","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hexiaoqiao&avatarId=26980","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hexiaoqiao&avatarId=26980","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hexiaoqiao&avatarId=26980"},"displayName":"He Xiaoqiao","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-04-17T12:16:59.091+0000","updated":"2018-04-17T12:17:56.243+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16441110","id":"16441110","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"[~hexiaoqiao], DataNode can't use NamenodeProtocol.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-17T16:29:28.616+0000","updated":"2018-04-17T16:29:28.616+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16442824","id":"16442824","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"This is a bad approach for a couple reasons.  Checking if the exception contains \"Can't recompute\" is very fragile.  Exception messages should be considered opaque.\r\n\r\nAlso consider that an invalid token hash caused by a missed key update is rare.  The more common case is something like the balancer using an expired secret.  Or consider a faulty or malicious client using an expired token.  This approach may easily cause DNs to go into re-registration loops and ruin a cluster.\r\n\r\nPlease see discussion on HDFS-13473 for a cleaner way to handle this problem.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2018-04-18T16:45:05.428+0000","updated":"2018-04-18T16:45:05.428+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16443349","id":"16443349","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"[~daryn] , you are right, it's not the best and reliable way to fix this issue.\r\n\r\nAfter some rethink, I think one line code should fix this issue.\r\n\r\nWhen NameNode startActiveServices, it will call \r\n{code:java}\r\nblockManager.getDatanodeManager().markAllDatanodesStale();\r\n{code}\r\nInside markAllDatanodesStale, add one line code to make sure DataNode have the current key from active NameNode.\r\n{code:java}\r\ndn.setNeedKeyUpdate(true);\r\n{code} ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-04-18T23:39:34.846+0000","updated":"2018-04-18T23:39:34.846+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13152093/comment/16443471","id":"16443471","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 41s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 26m 55s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 16s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 53s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  8s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 28s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  0s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 46s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  1m  7s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 19s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 59s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 99m 18s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 24s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}163m 14s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |\r\n| JIRA Issue | HDFS-13441 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12919706/HDFS-13441.003.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux 79a13594c2b2 3.13.0-143-generic #192-Ubuntu SMP Tue Feb 27 10:45:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / e4c39f3 |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_162 |\r\n| findbugs | v3.1.0-RC1 |\r\n| mvninstall | https://builds.apache.org/job/PreCommit-HDFS-Build/23992/artifact/out/patch-mvninstall-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/23992/testReport/ |\r\n| Max. process+thread count | 3243 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/23992/console |\r\n| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-04-19T02:24:09.482+0000","updated":"2018-04-19T02:24:09.482+0000"}],"maxResults":18,"total":18,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-13441/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3shqn:"}}