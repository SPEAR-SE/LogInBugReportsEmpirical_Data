{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12821545","self":"https://issues.apache.org/jira/rest/api/2/issue/12821545","key":"HDFS-8160","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-04-19T17:47:49.657+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Apr 24 09:33:51 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-8160/watchers","watchCount":5,"isWatching":false},"created":"2015-04-16T14:53:48.160+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12328970","id":"12328970","description":"2.5.2 release","name":"2.5.2","archived":false,"released":true,"releaseDate":"2014-11-19"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-04-24T09:33:51.086+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12313126","id":"12313126","name":"libhdfs","description":"The C++ interface to HDFS."}],"timeoriginalestimate":null,"description":"Calling hdfsOpenFile on a file residing on target 3-node Hadoop cluster (described in detail in Environment section) blocks for a long time (several minutes).  I've noticed that the delay is related to the size of the target file. \nFor example, attempting to hdfsOpenFile() on a file of filesize 852483361 took 121 seconds, but a file of 15458 took less than a second.\n\nAlso, during the long delay, the following stacktrace is routed to standard out:\n\n2015-04-16 10:32:13,943 WARN  [main] hdfs.BlockReaderFactory (BlockReaderFactory.java:getRemoteBlockReaderFromTcp(693)) - I/O error constructing remote block reader.\norg.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.40.8.10:50010]\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)\n\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)\n\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)\n\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:854)\n\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:143)\n2015-04-16 10:32:13,946 WARN  [main] hdfs.DFSClient (DFSInputStream.java:blockSeekTo(612)) - Failed to connect to /10.40.8.10:50010 for block, add to deadNodes and continue. org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.40.8.10:50010]\norg.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.40.8.10:50010]\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3101)\n\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:755)\n\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:670)\n\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:337)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:576)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:800)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:854)\n\tat org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:143)\n\nI have also seen similar delays and stacktrace printout when executing dfs CL commands on those same files (df -cat, df -tail, etc.).\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Long delays when calling hdfsOpenFile()","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rpastrana","name":"rpastrana","key":"rpastrana","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rod","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rpastrana","name":"rpastrana","key":"rpastrana","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rod","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"3-node Apache Hadoop 2.5.2 cluster running on Ubuntu 14.04 \n\ndfshealth overview:\nSecurity is off.\nSafemode is off.\n\n8 files and directories, 9 blocks = 17 total filesystem object(s).\n\nHeap Memory used 45.78 MB of 90.5 MB Heap Memory. Max Heap Memory is 889 MB.\n\nNon Heap Memory used 36.3 MB of 70.44 MB Commited Non Heap Memory. Max Non Heap Memory is 130 MB.\nConfigured Capacity:\t118.02 GB\nDFS Used:\t2.77 GB\nNon DFS Used:\t12.19 GB\nDFS Remaining:\t103.06 GB\nDFS Used%:\t2.35%\nDFS Remaining%:\t87.32%\nBlock Pool Used:\t2.77 GB\nBlock Pool Used%:\t2.35%\nDataNodes usages% (Min/Median/Max/stdDev): \t2.35% / 2.35% / 2.35% / 0.00%\nLive Nodes\t3 (Decommissioned: 0)\nDead Nodes\t0 (Decommissioned: 0)\nDecommissioning Nodes\t0\nNumber of Under-Replicated Blocks\t0\nNumber of Blocks Pending Deletion\t0\n\nDatanode Information\nIn operation\nNode\tLast contact\tAdmin State\tCapacity\tUsed\tNon DFS Used\tRemaining\tBlocks\tBlock pool used\tFailed Volumes\tVersion\nhadoop252-3 (x.x.x.10:50010)\t1\tIn Service\t39.34 GB\t944.85 MB\t3.63 GB\t34.79 GB\t9\t944.85 MB (2.35%)\t0\t2.5.2\nhadoop252-1 (x.x.x.8:50010)\t0\tIn Service\t39.34 GB\t944.85 MB\t4.94 GB\t33.48 GB\t9\t944.85 MB (2.35%)\t0\t2.5.2\nhadoop252-2 (x.x.x.9:50010)\t1\tIn Service\t39.34 GB\t944.85 MB\t3.63 GB\t34.79 GB\t9\t944.85 MB (2.35%)\t0\t2.5.2\n\njava version \"1.7.0_76\"\nJava(TM) SE Runtime Environment (build 1.7.0_76-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 24.76-b04, mixed mode)","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12821545/comment/14502020","id":"14502020","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"From the stack trace\n{code}\norg.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.40.8.10:50010]\n{code}\nour server at 10.40.8.10 appears to be down or unreachable.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2015-04-19T17:47:49.657+0000","updated":"2015-04-19T17:47:49.657+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12821545/comment/14504154","id":"14504154","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rpastrana","name":"rpastrana","key":"rpastrana","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rod","active":true,"timeZone":"Etc/UTC"},"body":"I wish the answer were that simple, here's some cluster status taken at the time of the issue which indicates all nodes were running healthy. Also, the operation ultimately did not fail.\n\n3-node Apache Hadoop 2.5.2 cluster running on Ubuntu 14.04 \n\ndfshealth overview:\nSecurity is off.\nSafemode is off.\n\n8 files and directories, 9 blocks = 17 total filesystem object(s).\n\nHeap Memory used 45.78 MB of 90.5 MB Heap Memory. Max Heap Memory is 889 MB.\n\nNon Heap Memory used 36.3 MB of 70.44 MB Commited Non Heap Memory. Max Non Heap Memory is 130 MB.\nConfigured Capacity:\t118.02 GB\nDFS Used:\t2.77 GB\nNon DFS Used:\t12.19 GB\nDFS Remaining:\t103.06 GB\nDFS Used%:\t2.35%\nDFS Remaining%:\t87.32%\nBlock Pool Used:\t2.77 GB\nBlock Pool Used%:\t2.35%\nDataNodes usages% (Min/Median/Max/stdDev): \t2.35% / 2.35% / 2.35% / 0.00%\nLive Nodes\t3 (Decommissioned: 0)\nDead Nodes\t0 (Decommissioned: 0)\nDecommissioning Nodes\t0\nNumber of Under-Replicated Blocks\t0\nNumber of Blocks Pending Deletion\t0\n\nDatanode Information\nIn operation\nNode\tLast contact\tAdmin State\tCapacity\tUsed\tNon DFS Used\tRemaining\tBlocks\tBlock pool used\tFailed Volumes\tVersion\nhadoop252-3 (x.x.x.10:50010)\t1\tIn Service\t39.34 GB\t944.85 MB\t3.63 GB\t34.79 GB\t9\t944.85 MB (2.35%)\t0\t2.5.2\nhadoop252-1 (x.x.x.8:50010)\t0\tIn Service\t39.34 GB\t944.85 MB\t4.94 GB\t33.48 GB\t9\t944.85 MB (2.35%)\t0\t2.5.2\nhadoop252-2 (x.x.x.9:50010)\t1\tIn Service\t39.34 GB\t944.85 MB\t3.63 GB\t34.79 GB\t9\t944.85 MB (2.35%)\t0\t2.5.2","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rpastrana","name":"rpastrana","key":"rpastrana","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rod","active":true,"timeZone":"Etc/UTC"},"created":"2015-04-21T01:58:32.336+0000","updated":"2015-04-21T01:58:32.336+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12821545/comment/14504623","id":"14504623","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"it ultimately worked as after timing out, the DFS client tried a different host.\n\nwhat may be happening is that the datanodes are reporting in as healthy, but the address they publish for clients to get that data isn't accessible. Wrong hostname or firewalls being the common causes; network & routing problems another\n\ntry a telnet to the hostname & port listed, from the machine that isn't able to connect, and see what happens","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2015-04-21T08:47:22.964+0000","updated":"2015-04-21T08:47:22.964+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12821545/comment/14508981","id":"14508981","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rpastrana","name":"rpastrana","key":"rpastrana","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rod","active":true,"timeZone":"Etc/UTC"},"body":"The hadoop user can ssh paswordless into and from all 3 nodes. \nAlso, port 50010 is reachable on all nodes from all nodes using hostnames and ip address...\nhadoop@hadoop252-3:~$ nc 10.40.8.10 50010 < /dev/null; echo $?\n0\nhadoop@hadoop252-3:~$ nc 10.40.8.9 50010 < /dev/null; echo $?\n0\nhadoop@hadoop252-3:~$ nc 10.40.8.8 50010 < /dev/null; echo $?\n0\nhadoop@hadoop252-3:~$ nc hadoop252-1 50010 < /dev/null; echo $?\n0\nhadoop@hadoop252-3:~$ nc hadoop252-2 50010 < /dev/null; echo $?\n0\nhadoop@hadoop252-3:~$ nc hadoop252-3 50010 < /dev/null; echo $?\n0\n\nAny more ideas what this could be?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rpastrana","name":"rpastrana","key":"rpastrana","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rod","active":true,"timeZone":"Etc/UTC"},"created":"2015-04-23T12:37:25.955+0000","updated":"2015-04-23T12:37:25.955+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12821545/comment/14510716","id":"14510716","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"body":"nothing obvious springs to mind. what happens if you kill that first DN?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=stevel%40apache.org","name":"stevel@apache.org","key":"stevel@apache.org","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=stevel%40apache.org&avatarId=16513","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=stevel%40apache.org&avatarId=16513","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=stevel%40apache.org&avatarId=16513","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=stevel%40apache.org&avatarId=16513"},"displayName":"Steve Loughran","active":true,"timeZone":"Europe/London"},"created":"2015-04-24T09:33:51.086+0000","updated":"2015-04-24T09:33:51.086+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-8160/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2dcl3:"}}