{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12413821","self":"https://issues.apache.org/jira/rest/api/2/issue/12413821","key":"HDFS-196","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2009-02-04T10:19:03.374+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jun 10 06:15:54 UTC 2015","customfield_12310420":"17099","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-196/watchers","watchCount":8,"isWatching":false},"created":"2009-02-02T22:12:39.264+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-06-10T06:15:54.536+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/4","description":"This issue was once resolved, but the resolution was deemed incorrect. From here issues are either marked assigned or resolved.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/reopened.png","name":"Reopened","id":"4","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"Our application (Hypertable) creates a transaction log in HDFS.  This log is written with the following pattern:\n\nout_stream.write(header, 0, 7);\nout_stream.sync()\nout_stream.write(data, 0, amount);\nout_stream.sync()\n[...]\n\nHowever, if the application crashes and then comes back up again, the following statement\n\nlength = mFilesystem.getFileStatus(new Path(fileName)).getLen();\n\nreturns the wrong length.  Apparently this is because this method fetches length information from the NameNode which is stale.  Ideally, a call to getFileStatus() would return the accurate file length by fetching the size of the last block from the primary datanode.\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"108279","customfield_12312823":null,"summary":"File length not reported correctly after application crash","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nuggetwheat","name":"nuggetwheat","key":"nuggetwheat","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Judd","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nuggetwheat","name":"nuggetwheat","key":"nuggetwheat","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Doug Judd","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12413821/comment/12670298","id":"12670298","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"The staleness corrects itself if either another writer opens the file for \"appending\" to it or the hard limit of 1 hour (the lease recovery period) expires. But I agree that your proposal is better. +1\n\nIt introduces additional latency for the getFileSatus() call, but if we do this only for files that have a lease on it (i.e. a writer was writing to this file), then it should be ok.\n\nAdditionally, the current getFileStatus() call does not retrieve block location information from the namenode. This has to be enhanced to return the location of at least the last block of a file.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-02-04T10:19:03.374+0000","updated":"2009-02-04T10:19:03.374+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12413821/comment/14069093","id":"14069093","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"Ping!\n\nI'm tempted to close this as stale, but it would be good for someone more familiar with the issue to do that.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2014-07-21T19:29:13.394+0000","updated":"2014-07-21T19:29:13.394+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12413821/comment/14069108","id":"14069108","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"sync() does not update the length in NN.  So getFileSatus() will return the correct length immediately as Dhruba mentioned.\n\nAnyway, sync() is already removed from trunk (HDFS-3034).  hsync(..) with UPDATE_LENGTH flag could be used instead.  So this becomes not-a-problem anymore.  Resolving ...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-07-21T19:36:10.110+0000","updated":"2014-07-21T19:36:10.110+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12413821/comment/14069112","id":"14069112","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"> ... getFileSatus() will return the correct length ...\n\nOops, it should be \"... getFileSatus() will NOT return the correct length ...\".","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-07-21T19:37:07.265+0000","updated":"2014-07-21T19:37:07.265+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12413821/comment/14579616","id":"14579616","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kbeyer","name":"kbeyer","key":"kbeyer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kevin Beyer","active":true,"timeZone":"America/Los_Angeles"},"body":"HDFS file length reported by ls may be less than the number of bytes found when reading.  I created the mismatched file by kill -9 during a copy so that the client doesn't shutdown its connection to the namenode properly.  This misreported length persisted after restarting hdfs.\n\n{quote}\n$ hdfs dfs -copyFromLocal junk17 /tmp/.\n2015-06-09 13:09:25,742 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n^Z\n[1]+  Stopped                 hdfs dfs -copyFromLocal junk17 /tmp/.\n$ kill -9 %1\n\n[1]+  Stopped                 hdfs dfs -copyFromLocal junk17 /tmp/.\n$ fg\n-bash: fg: job has terminated\n[1]+  Killed: 9               hdfs dfs -copyFromLocal junk17 /tmp/.\n$ hdfs dfs -ls /tmp\n2015-06-09 13:09:45,730 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 3 items\ndrwxrwx---   - jane supergroup          0 2015-05-28 14:26 /tmp/hadoop-yarn\ndrwx-wx-wx   - jane supergroup          0 2015-05-28 14:26 /tmp/hive\n-rw-r--r--   1 jane supergroup 1073741824 2015-06-09 13:09 /tmp/junk17._COPYING_\n$ hdfs dfs -ls /tmp\n2015-06-09 13:09:55,345 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 3 items\ndrwxrwx---   - jane supergroup          0 2015-05-28 14:26 /tmp/hadoop-yarn\ndrwx-wx-wx   - jane supergroup          0 2015-05-28 14:26 /tmp/hive\n-rw-r--r--   1 jane supergroup 1073741824 2015-06-09 13:09 /tmp/junk17._COPYING_\n$ hdfs dfs -cat /tmp/junk17._COPYING_ | wc -c\n 1207959752\n$ hdfs dfs -ls /tmp\n2015-06-09 13:11:21,389 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 3 items\ndrwxrwx---   - jane supergroup          0 2015-05-28 14:26 /tmp/hadoop-yarn\ndrwx-wx-wx   - jane supergroup          0 2015-05-28 14:26 /tmp/hive\n-rw-r--r--   1 jane supergroup 1073741824 2015-06-09 13:09 /tmp/junk17._COPYING_\n$ hdfs dfs -cp /tmp/junk17._COPYING_ /tmp/junk18\n2015-06-09 13:13:38,963 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n$ hdfs dfs -ls /tmp\n2015-06-09 13:13:45,575 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 4 items\ndrwxrwx---   - jane supergroup          0 2015-05-28 14:26 /tmp/hadoop-yarn\ndrwx-wx-wx   - jane supergroup          0 2015-05-28 14:26 /tmp/hive\n-rw-r--r--   1 jane supergroup 1073741824 2015-06-09 13:09 /tmp/junk17._COPYING_\n-rw-r--r--   1 jane supergroup 1207959552 2015-06-09 13:13 /tmp/junk18\n{quote}\n\n{quote}\n$ hdfs version\nHadoop 2.6.0\nSubversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1\nCompiled by jenkins on 2014-11-13T21:10Z\nCompiled with protoc 2.5.0\nFrom source with checksum 18e43357c8f927c0695f1e9522859d6a\n{quote}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kbeyer","name":"kbeyer","key":"kbeyer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kevin Beyer","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-09T21:43:39.442+0000","updated":"2015-06-09T21:43:39.442+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12413821/comment/14579641","id":"14579641","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kbeyer","name":"kbeyer","key":"kbeyer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kevin Beyer","active":true,"timeZone":"America/Los_Angeles"},"body":"Turns out the file is still considered \"open for write\"...  This is an hour or two after the client aborted, and after restarting hdfs.  The standard behavior would be the file would be closed shortly after the client aborting.\n\n{block}\nkevin-beyer-mbp2:t kevin$ hdfs fsck /tmp/junk17._COPYING_ -blocks\n2015-06-09 14:52:02,807 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nConnecting to namenode via http://localhost:50070\nFSCK started by kevin (auth:SIMPLE) from /127.0.0.1 for path /tmp/junk17._COPYING_ at Tue Jun 09 14:52:03 PDT 2015\nStatus: HEALTHY\n Total size:\t0 B (Total open files size: 1073741824 B)\n Total dirs:\t0\n Total files:\t0\n Total symlinks:\t\t0 (Files currently being written: 1)\n Total blocks (validated):\t0 (Total open file blocks (not validated): 8)\n Minimally replicated blocks:\t0\n Over-replicated blocks:\t0\n Under-replicated blocks:\t0\n Mis-replicated blocks:\t\t0\n Default replication factor:\t1\n Average block replication:\t0.0\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0\n Number of data-nodes:\t\t1\n Number of racks:\t\t1\nFSCK ended at Tue Jun 09 14:52:03 PDT 2015 in 2 milliseconds\n\n\nThe filesystem under path '/tmp/junk17._COPYING_' is HEALTHY\nkevin-beyer-mbp2:t kevin$ hdfs fsck -openforwrite\n2015-06-09 14:52:25,236 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nConnecting to namenode via http://localhost:50070\nFSCK started by kevin (auth:SIMPLE) from /127.0.0.1 for path / at Tue Jun 09 14:52:26 PDT 2015\n....................................................................................................\n./tmp/junk17._COPYING_ 1073741824 bytes, 8 block(s), OPENFORWRITE: ..................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n..........................................................................Status: HEALTHY\n Total size:\t3905069992 B\n Total dirs:\t276\n Total files:\t674\n Total symlinks:\t\t0\n Total blocks (validated):\t321 (avg. block size 12165327 B)\n Minimally replicated blocks:\t321 (100.0 %)\n Over-replicated blocks:\t0 (0.0 %)\n Under-replicated blocks:\t0 (0.0 %)\n Mis-replicated blocks:\t\t0 (0.0 %)\n Default replication factor:\t1\n Average block replication:\t1.0\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0 (0.0 %)\n Number of data-nodes:\t\t1\n Number of racks:\t\t1\nFSCK ended at Tue Jun 09 14:52:26 PDT 2015 in 167 milliseconds\n{block}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kbeyer","name":"kbeyer","key":"kbeyer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kevin Beyer","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-09T22:03:32.441+0000","updated":"2015-06-09T22:03:32.441+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12413821/comment/14579709","id":"14579709","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kbeyer","name":"kbeyer","key":"kbeyer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kevin Beyer","active":true,"timeZone":"America/Los_Angeles"},"body":"I am not alone with this problem:\n\nhttp://stackoverflow.com/questions/5347293/hdfs-says-file-is-still-open-but-process-writing-to-it-was-killed\n\nhttp://stackoverflow.com/questions/19565791/hbase-distributed-log-splitting-keeps-failing-because-unable-to-get-a-lease\n\nhttp://stackoverflow.com/questions/23833318/crashed-hdfs-client-how-to-close-remaining-open-files\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kbeyer","name":"kbeyer","key":"kbeyer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kevin Beyer","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-09T22:52:44.804+0000","updated":"2015-06-09T22:52:44.804+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12413821/comment/14580084","id":"14580084","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kbeyer","name":"kbeyer","key":"kbeyer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kevin Beyer","active":true,"timeZone":"America/Los_Angeles"},"body":"I've learned about soft and hard limits on the update lease.  After the hard limit expired, the file length corrected to same number of bytes found by reading.  So this is not a bug. \n\nHowever, I have a few ideas that might help:\n\n1. The file stats could update when the soft limit expires.  This would reduce the window of inconsistency to 1 minute instead of 1 hour.\n\n2. Allow the writing application to control the \"safe\" length and limit readers to the safe length.  Readers could set an option to read the unsafe bytes (or the default could read the full length, but that seems more dangerous although backwards compatible to current behavior).  If the lease is not recovered before the hard limit expires, the unsafe bytes are discarded (a writer option could control this as well).  This would allow applications to avoid partial record reads. \n\n3. A simple way for readers to detect that there is an active soft/hard lease on a file, probably in the FileStatus.\n\n4. The hard limit duration should be an option when opening for write.  The default should be zero.\n\n5. A simple way to terminate a hard lease.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kbeyer","name":"kbeyer","key":"kbeyer","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Kevin Beyer","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-10T06:15:54.536+0000","updated":"2015-06-10T06:15:54.536+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-196/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0iw07:"}}