{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13028415","self":"https://issues.apache.org/jira/rest/api/2/issue/13028415","key":"HDFS-11251","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334218","id":"12334218","description":"2.9.0 release","name":"2.9.0","archived":false,"released":true,"releaseDate":"2017-11-17"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12337976","id":"12337976","name":"3.0.0-alpha2","archived":false,"released":true,"releaseDate":"2017-01-25"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2016-12-20T23:31:13.097+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Dec 29 08:12:24 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_533649667_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_1_*:*_646563162","customfield_12312321":null,"resolutiondate":"2016-12-29T07:37:36.953+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-11251/watchers","watchCount":8,"isWatching":false},"created":"2016-12-15T15:47:24.572+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"4.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12337976","id":"12337976","name":"3.0.0-alpha2","archived":false,"released":true,"releaseDate":"2017-01-25"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-01-03T20:09:54.052+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"The testAddVolumesDuringWrite case failed with a ReconfigurationException which appears to have been caused by a ConcurrentModificationException.  Stacktrace details to follow.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12334218","id":"12334218","description":"2.9.0 release","name":"2.9.0","archived":false,"released":true,"releaseDate":"2017-11-17"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12337976","id":"12337976","name":"3.0.0-alpha2","archived":false,"released":true,"releaseDate":"2017-01-25"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12844289","id":"12844289","filename":"HDFS-11251.01.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-21T20:00:55.209+0000","size":5355,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12844289/HDFS-11251.01.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12844838","id":"12844838","filename":"HDFS-11251.02.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-27T23:58:01.852+0000","size":12702,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12844838/HDFS-11251.02.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12844856","id":"12844856","filename":"HDFS-11251.03.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-28T03:15:47.801+0000","size":13005,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12844856/HDFS-11251.03.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12844966","id":"12844966","filename":"HDFS-11251.04.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-28T18:48:55.647+0000","size":13003,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12844966/HDFS-11251.04.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"ConcurrentModificationException during DataNode#refreshVolumes","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15751703","id":"15751703","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"body":"The test failed with this stacktrace:\n{noformat}\norg.apache.hadoop.conf.ReconfigurationException: Could not change property dfs.datanode.data.dir from '[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data1,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data2,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data4' to '[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data1,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data2,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data3,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data4'\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.refreshVolumes(DataNode.java:777)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.reconfigurePropertyImpl(DataNode.java:532)\n\tat org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.addVolumes(TestDataNodeHotSwapVolumes.java:310)\n\tat org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.testAddVolumesDuringWrite(TestDataNodeHotSwapVolumes.java:404)\n\n{noformat}\n\nIn the test output I found a CME which appears to be the cause.  If so, it'd be nice if ReconfigurationException relayed the exception that caused the failure.\n{noformat}\n2016-12-15 00:33:21,848 [pool-239-thread-2] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(320)) - Added new volume: DS-6c2d1743-ee6f-4011-8042-b47d45d5279b\n2016-12-15 00:33:21,848 [pool-239-thread-2] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(494)) - Added volume - [DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data4, StorageType: DISK\n2016-12-15 00:33:21,851 [Thread-1888] ERROR datanode.DataNode (DataNode.java:refreshVolumes(764)) - Failed to add volume: [DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data3\njava.util.concurrent.ExecutionException: java.util.ConcurrentModificationException\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.refreshVolumes(DataNode.java:750)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.reconfigurePropertyImpl(DataNode.java:532)\n\tat org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.addVolumes(TestDataNodeHotSwapVolumes.java:310)\n\tat org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.testAddVolumesDuringWrite(TestDataNodeHotSwapVolumes.java:404)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\nCaused by: java.util.ConcurrentModificationException\n\tat java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)\n\tat java.util.ArrayList$Itr.next(ArrayList.java:851)\n\tat org.apache.hadoop.hdfs.server.common.Storage.containsStorageDir(Storage.java:999)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadBpStorageDirectories(BlockPoolSliceStorage.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.prepareVolume(DataStorage.java:332)\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addVolume(FsDatasetImpl.java:455)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$2.call(DataNode.java:737)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$2.call(DataNode.java:733)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jlowe","name":"jlowe","key":"jlowe","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jason Lowe","active":true,"timeZone":"America/Chicago"},"created":"2016-12-15T15:50:03.522+0000","updated":"2016-12-15T15:50:03.522+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15765566","id":"15765566","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~jlowe], \n\nAmny chance you have the full log file for this failure case ? Would like to take a look.\n\n\n{{Storage#storageDirs}} is not a concurrent list. So, parallel addition or removal of volumes with list iteration can throw ConcurrentModificationException. Want to look at logs to find the parallel operations on the storageDir. One of the fixes could be building {{Storage#storageDirs}} as a Collections.synchronizedList(..). Other could be locking down the modification operations and iteration operations. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-20T23:31:13.097+0000","updated":"2016-12-20T23:31:13.097+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15765882","id":"15765882","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks [~manojg] for the analysis. I think that's the reason of the failure case. Here the add volume or remove volume is a  asynchronized operation so there is a chance to lead the CME.\n{quote}\nWant to look at logs to find the parallel operations on the storageDir\n{quote}\nHere it's the {{addVolume}} operation caused this as you can see the stack info that [~jlowe] provided above. Hope this can help you.\n{code}\norg.apache.hadoop.conf.ReconfigurationException: Could not change property dfs.datanode.data.dir from '[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data1,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data2,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data4' to '[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data1,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data2,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data3,[DISK]file:/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/4/dfs/data/data4'\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.refreshVolumes(DataNode.java:777)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.reconfigurePropertyImpl(DataNode.java:532)\n\tat org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.addVolumes(TestDataNodeHotSwapVolumes.java:310)\n\tat org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.testAddVolumesDuringWrite(TestDataNodeHotSwapVolumes.java:404)\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-12-21T02:19:46.311+0000","updated":"2016-12-21T02:26:04.755+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15765950","id":"15765950","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks [~linyiqun].\n\nYes, I do see the {{addVolume}} volume operation from the stack trace. But, that volume adding thread shown in the stack trace is just in the {{DataStorage#prepareVolume}} phase and it is only traversing the {{storageDirs}} in {{Storage#containsStorageDir}}. That is, there is yet another thread which is mutating the same ArrayList around the same time when the first Volume Add was happening.\n\nLooked at the test code again and there are 2 Volume Add happening as part of the test. As you said, these Volume add operations are run in Executors via FutureTask and hence these are submitted in quick succession and are running parallely and are mutating the same {{storageDirs}} list. \n\nI am able to recreate the {{ConcurrentModificationException}} by following 2 ways:\n* Running a new Thread which continuously runs a read operation on the StorageDirs (like listStorageDirectories) and then having Volume add in parallel,  OR\n* Adding 10 volumes with a small delay between each other, so that each of volume add's list traversing will trip over previous volume add list modification. \n\nMy fix proposal is to create {{Storage#storageDirs}} as {{new CopyOnWriteArrayList<StorageDirectory>()}}; Will test out and submit a patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-21T03:03:50.896+0000","updated":"2016-12-21T03:03:50.896+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15768026","id":"15768026","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Attaching v01 patch to address the following.\n1. Made {{Storage#storageDirs}} a CopyOnWriteArrayList instead of a normal ArrayList so that concurrent iterators and writers don't end up in {{ConcurrentModificationException}}. In this case, substitution with CopyOnWriteArrayList is ok and should not cause any peformance degradation as Volume add and removal are not frequent operations. \n2. Tests updated to add more volumes during write and to expose the race condition. On a private branched I recreated the original CME problem with this test along with some delays introduced in {{Storage#containsStorageDir}}\n\nThis patch will throw a CheckStyle issue on {{Storage#storageDirs}} not being private. But, this checkstyle issue existed even before the patch and touching the same line again will cause the checkstyle to show up again. Will file a new jira to cleanup the specifier for {{Storage#storageDirs}} and its access patterns in the derived classes.\n\n[~eddyxu], [~xiaochen], [~linyiqun], Can you please review the patch ?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-21T20:00:55.240+0000","updated":"2016-12-21T20:00:55.240+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15768364","id":"15768364","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (/) *{color:green}+1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 47s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 28s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 50s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 43s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 25s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 150 unchanged - 2 fixed = 151 total (was 152) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 46s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 70m 23s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 25s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 96m 51s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:a9ad5d6 |\n| JIRA Issue | HDFS-11251 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12844289/HDFS-11251.01.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 3e521a785221 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 8b042bc |\n| Default Java | 1.8.0_111 |\n| findbugs | v3.0.0 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/17925/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/17925/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17925/console |\n| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-12-21T22:33:34.241+0000","updated":"2016-12-21T22:33:34.241+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15768763","id":"15768763","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks for providing the patch, [~manojg]. The patch almost looks good to me, only some comments for your test. I ran the test in my local, it can run well without using {{CopyOnWriteArrayList}}. Can you add the test that can trigger the CME here as you mentioned in your comments? I think this will be good to test for this. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-12-22T02:08:28.344+0000","updated":"2016-12-22T02:08:28.344+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15768948","id":"15768948","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"I looked into this again, we can create some threads to execute {{addVolumes}} concurrently here. This will be easy to test this case.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-12-22T04:01:20.005+0000","updated":"2016-12-22T04:01:20.005+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15768964","id":"15768964","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Sure, this is doable. I was inducing a delay in {{refreshVolumes}} to simulate race. I can try something with Mockito to simulate my testing. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-22T04:10:02.477+0000","updated":"2016-12-22T04:10:02.477+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15768965","id":"15768965","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"sure, will give a try on this and upload a new patch soon. thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-22T04:10:22.754+0000","updated":"2016-12-22T04:10:22.754+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15781569","id":"15781569","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Attached v02 patch to address the following:\n1. Added a new test {{TestDataNodeHotSwapVolumes#testAddVolumesConcurrently}} to recreate the problem of ConcurrentModificationException during addVolume and fail when run without the proposed fix. \n2. HDFS-11267 has been filed to track the checkstyle issue.\n\nPS: testAddVolumesConcurrently currently has 40 new volumes to be added concurrently. Without fix, this test fails most of the times. You might want to increase the volume count to a higher number to make it fail all the times. \n\n\n[~eddyxu], [~linyiqun], can you please take a look at the patch ?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-27T23:58:01.857+0000","updated":"2016-12-27T23:58:01.857+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15781823","id":"15781823","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"Thanks [~manojg] for updating the patch. The latest patch looks pretty good now. Two minor comments:\n\n* Can we define a var named {{DEFAULT_STORAGES_PER_DATANODE}} to replace {{2}}? That will be easily understood.\n\n{code}\n   private void startDFSCluster(int numNameNodes, int numDataNodes)\n       throws IOException {\n+    startDFSCluster(numNameNodes, numDataNodes, 2);\n+  }\n{code}\n\n* The delay time of {{addVolume}} is a little short. I tested your patch in my local many times, the most of the results were still passed with the {{ArrayList}}. \n\n{code}\nif (r.nextInt(10) > 4) {\n  int s = r.nextInt(10) + 1;\n  Thread.sleep(s);\n }\n{code}\n\nI increased the delay here, change {{Thread.sleep(s)}} to {{Thread.sleep(s * 100)}}, then the tests runs as we expected,\n\n+1 once these are addressed. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-12-28T02:20:01.337+0000","updated":"2016-12-28T02:21:01.220+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15781913","id":"15781913","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the review [~linyiqun]. Updated the patch with above changes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-28T03:19:10.366+0000","updated":"2016-12-28T03:19:10.366+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15782124","id":"15782124","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi, [~manojg]\n\nThe latest patch hangs on my local machine:\n\n{code}\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes\n\n\n\n\n====> TEST TIMED OUT. PRINTING THREAD DUMP. <====\nTimestamp: 2016-12-28 01:41:15,000\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data34)\" daemon prio=5 tid=34502 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"pool-3452-thread-1\"  prio=5 tid=30550 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server Responder\" daemon prio=5 tid=28901 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1328)\n        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1311)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data58)\" daemon prio=5 tid=34527 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 8 on 50718\" daemon prio=5 tid=30559 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data14)\" daemon prio=5 tid=34482 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data86)\" daemon prio=5 tid=34556 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"qtp282733206-32102\" daemon prio=5 tid=32102 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:243)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:191)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:249)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\n        at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:745)\n\"org.eclipse.jetty.server.session.HashSessionManager@3bfa7babTimer\" daemon prio=5 tid=30538 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data20)\" daemon prio=5 tid=34488 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 7 on 50708\" daemon prio=5 tid=28914 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"qtp1387697241-30532-acceptor-0@1d1c322a-ServerConnector@6ca16f0f{HTTP/1.1,[http/1.1]}{localhost:50716}\" daemon prio=3 tid=30532 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\n        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:373)\n        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:593)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data78)\" daemon prio=5 tid=34548 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 7 on 50723\" daemon prio=5 tid=32129 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data52)\" daemon prio=5 tid=34521 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data26)\" daemon prio=5 tid=34494 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data17)\" daemon prio=5 tid=34485 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data83)\" daemon prio=5 tid=34553 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data76)\" daemon prio=5 tid=34546 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 9 on 50718\" daemon prio=5 tid=30560 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"org.eclipse.jetty.server.session.HashSessionManager@4fab38daTimer\" daemon prio=5 tid=30540 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server listener on 50723\" daemon prio=5 tid=32114 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1150)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data23)\" daemon prio=5 tid=34491 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"qtp1387697241-30535\" daemon prio=5 tid=30535 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data70)\" daemon prio=5 tid=34540 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"StorageInfoMonitor\" daemon prio=5 tid=28894 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:4322)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data65)\" daemon prio=5 tid=34534 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data71)\" daemon prio=5 tid=34541 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data40)\" daemon prio=5 tid=34509 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"Socket Reader #1 for port 50723\"  prio=5 tid=32115 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1088)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1067)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data16)\" daemon prio=5 tid=34484 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"qtp490528317-28885\" daemon prio=5 tid=28885 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data85)\" daemon prio=5 tid=34555 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"qtp282733206-32106\" daemon prio=5 tid=32106 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"org.eclipse.jetty.server.session.HashSessionManager@4726bdbeTimer\" daemon prio=5 tid=32110 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data11)\" daemon prio=5 tid=34479 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 1 on 50723\" daemon prio=5 tid=32123 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data73)\" daemon prio=5 tid=34543 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data59)\" daemon prio=5 tid=34528 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data41)\" daemon prio=5 tid=34510 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 0 on 50718\" daemon prio=5 tid=30551 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"Block report processor\" daemon prio=5 tid=28895 in Object.wait()\njava.lang.Thread.State: WAITING (on object monitor)\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\n        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.processQueue(BlockManager.java:4623)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.run(BlockManager.java:4612)\n\"IPC Server handler 2 on 50718\" daemon prio=5 tid=30553 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"qtp282733206-32105\" daemon prio=5 tid=32105 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data30)\" daemon prio=5 tid=34498 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)\" daemon prio=5 tid=34476 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"AsyncAppender-Dispatcher-Thread-1083\" daemon prio=5 tid=1719 in Object.wait()\njava.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Object.wait(Object.java:502)\n        at org.apache.log4j.AsyncAppender$Dispatcher.run(AsyncAppender.java:548)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server handler 0 on 50723\" daemon prio=5 tid=32122 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data75)\" daemon prio=5 tid=34545 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data50)\" daemon prio=5 tid=34519 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"qtp1387697241-30536\" daemon prio=5 tid=30536 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"qtp490528317-28883\" daemon prio=5 tid=28883 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:243)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:191)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:249)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\n        at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:745)\n\"qtp282733206-32108\" daemon prio=5 tid=32108 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data19)\" daemon prio=5 tid=34487 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data57)\" daemon prio=5 tid=34526 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data81)\" daemon prio=5 tid=34551 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data33)\" daemon prio=5 tid=34501 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server idle connection scanner for port 50708\" daemon prio=5 tid=28900 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at java.util.TimerThread.mainLoop(Timer.java:552)\n        at java.util.TimerThread.run(Timer.java:505)\n\"qtp490528317-28882\" daemon prio=5 tid=28882 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:243)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:191)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:249)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\n        at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data21)\" daemon prio=5 tid=34489 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"qtp490528317-28889\" daemon prio=5 tid=28889 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)\" daemon prio=5 tid=34470 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data74)\" daemon prio=5 tid=34544 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data69)\" daemon prio=5 tid=34538 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data68)\" daemon prio=5 tid=34537 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data48)\" daemon prio=5 tid=34517 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@629b7761\" daemon prio=5 tid=30528 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\n        at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:100)\n        at org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:85)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:145)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data84)\" daemon prio=5 tid=34554 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.apache.hadoop.util.JvmPauseMonitor$Monitor@78a9fb45\" daemon prio=5 tid=30542 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server handler 7 on 50718\" daemon prio=5 tid=30558 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"IPC Server Responder\" daemon prio=5 tid=30546 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1328)\n        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1311)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data13)\" daemon prio=5 tid=34481 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor@22760570\" daemon prio=5 tid=28919 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor.run(LeaseManager.java:339)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server handler 5 on 50718\" daemon prio=5 tid=30556 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data77)\" daemon prio=5 tid=34547 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)\" daemon prio=5 tid=34474 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 1 on 50718\" daemon prio=5 tid=30552 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"pool-3451-thread-1\"  prio=5 tid=30529 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"org.apache.hadoop.util.JvmPauseMonitor$Monitor@7ab86d15\" daemon prio=5 tid=32113 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data38)\" daemon prio=5 tid=34507 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data29)\" daemon prio=5 tid=34497 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.eclipse.jetty.server.session.HashSessionManager@604680beTimer\" daemon prio=5 tid=28891 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"qtp282733206-32101\" daemon prio=5 tid=32101 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:243)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:191)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:249)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\n        at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data79)\" daemon prio=5 tid=34549 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)\" daemon prio=5 tid=34475 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"Socket Reader #1 for port 50708\"  prio=5 tid=28899 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1088)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1067)\n\"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@7029c709\" daemon prio=5 tid=28920 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run(FSNamesystem.java:3752)\n        at java.lang.Thread.run(Thread.java:745)\n\"RedundancyMonitor\" daemon prio=5 tid=28893 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at java.lang.Thread.sleep(Thread.java:340)\n        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor.run(BlockManager.java:4287)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data72)\" daemon prio=5 tid=34542 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"main\"  prio=5 tid=1 runnable\njava.lang.Thread.State: RUNNABLE\n        at java.lang.Thread.dumpThreads(Native Method)\n        at java.lang.Thread.getAllStackTraces(Thread.java:1607)\n        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:87)\n        at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:73)\n        at org.apache.hadoop.test.TimedOutTestsListener.testFailure(TimedOutTestsListener.java:62)\n        at org.junit.runner.notification.RunNotifier$4.notifyListener(RunNotifier.java:139)\n        at org.junit.runner.notification.RunNotifier$SafeNotifier.run(RunNotifier.java:61)\n        at org.junit.runner.notification.RunNotifier.fireTestFailures(RunNotifier.java:134)\n        at org.junit.runner.notification.RunNotifier.fireTestFailure(RunNotifier.java:128)\n        at org.junit.internal.runners.model.EachTestNotifier.addFailure(EachTestNotifier.java:23)\n        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:275)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\n\"Reference Handler\" daemon prio=10 tid=2 in Object.wait()\njava.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Object.wait(Object.java:502)\n        at java.lang.ref.Reference.tryHandlePending(Reference.java:191)\n        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)\n\"qtp1387697241-30531\" daemon prio=5 tid=30531 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:243)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:191)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:249)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\n        at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server handler 8 on 50708\" daemon prio=5 tid=28915 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"IPC Server idle connection scanner for port 50723\" daemon prio=5 tid=32116 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at java.util.TimerThread.mainLoop(Timer.java:552)\n        at java.util.TimerThread.run(Timer.java:505)\n\"pool-3448-thread-1\"  prio=5 tid=28917 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data37)\" daemon prio=5 tid=34506 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data43)\" daemon prio=5 tid=34512 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 2 on 50708\" daemon prio=5 tid=28909 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"Timer for 'NameNode' metrics system\" daemon prio=5 tid=28879 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at java.util.TimerThread.mainLoop(Timer.java:552)\n        at java.util.TimerThread.run(Timer.java:505)\n\"IPC Server Responder\" daemon prio=5 tid=32117 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1328)\n        at org.apache.hadoop.ipc.Server$Responder.run(Server.java:1311)\n\"qtp282733206-32107\" daemon prio=5 tid=32107 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"qtp1387697241-30537\" daemon prio=5 tid=30537 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"pool-3446-thread-1\"  prio=5 tid=28881 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server handler 5 on 50708\" daemon prio=5 tid=28912 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data28)\" daemon prio=5 tid=34496 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@6ffc0448\" daemon prio=5 tid=28896 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run(HeartbeatManager.java:451)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data45)\" daemon prio=5 tid=34514 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)\" daemon prio=5 tid=34468 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data55)\" daemon prio=5 tid=34524 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data62)\" daemon prio=5 tid=34531 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 1 on 50708\" daemon prio=5 tid=28908 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data60)\" daemon prio=5 tid=34529 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data42)\" daemon prio=5 tid=34511 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"Attach Listener\" daemon prio=9 tid=27524 runnable\njava.lang.Thread.State: RUNNABLE\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data87)\" daemon prio=5 tid=34557 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data10)\" daemon prio=5 tid=34478 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data67)\" daemon prio=5 tid=34536 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data61)\" daemon prio=5 tid=34530 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 6 on 50708\" daemon prio=5 tid=28913 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data63)\" daemon prio=5 tid=34532 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@7ef2b559\" daemon prio=5 tid=28922 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber.run(FSNamesystem.java:3881)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data44)\" daemon prio=5 tid=34513 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"pool-3457-thread-1\"  prio=5 tid=32100 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"org.eclipse.jetty.server.session.HashSessionManager@6a64387cTimer\" daemon prio=5 tid=32109 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"qtp490528317-28888\" daemon prio=5 tid=28888 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"qtp490528317-28887\" daemon prio=5 tid=28887 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server handler 3 on 50723\" daemon prio=5 tid=32125 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"CacheReplicationMonitor(1664877657)\"  prio=5 tid=28923 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2163)\n        at org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:181)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)\" daemon prio=5 tid=34472 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.eclipse.jetty.server.session.HashSessionManager@5bb89342Timer\" daemon prio=5 tid=28892 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data36)\" daemon prio=5 tid=34505 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 0 on 50708\" daemon prio=5 tid=28907 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data12)\" daemon prio=5 tid=34480 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data15)\" daemon prio=5 tid=34483 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)\" daemon prio=5 tid=34471 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data64)\" daemon prio=5 tid=34533 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data18)\" daemon prio=5 tid=34486 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"nioEventLoopGroup-22-1\"  prio=10 tid=30541 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:621)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:309)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:703)\n        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)\n        at java.lang.Thread.run(Thread.java:745)\n\"Socket Reader #1 for port 50718\"  prio=5 tid=30544 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1088)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1067)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data35)\" daemon prio=5 tid=34504 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner\" daemon prio=5 tid=64 in Object.wait()\njava.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)\n        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)\n        at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:3698)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data47)\" daemon prio=5 tid=34516 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.eclipse.jetty.server.session.HashSessionManager@1c38d141Timer\" daemon prio=5 tid=32111 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"DecommissionMonitor-0\" daemon prio=5 tid=28905 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data9)\" daemon prio=5 tid=34477 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 9 on 50723\" daemon prio=5 tid=32131 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"org.eclipse.jetty.server.session.HashSessionManager@2eaae4c6Timer\" daemon prio=5 tid=28890 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data49)\" daemon prio=5 tid=34518 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 2 on 50723\" daemon prio=5 tid=32124 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"IPC Server handler 6 on 50723\" daemon prio=5 tid=32128 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"IPC Server listener on 50708\" daemon prio=5 tid=28898 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.apache.hadoop.ipc.Server$Listener.run(Server.java:1150)\n\"Thread-26471\"  prio=5 tid=34580 runnable\njava.lang.Thread.State: RUNNABLE\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.doStop(QueuedThreadPool.java:181)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89)\n        at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143)\n        at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:161)\n        at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73)\n        at org.eclipse.jetty.server.Server.doStop(Server.java:482)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89)\n        at org.apache.hadoop.http.HttpServer2.stop(HttpServer2.java:1071)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.stop(NameNodeHttpServer.java:243)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.stopHttpServer(NameNode.java:884)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.stopCommonServices(NameNode.java:841)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1009)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.stopAtException(NameNode.java:960)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:949)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:919)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1636)\n        at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1263)\n        at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1032)\n        at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:907)\n        at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:839)\n        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:491)\n        at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:450)\n        at org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.startDFSCluster(TestDataNodeHotSwapVolumes.java:137)\n        at org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.startDFSCluster(TestDataNodeHotSwapVolumes.java:108)\n        at org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.testAddVolumesDuringWrite(TestDataNodeHotSwapVolumes.java:421)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data39)\" daemon prio=5 tid=34508 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@6b2cbc8f\" daemon prio=5 tid=28921 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller.run(FSNamesystem.java:3794)\n        at java.lang.Thread.run(Thread.java:745)\n\"qtp1387697241-30530\" daemon prio=5 tid=30530 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)\n        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198)\n        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:117)\n        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:243)\n        at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:191)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:249)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\n        at org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:745)\n\"IPC Server handler 4 on 50723\" daemon prio=5 tid=32126 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data54)\" daemon prio=5 tid=34523 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data80)\" daemon prio=5 tid=34550 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 3 on 50708\" daemon prio=5 tid=28910 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"org.apache.hadoop.util.JvmPauseMonitor$Monitor@7e0a2aa\" daemon prio=5 tid=28880 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192)\n        at java.lang.Thread.run(Thread.java:745)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data56)\" daemon prio=5 tid=34525 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"IPC Server handler 5 on 50723\" daemon prio=5 tid=32127 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at sun.misc.Unsafe.park(Native Method)\n        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n        at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)\n        at org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:218)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2639)\n\"VolumeScannerThread(/Users/lei/work/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data82)\" daemon prio=5 tid=34552 timed_waiting\njava.lang.Thread.State: TIMED_WAITING\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:620)\n\"qtp282733206-32103-acceptor-0@59ab2598-ServerConnector@2635a54a{HTTP/1.1,[http/1.1]}{localhost:50721}\" daemon prio=3 tid=32103 runnable\njava.lang.Thread.State: RUNNABLE\n        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\n        at org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:373)\n        at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:593)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:745)\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-28T05:45:17.005+0000","updated":"2016-12-28T05:45:17.005+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15782158","id":"15782158","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"body":"It seems this change lead this\n{code}\n+  private static final int DEFAULT_STORAGES_PER_DATANODE = 512;\n   private MiniDFSCluster cluster;\n{code}\nIs that your intended change here, [~manojg]? Actually the default value {{2}} is enough here since {{startDFSCluster(int numNameNodes, int numDataNodes)}} is used by other tests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=linyiqun","name":"linyiqun","key":"linyiqun","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=linyiqun&avatarId=25258","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=linyiqun&avatarId=25258","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=linyiqun&avatarId=25258","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=linyiqun&avatarId=25258"},"displayName":"Yiqun Lin","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-12-28T06:00:56.507+0000","updated":"2016-12-28T06:00:56.507+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15782204","id":"15782204","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 10s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 12m 34s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 45s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 42s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |\n| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 24s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 1 new + 150 unchanged - 2 fixed = 151 total (was 152) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  1s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 45s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red}163m  8s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black}187m 17s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| Failed junit tests | hadoop.hdfs.TestDFSClientRetries |\n|   | hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:a9ad5d6 |\n| JIRA Issue | HDFS-11251 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12844856/HDFS-11251.03.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux ea28270ca463 3.13.0-93-generic #140-Ubuntu SMP Mon Jul 18 21:21:05 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / 9262797 |\n| Default Java | 1.8.0_111 |\n| findbugs | v3.0.0 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/17965/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/17965/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/17965/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17965/console |\n| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-12-28T06:26:05.732+0000","updated":"2016-12-28T06:26:05.732+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15783440","id":"15783440","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Oops, not intentional. Copy paste from the previous line mistake. 2 is the desired number here. Thanks for catching this. Will upload the new patch soon.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-28T18:30:08.554+0000","updated":"2016-12-28T18:30:08.554+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15783443","id":"15783443","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for verifying the fix. The latest patch has one unintentional change on the constant. Fixing it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-28T18:31:06.932+0000","updated":"2016-12-28T18:31:06.932+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15783479","id":"15783479","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Fixed the DEFAULT_STORAGES_PER_DATANODE to 2 in TestDataNodeHotSwapVolumes. Uploaded the patch v04. \n[~eddyxu], can you please take a look ?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-28T18:48:55.651+0000","updated":"2016-12-28T18:48:55.651+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15784764","id":"15784764","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"body":"+1. Thanks [~manojg]\n\nThe last patch passed test here and the jenkins failure is not related. \n\nCommitted to {{trunk}} and {{branch-2}}.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eddyxu","name":"eddyxu","key":"eddyxu","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Lei (Eddy) Xu","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-12-29T07:37:37.172+0000","updated":"2016-12-29T07:37:37.172+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13028415/comment/15784823","id":"15784823","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11054 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11054/])\nHDFS-11251. ConcurrentModificationException during (lei: rev e9f1396834174646a8d7aa8fc6c4a4f724ca5b28)\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeHotSwapVolumes.java\n* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2016-12-29T08:12:24.462+0000","updated":"2016-12-29T08:12:24.462+0000"}],"maxResults":21,"total":21,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-11251/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i37nan:"}}