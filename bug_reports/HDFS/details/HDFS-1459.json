{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12477575","self":"https://issues.apache.org/jira/rest/api/2/issue/12477575","key":"HDFS-1459","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2010-10-20T18:21:05.115+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sun Oct 24 10:54:15 UTC 2010","customfield_12310420":"15433","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-1459/watchers","watchCount":1,"isWatching":false},"created":"2010-10-17T18:07:50.922+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":"NullPointerException, NPE, HDFS, readInt, read, blockSeekTo","customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12314048","id":"12314048","description":"","name":"0.20.1","archived":false,"released":true,"releaseDate":"2009-09-01"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2010-10-24T10:54:15.540+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[],"timeoriginalestimate":null,"description":"First, here's my source code accessing the HDFS:\n\n\nfinal FSDataInputStream indexFile = getFile(bucketPathStr, Integer.toString(hashTableId) + \".index\");\nindexFile.seek(bucketId * 4);\nint bucketStart = ByteSwapper.swap(indexFile.readInt());\nint bucketEnd = ByteSwapper.swap(indexFile.readInt());\n\nfinal FSDataInputStream dataFile = getFile(bucketPathStr, Integer.toString(hashTableId) + \".data\");\ndataFile.seek(bucketStart * (2 + Hasher.getConfigHashLength()) * 4);\n\nfor (int hash = bucketStart; hash < bucketEnd; hash++) {\n\tint RimageIdA = ByteSwapper.swap(dataFile.readInt());\n\tint RimageIdB = ByteSwapper.swap(dataFile.readInt());\n\t....... read hash of length Hasher.getConfigHashLength() and work with it ....\n}\n\n\nAs you can see, i am reading the range to be read from an X.index file and then read these rows from X.data. The index file is always exactly 6.710.888 bytes in length.\n\nAs for the data file, everything works fine with 50 different 1.35 GB (22 blocks) data files and it fails every time i tried with 50 different 2.42 GB (39 blocks) data files. So the cause of the bug is clearly dependent on the file size.\n\nI checked for ulimit and for the number of network connections and they are both not maxed out when the error occurs. The stack trace i get is:\n\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1703)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1755)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1680)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:370)\n...\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\n\nwhich leads me to believe that DFSClient.blockSeekTo returns with a non-null chosenNode but with blockReader = null.\n\nSince the exact same jar works flawlessly with small data files and fails reliably with big data files, i'm wondering how this could possibly dependent on the file's size or block count (DFSClient.java line 1628+):\n\ns = socketFactory.createSocket();\nNetUtils.connect(s, targetAddr, socketTimeout);\ns.setSoTimeout(socketTimeout);\nBlock blk = targetBlock.getBlock();\n\nblockReader = BlockReader.newBlockReader(s, src, blk.getBlockId(), \n    blk.getGenerationStamp(),\n    offsetIntoBlock, blk.getNumBytes() - offsetIntoBlock,\n    buffersize, verifyChecksum, clientName);\nreturn chosenNode;\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"113558","customfield_12312823":null,"summary":"NullPointerException in DataInputStream.readInt caused by reaching xceiverCount","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fxtentacle","name":"fxtentacle","key":"fxtentacle","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hajo Nils Krabbenhöft","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fxtentacle","name":"fxtentacle","key":"fxtentacle","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hajo Nils Krabbenhöft","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Debian 64 bit\nCloudera Hadoop","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12477575/comment/12923075","id":"12923075","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for opening new JIRA, Hajo. One thing: in the future, please try to limit 'Description' field to a self-explanatory short diagnosis of a problem and post any error messages, code snippets, etc. as 'Comment' messages.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-10-20T18:21:05.115+0000","updated":"2010-10-20T18:21:05.115+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12477575/comment/12924306","id":"12924306","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fxtentacle","name":"fxtentacle","key":"fxtentacle","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hajo Nils Krabbenhöft","active":true,"timeZone":"Etc/UTC"},"body":"I found this in my datanode logs:\n\n2010-10-20 15:31:17,154 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.17.5.3:50010, storageID=DS-266784496-78.46.65.54-50010-1287004808819, infoPort=50075, ipcPort=50020):DataXceiver\njava.io.IOException: xceiverCount 257 exceeds the limit of concurrent xcievers 256\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:88)\n\tat java.lang.Thread.run(Thread.java:619)\n\n2010-10-20 15:31:19,115 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.17.5.3:50010, storageID=DS-266784496-78.46.65.54-50010-1287004808819, infoPort=50075, ipcPort=50020):Got exception while serving blk_-8099607957427967059_1974 to /10.17.5.4:\njava.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.17.5.3:50010 remote=/10.17.5.4:51336]\n\tat org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)\n\tat org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)\n\tat org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:313)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:401)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:180)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:95)\n\tat java.lang.Thread.run(Thread.java:619)\n\nand so far using this configuration snippet seems to fix the problem:\n\n<property>\n  <name>dfs.datanode.handler.count</name>\n  <value>40</value>\n  <description>The number of server threads for the datanode.</description>\n</property>\n\n<property>\n  <name>dfs.namenode.handler.count</name>\n  <value>40</value>\n  <description>The number of server threads for the namenode.</description>\n</property>\n\n<property>      \n  <name>dfs.datanode.max.xcievers</name>        \n  <value>2048</value>   \n  <description>The maximum # of threads that can be connected to a data\nndoe simultaneously. Default value is 256.      \n  </description>        \n</property>\n\n\nSo the underlying problem seems to be that when max xcievers is reached that the client does not get notified and thus reports unusable error messages.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fxtentacle","name":"fxtentacle","key":"fxtentacle","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hajo Nils Krabbenhöft","active":true,"timeZone":"Etc/UTC"},"created":"2010-10-24T10:53:07.874+0000","updated":"2010-10-24T10:53:07.874+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12477575/comment/12924307","id":"12924307","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fxtentacle","name":"fxtentacle","key":"fxtentacle","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hajo Nils Krabbenhöft","active":true,"timeZone":"Etc/UTC"},"body":"The NPE on client side is caused by this on datanode side:\n\n\n2010-10-20 15:31:17,177 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.17.5.3:50010, storageID=DS-266784496-78.46.65.54-50010-1287004808819, infoPort=50075, ipcPort=50020):DataXceiver\njava.io.IOException: xceiverCount 257 exceeds the limit of concurrent xcievers 256\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:88)\n\tat java.lang.Thread.run(Thread.java:619)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=fxtentacle","name":"fxtentacle","key":"fxtentacle","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hajo Nils Krabbenhöft","active":true,"timeZone":"Etc/UTC"},"created":"2010-10-24T10:54:15.502+0000","updated":"2010-10-24T10:54:15.502+0000"}],"maxResults":3,"total":3,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-1459/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0jskf:"}}