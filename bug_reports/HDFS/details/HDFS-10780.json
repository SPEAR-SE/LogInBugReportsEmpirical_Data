{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12998565","self":"https://issues.apache.org/jira/rest/api/2/issue/12998565","key":"HDFS-10780","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2016-08-22T15:28:22.457+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Aug 31 02:50:34 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-10780/watchers","watchCount":8,"isWatching":false},"created":"2016-08-19T18:32:18.619+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12335732","id":"12335732","description":"3.0.0-alpha1 release","name":"3.0.0-alpha1","archived":false,"released":true,"releaseDate":"2016-09-03"}],"issuelinks":[{"id":"12479112","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12479112","type":{"id":"10020","name":"Cloners","inward":"is cloned by","outward":"is a clone of","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10020"},"inwardIssue":{"id":"13001537","key":"HDFS-10819","self":"https://issues.apache.org/jira/rest/api/2/issue/13001537","fields":{"summary":"BlockManager fails to store a good block for a datanode storage after it reported a corrupt block — block replication stuck","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-08-31T02:50:34.139+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12329603","id":"12329603","name":"hdfs"}],"timeoriginalestimate":null,"description":"TestDataNodeHotSwapVolumes occasionally fails in the unit test testRemoveVolumeBeingWrittenForDatanode.  Data write pipeline can have issues as there could be timeouts, data node not reachable etc, and in this test case it was more of induced one as one of the volumes in a datanode is removed while block write is in progress. Digging further in the logs, when the problem happens in the write pipeline, the error recovery is not happening as expected leading to block replication never catching up.\n\nRunning org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 44.495 sec <<< FAILURE! - in org.apache.hadoop.hdfs.serv\ntestRemoveVolumeBeingWritten(org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes)  Time elapsed: 44.354 se\njava.util.concurrent.TimeoutException: Timed out waiting for /test to reach 3 replicas\nResults :\nTests in error: \n  TestDataNodeHotSwapVolumes.testRemoveVolumeBeingWritten:637->testRemoveVolumeBeingWrittenForDatanode:714 » Timeout\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0\n\nFollowing exceptions are not expected in this test run\n{noformat}\n 614 2016-08-10 12:30:11,269 [DataXceiver for client DFSClient_NONMAPREDUCE_-640082112_10 at /127.0.0.1:58805 [Receiving block BP-1852988604-172.16.3.66-1470857409044:blk_1073741825_1001]] DEBUG datanode.Da     taNode (DataXceiver.java:run(320)) - 127.0.0.1:58789:Number of active connections is: 2\n 615 java.lang.IllegalMonitorStateException\n 616         at java.lang.Object.wait(Native Method)\n 617         at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.waitVolumeRemoved(FsVolumeList.java:280)\n 618         at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.removeVolumes(FsDatasetImpl.java:517)\n 619         at org.apache.hadoop.hdfs.server.datanode.DataNode.removeVolumes(DataNode.java:832)\n 620         at org.apache.hadoop.hdfs.server.datanode.DataNode.removeVolumes(DataNode.java:798)\n{noformat}\n\n{noformat}\n 720 2016-08-10 12:30:11,287 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-projec     t/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:58788] ERROR datanode.DataNode (BPServiceActor.java:run(768)) - Exception in BPOfferService for Block pool BP-18529     88604-172.16.3.66-1470857409044 (Datanode Uuid 711d58ad-919d-4350-af1e-99fa0b061244) service to localhost/127.0.0.1:58788\n 721 java.lang.NullPointerException\n 722         at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockReports(FsDatasetImpl.java:1841)\n 723         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:336)\n 724         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:624)\n 725         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:766)\n 726         at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12826293","id":"12826293","filename":"HDFS-10780.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-31T01:21:43.326+0000","size":8519,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12826293/HDFS-10780.001.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Block replication not proceeding after pipeline recovery -- TestDataNodeHotSwapVolumes fails","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12998565/comment/15430997","id":"15430997","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shahrs87","name":"shahrs87","key":"shahrs87","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rushabh S Shah","active":true,"timeZone":"Etc/UTC"},"body":"Is this somehow related to HDFS-9781 ?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shahrs87","name":"shahrs87","key":"shahrs87","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rushabh S Shah","active":true,"timeZone":"Etc/UTC"},"created":"2016-08-22T15:28:22.457+0000","updated":"2016-08-22T15:28:22.457+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12998565/comment/15432306","id":"15432306","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"[~shahrs87], I do see HDFS-9781 (NPE during Full Block Report and especially after a volume removal) quite frequently in my test (TestDataNodeHotSwapVolumes) runs. But for these tests Incremental Block Reports from DataNodes are sufficient and they do work as expected. Block Report generations are happening within a try catch block and they are ignoring any encountered exceptions. Thanks for pointing me to the other jira, will follow up on that as well.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-23T07:36:28.695+0000","updated":"2016-08-23T07:36:28.695+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12998565/comment/15433995","id":"15433995","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"*Summary:* \n-- I can see an issue in the current code where by the NameNode can miss out permantnetly to replicate a block to a DataNode. \n-- If the write pipeline doesn’t have enough DataNodes to match the expected replication factor, then NameNode schedules replication of the block moments after the block has been COMMITTED or COMPLETED.  But, if the block COMMIT at NameNode arrives/happens after all BlockManager::addStoredBlock() (happens when processing block reports from DataNodes in the current write pipeline) then NameNode totally misses out to replicate the block because of the way it manages the {{LowRedundancyBlocks}} and {{PendingReconstructionBlocks}}.\n\n*Details:*\nHere are the events and their rough timeline order I noticed in one of the failures.\n1. Say the setup has 3 DataNodes ( DN(1..3) ) and the replication factor configured to be 3\n2. Client is writing a block (BLK_xyz_GS1) with the constructed pipeline DN1 —> DN2 —> DN3\n3. Say DN1 encounters an issue (like downstream communication, or interrupts or storage volume issue etc.,) before the block is FINALIZED\n4. Client detects the pipeline issue and gets a new write pipeline DN2 —> DN3 \n5. Generation stamp for the current block is bumped up (BLK_xyz_GS2)\n6. Client write (BLK_xyz_GS2) to DN2 and DN3\n7. NameNode (NN) is getting Incremental Block Reports (IBR) from DN1, DN2 and  DN3\n8. NN sees a mix of (BLK_xyz_GS1) and (BLK_xyz_GS2) in IBRs from DN1, DN2 and DN3\n9. NN marks (BLK_xyz_GS1) as corrupted and puts these DNs in invalidation list to remove that particular block\n\n10. After seeing the first BLK_xyz_GS2 from one the DNs (say DN2)\n— — NN adds  BLK_xyz_GS2 to the respective {{DatanodeStorageInfo}} (Refer: {{BlockManager::addStoredBlock}} )\n— — Since  BLK_xyz_GS2 is not COMMITTED from the Client yet, NN cannot move BLK_xyz_GS2 to COMPLETE state\n— — NN does not update the {{LowRedundancyBlocks}} neededReconstruction as it is done only after BLK_xyz_GS2 is COMMITTED by the Client \n11. NN sees BLK_xyz_GS2 from the other DN (DN3)\n— — NN adds BLK_xyz_GS2\n— — At NN, BLK_xyz_GS2 is still in UNDER_CONSTRUCTION state as the Client has not COMMITTED yet. So, neededReconstruction is still not updated.\n\n\n12. At this point, for BLK_xyz_GS2 the live replica count is 2 and the expected replica count is 3.\n— — For the {{ReplicationMonitor}} to pick up a replica work, the intention must be available in {{LowRedundancyBlocks}} neededReconstruction\n— — Since no event triggered the addition of intention yet, no block replication work scheduled yet to the missing DN1\n\n13. Client closes the file\n— — NN moves the block to COMMITTED state\n— — Since there are already 2 love copies of the block, NN moves the block to COMPLETED state\n— — But, before moving the block to COMPLETED state, NN does something like below \n\n{{BlockManager.java}}\n{noformat}\npublic boolean commitOrCompleteLastBlock(BlockCollection bc,     Block commitBlock) throws IOException { \n  ...\n  final boolean committed = commitBlock(lastBlock, commitBlock);   \n  if (committed && lastBlock.isStriped()) {   \n  ...\n  if (hasMinStorage(lastBlock)) {     \n        if (committed) {       \n          //XXX: Is adding to {{PendingReconstructionBlocks}} list without adding to\n          // \t {{LowRedundancyBlocks}} list right ?\n          //     This fakes block replication task in progress without even\n          //     any {{BlockReconstructionWork}} scheduled. \n\n          addExpectedReplicasToPending(lastBlock);     \n         }\n         completeBlock(lastBlock, false); \n         }   \n    ... \n }\n{noformat}\n\n— — Since the block is added the {{pendingReconstruction}} list without any actual reconstruction work, the follow on {{checkRedundancy}} mistakenly believes that there is sufficient redundancy for the block and does not trigger any further block reconstruction works.\n\n{noformat}\n839 2016-08-23 10:57:21,450 [IPC Server handler 4 on 57980] INFO  blockmanagement.BlockManager (BlockManager.java:checkRedundancy(4046)) - CheckRedun Block: blk_1073741825_1002, exp: 3, replica: *LIVE=2*, READONLY=0, DECOMMISSIONING=0, DECOMMISSIONED=0, CORRUPT=0, EXCESS=0, STALESTORAGE=     0, REDUNDANT=0, *pending: 1*\t\n840 2016-08-23 10:57:21,451 [IPC Server handler 4 on 57980] INFO  blockmanagement.BlockManager (BlockManager.java:hasEnoughEffectiveReplicas(1685)) - Blk: blk_1073741825_1002, numEffectiveReplicas: 3, required: 3, pendingReplicaNum: 1, placementPolicy: true\n{noformat}\n\n— — {{ReplicationMonitor}} which runs once in every 3 seconds, is also unable to help here as {{LowRedundancyBlocks}} list {{neededReplication}} is empty. \n\n— — So the test fails with following signature \n\n{noformat}\nRunning org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 44.057 sec <<< FAILURE! - in org.apache.hadoop.hdfs.serv\ntestRemoveVolumeBeingWritten(org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes)  Time elapsed: 43.927 se\njava.util.concurrent.TimeoutException: Timed out waiting for /test to reach 3 replicas\nResults :\nTests in error: \n  TestDataNodeHotSwapVolumes.testRemoveVolumeBeingWritten:637->testRemoveVolumeBeingWrittenForDatanode:714 » Timeout\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0\n[INFO] Apache Hadoop HDFS Client .......................... SUCCESS [  2.345 s]\n[INFO] Apache Hadoop HDFS ................................. FAILURE [ 45.878 s]\n[INFO] BUILD FAILURE\n[INFO] Total time: 48.995 s\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.17:test (default-test) on project hadoop\n{noformat}\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-24T01:09:43.807+0000","updated":"2016-08-24T01:14:03.460+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12998565/comment/15440406","id":"15440406","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"The core issue here is in the handling *write pipeline errors* and the follow on *race condition* between the following events\n-- Client sending Block COMMIT to the NN\n-- DN sending IBR with stale Block (the one with old generation stamp) info and \n-- DN sending IBR with right Block (the one with expected generation stamp) info\n\nI have been seeing TestDataNodeHotSwapVolumes#testRemoveVolumeBeingWrittenForDatanode failing very frequently on the latest trunk. Though they all fail with same signature \"Timed out waiting for /test to reach 3 replicas\", there could be more than one issue here as I can see different code paths being taken in the failed logs. One common thing among the failures is they are all happening under pipeline error recovery case.\n\n*Problem 1:*\n-- BlockManager failing to trigger block replication in time to the missed out DN (the DN which is not in the write pipeline after pipeline error recovery)\n-- BlockManager mistakenly believes that there is already a block reconstruction to the last DN in progress and starts monitoring it using pendingReconstruction list\n-- Previous comment explains why BlockManager trapped into such belief. \n\n*Problem 2:*\n-- This is totally different from the previous case\n-- A DN reported (in the IBR) the receipt of Block with the right generation stamp. But, BlockManager failed to add this StoredBlock for the reporting DN\n-- Later BlockManager detects (erroneously) the replication factor not caught up and tries to replicate the block the missing DN. Except, the {{BlockPlacementPolicy}} engine fails to find a target node as it sees all the given nodes already have the given Block. This detection and failed replication continues on and on. \n\nI have theories for both of the above problems. Will try to elaborate more on further comments and will love to have your feedback on my understandings.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-27T01:27:47.941+0000","updated":"2016-08-27T01:28:16.237+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12998565/comment/15450693","id":"15450693","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"More details on the Issue 1:\n\n*Problem:*\n— After pipeline recovery (from data streaming failures), block replication to stale replicas are not happening\n— TestDataNodeHotSwapVolumes fails with “TimeoutException: Timed out waiting for /test to reach 3 replicas” signature\n\n*Analysis:*\n— Assume write pipeline DN1 —> DN2 —> DN3\n— For the {{UNDER_CONSTRUCTION}} Block, NameNode sets the *expected replicas* as DN1, DN2, DN3\n— DN1 encounters a write issue (here the volume is removed while write is in-progress)\n— Client detects pipeline issue, triggers pipeline recovery and gets the new write pipeline as DN2 —> DN3\n\n— On a successful {{FSNameSystem::updatePipeline}} request from Client, NameNode bumps up the Generation Stamp (from 001 to 002) of the UnderConstruction (that is, the last) block of the file.\n— All the current *expected replicas* are stale as they have lesser Generation Stamp compared to the new one after the pipeline update.\n— NameNode resets *expected replicas* with the new set of storage ids from the update pipeline, which is {DN2, DN3}\n\n— DNs send their Incremental Block Reports to NameNode. IBRs can have Blocks with old or new Generation Stamp. And these replica blocks can be in any states — FINALIZED, RBW, RBR, etc.,\n— Assume, the stale replica DN1 sending IBR with the following\n— — Replica Block State: RBW\n— — Replica Block GS: 001 (stale)\n— Assume, the good replica DN2, DN3 sending IBR with the following\n— — Replica Block State: FINALIZED\n— — Replica Block GS: 002 (good)\n\n\n— {{BlockManager::processAndHandleReportedBlock}} when processing Incremental Block Reports, for Replica blocks in RBW/RBR states, NameNode does not check block Generation Stamps until the stored block is COMPLETE. Since the Block state at NN is still in UNDER_CONSTRUCTION, the *Stale RBW block from DN1 gets accepted*\n\n— {{BlockManager::addStoredBlockUnderConstruction}} assumes the replica block from corrupt DN1 to be a good one and adds DN1’s StorageInfo to the expected replica locations. Refer: {{BlockUnderConstructionFeature::addReplicaIfNotPresent}}. Thus *expected replicas* again become (DN1, DN2, DN3).\n\n— Later when the Client closes the file, {{FSNameSystem}} moves all the *expected replicas* to pendingReconstrcution. Refer: {{FSNameSystem::addComittedBlocksToPending}}\n\n— {{BlockManager::checkRedundancy}} mistakenly believes pendingReconstruction count 1 (for DN1) is currently in-porgress and adding this to live replicas count 2 (for DN2, DN3), it decides no more reconstruction needed as it matches the configured replication factor of 3.\n\n— Since there wasn’t any block reconstruction triggered for DN1, test times out waiting for the replication factor of 3. \n\n\n*Fix:*\n\n— I believe the core issue here is in the processing of IBRs from stale replicas. Either \n— — (A) {{BlockManager::checkReplicaCorrupt}} has to tag the block as corrupt, when the replica state is RBW and when the block is not complete  OR\n— — (B) {{BlockManager::addStoredBlockUnderConstruction}} should not ADD the corrupt replica in the *expected replicas* for the under construction block\n\nAttached patch has the fix (B). Also, wrote a unit test to explicitly check for expected replica count under above line of events. \n\n[~eddyxu], [~andrew.wang], [~yzhangal] can you please take a look at the patch ?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-31T01:21:43.330+0000","updated":"2016-08-31T01:21:43.330+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12998565/comment/15450779","id":"15450779","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"body":"Filed HDFS-10819 to track the Problem 2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=manojg","name":"manojg","key":"manojg","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Manoj Govindassamy","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-08-31T02:02:36.455+0000","updated":"2016-08-31T02:02:36.455+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12998565/comment/15450870","id":"15450870","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (/) *{color:green}+1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 17s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 47s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 28s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 45s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 48s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} unit {color} | {color:green} 62m  1s{color} | {color:green} hadoop-hdfs in the patch passed. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 21s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 81m 45s{color} | {color:black} {color} |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:9560f25 |\n| JIRA Issue | HDFS-10780 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12826293/HDFS-10780.001.patch |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 6c1deef2b562 3.13.0-92-generic #139-Ubuntu SMP Tue Jun 28 20:42:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / d6d9cff |\n| Default Java | 1.8.0_101 |\n| findbugs | v3.0.0 |\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/16584/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/16584/console |\n| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-08-31T02:50:34.139+0000","updated":"2016-08-31T02:50:34.139+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-10780/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i32jcf:"}}