{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12631148","self":"https://issues.apache.org/jira/rest/api/2/issue/12631148","key":"HDFS-4475","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2013-02-06T22:18:07.899+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Apr 12 15:37:42 UTC 2013","customfield_12310420":"311644","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_506654325_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2013-02-12T18:29:53.510+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-4475/watchers","watchCount":9,"isWatching":false},"created":"2013-02-06T21:45:39.220+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12323274","id":"12323274","description":"2.0.3-alpha release","name":"2.0.3-alpha","archived":false,"released":true,"releaseDate":"2013-02-14"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12335732","id":"12335732","description":"3.0.0-alpha1 release","name":"3.0.0-alpha1","archived":false,"released":true,"releaseDate":"2016-09-03"}],"issuelinks":[{"id":"12366906","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12366906","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12627586","key":"HADOOP-9211","self":"https://issues.apache.org/jira/rest/api/2/issue/12627586","fields":{"summary":"HADOOP_CLIENT_OPTS default setting fixes max heap size at 128m, disregards HADOOP_HEAPSIZE","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12364020","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12364020","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12527380","key":"HDFS-2452","self":"https://issues.apache.org/jira/rest/api/2/issue/12527380","fields":{"summary":"OutOfMemoryError in DataXceiverServer takes down the DataNode","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-05-12T18:16:04.089+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"In DataNode, there are catchs around BPServiceActor.offerService() call but no catch for OutOfMemory as there is for the DataXeiver as introduced in 0.22.0.\n\nThe issue can be replicated like this:\n1) Create a cluster of X DataNodes and 1 NameNode and low memory settings (-Xmx128M or something similar).\n2) Flood HDFS with small file creations (any should work actually).\n3) DataNodes will hit OoM, stop blockpool service, and shutdown.\n\nThe resolution is to catch the OoMException and handle it properly when calling BPServiceActor.offerService() in DataNode.java; like as done in 0.22.0 of Hadoop. DataNodes should not shutdown or crash but remain in a sort of frozen state until memory issues are resolved by GC.\n\nLOG ERROR:\n2013-02-04 11:46:01,854 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected exception in block pool Block pool BP-1105714849-10.10.10.110-1360005776467 (storage id DS-1952316202-10.10.10.112-50010-1360005820993) service to vmhost2-vm0/10.10.10.110:8020\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n2013-02-04 11:46:01,854 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-1105714849-10.10.10.110-1360005776467 (storage id DS-1952316202-10.10.10.112-50010-1360005820993) service to vmhost2-vm0/10.10.10.110:8020","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12323274","id":"12323274","description":"2.0.3-alpha release","name":"2.0.3-alpha","archived":false,"released":true,"releaseDate":"2013-02-14"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"311990","customfield_12312823":null,"summary":"OutOfMemory by BPServiceActor.offerService() takes down DataNode","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13572916","id":"13572916","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. The resolution is to catch the OoMException and handle it properly when calling BlockPool.offerService() in DataNode.java; like as done in 0.22.0 of Hadoop. DataNodes should not shutdown or crash but remain in a sort of frozen state until memory issues are resolved by GC\nFirst, OOM is not an exception but an error. Catching an error and trying to handle it is a bad idea. Datanode should shutdown when this happens and staying in frozen state is incorrect.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-06T22:18:07.899+0000","updated":"2013-02-06T22:18:07.899+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13572923","id":"13572923","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"Please see some related discussion in HDFS-2911. BTW the right solution here is to configure the process with right heap size.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-06T22:22:13.444+0000","updated":"2013-02-06T22:22:13.444+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13572989","id":"13572989","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"body":"Yes, it is an error not an exception. Yes, configuration for more memory will resolve the issue. Maybe we should specify not to run DataNodes with low memory somehow? I mean, even the simplest stress test will result in this crash.\n\nI read your discussions in HDFS-2911 and understand why you don't want to handle OoM. If you would prefer to let the DataNode crash than I understand. To be honest, I was skeptical about raising this JIRA but I felt that this particular case can be resolved with waiting for receiver threads to free up.\n\nIf we do not want to deal with it that is fine.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-06T23:26:22.692+0000","updated":"2013-02-06T23:26:22.692+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13573874","id":"13573874","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"body":"I think recovery from an error might be an overkill indeed. However, catching it and re-throwing with a hint about possible cause of an issue might be very helpful for the people trying to debug the problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-07T20:13:15.865+0000","updated":"2013-02-07T20:13:15.865+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13573939","id":"13573939","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. However, catching it and re-throwing with a hint about possible cause of an issue might be very helpful for the people trying to debug the problem.\nI think OOM is fairly descriptive and widely understood (hopefully!). Not sure how a cause can be pointed out - among many it could be incorrect heap size, possible memory leak or some other bug.\n\nMay be, additional information as done in HADOOP-7469 could be added to describe many reasons why it could happen. It should be done in another jira.\n\nIf no one comments back or disagrees, I will close this jira as Invalid shortly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-07T21:17:46.404+0000","updated":"2013-02-07T21:17:46.404+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13573980","id":"13573980","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"body":"I believe OP has came across the problem with default mem. config at 128M.\nSo, a descriptive hint like \n\"Make sure the datanode JVM has big enough heapsize configured in... \"\nwould be helpful. \n\nNot like it is a high priority ticket, but it is a usability issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cos","name":"cos","key":"cos","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cos&avatarId=16741","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cos&avatarId=16741","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cos&avatarId=16741","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cos&avatarId=16741"},"displayName":"Konstantin Boudnik","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-07T22:02:41.061+0000","updated":"2013-02-07T22:02:41.061+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13573985","id":"13573985","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"body":"Indeed the issue why I bothered raising the JIRA was because the crash was vague and the OOM does not stand out compared to lines of \"Block Pool shutting down\", etc, that got printed in the logs.\n\nLook at my log snippet I posted. That line was nowhere near the bottom of the logs by the time DataNode shutdown. It would be nice if it could be made more apparent as it was the root cause.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-07T22:06:20.015+0000","updated":"2013-02-07T22:06:20.015+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13573992","id":"13573992","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Indeed the issue why I bothered raising the JIRA was because the crash was vague and the OOM does not stand out compared to lines of \"Block Pool shutting down\", etc, that got printed in the logs.\nSorry I might have misunderstood. The following line made me think you want to handle OOM.\n\nbq. DataNodes should not shutdown or crash but remain in a sort of frozen state until memory issues are resolved by GC.\n\nI am okay if you want to add more details. But the above line from the description confuses the intent of the jira. How about creating another jira that just says improve the message printed on OOM (if you want) and close this jira as invalid?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-07T22:23:08.795+0000","updated":"2013-02-07T22:23:08.795+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13574112","id":"13574112","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"body":"No you are correct. My initial intent was to handle the OOM. Your discussion post made me understand why we should not. Thank you. These were both concerns of mine, but I agree now to not handle the OOME.\n\nPlease understand, this error was reached on a freshly installed vanilla cluster of 3 DataNodes and 1 NameNode with minor changes to configs for distributed mode. This error will hit anyone using the default 128M DataNode memory configuration and trying to use a distributed cluster. Let's not handle the OOME, but maybe another path to consider is lowering the default number of receiver threads in the related default.xml so that we do not OOME so easily by default? If not, then I think the consensus is that we improve the error message handled by the try/catch around BPServiceActor.offerService().\n\nPlease let me know your final thoughts and you can close this JIRA as invalid. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-08T00:45:46.659+0000","updated":"2013-02-08T00:45:46.659+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13574152","id":"13574152","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shv","name":"shv","key":"shv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Konstantin Shvachko","active":true,"timeZone":"America/Los_Angeles"},"body":"I understand we are not catching OOM. But the problem still remains. People starting the cluster with default configuration ending up with dysfunctional cluster and dead DataNodes.\n\nI propose to adjust the default configuration to avoid the problem. There is clear unbalance between the default heap size (128 MB) and number of threads we allow. Either the default heap size should increase or the # of threads should go down.\nPlamen you have a reproducible configuration to crash the cluster. Could you investigate how many BP threads 128 MB can hold? You can reduce the thread count gradually until the cluster doesn't crash.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=shv","name":"shv","key":"shv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Konstantin Shvachko","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-08T01:47:19.825+0000","updated":"2013-02-08T01:47:19.825+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13576862","id":"13576862","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Either the default heap size should increase or the # of threads should go down.\nEither one of them should be okay. Please do that in a separate jira.\n\nClosing this jira as Invalid.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=sureshms","name":"sureshms","key":"sureshms","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10450","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10450","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10450","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10450"},"displayName":"Suresh Srinivas","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-02-12T18:29:45.506+0000","updated":"2013-02-12T18:29:45.506+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13630144","id":"13630144","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=raviprak","name":"raviprak","key":"raviprak","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=raviprak&avatarId=10113","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=raviprak&avatarId=10113","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=raviprak&avatarId=10113","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=raviprak&avatarId=10113"},"displayName":"Ravi Prakash","active":true,"timeZone":"America/Los_Angeles"},"body":"I know I'm coming in late on this and I agree this problem is easily solved by increasing heap sizes, restricting user quotas. But perhaps inside the daemons we should be monitoring [totalMemory()|http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Runtime.html#totalMemory%28%29] and enter a \"safemode\" when we come too close to OOMing?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=raviprak","name":"raviprak","key":"raviprak","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=raviprak&avatarId=10113","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=raviprak&avatarId=10113","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=raviprak&avatarId=10113","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=raviprak&avatarId=10113"},"displayName":"Ravi Prakash","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-04-12T15:05:42.204+0000","updated":"2013-04-12T15:05:42.204+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12631148/comment/13630191","id":"13630191","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"body":"I think the agreement made was to not try to handle OOM on DataNodes, but to let them crash. If you can, you should increase the heap size of your DataNode. In HADOOP-9211, I got that 512mb was enough to not cause any issues while under stress, with a vanilla setup.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zero45","name":"zero45","key":"zero45","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zero45&avatarId=35141","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zero45&avatarId=35141","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zero45&avatarId=35141","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zero45&avatarId=35141"},"displayName":"Plamen Jeliazkov","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-04-12T15:37:42.747+0000","updated":"2013-04-12T15:37:42.747+0000"}],"maxResults":13,"total":13,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-4475/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1hs5z:"}}