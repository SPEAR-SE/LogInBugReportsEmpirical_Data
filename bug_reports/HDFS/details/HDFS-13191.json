{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13140672","self":"https://issues.apache.org/jira/rest/api/2/issue/13140672","key":"HDFS-13191","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2018-02-24T06:08:33.768+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Mon Mar 05 17:23:29 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-13191/watchers","watchCount":6,"isWatching":false},"created":"2018-02-24T02:15:55.692+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-03-05T22:11:27.768+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12329603","id":"12329603","name":"hdfs"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"{color:red}colored text{color}The org.apache.hadoop.io.DataOutputBuffer is used as an \"optimization\" in many places to allow a reusable form of ByteArrayOutputStream, but requires the caller to be careful to use getLength() instead of getData().length to determine the number of actually valid bytes to consume.\r\n\r\nAt least three places in the path of constructing FileChecksums have incorrect usage of DataOutputBuffer:\r\n\r\n[FileChecksumHelper digesting block MD5s|https://github.com/apache/hadoop/blob/329a4fdd07ab007615f34c8e0e651360f988064d/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java#L239]\r\n\r\n[BlockChecksumHelper digesting striped block MD5s to construct block-group checksum|https://github.com/apache/hadoop/blob/329a4fdd07ab007615f34c8e0e651360f988064d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java#L412]\r\n\r\n[MD5MD5CRC32FileChecksum.getBytes()|https://github.com/apache/hadoop/blob/329a4fdd07ab007615f34c8e0e651360f988064d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java#L76]\r\n\r\nThe net effect is that FileChecksum consumes exact BlockChecksums if there are 1 or 2 blocks (at 16 and 32 bytes respectively), but at 3 blocks will round up to 64 bytes, effectively returning the same FileChecksum as if there were 4 blocks and the 4th block happened to have an MD5 exactly equal to 0x00...00. Similarly, BlockGroupChecksum will behave as if there is a power-of-2 number of bytes from BlockChecksums in the BlockGroup.\r\n\r\nThis appears to have been a latent bug for at least 9 years for FileChecksum (and since inception for the implementation of striped files), and works fine as long as HDFS implementations strictly stick to the same internal buffering semantics.\r\n\r\nHowever, this also makes the implementation extremely brittle unless carefully documented. For example, if code is ever refactored to pass around a MessageDigest that consumes block MD5s as they come rather than writing into a DataOutputBuffer before digesting the entire buffer, then the resulting checksum calculations will change unexpectedly.\r\n\r\nAt the same time, \"fixing\" the bug would also be backwards-incompatible, so the bug might need to stick around. At least for the FileChecksum-level calculation, it seems the bug has been latent for a very long time. Since striped files are fairly new, the BlockChecksumHelper could probably be fixed sooner rather than later to avoid perpetuating a bug. The getBytes() method for FileChecksum is more innocuous, so could likely be fixed or left as-is without too much impact either way.\r\n\r\nThe bug can be highlighted by changing the internal buffer-growing semantics of the DataOutputBuffer, or simply returning a randomly-sized byte buffer in getData() while only ensuring the first getLength() bytes are actually present, for example:\r\n\r\n \r\n{code:java}\r\ndiff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java\r\nindex 4c2fa67f8f2..f2df94e898f 100644\r\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java\r\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java\r\n@@ -103,7 +103,17 @@ private DataOutputBuffer(Buffer buffer) {\r\n/** Returns the current contents of the buffer.\r\n* Data is only valid to {@link #getLength()}.\r\n*/\r\n- public byte[] getData() { return buffer.getData(); }\r\n+ public byte[] getData() {\r\n+ java.util.Random rand = new java.util.Random();\r\n+ byte[] bufferData = buffer.getData();\r\n+ byte[] ret = new byte[rand.nextInt(bufferData.length) + bufferData.length];\r\n+ System.arraycopy(bufferData, 0, ret, 0, getLength());\r\n+ return ret;\r\n+ }\r\n\r\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12911860","id":"12911860","filename":"HDFS-13191.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dennishuo","name":"dennishuo","key":"dennishuo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dennis Huo","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-02-24T02:23:44.360+0000","size":5884,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12911860/HDFS-13191.001.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Internal buffer-sizing details are inadvertently baked into FileChecksum and BlockGroupChecksum","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dennishuo","name":"dennishuo","key":"dennishuo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dennis Huo","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dennishuo","name":"dennishuo","key":"dennishuo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dennis Huo","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13140672/comment/16375253","id":"16375253","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dennishuo","name":"dennishuo","key":"dennishuo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dennis Huo","active":true,"timeZone":"America/Los_Angeles"},"body":"[^HDFS-13191.001.patch] illustrates a basic fix without adding new tests, which is fairly straightforward. Unittests would be tricky without duplicating all the layers of chunk -> block -> file checksum computation in the test case, though doing so could be worthwhile as a regression test to detect when checksum implementations change even if they're internally consistent within a fresh set of tests. If I apply the DataOutputBuffer randomization change without these fixes, then TestFileChecksum fails; the test succeeds with the patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dennishuo","name":"dennishuo","key":"dennishuo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dennis Huo","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-02-24T02:26:57.800+0000","updated":"2018-02-24T02:26:57.800+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13140672/comment/16375334","id":"16375334","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\r\n\\\\\r\n\\\\\r\n|| Vote || Subsystem || Runtime || Comment ||\r\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 16s{color} | {color:blue} Docker mode activated. {color} |\r\n|| || || || {color:brown} Prechecks {color} ||\r\n| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |\r\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\r\n|| || || || {color:brown} trunk Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 13s{color} | {color:blue} Maven dependency ordering for branch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m  0s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 35s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 11s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 19s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 16m 12s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 17s{color} | {color:green} trunk passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 37s{color} | {color:green} trunk passed {color} |\r\n|| || || || {color:brown} Patch Compile Tests {color} ||\r\n| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for patch {color} |\r\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 32s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 42s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 42s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  7s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  2s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |\r\n| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 26s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |\r\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 51s{color} | {color:green} the patch passed {color} |\r\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 32s{color} | {color:green} the patch passed {color} |\r\n|| || || || {color:brown} Other Tests {color} ||\r\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 38s{color} | {color:green} hadoop-common in the patch passed. {color} |\r\n| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 32s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |\r\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 98m  7s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |\r\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 38s{color} | {color:green} The patch does not generate ASF License warnings. {color} |\r\n| {color:black}{color} | {color:black} {color} | {color:black}207m 52s{color} | {color:black} {color} |\r\n\\\\\r\n\\\\\r\n|| Reason || Tests ||\r\n| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |\r\n|   | hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages |\r\n|   | hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation |\r\n|   | hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |\r\n\\\\\r\n\\\\\r\n|| Subsystem || Report/Notes ||\r\n| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |\r\n| JIRA Issue | HDFS-13191 |\r\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12911860/HDFS-13191.001.patch |\r\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |\r\n| uname | Linux c40c49a3aaf7 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | /testptch/patchprocess/precommit/personality/provided.sh |\r\n| git revision | trunk / 329a4fd |\r\n| maven | version: Apache Maven 3.3.9 |\r\n| Default Java | 1.8.0_151 |\r\n| findbugs | v3.1.0-RC1 |\r\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/23185/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |\r\n|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/23185/testReport/ |\r\n| Max. process+thread count | 4194 (vs. ulimit of 10000) |\r\n| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |\r\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/23185/console |\r\n| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=genericqa","name":"genericqa","key":"genericqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=genericqa&avatarId=33630","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=genericqa&avatarId=33630","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=genericqa&avatarId=33630","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=genericqa&avatarId=33630"},"displayName":"genericqa","active":true,"timeZone":"Etc/UTC"},"created":"2018-02-24T06:08:33.768+0000","updated":"2018-02-24T06:08:33.768+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13140672/comment/16385613","id":"16385613","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajayydv","name":"ajayydv","key":"ajayydv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ajay Kumar","active":true,"timeZone":"America/Los_Angeles"},"body":"[~dennishuo], thanks for finding this subtle bug. How about fixing original bug by overriding {{DataOutputBuffer#getData}} to something like:\r\n{code}public byte[] getData() { return Arrays.copyOfRange(buf, 0, count); }{code}\r\nThis will not break syntactic compatibility.\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajayydv","name":"ajayydv","key":"ajayydv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ajay Kumar","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-03-05T05:07:04.729+0000","updated":"2018-03-05T05:07:20.174+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13140672/comment/16385621","id":"16385621","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dennishuo","name":"dennishuo","key":"dennishuo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dennis Huo","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the input! I personally don't feel too strongly about any particular approach and the uploaded patch is intended more to illustrate the issue than necessarily be the right fix, but I do foresee the downside of modifying getData() to return only the trimmed copy of the data would include:\r\n # The original intent of DataOutputBuffer appears to be for the sole purpose of being able to reuse a single byte array to avoid re-allocating an array. If getData() always allocates a new array, it likely eliminates any (perceived) advantage of using DataOutputBuffer in the first place, and in fact would become the same as just using a raw ByteArrayOutputStream instead.\r\n # Even though its interface doesn't promise anything about the size of the returned byte[], the fact that the checksums are sensitive to it means at least some places are apparently (unintentionally) relying on the exact buffer-sizing behavior already, so changing it inside DataOutputBuffer directly has a higher risk of breaking unexpected system components.\r\n\r\nHonestly I can't think of a good solution for any of this though. One approach that's safer but much slower is to very gradually deprecate the DataOutputBuffer entirely in favor of just using a ByteArrayOutputStream; the majority of places I've seen using it don't even benefit from \"reuse\" at all. Then each call site could be assessed case-by-case for impact and carefully mitigated/documented. For example, in the ones I listed in this JIRA, it would mean checksums become inconsistent during an in-place upgrade, or when comparing across HDFS versions, so an admin would have to somehow coordinate around any distcp jobs in-progress.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dennishuo","name":"dennishuo","key":"dennishuo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dennis Huo","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-03-05T05:27:20.415+0000","updated":"2018-03-05T05:27:20.415+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13140672/comment/16386395","id":"16386395","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajayydv","name":"ajayydv","key":"ajayydv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ajay Kumar","active":true,"timeZone":"America/Los_Angeles"},"body":" Ya, it seems there is no good solution to this.\r\n{quote} If getData() always allocates a new array, it likely eliminates any (perceived) advantage of using DataOutputBuffer in the first place{quote}\r\nModifying getData() to return only the trimmed copy of the array will result in overhead of new byte array allocation. Although we can minimize this by wrapping it with an if condition. (Create new copy only when count< length). \r\n{quote}Even though its interface doesn't promise anything about the size of the returned byte[], the fact that the checksums are sensitive to it means at least some places are apparently (unintentionally) relying on the exact buffer-sizing behavior already, so changing it inside DataOutputBuffer directly has a higher risk of breaking unexpected system components.{quote} Ya, but leaving it as it is may also involve risk of breaking some future functionality.\r\nI don't feel strongly about any specific approach but leaving bug as it is may result in more innocuous  critical bugs somewhere else. At the minimum we should add more documentation to it to warn users about this bug.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajayydv","name":"ajayydv","key":"ajayydv","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ajay Kumar","active":true,"timeZone":"America/Los_Angeles"},"created":"2018-03-05T17:23:29.804+0000","updated":"2018-03-05T22:11:27.759+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-13191/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3qjwn:"}}