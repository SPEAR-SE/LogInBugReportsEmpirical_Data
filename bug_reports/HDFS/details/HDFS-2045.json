{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12509510","self":"https://issues.apache.org/jira/rest/api/2/issue/12509510","key":"HDFS-2045","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2011-06-07T21:20:08.580+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sun Feb 08 18:25:39 UTC 2015","customfield_12310420":"15207","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_115940316696_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-02-08T18:25:39.253+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-2045/watchers","watchCount":6,"isWatching":false},"created":"2011-06-07T20:47:02.601+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-02-08T18:25:39.293+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312931","id":"12312931","name":"scripts"}],"timeoriginalestimate":null,"description":"It used to be that you could do the following:\n\n# Run `ant bin-package' in your hadoop-common checkout.\n# Set HADOOP_COMMON_HOME to the built directory of hadoop-common.\n# Run `ant bin-package' in your hadoop-hdfs checkout.\n# Set HADOOP_HDFS_HOME to the built directory of hadoop-hdfs.\n# Set PATH to have HADOOP_HDFS_HOME/bin and HADOOP_COMMON_HOME/bin on it.\n# Run `hdfs'.\n\\\\\n\\\\\nAs of HDFS-1963, this no longer works since hdfs-config.sh is looking in HADOOP_COMMON_HOME/bin/ for hadoop-config.sh, but it's being placed in HADOOP_COMMON_HOME/libexec.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"113757","customfield_12312823":null,"summary":"HADOOP_*_HOME environment variables no longer work for tar ball distributions","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=atm","name":"atm","key":"atm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=atm&avatarId=14136","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=atm&avatarId=14136","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=atm&avatarId=14136","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=atm&avatarId=14136"},"displayName":"Aaron T. Myers","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=atm","name":"atm","key":"atm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=atm&avatarId=14136","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=atm&avatarId=14136","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=atm&avatarId=14136","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=atm&avatarId=14136"},"displayName":"Aaron T. Myers","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12509510/comment/13045631","id":"13045631","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"The HADOOP_*_HOME variables were a bad idea that I take full responsibility for.\n\nThe goal should be to move the deployment and development environments closer together rather than have two completely different structures.\n\nCurrently, you can do:\n\n{code}\ncd common\nant rpm\nrpm -i build/hadoop-common-*.rpm\ncd ../hdfs\nant rpm\nrpm -i build/hadoop-hdfs-*.rpm\n{code}\n\nwhich is far easier to understand and you are running it like it is deployed.\n\nOf course, you can also deploy using the tarballs instead of rpm or debs.\n\nDo you have ideas for making the dev easier? I assume part of Nigel's goal of moving the subversion trees to gether is to eventually have a shared build directory.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-07T21:20:08.580+0000","updated":"2011-06-07T21:20:08.580+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12509510/comment/13045633","id":"13045633","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"I'm sorry, I read too fast. Are you just proposing to fix the hdfs-config.sh?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-07T21:23:01.256+0000","updated":"2011-06-07T21:23:01.256+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12509510/comment/13045648","id":"13045648","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"The state of current code in trunk.\n\nDevelopers:\n\n# Direct source tree execution, specify HADOOP_COMMON_HOME, HADOOP_HDFS_HOME, HADOOP_MAPRED_HOME works.\n# Source tarball, same as 1.\n\nOps:\n\n# Binary tarball, decompress all binary tarball to the same destination without prefix.  There is no need to use environment variable.\n# RPM/DEB works as described in Owen's comment, no need to specify environment variables.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-07T21:53:12.815+0000","updated":"2011-06-07T21:53:12.815+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12509510/comment/13045688","id":"13045688","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=atm","name":"atm","key":"atm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=atm&avatarId=14136","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=atm&avatarId=14136","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=atm&avatarId=14136","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=atm&avatarId=14136"},"displayName":"Aaron T. Myers","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. I'm sorry, I read too fast. Are you just proposing to fix the hdfs-config.sh?\n\nUnfortunately, I think it would take more than that to restore the previous behavior. Since the jars, etc are now located based solely on the single HADOOP_PREFIX env var, one cannot have separate distribution directories for HDFS, MR, and Common.\n\nMy main issue is that all of this rearranging/reworking of the tarball and development layouts were done under the auspices of adding RPM support. I'll be honest and say that I didn't review the RPM work hardly at all, because I assumed from the title that it would not affect tarballs or the dev environment. Had there been separate JIRAs along the lines of \"rearrange the layout of distribution builds\" and \"add RPM support\", I likely would have reviewed the former.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=atm","name":"atm","key":"atm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=atm&avatarId=14136","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=atm&avatarId=14136","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=atm&avatarId=14136","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=atm&avatarId=14136"},"displayName":"Aaron T. Myers","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-07T22:47:31.804+0000","updated":"2011-06-07T22:47:31.804+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12509510/comment/13045694","id":"13045694","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"body":"HADOOP_COMMON_HOME, HADOOP_HDFS_HOME, HADOOP_MAPRED_HOME were results of splitting the source code into three different submodules.  While this works fine for developer to isolate each project, it makes configuration difficult for production use. HDFS and MAPRED run as their own uid.  The amount of configuration just multiples.\n\nTo solve this problem, there are a couple options:\n\nOption 1.  Modify jar file which contains all common shell script in common jar file, when binary tarball is built, the common shell scripts are rearranged submerged into the binary tarball distribution, and completely remove HADOOP_*_HOME environment variables.  $HADOOP_PREFIX is the only hint (generated from shell script path, no need to define in the environment) to all hadoop programs where the bits are exactly layout.  When HDFS or MAPREDUCE is deployed, there is no need to deploy COMMON tarball.  To make this work for developers, *-config.sh should be moved to $HADOOP_PREFIX/libexec.  During the build process, hadoop-common-*.jar is extract for common shell scripts.  Both developer and binary layout are closer to each other.  (When project is converted to maven, this keeps hdfs/mapreduce loosely coupled and reduce duplicated shell scripts.)\n\nOption 2. Preserve HADOOP_*_HOME for source code execution.  Environment driven layout does not work on binary tarball. Change the prefix tarball from hadoop-[common|mapred|hdfs]-0.23.0-SNAPSHOT to hadoop-[version] for easy extraction.\n\nOption 3.  Enable HADOOP_*_HOME for binary tarball.  (Risk of crashing the system due to bad environment variable setup)\n\nOption 4.  Merge hdfs/mapreduce back to the same project, but create as subdirectories to reduce duplicated shell scripts.\n\nI am incline to vote for option 2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eyang","name":"eyang","key":"eyang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eric Yang","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-07T22:54:48.839+0000","updated":"2011-06-07T22:54:48.839+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12509510/comment/13049576","id":"13049576","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=atm","name":"atm","key":"atm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=atm&avatarId=14136","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=atm&avatarId=14136","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=atm&avatarId=14136","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=atm&avatarId=14136"},"displayName":"Aaron T. Myers","active":true,"timeZone":"America/Los_Angeles"},"body":"In the absence of compatibility concerns, I agree with option 2. I'm sorry we didn't go with this in the first place.\n\nThat said, I don't feel strongly about maintaining the backward compatibility of this \"feature\" of being able to run Hadoop from distinct mr/hdfs/common build directories.\n\nSo, if no one else feels strongly about this, then I'm fine rolling with option 2 (and closing this issue.)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=atm","name":"atm","key":"atm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=atm&avatarId=14136","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=atm&avatarId=14136","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=atm&avatarId=14136","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=atm&avatarId=14136"},"displayName":"Aaron T. Myers","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-15T02:20:16.185+0000","updated":"2011-06-15T02:20:16.185+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12509510/comment/13053093","id":"13053093","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"If doesn't impose a big burden I think we should preserve the ability to run Hadoop from distinct build dirs. I use this pretty frequently, and from recent list xfic it seems others do as well.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-06-22T07:19:57.806+0000","updated":"2011-06-22T07:19:57.806+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12509510/comment/14311473","id":"14311473","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"Fixed in trunk.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2015-02-08T18:25:39.284+0000","updated":"2015-02-08T18:25:39.284+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-2045/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0jtsn:"}}