{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12837016","self":"https://issues.apache.org/jira/rest/api/2/issue/12837016","key":"HDFS-8574","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2015-06-11T03:32:32.474+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Nov 10 19:17:49 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_4681302422_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-08-04T07:28:00.489+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-8574/watchers","watchCount":11,"isWatching":false},"created":"2015-06-11T03:06:18.109+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327584","id":"12327584","description":"2.7.0 release","name":"2.7.0","archived":false,"released":true,"releaseDate":"2015-04-20"}],"issuelinks":[{"id":"12448376","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12448376","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12861429","key":"HDFS-9011","self":"https://issues.apache.org/jira/rest/api/2/issue/12861429","fields":{"summary":"Support splitting BlockReport of a storage into multiple RPC","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-11-10T19:17:49.194+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"This piece of code in {{org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport()}}\n\n{code}\n// Send one block report per message.\n        for (int r = 0; r < reports.length; r++) {\n          StorageBlockReport singleReport[] = { reports[r] };\n          DatanodeCommand cmd = bpNamenode.blockReport(\n              bpRegistration, bpos.getBlockPoolId(), singleReport,\n              new BlockReportContext(reports.length, r, reportId));\n          numReportsSent++;\n          numRPCs++;\n          if (cmd != null) {\n            cmds.add(cmd);\n          }\n{code}\n\nwhen a single volume contains many blocks, i.e more than the threshold, it is trying to send the entire blockreport in one RPC, causing exception\n{code}\njava.lang.IllegalStateException: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.\n        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:369)\n        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:347)\n        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder.getBlockListAsLongs(BlockListAsLongs.java:325)\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.blockReport(DatanodeProtocolClientSideTranslatorPB.java:190)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:473)\n{code}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"When block count for a volume exceeds dfs.blockreport.split.threshold, block report causes exception","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14581389","id":"14581389","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=walter.k.su","name":"walter.k.su","key":"walter.k.su","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Walter Su","active":true,"timeZone":"Asia/Shanghai"},"body":"I don't know if it's necessary. If threshold is exceeded, the number of RPCs equals to the number of storages. The number of storages is not quite many, don't you think? What if threshold is exceeded per storage? The approach increases RPCs actually.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=walter.k.su","name":"walter.k.su","key":"walter.k.su","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Walter Su","active":true,"timeZone":"Asia/Shanghai"},"created":"2015-06-11T03:32:32.474+0000","updated":"2015-06-11T03:32:32.474+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14581674","id":"14581674","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi walter, thanks for that info. You are right, the number of RPCs is equal to number of volumes.\nBut in my scenario, there is one volume which contains files way more than {{dfs.blockreport.split.threshold}} (may be 10 times)\n\nSo the previous loop has created one report with all the blocklist on that volume\n{code}\n    for(Map.Entry<DatanodeStorage, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n      BlockListAsLongs blockList = kvPair.getValue();\n      reports[i++] = new StorageBlockReport(kvPair.getKey(), blockList); \n      totalBlockCount += blockList.getNumberOfBlocks();\n    }\n{code}\nSo next, when it tries to send this block report to NN, it receives \n{code}\njava.lang.IllegalStateException: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.\n        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:369)\n        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:347)\n        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder.getBlockListAsLongs(BlockListAsLongs.java:325)\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.blockReport(DatanodeProtocolClientSideTranslatorPB.java:190)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:473)\n{code}\n\nSo may be we can redesign so that multiple block reports can be sent per volume.? what do you suggest.?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-11T08:29:51.695+0000","updated":"2015-06-11T08:29:51.695+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14581677","id":"14581677","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"body":"Updated the issue based on your comment","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-11T08:33:59.598+0000","updated":"2015-06-11T08:33:59.598+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14581979","id":"14581979","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~ajithshetty], \n\nThanks for reporting this. \n\nbq. there is one volume which contains files way more than dfs.blockreport.split.threshold (may be 10 times)\nThe default value of {{dfs.blockreport.split.threshold}} is 1 Million. Even if you have a 10TB drive, 10 Million blocks per drive gives a mean block size of 1MB. HDFS is not designed to deal well with large numbers of such tiny blocks. Would you mind sharing some metrics about your target use case?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-11T14:28:26.896+0000","updated":"2015-06-11T14:28:26.896+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14587324","id":"14587324","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi [~arpitagarwal]\n\nThanks for the input. Yes you are right, HDFS was not designed for tiny blocks. My scenario was like, i wanted to test the NN limits so i inserted 10 million files with size ~10KB(10KB because i had smaller disk). My DN had one {{data.dir}} directory, when i faced this exception. But when i increased the {{data.dir}} to 5, the issue was resolved. Later i checked and came across this piece of code where the block report was sent per volume of DN. My question is when we check for overflow, based on number of blocks, then why we split based on report, as in a single report, there might be still overflow for given limit {{dfs.blockreport.split.threshold}}\n\nPlease correct me if i am wrong","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-06-16T02:01:05.576+0000","updated":"2015-06-16T02:01:05.576+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14588457","id":"14588457","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~ajithshetty], I don't expect this limit to be hit for a single drive in a real deployment soon given the NN scalability limits and typical disk sizes. Splitting based on storage directories was easier to implement and will handles the block report size issue.\n\nIf you just want to test NN namespace/block space limits you can create multiple storage directories on a single volume. The free space calculations will be off but that won't matter for your test. If you feel strongly about splitting reports for a single storage we can keep this Jira around.\n\nAlso the message is likely due to hitting the default protobuf message size limit of 64MB which is hit around 9-10M blocks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-16T17:49:04.450+0000","updated":"2015-06-16T17:49:04.450+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14589701","id":"14589701","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nijel","name":"nijel","key":"nijel","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"nijel","active":true,"timeZone":"Etc/UTC"},"body":"Can we think about making the protobuf size configurable ? is it feasible ? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nijel","name":"nijel","key":"nijel","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"nijel","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-17T12:28:55.872+0000","updated":"2015-06-17T12:28:55.872+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14593864","id":"14593864","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Can we think about making the protobuf size configurable ? is it feasible ?\nnijel, it is configurable via {{ipc.maximum.data.length}}.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-19T19:51:23.976+0000","updated":"2015-06-19T19:51:23.976+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14597253","id":"14597253","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nijel","name":"nijel","key":"nijel","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"nijel","active":true,"timeZone":"Etc/UTC"},"body":"In that case, i think there is no need of any change.\nUser can configure this accordingly, since the scenario is to verify the limits.\nThanks","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=nijel","name":"nijel","key":"nijel","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"nijel","active":true,"timeZone":"Etc/UTC"},"created":"2015-06-23T06:36:47.900+0000","updated":"2015-06-23T06:36:47.900+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14653196","id":"14653196","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"body":"Closing this issue as per comments","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ajithshetty","name":"ajithshetty","key":"ajithshetty","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10432"},"displayName":"Ajith S","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-08-04T07:27:04.485+0000","updated":"2015-08-04T07:27:04.485+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14991717","id":"14991717","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"body":"[~arpitagarwal]/[~walter.k.su]/[~ajithshetty]/[~nijel]\n\nSorry for coming late.Had seen similar issue(where user might not aware ),I am thinking ,shutting down the Datanode is better when this issue occurs,as of now sliently skips {{blockreports}}.\n\nany thoughts..?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"created":"2015-11-05T14:22:34.995+0000","updated":"2015-11-05T14:22:34.995+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12837016/comment/14999153","id":"14999153","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~brahmareddy], it's non-trivial to handle it correctly. Shutting down the DataNode will trigger re-replication and could have a cascading effect.\n\nI still don't expect this situation to be encountered in real clusters soon. [~jingzhao] planned some related work to split block reports per volume via HDFS-9011 (thanks to [~nijel] for commenting on HDFS-9011 which reminded me of that effort).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitagarwal","name":"arpitagarwal","key":"arpitagarwal","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Agarwal","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-11-10T19:17:49.194+0000","updated":"2015-11-10T19:17:49.194+0000"}],"maxResults":12,"total":12,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-8574/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2fwrb:"}}