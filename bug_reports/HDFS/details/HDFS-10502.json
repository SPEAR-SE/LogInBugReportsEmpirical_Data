{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12976839","self":"https://issues.apache.org/jira/rest/api/2/issue/12976839","key":"HDFS-10502","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/6","id":"6","description":"The problem isn't valid and it can't be fixed.","name":"Invalid"},"customfield_12312322":null,"customfield_12310220":"2016-06-08T16:55:00.507+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jun 08 16:55:00 UTC 2016","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_18638561_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2016-06-08T16:55:00.483+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-10502/watchers","watchCount":2,"isWatching":false},"created":"2016-06-08T11:44:21.957+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12332790","id":"12332790","description":"2.7.2 release","name":"2.7.2","archived":false,"released":true,"releaseDate":"2016-01-25"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-06-08T16:55:00.516+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12327021","id":"12327021","name":"fs"}],"timeoriginalestimate":null,"description":"My goal is to speed up reads.  I have about 500k small files (2k to 15k) and I'm trying to use HDFS as a cache for serialized instances of java objects.\n\nI've written the code to construct and serialize all the objects out to HDFS, and am now hoping to improve read performance, because accessing the objects from disk-based storage is proving to be too slow for my application's SLA's.\n\nSo my first question is, is using memory locking and hdfs cacheadmin pools and directives the right way to go, to cache my objects into memory, or should I create RAM disks, and do memory-based storage instead?\n\nIf hdfs cacheadmin is the way to go (it's the path I'm going down so far), then I need to figure out if what's happening is a bug or if I've configured something wrong, because when I start up HDFS with a gig of memory locked (both in limits.d for ulimit -l and also in hdfs-site.xml) and the server starts up, and presumably tries to cache things into memory, I get hours and hours of timeouts in the logs like this:\n\n2016-06-08 07:42:50,856 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService\njava.net.SocketTimeoutException: Call From stgb-fe1.litle.com/10.1.9.66 to localhost:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51647 remote=localhost/127.0.0.1:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n\tat sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:554)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51647 remote=localhost/127.0.0.1:8020]\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:520)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:254)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1084)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:979)\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Enabled memory locking and now HDFS won't start up","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=machey","name":"machey","key":"machey","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Machemer","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=machey","name":"machey","key":"machey","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chris Machemer","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"RHEL 6.8","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12976839/comment/15320922","id":"15320922","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"Hello [~machey].  I recommend taking these questions to the user@hadoop.apache.org mailing list.  We use JIRA for tracking confirmed bugs and feature requests.  We use user@hadoop.apache.org for usage advice and troubleshooting.\n\nRegarding whether or not this is a recommended approach, I think it depends on a few other factors.  Is the intent to use these cached files from Hadoop workloads, such as MapReduce jobs or Hive queries?  If not, then I wonder if your use case might be better served by something more directly focused on general caching use cases, such as Redis or memcached.  If your use case does involve Hadoop integration, then certainly Centralized Cache Management is worth exploring.\n\nRegarding the timeouts, I can tell from the exception that this is the heartbeat RPC sent from the DataNode to the NameNode.  I recommend investigating connectivity between the DataNode and the NameNode and examining the logs from both sides to try to determine if something is going wrong in the handling of the heartbeat message.  On one hand, a heartbeat timeout is not an error condition that is specific to Centralized Cache Management.  It could happen whether or not you're using that feature.  On the other hand, the heartbeat message does contain some optional information about the state of cache capacity and current usage at the DataNode.  That information would trigger special handling logic at the NameNode side, so I suppose there is a chance that something in that logic is hanging up the heartbeat handling.  Investigating the logs might reveal more.\n\nuser@hadoop.apache.org would be a good forum for further discussion of both of these topics.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-06-08T16:55:00.507+0000","updated":"2016-06-08T16:55:00.507+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-10502/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i2z5n3:"}}