{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12597814","self":"https://issues.apache.org/jira/rest/api/2/issue/12597814","key":"HDFS-3605","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12322472","id":"12322472","description":"2.0.2-alpha release","name":"2.0.2-alpha","archived":false,"released":true,"releaseDate":"2012-10-09"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2012-07-06T19:25:47.353+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Jul 19 14:02:53 UTC 2012","customfield_12310420":"241006","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_431084925_*|*_1_*:*_1_*:*_623902309_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2012-07-18T23:43:04.263+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-3605/watchers","watchCount":10,"isWatching":false},"created":"2012-07-06T18:39:57.063+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"5.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320353","id":"12320353","description":"hadoop-2.0.0-alpha release","name":"2.0.0-alpha","archived":false,"released":true,"releaseDate":"2012-05-23"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-09-04T00:59:01.909+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12316609","id":"12316609","name":"ha","description":"High Availability"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12312926","id":"12312926","name":"namenode"}],"timeoriginalestimate":null,"description":"Open file for append\nWrite data and sync.\nAfter next log roll and editlog tailing in standbyNN close the append stream.\nCall append multiple times on the same file, before next editlog roll.\nNow abruptly kill the current active namenode.\n\nHere block is missed..\n\nthis may be because of All latest blocks were queued in StandBy Namenode. \nDuring failover, first OP_CLOSE was processing the pending queue and adding the block to corrupted block. ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12536372","id":"12536372","filename":"HDFS-3605.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-13T12:26:36.033+0000","size":12293,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12536372/HDFS-3605.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12536961","id":"12536961","filename":"hdfs-3605.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-18T06:38:57.633+0000","size":10469,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12536961/hdfs-3605.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12536725","id":"12536725","filename":"hdfs-3605.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-16T21:24:06.986+0000","size":11252,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12536725/hdfs-3605.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12536481","id":"12536481","filename":"hdfs-3605.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-13T23:57:55.783+0000","size":10918,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12536481/hdfs-3605.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12535784","id":"12535784","filename":"TestAppendBlockMiss.java","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-07-10T05:36:23.643+0000","size":3650,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12535784/TestAppendBlockMiss.java"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"5025","customfield_12312823":null,"summary":"Block mistakenly marked corrupt during edit log catchup phase of failover","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13408238","id":"13408238","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"Great catch Brahma.\n\nHere I have one question, why we are keeping all the blocks which are having the same blockID and different genstamps due to append recovery etc.? I think we should maintain only the latest block which is reported recently. Mostly this block will have the higher genstamp.\n\nOther part is:\n{code}\nif (changeMade) {\n        // The state or gen-stamp of the block has changed. So, we may be\n        // able to process some messages from datanodes that we previously\n        // were unable to process.\n        fsNamesys.getBlockManager().processQueuedMessagesForBlock(newBlock);\n      }\n{code}\nIn updateBlocks we have done this, because of optimizing the queued block processing?\nDue to this, it may mark block as corrupt right if have queued older genstamp block?\n \nWhat if we maintain only recently reported genstamp block in postPonedDNMessages and do only after loading complete edits?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-06T19:25:47.353+0000","updated":"2012-07-06T19:25:47.353+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13409923","id":"13409923","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"I'm sorry, I'm not entirely understanding the description. Can you post a unit test which reproduces the issue?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-09T22:44:04.291+0000","updated":"2012-07-09T22:44:04.291+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410062","id":"13410062","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi Todd,\nThanks for taking a look..Attaching unit test to reproduce this issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brahmareddy","name":"brahmareddy","key":"brahmareddy","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=brahmareddy&avatarId=24624","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=brahmareddy&avatarId=24624","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=brahmareddy&avatarId=24624","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=brahmareddy&avatarId=24624"},"displayName":"Brahma Reddy Battula","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-07-10T05:36:23.734+0000","updated":"2012-07-10T05:36:23.734+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410066","id":"13410066","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Thanks for the test. Very helpful. I'll take a look at this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-10T05:46:28.081+0000","updated":"2012-07-10T05:46:28.081+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410077","id":"13410077","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"{code}\npublic void processQueuedMessagesForBlock(Block b) throws IOException {\n    Queue<ReportedBlockInfo> queue = pendingDNMessages.takeBlockQueue(b);\n    if (queue == null) {\n      // Nothing to re-process\n      return;\n    }\n    processQueuedMessages(queue);\n  }\n{code}\n\nI think here, on first OP_CLOSE edit processing it is trying to process the QueuedMessagesForBlock. But here queued messages may contains the more future block as well, because duw to many append calls, SNN queued that messages.\nInstead processing all the queued messages for that block, it is make sense to process that current block(current OP_CLOSE genstamp block)?\n\npendingDNMessages.takeBlockQueue(b); will give set of blocks which are matching to the blockID, because it was queuse by block ID. did not consider genstamp. But, by considering current case, do we need to consider getstamp also, while getting the blcok from queued message and process. Because there moght be some more OPs which will come with that block as part of furtheer appends. At that time, anyway that respective genstamp blocks will get processed right?\n \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-10T06:08:35.401+0000","updated":"2012-07-10T06:08:35.401+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410080","id":"13410080","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"The design of the code should be such that, it will re-process those \"future\" events, but they'll get re-postponed at that point. Maybe the issue is specifically in the case where these opcodes get read during the \"catchup\" during transition to active?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-10T06:14:45.863+0000","updated":"2012-07-10T06:14:45.863+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410083","id":"13410083","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"{quote}\nThe design of the code should be such that, it will re-process those \"future\" events, but they'll get re-postponed at that point\n{quote}\nThis is what I mean, if i understand you intent correctly here. leave the future genstamps here for processing. Once all the OP codes read and processed anyway it is processing all quequed messages again if anything left I remeber. So, this should help us in this case.\n\n{quote}\nMaybe the issue is specifically in the case where these opcodes get read during the \"catchup\" during transition to active?\n{quote}\nWhat issue you are pointing here. Edits are getting read in correct order only right?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-10T06:21:30.104+0000","updated":"2012-07-10T06:21:30.104+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410090","id":"13410090","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"{quote}The design of the code should be such that, it will re-process those \"future\" events, but they'll get re-postponed at that point. Maybe the issue is specifically in the case where these opcodes get read during the \"catchup\" during transition to active?{quote}\nYou are correct Todd. this problem will come in case of catch up during failover.\n\n{code}if (namesystem.isInStandbyState() &&\n        namesystem.isGenStampInFuture(block.getGenerationStamp())) {\n      queueReportedBlock(dn, block, reportedState,\n          QUEUE_REASON_FUTURE_GENSTAMP);\n      return null;\n    }{code}\nSince during failover the state is already changed to ACTIVE, block will not be added again to queue even though it is in future.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-07-10T06:41:54.972+0000","updated":"2012-07-10T06:41:54.972+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410091","id":"13410091","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"@Uma\n{quote}Here I have one question, why we are keeping all the blocks which are having the same blockID and different genstamps due to append recovery etc.? I think we should maintain only the latest block which is reported recently. Mostly this block will have the higher genstamp.{quote}\n\nI agree with your point, we can keep only the latest reported state of the block regardless of genstamp from each datanode instead of keeping all previous states in queue which may be outdated by the time those are processed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-07-10T06:46:56.903+0000","updated":"2012-07-10T06:46:56.903+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410100","id":"13410100","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"So, I think the fix needs to be that, while we're doing the \"catch-up\" tailing, we continue to treat everything as if it were still in standby state, from the block management perspective. Then process any remaining queued messages only after becoming active. Right?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-10T07:18:01.703+0000","updated":"2012-07-10T07:18:01.703+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410132","id":"13410132","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"Exactly Todd, This is what we have done in our internal branch as a work around for this fix. Still we can discuss about the optimization as we discussed in above comment, like we can process only the current genstamp blkID, and postpone remaining stuff. Other case could be that maintain only, renecnt block report with the greater genstamp. Do you feel any issues with this?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-10T08:31:05.456+0000","updated":"2012-07-10T08:31:05.456+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13410138","id":"13410138","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"Note that, In this case we have 2 options\n- we have to maintain only recently reoprted blocks on QueuedDNMessages(with latest genstamp). we have to think the impacts of this, if  any scenario we are missing. Currently we have opted this for our work around. \n  (or) \n- process the current genstamp then and there and postpone higher genstamp blcoks again here. Otherwise if we just postpone everything till all edits loading, then the older genstamp blocks will create issue as genstamp already might have updated to higher in blocksMap while loading all edits, So, still block may get marked as corrupt.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-10T08:44:18.931+0000","updated":"2012-07-10T08:44:18.931+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13413511","id":"13413511","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Hey Uma. I think we should separate the discussion of a potential optimization from the discussion of fixing this bug.\n\nDo you already have a patch for the bug? If not, I'll make one following the approach described above.\n\nOnce the bug fix is in, we can talk about how to optimize the memory usage.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-13T05:35:25.427+0000","updated":"2012-07-13T05:35:25.427+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13413690","id":"13413690","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Todd, \n \n Attached a patch which I am thiking of currently.\n \n{quote}\n I think we should separate the discussion of a potential optimization from the discussion of fixing this bug.\n{quote}\nSure. Infact that is not an optimization, its required in the approach which I am thinking.\n\nPlease take a look and correct me, if I miss something in the patch as you and Jitendra involved in that changes mainly.\n\nThanks a lot,\n\nUma","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-13T12:34:07.659+0000","updated":"2012-07-13T12:34:07.659+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13414167","id":"13414167","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Hey Uma. I took your unit test (thanks) and modified it to be minimal and remove sleeps. Then I prepared a patch with a slightly different approach: I now use a boolean inside BlockManager to determine whether to do the block postponement. I think this is a bit simpler, and still fixes the issue.\n\nAm I missing another case with this fix? The optimization you did might be useful but per above I think we can make this minimal and optimize separately. I don't think it's required for the bugfix.\n\nThis patch isn't quite final - I want to add a few javadocs, etc.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-13T23:57:55.832+0000","updated":"2012-07-13T23:57:55.832+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13414168","id":"13414168","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"submitting patch to make sure it doesn't break other tests","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-13T23:58:19.357+0000","updated":"2012-07-13T23:58:19.357+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13414292","id":"13414292","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12536481/hdfs-3605.txt\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.namenode.TestBackupNode\n                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2821//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2821//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-14T04:23:47.567+0000","updated":"2012-07-14T04:23:47.567+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13414546","id":"13414546","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"TestPipelinesFailover is probably related. I'll look into that.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-14T23:35:02.490+0000","updated":"2012-07-14T23:35:02.490+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13414580","id":"13414580","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":" Thanks a lot, Todd for the patch.\n\nI have taken a quick look on the patch. Yes, this approach should work as well. \nBlocks will get processed for all the ops, so, the matching to current genStamp will get processed in current iteration and future ones will get postponed again. \n\nA few comments on patch: Did not check for any javadoc issues as you mentioned already, i.e,  will work on javadocs.\n\n{quote}\n+      out.writeBytes(\"/data\");\n+      \n+      // TODO: why do we need an hflush for this test case to fail?\n{quote}\nI remember, this is just added to ensure tthat the current packet will be en-queued and block will get allocated.\nOther wise less than 64K content may not be flushed at that time.\n\n{quote}\nDFSTestUtil.appendFile(fs, fileToAppend, \"data\");\n{quote}\nHaving the multiple append calls can give the regression for the case where we have many genstamp and they got processed in order and future ones will get postponed.\n\n{quote}\n// Wait till DN reports blocks\n+      cluster.triggerBlockReports();\n{quote}\ncomment need to update?\n\n{quote}\nshouldPostponeInvalidBlocks  \n{quote}\ndo we need to change the variable name? Since blocks are not declared as invalid yet.\n\nWill take a look deeply on the patch tomorrow again. ( not able to concentrate much, as I am traveling today)\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-15T05:52:22.198+0000","updated":"2012-07-15T05:52:22.198+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13415656","id":"13415656","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"I looked into the TestPipelinesFailover issue, and saw a \"Too many open files\" in the log. But I looked for leaks using MAT and some lsofing, and couldn't figure out what the leak issue is. Resubmitting an updated patch, which also addresses Uma's comments above.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-16T21:24:07.011+0000","updated":"2012-07-16T21:24:07.011+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13415764","id":"13415764","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12536725/hdfs-3605.txt\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2835//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2835//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-16T23:03:00.601+0000","updated":"2012-07-16T23:03:00.601+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13416779","id":"13416779","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"What do you think, Uma. Does this look good to you?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-18T00:52:33.660+0000","updated":"2012-07-18T00:52:33.660+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13416862","id":"13416862","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Todd, \n\n sorry for the late reply on this. Stucked in someother work yesterday.\n\n Patch looks great to me. \n \n +1 on addressing a small nit.\n\n{code}\n+  public void setPostponeInvalidBlockReports(boolean postpone) {\n+    this.shouldPostponeBlocksFromFuture  = postpone;\n+  }\n{code}\nforgot to update method name? variable and method names looks different.\n\n\nI also ran some tests with this change by adding some debug points. Worked well for me.\n\n\nThanks,\nUma\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=umamaheswararao","name":"umamaheswararao","key":"umamaheswararao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Uma Maheswara Rao G","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-18T04:55:50.195+0000","updated":"2012-07-18T04:55:50.195+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13416908","id":"13416908","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Addresses Uma's nit above","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-18T06:38:57.711+0000","updated":"2012-07-18T06:38:57.711+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13416935","id":"13416935","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12536961/hdfs-3605.txt\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.namenode.TestListCorruptFileBlocks\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2853//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2853//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-18T08:23:25.276+0000","updated":"2012-07-18T08:23:25.276+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13417834","id":"13417834","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"I'll commit this momentarily based on Uma's +1 above, since it was only a small nit.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-18T23:36:44.564+0000","updated":"2012-07-18T23:36:44.564+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13417853","id":"13417853","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"body":"Committed to branch-2 and trunk. Thanks Brahma for reporting the bug and writing a test case, and thanks Uma for the reviews and discussion above!","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tlipcon","name":"tlipcon","key":"tlipcon","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=tlipcon&avatarId=26804","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tlipcon&avatarId=26804","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tlipcon&avatarId=26804","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tlipcon&avatarId=26804"},"displayName":"Todd Lipcon","active":true,"timeZone":"America/Tijuana"},"created":"2012-07-18T23:43:04.287+0000","updated":"2012-07-18T23:43:04.287+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13417903","id":"13417903","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Mapreduce-trunk-Commit #2520 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2520/])\n    HDFS-3605. Block mistakenly marked corrupt during edit log catchup phase of failover. Contributed by Todd Lipcon and Brahma Reddy Battula. (Revision 1363175)\n\n     Result = FAILURE\ntodd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363175\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAAppend.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-19T00:37:40.357+0000","updated":"2012-07-19T00:37:40.357+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13417941","id":"13417941","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Hdfs-trunk-Commit #2563 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2563/])\n    HDFS-3605. Block mistakenly marked corrupt during edit log catchup phase of failover. Contributed by Todd Lipcon and Brahma Reddy Battula. (Revision 1363175)\n\n     Result = SUCCESS\ntodd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363175\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAAppend.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-19T01:27:25.912+0000","updated":"2012-07-19T01:27:25.912+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13417942","id":"13417942","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Common-trunk-Commit #2498 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2498/])\n    HDFS-3605. Block mistakenly marked corrupt during edit log catchup phase of failover. Contributed by Todd Lipcon and Brahma Reddy Battula. (Revision 1363175)\n\n     Result = SUCCESS\ntodd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363175\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAAppend.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-19T01:28:31.530+0000","updated":"2012-07-19T01:28:31.530+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13418233","id":"13418233","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Hdfs-trunk #1108 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1108/])\n    HDFS-3605. Block mistakenly marked corrupt during edit log catchup phase of failover. Contributed by Todd Lipcon and Brahma Reddy Battula. (Revision 1363175)\n\n     Result = FAILURE\ntodd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363175\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAAppend.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-19T11:40:47.241+0000","updated":"2012-07-19T11:40:47.241+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12597814/comment/13418303","id":"13418303","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Mapreduce-trunk #1141 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1141/])\n    HDFS-3605. Block mistakenly marked corrupt during edit log catchup phase of failover. Contributed by Todd Lipcon and Brahma Reddy Battula. (Revision 1363175)\n\n     Result = FAILURE\ntodd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363175\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAAppend.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-19T14:02:53.251+0000","updated":"2012-07-19T14:02:53.251+0000"}],"maxResults":32,"total":32,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-3605/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i017r3:"}}