{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13008495","self":"https://issues.apache.org/jira/rest/api/2/issue/13008495","key":"HDFS-10927","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2017-02-10T21:36:56.140+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Feb 17 07:07:07 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-10927/watchers","watchCount":9,"isWatching":false},"created":"2016-09-29T10:48:18.979+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12331979","id":"12331979","description":"2.7.1 release","name":"2.7.1","archived":false,"released":true,"releaseDate":"2015-07-06"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-02-17T07:25:25.463+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12327021","id":"12327021","name":"fs"}],"timeoriginalestimate":null,"description":"HDFS was unable to close a file when block write operation failed because of too high disk usage.\n\nScenario:\n\nHBase was writing WAL logs on HDFS and the disk usage was too high at that time. While writing these WAL logs, one of the blocks writes operation failed with the following exception:\n\n2016-09-13 10:00:49,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-337226066-192.168.193.217-1468912147102:blk_1074859607_1160899\njava.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.194.144:50010 remote=/192.168.192.162:43105]\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at java.io.BufferedInputStream.fill(Unknown Source)\n        at java.io.BufferedInputStream.read1(Unknown Source)\n        at java.io.BufferedInputStream.read(Unknown Source)\n        at java.io.DataInputStream.read(Unknown Source)\n        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:472)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:849)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:807)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Unknown Source)\n\nAfter this exception, HBase tried to close/rollover the WAL file but that call also failed and WAL file couldn't be closed. After this HBase closed the region server\n\nAfter some time, Lease Recovery got triggered for this file and following exceptions starts occurring:\n\n2016-09-13 11:51:11,743 WARN org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol: Failed to obtain replica info for block (=BP-337226066-192.168.193.217-1468912147102:blk_1074859607_1161187) from datanode (=DatanodeInfoWithStorage[192.168.192.162:50010,null,null])\njava.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_1074859607_1161187, RBW\n  getNumBytes()     = 45524696\n  getBytesOnDisk()  = 45483527\n  getVisibleLength()= 45511557\n  getVolume()       = /opt/reflex/data/yarn/datanode/current\n  getBlockFile()    = /opt/reflex/data/yarn/datanode/current/BP-337226066-192.168.193.217-1468912147102/current/rbw/blk_1074859607\n  bytesAcked=45511557\n  bytesOnDisk=45483527\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2278)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2254)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2542)\n        at org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolServerSideTranslatorPB.initReplicaRecovery(InterDatanodeProtocolServerSideTranslatorPB.java:55)\n        at org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(InterDatanodeProtocolProtos.java:3105)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Unknown Source)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\n        at java.lang.reflect.Constructor.newInstance(Unknown Source)\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2555)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2625)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:243)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2527)\n        at java.lang.Thread.run(Unknown Source)\n\nExpected Behaviour: Under all conditions lease recovery should have been done and file should have been closed.\n\nImpact: Since the file couldn't be closed, HBase went into an incosistent state as it wasn't able to run through the WAL file after the region server restart.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Lease Recovery: File not getting closed on HDFS when block write operation fails","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ngoswami","name":"ngoswami","key":"ngoswami","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nitin Goswami","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ngoswami","name":"ngoswami","key":"ngoswami","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Nitin Goswami","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13008495/comment/15861877","id":"15861877","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~ngoswami] thanks for reporting this issue and attach the relevant logs.\n\nWe have seen the same error message a few times in the past:\n{quote}\njava.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), \n{quote}\n\nIn one such incidence, the DataNode had a long GC pause, lasting more than 1 minute. I have a theory why this inconsistency happens and I am working on a unit test to reproduce it. Could you confirm if the DataNode in your cluster also suffered from a long GC?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jojochuang","name":"jojochuang","key":"jojochuang","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=jojochuang&avatarId=25508","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jojochuang&avatarId=25508","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jojochuang&avatarId=25508","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jojochuang&avatarId=25508"},"displayName":"Wei-Chiu Chuang","active":true,"timeZone":"America/Los_Angeles"},"created":"2017-02-10T21:36:56.140+0000","updated":"2017-02-10T21:36:56.140+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13008495/comment/15871310","id":"15871310","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhangchen","name":"zhangchen","key":"zhangchen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Zhang","active":true,"timeZone":"Etc/UTC"},"body":"Hey guys, I just found an issue like this.\nHBase RegionServer got an DiskFull exception while writing WAL files, and client failed after several times retry. When Master trying to use recoverLease to recover these file, we got almost same logs as [~ngoswami] attached\n{quote}\njava.io.IOException: File length mismatched.  The length of /home/work/ssd11/hdfs/xxxx/datanode/current/BP-228094273-10.136.5.10-1486630815208/current/rbw/blk_1073970099 is 174432256 but r=ReplicaBeingWritten, blk_1073970099_229357, RBW\n  getNumBytes()     = 174437376\n  getBytesOnDisk()  = 174429752\n  getVisibleLength()= 174429752\n  getVolume()       = /home/work/ssd11/hdfs/xxxxx/datanode/current\n  getBlockFile()    = /home/work/ssd11/hdfs/xxxxx/datanode/current/BP-228094273-10.136.5.10-1486630815208/current/rbw/blk_1073970099\n  bytesAcked=174429752\n  bytesOnDisk=174429752\n{quote}\n\nIn my case, it's caused by the exception while out.write() in receivePacket() of BlockReceiver. \nreceivePacket() first update numbytes in replicaInfo, then write data to disk, and update bytesOnDisk at last, the DiskFull exception makes the length not consistent.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhangchen","name":"zhangchen","key":"zhangchen","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Chen Zhang","active":true,"timeZone":"Etc/UTC"},"created":"2017-02-17T07:07:07.351+0000","updated":"2017-02-17T07:25:25.451+0000"}],"maxResults":2,"total":2,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-10927/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i348jj:"}}