{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12542071","self":"https://issues.apache.org/jira/rest/api/2/issue/12542071","key":"HDFS-2932","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2012-02-10T05:11:00.522+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Sep 18 06:16:11 UTC 2014","customfield_12310420":"227358","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_82172797839_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-09-18T06:16:11.588+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-2932/watchers","watchCount":8,"isWatching":false},"created":"2012-02-10T04:29:33.823+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320353","id":"12320353","description":"hadoop-2.0.0-alpha release","name":"2.0.0-alpha","archived":false,"released":true,"releaseDate":"2012-05-23"}],"issuelinks":[{"id":"12397099","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12397099","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12559116","key":"HDFS-3493","self":"https://issues.apache.org/jira/rest/api/2/issue/12559116","fields":{"summary":"Invalidate excess corrupted blocks as long as minimum replication is satisfied","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=usrikanth","name":"usrikanth","key":"usrikanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Srikanth Upputuri","active":true,"timeZone":"Asia/Kolkata"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-03-10T04:36:53.278+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"}],"timeoriginalestimate":null,"description":"Started 1NN,DN1,DN2,DN3 in the same machine.\nWritten a huge file of size 2 Gb\nwhile the write for the block-id-1005 is in progress bruought down DN3.\nafter the pipeline recovery happened.Block stamp changed into block_id_1006 in DN1,Dn2.\nafter the write is over.DN3 is brought up and fsck command is issued.\nthe following mess is displayed as follows\n\"block-id_1006 is underreplicatede.Target replicas is 3 but found 2 replicas\".","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"114038","customfield_12312823":null,"summary":"Under replicated block after the pipeline recovery.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andreina","name":"andreina","key":"andreina","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J.Andreina","active":true,"timeZone":"Asia/Kolkata"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andreina","name":"andreina","key":"andreina","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J.Andreina","active":true,"timeZone":"Asia/Kolkata"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12542071/comment/13205237","id":"13205237","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"body":"Could you please also elaborate on how this is a bug? As I see it from the steps provided, your write ended successfully with just two DNs in a 3-DN cluster with replication factor of 3, cause one DN was pulled down. This would naturally be the case afterwards -- an underreplication.\n\nOr do you mean to say the underreplication warning isn't passing away; that the NN isn't assigning it to newly living DN3 even after the monitor period has passed?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=qwertymaniac","name":"qwertymaniac","key":"qwertymaniac","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=qwertymaniac&avatarId=16780","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=qwertymaniac&avatarId=16780","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=qwertymaniac&avatarId=16780","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=qwertymaniac&avatarId=16780"},"displayName":"Harsh J","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-02-10T05:11:00.522+0000","updated":"2012-02-10T05:11:00.522+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12542071/comment/13205454","id":"13205454","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andreina","name":"andreina","key":"andreina","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J.Andreina","active":true,"timeZone":"Asia/Kolkata"},"body":"Initially Three Dn was running ..but while writing the block (block-id-1005) i brought down 3rdDN.so the write was successful to other two DN but the block stamp was changed to (block-id-1006) and the rest of the block was also written successfuly to those two datanodes.\n\nAfter the write was over ,The 3rd DN was restarted after 5 mins.then when fsck command was issued \"block-id_1006 is underreplicatede.Target replicas is 3 but found 2 replicas\" message was displayed.\n\nEver after the replication monitor period is over ,the blocks is still  under replicated only.I checked the same after 10 hrs. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=andreina","name":"andreina","key":"andreina","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"J.Andreina","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-02-10T13:36:54.025+0000","updated":"2012-02-10T13:36:54.025+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12542071/comment/13235301","id":"13235301","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Scenario:\n 1. Client is writing to a Pipeline DN1--> DN2 -->DN3. Block id Ex; b1k_1_1001\n 2. DN3 is stopped in between. Now pipeline recovery happens and block id is changed to b1k_1_1002.\n 3. write is complete, and stream is closed.\n 4. DN3 is restarted.\n\n*Issue Case 1: DN3 coming back after file is closed.*\n----------------------------------------------------\n>> Now DN3 will send the block reports to NN, which contains b1k_1_1001 report in RBW state.\n>> by this time, Since the file is closed, NN will mark this as replica as corrupt.\n>> Now Replication will not succeed since It cannot find one more datanode.\n\n*Issue Case 2: DN3 coming back before the file closure.*\n------------------------------------------------------\n>> Now DN3 will send the block reports to NN, which contains b1k_1_1001 report in RBW state. but by this time file is not closed, then this DN is just added to targets array.\n>> Replication request sent to Other DN (Ex DN2) to replicate this block to DN3.\n>> Now DN3 will refuse the Replication throwing ReplicaAlreadyExistsException. because while checking for the existence of the Block, generation stamp is not considered.\n\t{noformat}2012-03-22 08:30:39,406 ERROR datanode.DataNode (DataXceiver.java:run(171)) - 127.0.0.1:59082:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:59124 dest: /127.0.0.1:59082\norg.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-1348337625-169.254.103.145-1332385233856:blk_-4842149393874243436_1003 already exists in state RWR and thus cannot be created.\n\tat org.apache.hadoop.hdfs.server.datanode.FSDataset.createTemporary(FSDataset.java:1740)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:151)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:340)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:98)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:66)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:167)\n\tat java.lang.Thread.run(Unknown Source){noformat}\n\n\n*Basic Queries..?*\n 1. Why while comparing the Block, Generationstamp is not considered...? \n       This behaviour is different compare to version 1.0\n    ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2012-03-22T03:19:08.854+0000","updated":"2012-03-22T03:19:08.854+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12542071/comment/14125490","id":"14125490","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=usrikanth","name":"usrikanth","key":"usrikanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Srikanth Upputuri","active":true,"timeZone":"Asia/Kolkata"},"body":"Further analysing the two cases detailed by Vinay:\n\n*Case 1*. I think the fix given for HDFS-3493 will solve this case as the corrupt replica(result of pipeline failure) will be eventually invalidated, inspite of the fact that total replicas = replication factor. Please confirm.\n\n*Case 2*. If a write-pipeline-failed replica from a restarted DN arrives before the stored block is 'completed', it will not be marked as corrupt. Later when NN computes the replication work it is not aware of the fact that a corrupt replica exists on DN3, so it will keep scheduling replication from say DN2 to DN3 without success till next block report from DN3 is processed.\n\n\n{code}\n//BlockManager#checkReplicaCorrupt\n\n    case RBW:\n    case RWR:\n      if (!storedBlock.isComplete()) {\n        return null; // not corrupt\n      } \n{code}\n\nThere are two exclusive time windows when such a replica can be reported.\nDN restarts and replica is reported before the client finished writing the block, i.e the block is not 'committed'.\nDN restarts and replica is reported after 'commit' but before 'complete'. \n\n\nSolution is to be able to detect and capture a write-pipeline-failed replica as early as possible. First fix may be to change the check from 'isCompleted' to 'isCommitted'. This will capture write-pipeline-failed replicas reported just after commit and before 'complete' and mark them as corrupt.\n\nThen to capture write-pipeline-failed replicas reported before commit, I am investigating if this can be solved by marking them as corrupt as part of commit. There already exists a check to find any mis-stamped replicas during commit but we only remove them from the blocksMap. In addition can we not mark such replicas as corrupt?\n\n{code}\n//BlockInfoUnderConstruction#commitBlock\n\n    // Sort out invalid replicas.\n    setGenerationStampAndVerifyReplicas(block.getGenerationStamp());\n{code}\n\nAny thoughts/suggestions?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=usrikanth","name":"usrikanth","key":"usrikanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Srikanth Upputuri","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-09-08T13:41:33.100+0000","updated":"2014-09-08T13:41:33.100+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12542071/comment/14133717","id":"14133717","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"body":"Hi [~usrikanth],\nThanks for looking into this issue.\n\n1. Case1  is solved in HDFS-3493\n\n2. Case 2: I understand your point,\n    I agree that it will be better to detect the failed replicas as early as possible and delete it. but we cannot do that on the fly while writing itself. It may delete the working copy itself if the report from datanode is little delayed. and this is possible in case of huge cluster. Its better to keep the corrupt replica for sometime instead of loosing the valid replica. Similar cases observed and code has been added to ignore such variations. see below comment in BlockManager.java.\n{code}          // If it's a RBW report for a COMPLETE block, it may just be that\n          // the block report got a little bit delayed after the pipeline\n          // closed. So, ignore this report, assuming we will get a\n          // FINALIZED replica later. See HDFS-2791{code}\n   {quote}\nSolution is to be able to detect and capture a write-pipeline-failed replica as early as possible. First fix may be to change the check from 'isCompleted' to 'isCommitted'. This will capture write-pipeline-failed replicas reported just after commit and before 'complete' and mark them as corrupt.\n    {quote}\n the timegap between 'isCommitted' and 'isCompleted' is not so huge, so ideally this will not change much.\n\n{quote}\nThen to capture write-pipeline-failed replicas reported before commit, I am investigating if this can be solved by marking them as corrupt as part of commit. There already exists a check to find any mis-stamped replicas during commit but we only remove them from the blocksMap. In addition can we not mark such replicas as corrupt?\n{quote}\n\"setGenerationStampAndVerifyReplicas\" is just updating the inmemory states of the replicas being written in Namenode. its not changing the blocksMap. I dont think this is the right place to decide about the corrupt replicas.\n\nI think its always better to handle the block validations when reported from datanode, yes it takes time :(","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=vinayrpet","name":"vinayrpet","key":"vinayrpet","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Vinayakumar B","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-09-15T08:56:13.913+0000","updated":"2014-09-15T08:56:25.879+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12542071/comment/14133800","id":"14133800","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=usrikanth","name":"usrikanth","key":"usrikanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Srikanth Upputuri","active":true,"timeZone":"Asia/Kolkata"},"body":"[~vinayrpet], though I really don't see a reason why we should not delete a mis-stamped replica (during block report processing) after the block is committed, I agree with you that this improvement in early detection may be unnecessary (or even slightly risky?) particularly when the benefit is very little.\n\nCan I mark it duplicate of HDFS-3493?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=usrikanth","name":"usrikanth","key":"usrikanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Srikanth Upputuri","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-09-15T11:25:33.384+0000","updated":"2014-09-15T11:25:33.384+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12542071/comment/14138570","id":"14138570","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=usrikanth","name":"usrikanth","key":"usrikanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Srikanth Upputuri","active":true,"timeZone":"Asia/Kolkata"},"body":"Closed as duplicate of HDFS-3493. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=usrikanth","name":"usrikanth","key":"usrikanth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Srikanth Upputuri","active":true,"timeZone":"Asia/Kolkata"},"created":"2014-09-18T06:16:11.620+0000","updated":"2014-09-18T06:16:11.620+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-2932/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0jvj3:"}}