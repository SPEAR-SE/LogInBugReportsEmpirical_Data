{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12773906","self":"https://issues.apache.org/jira/rest/api/2/issue/12773906","key":"HDFS-7765","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2015-02-18T14:12:06.922+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jun 27 08:57:09 UTC 2018","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-7765/watchers","watchCount":17,"isWatching":false},"created":"2015-02-10T17:54:54.014+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327181","id":"12327181","description":"2.6.0 release","name":"2.6.0","archived":false,"released":true,"releaseDate":"2014-11-18"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=janmejay","name":"janmejay","key":"janmejay","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Janmejay Singh","active":true,"timeZone":"Etc/UTC"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-06-27T08:57:09.936+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"While running an Accumulo test, saw exceptions like the following while trying to write to write ahead log in HDFS. \n\nThe exception occurrs at [FSOutputSummer.java:76|https://github.com/apache/hadoop/blob/release-2.6.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSOutputSummer.java#L76] which is attempting to update a byte array.\n\n{noformat}\n2015-02-06 19:46:49,769 [log.DfsLogger] WARN : Exception syncing java.lang.reflect.InvocationTargetException\njava.lang.ArrayIndexOutOfBoundsException: 4608\n        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:76)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:50)\n        at java.io.DataOutputStream.write(DataOutputStream.java:88)\n        at java.io.DataOutputStream.writeByte(DataOutputStream.java:153)\n        at org.apache.accumulo.tserver.logger.LogFileKey.write(LogFileKey.java:87)\n        at org.apache.accumulo.tserver.log.DfsLogger.write(DfsLogger.java:526)\n        at org.apache.accumulo.tserver.log.DfsLogger.logFileData(DfsLogger.java:540)\n        at org.apache.accumulo.tserver.log.DfsLogger.logManyTablets(DfsLogger.java:573)\n        at org.apache.accumulo.tserver.log.TabletServerLogger$6.write(TabletServerLogger.java:373)\n        at org.apache.accumulo.tserver.log.TabletServerLogger.write(TabletServerLogger.java:274)\n        at org.apache.accumulo.tserver.log.TabletServerLogger.logManyTablets(TabletServerLogger.java:365)\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.flush(TabletServer.java:1667)\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.closeUpdate(TabletServer.java:1754)\n        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.accumulo.trace.instrument.thrift.RpcServerInvocationHandler.invoke(RpcServerInvocationHandler.java:46)\n        at org.apache.accumulo.server.util.RpcWrapper$1.invoke(RpcWrapper.java:47)\n        at com.sun.proxy.$Proxy22.closeUpdate(Unknown Source)\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$closeUpdate.getResult(TabletClientService.java:2370)\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$closeUpdate.getResult(TabletClientService.java:2354)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:168)\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:516)\n        at org.apache.accumulo.server.util.CustomNonBlockingServer$1.run(CustomNonBlockingServer.java:77)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\n        at java.lang.Thread.run(Thread.java:744)\n2015-02-06 19:46:49,769 [log.DfsLogger] WARN : Exception syncing java.lang.reflect.InvocationTargetException\n2015-02-06 19:46:49,772 [log.DfsLogger] ERROR: java.lang.ArrayIndexOutOfBoundsException: 4609\njava.lang.ArrayIndexOutOfBoundsException: 4609\n        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:76)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:50)\n        at java.io.DataOutputStream.write(DataOutputStream.java:88)\n        at java.io.DataOutputStream.writeByte(DataOutputStream.java:153)\n        at org.apache.accumulo.tserver.logger.LogFileKey.write(LogFileKey.java:87)\n        at org.apache.accumulo.tserver.log.DfsLogger.write(DfsLogger.java:526)\n        at org.apache.accumulo.tserver.log.DfsLogger.logFileData(DfsLogger.java:540)\n        at org.apache.accumulo.tserver.log.DfsLogger.logManyTablets(DfsLogger.java:573)\n        at org.apache.accumulo.tserver.log.TabletServerLogger$6.write(TabletServerLogger.java:373)\n        at org.apache.accumulo.tserver.log.TabletServerLogger.write(TabletServerLogger.java:274)\n        at org.apache.accumulo.tserver.log.TabletServerLogger.logManyTablets(TabletServerLogger.java:365)\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.flush(TabletServer.java:1667)\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.applyUpdates(TabletServer.java:1574)\n        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.accumulo.trace.instrument.thrift.RpcServerInvocationHandler.invoke(RpcServerInvocationHandler.java:46)\n        at org.apache.accumulo.server.util.RpcWrapper$1.invoke(RpcWrapper.java:47)\n        at com.sun.proxy.$Proxy22.applyUpdates(Unknown Source)\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2349)\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2335)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:168)\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:516)\n        at org.apache.accumulo.server.util.CustomNonBlockingServer$1.run(CustomNonBlockingServer.java:77)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\n        at java.lang.Thread.run(Thread.java:744)\n\n  .\n  .\n  .\n\njava.lang.ArrayIndexOutOfBoundsException: 4632\n        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:76)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:50)\n        at java.io.DataOutputStream.write(DataOutputStream.java:88)\n        at java.io.DataOutputStream.writeByte(DataOutputStream.java:153)\n        at org.apache.accumulo.tserver.logger.LogFileKey.write(LogFileKey.java:87)\n        at org.apache.accumulo.tserver.log.DfsLogger.write(DfsLogger.java:526)\n        at org.apache.accumulo.tserver.log.DfsLogger.logFileData(DfsLogger.java:540)\n        at org.apache.accumulo.tserver.log.DfsLogger.logManyTablets(DfsLogger.java:573)\n        at org.apache.accumulo.tserver.log.TabletServerLogger$6.write(TabletServerLogger.java:373)\n        at org.apache.accumulo.tserver.log.TabletServerLogger.write(TabletServerLogger.java:274)\n        at org.apache.accumulo.tserver.log.TabletServerLogger.logManyTablets(TabletServerLogger.java:365)\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.flush(TabletServer.java:1667)\n        at org.apache.accumulo.tserver.TabletServer$ThriftClientHandler.applyUpdates(TabletServer.java:1574)\n        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.accumulo.trace.instrument.thrift.RpcServerInvocationHandler.invoke(RpcServerInvocationHandler.java:46)\n        at org.apache.accumulo.server.util.RpcWrapper$1.invoke(RpcWrapper.java:47)\n        at com.sun.proxy.$Proxy22.applyUpdates(Unknown Source)\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2349)\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor$applyUpdates.getResult(TabletClientService.java:2335)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:168)\n        at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:516)\n        at org.apache.accumulo.server.util.CustomNonBlockingServer$1.run(CustomNonBlockingServer.java:77)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\n        at java.lang.Thread.run(Thread.java:744)\n{noformat}\n\nImmediately before the above exception occurred, the following exception was logged by a hdfs client background thread.\n\n{noformat}\n2015-02-06 19:46:49,767 [hdfs.DFSClient] WARN : DataStreamer Exception\njava.net.SocketTimeoutException: 70000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.1.2.17:59411 remote=/10.1.2.24:50010]\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at java.io.FilterInputStream.read(FilterInputStream.java:83)\n        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2201)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1142)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1112)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1253)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1004)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:548)\n{noformat}\n\nInformation on how accumulo opened the file.\n\n{noformat}\n2015-02-06 19:43:10,051 [fs.VolumeManagerImpl] DEBUG: Found CREATE enum CREATE\n2015-02-06 19:43:10,051 [fs.VolumeManagerImpl] DEBUG: Found synch enum SYNC_BLOCK\n2015-02-06 19:43:10,051 [fs.VolumeManagerImpl] DEBUG: CreateFlag set: [CREATE, SYNC_BLOCK]\n2015-02-06 19:43:10,051 [fs.VolumeManagerImpl] DEBUG: creating hdfs://ip-10-1-2-11:9000/accumulo/wal/ip-10-1-2-17+9997/2e548d95-d075-484d-abac-bdd877ea205b with SYNCH_BLOCK flag\n{noformat}","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12752219","id":"12752219","filename":"0001-PATCH-HDFS-7765-FSOutputSummer-throwing-ArrayIndexOu.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=janmejay","name":"janmejay","key":"janmejay","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Janmejay Singh","active":true,"timeZone":"Etc/UTC"},"created":"2015-08-25T11:52:18.101+0000","size":1641,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12752219/0001-PATCH-HDFS-7765-FSOutputSummer-throwing-ArrayIndexOu.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12699476","id":"12699476","filename":"HDFS-7765.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=surendrasingh","name":"surendrasingh","key":"surendrasingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=surendrasingh&avatarId=30759","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=surendrasingh&avatarId=30759","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=surendrasingh&avatarId=30759","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=surendrasingh&avatarId=30759"},"displayName":"Surendra Singh Lilhore","active":true,"timeZone":"Etc/UTC"},"created":"2015-02-18T14:14:51.490+0000","size":1942,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12699476/HDFS-7765.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"FSOutputSummer throwing ArrayIndexOutOfBoundsException","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kturner","name":"kturner","key":"kturner","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Keith Turner","active":true,"timeZone":"America/New_York"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kturner","name":"kturner","key":"kturner","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Keith Turner","active":true,"timeZone":"America/New_York"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Centos 6, Open JDK 7, Amazon EC2, Accumulo 1.6.2RC4","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12773906/comment/14325886","id":"14325886","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=surendrasingh","name":"surendrasingh","key":"surendrasingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=surendrasingh&avatarId=30759","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=surendrasingh&avatarId=30759","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=surendrasingh&avatarId=30759","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=surendrasingh&avatarId=30759"},"displayName":"Surendra Singh Lilhore","active":true,"timeZone":"Etc/UTC"},"body":"By seeing the logs we are getting ArrayIndexOutOfBoundsException when count reached 4608 (default buffer size 512*9=4608)\n\n{code}java.lang.ArrayIndexOutOfBoundsException: 4608{code}\n\nBut in code for each increment we are checking the buffer length with count, if it is equal we will flush it.\n\n{code}\nbuf[count++] = (byte)b;\n    if(count == buf.length) {\n      flushBuffer();\n    }\n{code}\n{code}\n    count += bytesToCopy;\n    if (count == buf.length) {\n      // local buffer is full\n      flushBuffer();\n    }\n{code}\nMeans something happened wrong and count reached till full length but buffer not flushed.\n\n\nAccording to me it is possible when {{write1(byte b[], int off, int len)}} and {{write(int b)}} executed parallel.\n\nSequence is like this\n1. In {{write1()}} method line {{count += bytesToCopy}} executed and count reached 4608.\n2. Now before {{if (count == buf.length)}} line, {{write()}} method executed.\n3. {{buf(count++] = (byte)b;}} will throw ArrayIndexOutOfBoundsException but it will increase the count to 4609.\n4. Now line {{if (count == buf.length)}} never true and continue.\n\nIf I am wrong please correct me, thanks in advance.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=surendrasingh","name":"surendrasingh","key":"surendrasingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=surendrasingh&avatarId=30759","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=surendrasingh&avatarId=30759","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=surendrasingh&avatarId=30759","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=surendrasingh&avatarId=30759"},"displayName":"Surendra Singh Lilhore","active":true,"timeZone":"Etc/UTC"},"created":"2015-02-18T14:12:06.922+0000","updated":"2015-02-18T14:12:06.922+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12773906/comment/14325889","id":"14325889","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=surendrasingh","name":"surendrasingh","key":"surendrasingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=surendrasingh&avatarId=30759","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=surendrasingh&avatarId=30759","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=surendrasingh&avatarId=30759","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=surendrasingh&avatarId=30759"},"displayName":"Surendra Singh Lilhore","active":true,"timeZone":"Etc/UTC"},"body":"Attached initial patch. Please review and give your commen","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=surendrasingh","name":"surendrasingh","key":"surendrasingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=surendrasingh&avatarId=30759","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=surendrasingh&avatarId=30759","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=surendrasingh&avatarId=30759","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=surendrasingh&avatarId=30759"},"displayName":"Surendra Singh Lilhore","active":true,"timeZone":"Etc/UTC"},"created":"2015-02-18T14:14:51.496+0000","updated":"2015-02-18T14:14:51.496+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12773906/comment/14325964","id":"14325964","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kturner","name":"kturner","key":"kturner","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Keith Turner","active":true,"timeZone":"America/New_York"},"body":"Seems like this could not happen.\n\n * In 2.6.0 {{write(byte b[], int off, int len)}} and {{write(int b)}} are both synchronized.\n * In 2.6.0 {{write1(byte b[], int off, int len)}} is only called by {{write(byte b[], int off, int len)}}.  Therefore {{write1(...)}} is effectively synchronized in 2.6.0.\n\nSo it does not seem possible that {{write1(...)}} could be executed concurrently w/ {{write(int b)}}.  Does this reasoning seem correct?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=kturner","name":"kturner","key":"kturner","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Keith Turner","active":true,"timeZone":"America/New_York"},"created":"2015-02-18T14:49:58.429+0000","updated":"2015-02-18T14:49:58.429+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12773906/comment/14711121","id":"14711121","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=janmejay","name":"janmejay","key":"janmejay","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Janmejay Singh","active":true,"timeZone":"Etc/UTC"},"body":"The attached patch won't work, because it doesn't keep the invariant that count will always be <= array size. Attaching another patch: 0001-PATCH-HDFS-7765-FSOutputSummer-throwing-ArrayIndexOu.patch\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=janmejay","name":"janmejay","key":"janmejay","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Janmejay Singh","active":true,"timeZone":"Etc/UTC"},"created":"2015-08-25T11:52:18.105+0000","updated":"2015-08-25T11:52:18.105+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12773906/comment/15832612","id":"15832612","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mserrano","name":"mserrano","key":"mserrano","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Serrano","active":true,"timeZone":"America/New_York"},"body":"We've seen this as well under conditions which generate large I/O waits:\n\n{noformat}\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 321984\n        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:76)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:50)\n        at java.io.DataOutputStream.writeInt(DataOutputStream.java:197)\n{noformat}\n\nCloudera 5.8.2 (HDFS 2.6.0)","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=mserrano","name":"mserrano","key":"mserrano","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Martin Serrano","active":true,"timeZone":"America/New_York"},"created":"2017-01-20T23:33:13.004+0000","updated":"2017-01-20T23:33:13.004+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12773906/comment/16290169","id":"16290169","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wankunde","name":"wankunde","key":"wankunde","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"wan kun","active":true,"timeZone":"Asia/Shanghai"},"body":"[~kturner]\r\n{code:java}\r\n public synchronized void write(int b) throws IOException {\r\n     buf[count++] = (byte)b;\r\n     if(count == buf.length) {\r\n       flushBuffer();\r\n{code}\r\nIf the flushBuffer() throw IOException in the first time ,the count will larger than buf.length  the second time.\r\n\r\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wankunde","name":"wankunde","key":"wankunde","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"wan kun","active":true,"timeZone":"Asia/Shanghai"},"created":"2017-12-14T00:58:37.798+0000","updated":"2017-12-14T00:58:37.798+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12773906/comment/16428532","id":"16428532","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jeagles","name":"jeagles","key":"jeagles","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jonathan Eagles","active":true,"timeZone":"America/Chicago"},"body":"[~janmejay], I think [~wankunde]'sÂ assessment matches my experience for when this issue happens. Once an IOException happens at max buffer size, this class becomes unusable.\r\n\r\nMuch like this other apache stream class as reference, flush if we can't write, then write. That way the state is not modified until safe. \r\nhttps://github.com/apache/commons-io/blob/master/src/main/java/org/apache/commons/io/output/ByteArrayOutputStream.java#L171\r\n{code}\r\n  public synchronized void write(int b) throws IOException {\r\n    int newcount = count + 1;\r\n    if (newcount > buf.length) {\r\n      flushBuffer();\r\n    }\r\n    buf[count++] = (byte)b;\r\n  }\r\n{code}\r\n\r\nI haven't checked the rest of the FSOutputSummer for correctness. That is worth checking.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jeagles","name":"jeagles","key":"jeagles","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jonathan Eagles","active":true,"timeZone":"America/Chicago"},"created":"2018-04-06T16:23:56.753+0000","updated":"2018-04-06T16:23:56.753+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12773906/comment/16524777","id":"16524777","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yuanbo","name":"yuanbo","key":"yuanbo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yuanbo&avatarId=25581","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yuanbo&avatarId=25581","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yuanbo&avatarId=25581","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yuanbo&avatarId=25581"},"displayName":"Yuanbo Liu","active":true,"timeZone":"Asia/Shanghai"},"body":"Any update about this issue? We've met the same issue in our env while writing sequence file into Hadoop Cluster.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=yuanbo","name":"yuanbo","key":"yuanbo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=yuanbo&avatarId=25581","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=yuanbo&avatarId=25581","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=yuanbo&avatarId=25581","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=yuanbo&avatarId=25581"},"displayName":"Yuanbo Liu","active":true,"timeZone":"Asia/Shanghai"},"created":"2018-06-27T08:57:09.936+0000","updated":"2018-06-27T08:57:09.936+0000"}],"maxResults":8,"total":8,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-7765/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i25fk7:"}}