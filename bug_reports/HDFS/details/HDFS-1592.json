{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12496444","self":"https://issues.apache.org/jira/rest/api/2/issue/12496444","key":"HDFS-1592","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12316319","id":"12316319","description":"","name":"0.20.204.0","archived":false,"released":true,"releaseDate":"2011-09-02"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12315571","id":"12315571","description":"","name":"0.23.0","archived":false,"released":true,"releaseDate":"2011-11-11"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2011-01-24T02:41:40.155+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Fri Sep 02 22:16:02 UTC 2011","customfield_12310420":"16439","customfield_12312320":null,"customfield_12310222":"10002_*:*_2_*:*_1121828846_*|*_1_*:*_2_*:*_9508633575_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2011-05-26T17:36:29.028+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-1592/watchers","watchCount":5,"isWatching":false},"created":"2011-01-23T16:42:06.732+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"6.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12316319","id":"12316319","description":"","name":"0.20.204.0","archived":false,"released":true,"releaseDate":"2011-09-02"}],"issuelinks":[{"id":"12360747","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12360747","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"inwardIssue":{"id":"12617586","key":"HADOOP-9091","self":"https://issues.apache.org/jira/rest/api/2/issue/12617586","fields":{"summary":"Allow daemon startup when at least 1 (or configurable) disk is in an OK state.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12339100","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12339100","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"inwardIssue":{"id":"12504734","key":"HDFS-1849","self":"https://issues.apache.org/jira/rest/api/2/issue/12504734","fields":{"summary":"Respect failed.volumes.tolerated on startup","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12337781","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12337781","type":{"id":"12310010","name":"Incorporates","inward":"is part of","outward":"incorporates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310010"},"inwardIssue":{"id":"12497098","key":"HDFS-2137","self":"https://issues.apache.org/jira/rest/api/2/issue/12497098","fields":{"summary":"Datanode Disk Fail Inplace","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/2","id":"2","description":"A new feature of the product, which has yet to be developed.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype","name":"New Feature","subtask":false,"avatarId":21141}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2012-11-26T11:23:28.939+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"Datanode startup doesn't honor volumes.tolerated for hadoop 20 version.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12478919","id":"12478919","filename":"HDFS-1592-1.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-12T00:39:29.479+0000","size":3698,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12478919/HDFS-1592-1.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12479026","id":"12479026","filename":"HDFS-1592-2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-12T22:54:09.300+0000","size":5182,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12479026/HDFS-1592-2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12479721","id":"12479721","filename":"HDFS-1592-3.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-19T05:19:58.269+0000","size":6040,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12479721/HDFS-1592-3.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12479883","id":"12479883","filename":"HDFS-1592-4.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-20T08:03:05.000+0000","size":9480,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12479883/HDFS-1592-4.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12480337","id":"12480337","filename":"HDFS-1592-5.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-24T22:32:31.298+0000","size":8516,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12480337/HDFS-1592-5.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12472397","id":"12472397","filename":"HDFS-1592-rel20.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-03-02T07:46:36.129+0000","size":4569,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12472397/HDFS-1592-rel20.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"13671","customfield_12312823":null,"summary":"Datanode startup doesn't honor volumes.tolerated ","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/12985495","id":"12985495","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"The feature was added in 0.21, so it isn't too surprising that it doesn't work in 0.20....","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2011-01-24T02:41:40.155+0000","updated":"2011-01-24T02:41:40.155+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13001321","id":"13001321","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Attached the patch for release 20. \n\nThis patch takes care of two things.\n1. When Datanode is started, it checks if volumes tolerated is honored. \n2. Also, volumes required is calculated correctly at the startup.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-03-02T07:46:36.179+0000","updated":"2011-03-02T07:46:36.179+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13022818","id":"13022818","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Closed HDFS-1849 which is a dupe. \n\nBharath, this needs to be fixed on trunk as well. Are you submitting a patch for trunk too?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-04-21T16:37:18.655+0000","updated":"2011-04-21T16:37:18.655+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13023125","id":"13023125","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Yes, I will be submitting for the trunk too.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-04-22T05:57:33.087+0000","updated":"2011-04-22T05:57:33.087+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13032206","id":"13032206","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Attaching the patch for 0.23 version.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-12T00:39:29.512+0000","updated":"2011-05-12T00:39:29.512+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13032682","id":"13032682","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"body":"1. There seems to be a redundancy in following conditions \n   (volsFailed > volFailuresTolerated)\nand  validVolsRequired > storage.getNumStorageDirs().\n\nSince both checks throw the same exception, I will recommend doing it in one condition.\n\n2. Please don't remove the DataNode.LOG.error.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-12T21:11:51.315+0000","updated":"2011-05-12T21:11:51.315+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13032701","id":"13032701","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for the review, Jitendra.\n\n1. The conditions are there for better readability. Yes, we can change this into one condition.\n\n2. Error is logged where the exception is caught.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-12T21:54:12.194+0000","updated":"2011-05-12T21:54:12.194+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13032735","id":"13032735","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Attaching patch which address the review comments. I have add some more tests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-12T22:54:09.335+0000","updated":"2011-05-12T22:54:09.335+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13033231","id":"13033231","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12479026/HDFS-1592-2.patch\n  against trunk revision 1102833.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 5 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.cli.TestHDFSCLI\n                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery\n                  org.apache.hadoop.hdfs.TestFileConcurrentReader\n                  org.apache.hadoop.tools.TestJMXGet\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/518//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/518//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/518//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-13T19:18:41.643+0000","updated":"2011-05-13T19:18:41.643+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13033343","id":"13033343","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"These failing tests are not related to this patch.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-13T22:16:22.386+0000","updated":"2011-05-13T22:16:22.386+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13033356","id":"13033356","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"body":"+1 for the patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-13T22:30:05.038+0000","updated":"2011-05-13T22:30:05.038+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13033875","id":"13033875","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"The intent of this jira (as I understand it, see HDFS-1849) is that the DN should start even if there are failed volumes, as long as the number of failed volumes is <= {{dfs.datanode.failed.volumes.tolerated}}. The use case is that an admin configures n volume failures to tolerate, then when the cluster is restarted all the nodes with less than n failed volumes should startup, ie restarting the DN should respect the {{dfs.datanode.failed.volumes.tolerated}} value so you don't end up with a cluster with DNs that were running successfully but fail to restart.\n\nWith the current patch the DN will refuse to come up if *any* of the volumes have failed, no matter how dfs.datanode.failed.volumes.tolerated is configured. We need tests that verifies:\n* A DN will successfully start  with a failed volume as long as it's configured to tolerate a failed volume\n* A DN will fail to start if more than the number of tolerated volumes are failed\n\nMake sense?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-05-16T05:16:41.389+0000","updated":"2011-05-16T05:21:28.243+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13033885","id":"13033885","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Yes, what you mentioned w.r.t usecases are right.\n\n    * A DN will successfully start with a failed volume as long as it's configured to tolerate a failed volume\n    * A DN will fail to start if more than the number of tolerated volumes are failed\n\nThis is the expected behavior with this patch. \n\nI had some difficulty in failing the disks through the unit tests. If we set the directory permissions to not writable, then once we run datanode, it will reset the directory permissions and test will always succeed. \n\nThese tests were done outside of unit tests through umount -l etc. All the above mentioned cases were manually tested. \n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-16T06:01:08.376+0000","updated":"2011-05-16T06:01:08.376+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13034428","id":"13034428","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Did you test the patch on trunk?\n\nCurrently if a storage directory has failed the BPOfferService daemon will fail to start. This patch only throws an exception if there are an insufficient number of valid volumes, it doesn't do anything to ensure that the BP actually comes up even if there is a failed storage directory. Ie it doesn't implement the expected behavior.\n\nYou should be able to write a test that fails a volume using Mockito (see examples in other tests), the fault injection framework, or via having the test manage the data dirs itself (eg pass false for the 3rd argument in startDataNodes) and fail them individually yourself.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-05-16T23:39:58.590+0000","updated":"2011-05-16T23:39:58.590+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13035533","id":"13035533","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Eli, thanks for your review and comments. \n\nYes, I have tested against trunk. How did you test this? Did you configure volumes tolerated correctly?\nThe expected behavior is - if volumes failed are more than volumes tolerated, BPOfferService daemon will fail to start.\n\nAlso, note that, i have filed another Jira for - if all BPService exit due to some reason, Datanode should exit. This is a bug in the current code.  \n\nPlease see the following four tests i have performed and their outcome on trunk.\n\nCase 1: One disk failure (/grid/2) and Vol Tolerated = 0. Outcome: BP Service should exit.\n\n11/05/18 07:48:56 WARN common.Util: Path /grid/0/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 07:48:56 WARN common.Util: Path /grid/1/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 07:48:56 WARN common.Util: Path /grid/2/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 07:48:56 WARN common.Util: Path /grid/3/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 07:48:56 WARN datanode.DataNode: Invalid directory in: dfs.datanode.data.dir: \njava.io.FileNotFoundException: File file:/grid/2/testing/hadoop-logs/dfs/data does not exist.\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:424)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:315)\n\tat org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:131)\n\tat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:148)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.getDataDirsFromURIs(DataNode.java:2154)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2133)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2074)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2097)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2240)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2250)\n11/05/18 07:48:56 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n11/05/18 07:48:56 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n11/05/18 07:48:56 INFO impl.MetricsSystemImpl: DataNode metrics system started\n11/05/18 07:48:56 INFO impl.MetricsSystemImpl: Registered source UgiMetrics\n11/05/18 07:48:56 INFO datanode.DataNode: Opened info server at 50010\n11/05/18 07:48:56 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s\n11/05/18 07:48:56 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n11/05/18 07:48:56 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)\n11/05/18 07:48:56 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50075\n11/05/18 07:48:56 INFO http.HttpServer: listener.getLocalPort() returned 50075 webServer.getConnectors()[0].getLocalPort() returned 50075\n11/05/18 07:48:56 INFO http.HttpServer: Jetty bound to port 50075\n11/05/18 07:48:56 INFO mortbay.log: jetty-6.1.14\n11/05/18 07:48:56 WARN mortbay.log: Can't reuse /tmp/Jetty_0_0_0_0_50075_datanode____hwtdwq, using /tmp/Jetty_0_0_0_0_50075_datanode____hwtdwq_6441176730816569391\n11/05/18 07:49:01 INFO mortbay.log: Started SelectChannelConnector@0.0.0.0:50075\n11/05/18 07:49:01 INFO ipc.Server: Starting Socket Reader #1 for port 50020\n11/05/18 07:49:01 INFO ipc.Server: Starting Socket Reader #2 for port 50020\n11/05/18 07:49:01 INFO ipc.Server: Starting Socket Reader #3 for port 50020\n11/05/18 07:49:01 INFO ipc.Server: Starting Socket Reader #4 for port 50020\n11/05/18 07:49:01 INFO ipc.Server: Starting Socket Reader #5 for port 50020\n11/05/18 07:49:01 INFO impl.MetricsSystemImpl: Registered source RpcActivityForPort50020\n11/05/18 07:49:01 INFO impl.MetricsSystemImpl: Registered source RpcDetailedActivityForPort50020\n11/05/18 07:49:01 INFO impl.MetricsSystemImpl: Registered source JvmMetrics\n11/05/18 07:49:01 INFO impl.MetricsSystemImpl: Registered source DataNodeActivity-hadooplab40.yst.corp.yahoo.com-50010\n11/05/18 07:49:01 INFO datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=, infoPort=50075, ipcPort=50020, storageInfo=lv=0;cid=;nsid=0;c=0)In BPOfferService.run, data = null;bp=null\n11/05/18 07:49:01 INFO ipc.Server: IPC Server Responder: starting\n11/05/18 07:49:01 INFO ipc.Server: IPC Server listener on 50020: starting\n11/05/18 07:49:01 INFO ipc.Server: IPC Server handler 0 on 50020: starting\n11/05/18 07:49:01 INFO ipc.Server: IPC Server handler 1 on 50020: starting\n11/05/18 07:49:01 INFO ipc.Server: IPC Server handler 2 on 50020: starting\n11/05/18 07:49:01 INFO datanode.DataNode: handshake: namespace info = lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/18 07:49:01 INFO common.Storage: Locking is disabled\n11/05/18 07:49:01 INFO common.Storage: Locking is disabled\n11/05/18 07:49:01 INFO common.Storage: Locking is disabled\n11/05/18 07:49:01 INFO datanode.DataNode: setting up storage: nsid=0;bpid=BP-1694914230-10.72.86.55-1305704227822;lv=-35;nsInfo=lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/18 07:49:01 FATAL datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=DS-340618566-10.72.86.55-50010-1305704313207, infoPort=50075, ipcPort=50020, storageInfo=lv=-35;cid=test;nsid=413952175;c=0) initialization failed for block pool BP-1694914230-10.72.86.55-1305704227822\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Invalid value for volumes required - validVolsRequired: 4, Current valid volumes: 3, volsConfigured: 4, volFailuresTolerated: 0\n\tat org.apache.hadoop.hdfs.server.datanode.FSDataset.<init>(FSDataset.java:1160)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initFsDataSet(DataNode.java:1420)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.access$1100(DataNode.java:169)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.setupBPStorage(DataNode.java:804)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.setupBP(DataNode.java:774)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.run(DataNode.java:1191)\n\tat java.lang.Thread.run(Thread.java:619)\n11/05/18 07:49:01 WARN datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=DS-340618566-10.72.86.55-50010-1305704313207, infoPort=50075, ipcPort=50020, storageInfo=lv=-35;cid=test;nsid=413952175;c=0) ending block pool service for: BP-1694914230-10.72.86.55-1305704227822\n\n\n\nCase 2: One disk failure (/grid/2) and Vol Tolerated = 1. Outcome: BP Service should not exit \n\n11/05/18 08:48:39 WARN common.Util: Path /grid/0/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 08:48:39 WARN common.Util: Path /grid/1/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 08:48:39 WARN common.Util: Path /grid/2/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 08:48:39 WARN common.Util: Path /grid/3/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 08:48:39 WARN datanode.DataNode: Invalid directory in: dfs.datanode.data.dir: \njava.io.FileNotFoundException: File file:/grid/2/testing/hadoop-logs/dfs/data does not exist.\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:424)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:315)\n\tat org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:131)\n\tat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:148)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.getDataDirsFromURIs(DataNode.java:2154)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2133)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2074)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2097)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2240)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2250)\n11/05/18 08:48:40 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n11/05/18 08:48:40 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n11/05/18 08:48:40 INFO impl.MetricsSystemImpl: DataNode metrics system started\n11/05/18 08:48:40 INFO impl.MetricsSystemImpl: Registered source UgiMetrics\n11/05/18 08:48:40 INFO datanode.DataNode: Opened info server at 50010\n11/05/18 08:48:40 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s\n11/05/18 08:48:40 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n11/05/18 08:48:40 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)\n11/05/18 08:48:40 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50075\n11/05/18 08:48:40 INFO http.HttpServer: listener.getLocalPort() returned 50075 webServer.getConnectors()[0].getLocalPort() returned 50075\n11/05/18 08:48:40 INFO http.HttpServer: Jetty bound to port 50075\n11/05/18 08:48:40 INFO mortbay.log: jetty-6.1.14\n11/05/18 08:48:40 WARN mortbay.log: Can't reuse /tmp/Jetty_0_0_0_0_50075_datanode____hwtdwq, using /tmp/Jetty_0_0_0_0_50075_datanode____hwtdwq_4334063446071982759\n11/05/18 08:48:40 INFO mortbay.log: Started SelectChannelConnector@0.0.0.0:50075\n11/05/18 08:48:40 INFO ipc.Server: Starting Socket Reader #1 for port 50020\n11/05/18 08:48:40 INFO ipc.Server: Starting Socket Reader #2 for port 50020\n11/05/18 08:48:40 INFO ipc.Server: Starting Socket Reader #3 for port 50020\n11/05/18 08:48:40 INFO ipc.Server: Starting Socket Reader #4 for port 50020\n11/05/18 08:48:40 INFO ipc.Server: Starting Socket Reader #5 for port 50020\n11/05/18 08:48:40 INFO impl.MetricsSystemImpl: Registered source RpcActivityForPort50020\n11/05/18 08:48:40 INFO impl.MetricsSystemImpl: Registered source RpcDetailedActivityForPort50020\n11/05/18 08:48:40 INFO impl.MetricsSystemImpl: Registered source JvmMetrics\n11/05/18 08:48:40 INFO impl.MetricsSystemImpl: Registered source DataNodeActivity-hadooplab40.yst.corp.yahoo.com-50010\n11/05/18 08:48:40 INFO datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=, infoPort=50075, ipcPort=50020, storageInfo=lv=0;cid=;nsid=0;c=0)In BPOfferService.run, data = null;bp=null\n11/05/18 08:48:40 INFO ipc.Server: IPC Server Responder: starting\n11/05/18 08:48:40 INFO ipc.Server: IPC Server listener on 50020: starting\n11/05/18 08:48:40 INFO ipc.Server: IPC Server handler 0 on 50020: starting\n11/05/18 08:48:40 INFO ipc.Server: IPC Server handler 1 on 50020: starting\n11/05/18 08:48:40 INFO ipc.Server: IPC Server handler 2 on 50020: starting\n11/05/18 08:48:40 INFO datanode.DataNode: handshake: namespace info = lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/18 08:48:40 INFO common.Storage: Locking is disabled\n11/05/18 08:48:40 INFO common.Storage: Locking is disabled\n11/05/18 08:48:40 INFO common.Storage: Locking is disabled\n11/05/18 08:48:40 INFO datanode.DataNode: setting up storage: nsid=0;bpid=BP-1694914230-10.72.86.55-1305704227822;lv=-35;nsInfo=lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/18 08:48:40 INFO datanode.DataNode: FSDataset added volume - /grid/0/testing/hadoop-logs/dfs/data/current\n11/05/18 08:48:40 INFO datanode.DataNode: FSDataset added volume - /grid/1/testing/hadoop-logs/dfs/data/current\n11/05/18 08:48:40 INFO datanode.DataNode: FSDataset added volume - /grid/3/testing/hadoop-logs/dfs/data/current\n11/05/18 08:48:40 INFO datanode.DataNode: Registered FSDatasetState MBean\n11/05/18 08:48:40 INFO datanode.DataNode: Adding block pool BP-1694914230-10.72.86.55-1305704227822\n11/05/18 08:48:40 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1305719925918 with interval 21600000\n11/05/18 08:48:40 INFO datanode.DataNode: in register: sid=DS-340618566-10.72.86.55-50010-1305704313207;SI=lv=-35;cid=test;nsid=413952175;c=0\n11/05/18 08:48:40 INFO datanode.DataNode: bpReg after =lv=-35;cid=test;nsid=413952175;c=0;sid=DS-340618566-10.72.86.55-50010-1305704313207;name=127.0.0.1:50010\n11/05/18 08:48:40 INFO datanode.DataNode: in register:;bpDNR=lv=-35;cid=test;nsid=413952175;c=0\n11/05/18 08:48:40 INFO datanode.DataNode: For namenode localhost/127.0.0.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n11/05/18 08:48:40 INFO datanode.DataNode: BlockReport of 0 blocks got processed in 4 msecs\n11/05/18 08:48:40 INFO datanode.DataNode: sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.DatanodeCommand$Finalize@135ae7e\n11/05/18 08:48:40 INFO datanode.BlockPoolSliceScanner: Periodic Block Verification scan initialized with interval 1814400000.\n11/05/18 08:48:40 INFO datanode.DataBlockScanner: Added bpid=BP-1694914230-10.72.86.55-1305704227822 to blockPoolScannerMap, new size=1\n11/05/18 08:48:45 INFO datanode.BlockPoolSliceScanner: Starting a new period : work left in prev period : 0.00%\n\n\nCase 3: All good volumes and Vol Tolerated = 1. Outcome: BP Service should not exit.\n\n11/05/18 09:18:56 WARN common.Util: Path /grid/0/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 09:18:56 WARN common.Util: Path /grid/1/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 09:18:56 WARN common.Util: Path /grid/2/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 09:18:56 WARN common.Util: Path /grid/3/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 09:18:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n11/05/18 09:18:56 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n11/05/18 09:18:56 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n11/05/18 09:18:56 INFO impl.MetricsSystemImpl: DataNode metrics system started\n11/05/18 09:18:56 INFO impl.MetricsSystemImpl: Registered source UgiMetrics\n11/05/18 09:18:56 INFO datanode.DataNode: Opened info server at 50010\n11/05/18 09:18:56 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s\n11/05/18 09:18:56 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n11/05/18 09:18:56 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)\n11/05/18 09:18:56 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50075\n11/05/18 09:18:56 INFO http.HttpServer: listener.getLocalPort() returned 50075 webServer.getConnectors()[0].getLocalPort() returned 50075\n11/05/18 09:18:56 INFO http.HttpServer: Jetty bound to port 50075\n11/05/18 09:18:56 INFO mortbay.log: jetty-6.1.14\n11/05/18 09:18:56 WARN mortbay.log: Can't reuse /tmp/Jetty_0_0_0_0_50075_datanode____hwtdwq, using /tmp/Jetty_0_0_0_0_50075_datanode____hwtdwq_5832726280495656689\n11/05/18 09:18:56 INFO mortbay.log: Started SelectChannelConnector@0.0.0.0:50075\n11/05/18 09:18:57 INFO ipc.Server: Starting Socket Reader #1 for port 50020\n11/05/18 09:18:57 INFO ipc.Server: Starting Socket Reader #2 for port 50020\n11/05/18 09:18:57 INFO ipc.Server: Starting Socket Reader #3 for port 50020\n11/05/18 09:18:57 INFO ipc.Server: Starting Socket Reader #4 for port 50020\n11/05/18 09:18:57 INFO ipc.Server: Starting Socket Reader #5 for port 50020\n11/05/18 09:18:57 INFO impl.MetricsSystemImpl: Registered source RpcActivityForPort50020\n11/05/18 09:18:57 INFO impl.MetricsSystemImpl: Registered source RpcDetailedActivityForPort50020\n11/05/18 09:18:57 INFO impl.MetricsSystemImpl: Registered source JvmMetrics\n11/05/18 09:18:57 INFO impl.MetricsSystemImpl: Registered source DataNodeActivity-hadooplab40.yst.corp.yahoo.com-50010\n11/05/18 09:18:57 INFO datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=, infoPort=50075, ipcPort=50020, storageInfo=lv=0;cid=;nsid=0;c=0)In BPOfferService.run, data = null;bp=null\n11/05/18 09:18:57 INFO ipc.Server: IPC Server Responder: starting\n11/05/18 09:18:57 INFO ipc.Server: IPC Server listener on 50020: starting\n11/05/18 09:18:57 INFO ipc.Server: IPC Server handler 1 on 50020: starting\n11/05/18 09:18:57 INFO ipc.Server: IPC Server handler 0 on 50020: starting\n11/05/18 09:18:57 INFO ipc.Server: IPC Server handler 2 on 50020: starting\n11/05/18 09:18:57 INFO datanode.DataNode: handshake: namespace info = lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/18 09:18:57 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data is not formatted.\n11/05/18 09:18:57 INFO common.Storage: Formatting ...\n11/05/18 09:18:57 INFO common.Storage: Locking is disabled\n11/05/18 09:18:57 INFO common.Storage: Locking is disabled\n11/05/18 09:18:57 INFO common.Storage: Locking is disabled\n11/05/18 09:18:57 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 is not formatted.\n11/05/18 09:18:57 INFO common.Storage: Formatting ...\n11/05/18 09:18:57 INFO common.Storage: Formatting block pool BP-1694914230-10.72.86.55-1305704227822 directory /grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822/current\n11/05/18 09:18:57 INFO common.Storage: Locking is disabled\n11/05/18 09:18:57 INFO datanode.DataNode: setting up storage: nsid=413952175;bpid=BP-1694914230-10.72.86.55-1305704227822;lv=-35;nsInfo=lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/18 09:18:57 INFO datanode.DataNode: FSDataset added volume - /grid/0/testing/hadoop-logs/dfs/data/current\n11/05/18 09:18:57 INFO datanode.DataNode: FSDataset added volume - /grid/1/testing/hadoop-logs/dfs/data/current\n11/05/18 09:18:57 INFO datanode.DataNode: FSDataset added volume - /grid/2/testing/hadoop-logs/dfs/data/current\n11/05/18 09:18:57 INFO datanode.DataNode: FSDataset added volume - /grid/3/testing/hadoop-logs/dfs/data/current\n11/05/18 09:18:57 INFO datanode.DataNode: Registered FSDatasetState MBean\n11/05/18 09:18:57 INFO datanode.DataNode: Adding block pool BP-1694914230-10.72.86.55-1305704227822\n11/05/18 09:18:57 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1305728372371 with interval 21600000\n11/05/18 09:18:57 INFO datanode.DataNode: in register: sid=DS-340618566-10.72.86.55-50010-1305704313207;SI=lv=-35;cid=test;nsid=413952175;c=0\n11/05/18 09:18:57 INFO datanode.DataNode: bpReg after =lv=-35;cid=test;nsid=413952175;c=0;sid=DS-340618566-10.72.86.55-50010-1305704313207;name=127.0.0.1:50010\n11/05/18 09:18:57 INFO datanode.DataNode: in register:;bpDNR=lv=-35;cid=test;nsid=413952175;c=0\n11/05/18 09:18:57 INFO datanode.DataNode: For namenode localhost/127.0.0.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n11/05/18 09:18:57 INFO datanode.DataNode: BlockReport of 0 blocks got processed in 4 msecs\n11/05/18 09:18:57 INFO datanode.DataNode: sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.DatanodeCommand$Finalize@8de972\n11/05/18 09:18:57 INFO datanode.BlockPoolSliceScanner: Periodic Block Verification scan initialized with interval 1814400000.\n11/05/18 09:18:57 INFO datanode.DataBlockScanner: Added bpid=BP-1694914230-10.72.86.55-1305704227822 to blockPoolScannerMap, new size=1\n11/05/18 09:19:02 INFO datanode.BlockPoolSliceScanner: Starting a new period : work left in prev period : 0.00%\n\nCase 4: All good volumes and Vol Tolerated = 0. Outcome: BP Service should not exit.\n\n11/05/18 09:24:16 WARN common.Util: Path /grid/0/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 09:24:16 WARN common.Util: Path /grid/1/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 09:24:16 WARN common.Util: Path /grid/2/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 09:24:16 WARN common.Util: Path /grid/3/testing/hadoop-logs/dfs/data should be specified as a URI in configuration files. Please update hdfs configuration.\n11/05/18 09:24:16 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n11/05/18 09:24:16 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n11/05/18 09:24:16 INFO impl.MetricsSystemImpl: DataNode metrics system started\n11/05/18 09:24:16 INFO impl.MetricsSystemImpl: Registered source UgiMetrics\n11/05/18 09:24:16 INFO datanode.DataNode: Opened info server at 50010\n11/05/18 09:24:16 INFO datanode.DataNode: Balancing bandwith is 1048576 bytes/s\n11/05/18 09:24:16 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n11/05/18 09:24:16 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)\n11/05/18 09:24:16 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50075\n11/05/18 09:24:16 INFO http.HttpServer: listener.getLocalPort() returned 50075 webServer.getConnectors()[0].getLocalPort() returned 50075\n11/05/18 09:24:16 INFO http.HttpServer: Jetty bound to port 50075\n11/05/18 09:24:16 INFO mortbay.log: jetty-6.1.14\n11/05/18 09:24:16 WARN mortbay.log: Can't reuse /tmp/Jetty_0_0_0_0_50075_datanode____hwtdwq, using /tmp/Jetty_0_0_0_0_50075_datanode____hwtdwq_5258458250806180443\n11/05/18 09:24:17 INFO mortbay.log: Started SelectChannelConnector@0.0.0.0:50075\n11/05/18 09:24:17 INFO ipc.Server: Starting Socket Reader #1 for port 50020\n11/05/18 09:24:17 INFO ipc.Server: Starting Socket Reader #2 for port 50020\n11/05/18 09:24:17 INFO ipc.Server: Starting Socket Reader #3 for port 50020\n11/05/18 09:24:17 INFO ipc.Server: Starting Socket Reader #4 for port 50020\n11/05/18 09:24:17 INFO ipc.Server: Starting Socket Reader #5 for port 50020\n11/05/18 09:24:17 INFO impl.MetricsSystemImpl: Registered source RpcActivityForPort50020\n11/05/18 09:24:17 INFO impl.MetricsSystemImpl: Registered source RpcDetailedActivityForPort50020\n11/05/18 09:24:17 INFO impl.MetricsSystemImpl: Registered source JvmMetrics\n11/05/18 09:24:17 INFO impl.MetricsSystemImpl: Registered source DataNodeActivity-hadooplab40.yst.corp.yahoo.com-50010\n11/05/18 09:24:17 INFO datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=, infoPort=50075, ipcPort=50020, storageInfo=lv=0;cid=;nsid=0;c=0)In BPOfferService.run, data = null;bp=null\n11/05/18 09:24:17 INFO ipc.Server: IPC Server Responder: starting\n11/05/18 09:24:17 INFO ipc.Server: IPC Server listener on 50020: starting\n11/05/18 09:24:17 INFO ipc.Server: IPC Server handler 0 on 50020: starting\n11/05/18 09:24:17 INFO ipc.Server: IPC Server handler 1 on 50020: starting\n11/05/18 09:24:17 INFO ipc.Server: IPC Server handler 2 on 50020: starting\n11/05/18 09:24:17 INFO datanode.DataNode: handshake: namespace info = lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/18 09:24:17 INFO common.Storage: Locking is disabled\n11/05/18 09:24:17 INFO common.Storage: Locking is disabled\n11/05/18 09:24:17 INFO common.Storage: Locking is disabled\n11/05/18 09:24:17 INFO common.Storage: Locking is disabled\n11/05/18 09:24:17 INFO datanode.DataNode: setting up storage: nsid=0;bpid=BP-1694914230-10.72.86.55-1305704227822;lv=-35;nsInfo=lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/18 09:24:17 INFO datanode.DataNode: FSDataset added volume - /grid/0/testing/hadoop-logs/dfs/data/current\n11/05/18 09:24:17 INFO datanode.DataNode: FSDataset added volume - /grid/1/testing/hadoop-logs/dfs/data/current\n11/05/18 09:24:17 INFO datanode.DataNode: FSDataset added volume - /grid/2/testing/hadoop-logs/dfs/data/current\n11/05/18 09:24:17 INFO datanode.DataNode: FSDataset added volume - /grid/3/testing/hadoop-logs/dfs/data/current\n11/05/18 09:24:17 INFO datanode.DataNode: Registered FSDatasetState MBean\n11/05/18 09:24:17 INFO datanode.DataNode: Adding block pool BP-1694914230-10.72.86.55-1305704227822\n11/05/18 09:24:17 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1305719970633 with interval 21600000\n11/05/18 09:24:17 INFO datanode.DataNode: in register: sid=DS-340618566-10.72.86.55-50010-1305704313207;SI=lv=-35;cid=test;nsid=413952175;c=0\n11/05/18 09:24:17 INFO datanode.DataNode: bpReg after =lv=-35;cid=test;nsid=413952175;c=0;sid=DS-340618566-10.72.86.55-50010-1305704313207;name=127.0.0.1:50010\n11/05/18 09:24:17 INFO datanode.DataNode: in register:;bpDNR=lv=-35;cid=test;nsid=413952175;c=0\n11/05/18 09:24:17 INFO datanode.DataNode: For namenode localhost/127.0.0.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n11/05/18 09:24:17 INFO datanode.DataNode: BlockReport of 0 blocks got processed in 4 msecs\n11/05/18 09:24:17 INFO datanode.DataNode: sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.DatanodeCommand$Finalize@18c5e67\n11/05/18 09:24:17 INFO datanode.BlockPoolSliceScanner: Periodic Block Verification scan initialized with interval 1814400000.\n11/05/18 09:24:17 INFO datanode.DataBlockScanner: Added bpid=BP-1694914230-10.72.86.55-1305704227822 to blockPoolScannerMap, new size=1\n11/05/18 09:24:22 INFO datanode.BlockPoolSliceScanner: Starting a new period : work left in prev period : 0.00%","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-18T18:07:32.192+0000","updated":"2011-05-18T18:07:32.192+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13035834","id":"13035834","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks for the info Bharath. I tested on trunk, but also when I filed HDFS-1849 I knew the current code wouldn't tolerate a failed volume. There's an issue with the 2nd test case:\n\n{quote}\nCase 2: One disk failure (/grid/2) and Vol Tolerated = 1. Outcome: BP Service should not exit\n...\n11/05/18 08:48:39 WARN datanode.DataNode: Invalid directory in: dfs.datanode.data.dir: \njava.io.FileNotFoundException: File file:/grid/2/testing/hadoop-logs/dfs/data does not exist.\n{quote}\n\nA missing data directory is not a disk failure, the datanode will happily notice it and recreate the directory successfully. \n\nIf you swap out a disk from a host or just make part of the data directory inaccessible, eg by changing the perms on the host file system, you'll see that this is a fatal error for the DN, eg\n\n{noformat}\n11/05/18 15:57:23 FATAL datanode.DataNode: DatanodeRegistration(localhost.localdomain:50010, storageID=, infoPort=50075, ipcPort=50020, storageInfo=lv=0;cid=;nsid=0;c=0) initialization failed for block pool BP-1288327361-127.0.0.1-1305593076974\njava.io.IOException: Cannot remove current directory: /home/eli/hadoop-dirs1/dfs/data1/current\n\tat org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.clearDirectory(Storage.java:332)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.format(DataStorage.java:264)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:166)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:216)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.setupBPStorage(DataNode.java:797)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.setupBP(DataNode.java:774)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.run(DataNode.java:1186)\n\tat java.lang.Thread.run(Thread.java:662)\n{noformat}\n\n\nYour four test cases are great. Please write a unit test for each. This way we can make sure the patch works for each and that future changes don't break this feature.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-05-18T23:03:31.881+0000","updated":"2011-05-18T23:04:03.747+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13035987","id":"13035987","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Attaching a patch which addresses Eli's comments.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-19T05:19:58.307+0000","updated":"2011-05-19T05:19:58.307+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13035990","id":"13035990","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"First, Thank you for identifying this issue, Eli. Great job!\n\nCouple of comments,\n1. We did test couple of things like masking permissions still dfs level. That didn't catch this issue. You pointed in making specific directory permissions helped us to reproduce this case. Thanks again.\n2. We tested by unmounting disks also.\n3. Then we tested with injecting failures at kernel level. \n\nRegarding testcases,\nI agree with you that we need more tests, But I think, we should do that in another jira. Since, we have already spent lot of effort in manual testing. Can we file another Jira to track this? \n\nWith this new patch, i have tested following new cases. Can you please review and provide your feedback?\n\ncase 1: All four good volumes, Vol Tolerated=1, expected outcome = BPservice should start\n\n11/05/19 04:57:51 INFO datanode.DataNode: FSDataset added volume - /grid/0/testing/hadoop-logs/dfs/data/current\n11/05/19 04:57:51 INFO datanode.DataNode: FSDataset added volume - /grid/1/testing/hadoop-logs/dfs/data/current\n11/05/19 04:57:51 INFO datanode.DataNode: FSDataset added volume - /grid/2/testing/hadoop-logs/dfs/data/current\n11/05/19 04:57:51 INFO datanode.DataNode: FSDataset added volume - /grid/3/testing/hadoop-logs/dfs/data/current\n11/05/19 04:57:51 INFO datanode.DataNode: Registered FSDatasetState MBean\n11/05/19 04:57:51 INFO datanode.DataNode: Adding block pool BP-1694914230-10.72.86.55-1305704227822\n11/05/19 04:57:51 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1305782678947 with interval 21600000\n11/05/19 04:57:51 INFO datanode.DataNode: in register: sid=DS-340618566-10.72.86.55-50010-1305704313207;SI=lv=-35;cid=test;nsid=413952175;c=0\n11/05/19 04:57:51 INFO datanode.DataNode: bpReg after =lv=-35;cid=test;nsid=413952175;c=0;sid=DS-340618566-10.72.86.55-50010-1305704313207;name=127.0.0.1:50010\n11/05/19 04:57:51 INFO datanode.DataNode: in register:;bpDNR=lv=-35;cid=test;nsid=413952175;c=0\n11/05/19 04:57:51 INFO datanode.DataNode: For namenode localhost/127.0.0.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n11/05/19 04:57:51 INFO datanode.DataNode: BlockReport of 0 blocks got processed in 3 msecs\n11/05/19 04:57:51 INFO datanode.DataNode: sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.DatanodeCommand$Finalize@3e5a91\n11/05/19 04:57:51 INFO datanode.BlockPoolSliceScanner: Periodic Block Verification scan initialized with interval 1814400000.\n11/05/19 04:57:51 INFO datanode.DataBlockScanner: Added bpid=BP-1694914230-10.72.86.55-1305704227822 to blockPoolScannerMap, new size=1\n11/05/19 04:57:56 INFO datanode.BlockPoolSliceScanner: Starting a new period : work left in prev period : 0.00%\n\ncase 2: One failed volume(/grid/2), three good volumes, Vol Tolerated=1, expected outcome = BPService should start\n\n11/05/19 05:01:27 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data is not formatted.\n11/05/19 05:01:27 INFO common.Storage: Formatting ...\n11/05/19 05:01:27 WARN common.Storage: Invalid directory in: /grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822: File file:/grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:01:27 INFO common.Storage: Locking is disabled\n11/05/19 05:01:27 INFO common.Storage: Locking is disabled\n11/05/19 05:01:27 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:01:27 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:01:27 INFO common.Storage: Locking is disabled\n11/05/19 05:01:27 INFO datanode.DataNode: setting up storage: nsid=0;bpid=BP-1694914230-10.72.86.55-1305704227822;lv=-35;nsInfo=lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/19 05:01:27 INFO datanode.DataNode: FSDataset added volume - /grid/0/testing/hadoop-logs/dfs/data/current\n11/05/19 05:01:27 INFO datanode.DataNode: FSDataset added volume - /grid/1/testing/hadoop-logs/dfs/data/current\n11/05/19 05:01:27 INFO datanode.DataNode: FSDataset added volume - /grid/3/testing/hadoop-logs/dfs/data/current\n11/05/19 05:01:27 INFO datanode.DataNode: Registered FSDatasetState MBean\n11/05/19 05:01:27 INFO datanode.DataNode: Adding block pool BP-1694914230-10.72.86.55-1305704227822\n11/05/19 05:01:27 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1305789604425 with interval 21600000\n11/05/19 05:01:27 INFO datanode.DataNode: in register: sid=DS-340618566-10.72.86.55-50010-1305704313207;SI=lv=-35;cid=test;nsid=413952175;c=0\n11/05/19 05:01:27 INFO datanode.DataNode: bpReg after =lv=-35;cid=test;nsid=413952175;c=0;sid=DS-340618566-10.72.86.55-50010-1305704313207;name=127.0.0.1:50010\n11/05/19 05:01:27 INFO datanode.DataNode: in register:;bpDNR=lv=-35;cid=test;nsid=413952175;c=0\n11/05/19 05:01:27 INFO datanode.DataNode: For namenode localhost/127.0.0.1:8020 using BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n11/05/19 05:01:27 INFO datanode.DataNode: BlockReport of 0 blocks got processed in 4 msecs\n11/05/19 05:01:27 INFO datanode.DataNode: sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.DatanodeCommand$Finalize@1adb7b8\n11/05/19 05:01:27 INFO datanode.BlockPoolSliceScanner: Periodic Block Verification scan initialized with interval 1814400000.\n11/05/19 05:01:27 INFO datanode.DataBlockScanner: Added bpid=BP-1694914230-10.72.86.55-1305704227822 to blockPoolScannerMap, new size=1\n11/05/19 05:01:32 INFO datanode.BlockPoolSliceScanner: Starting a new period : work left in prev period : 0.00%\n\ncase 3: Two failed volumes(/grid/1,/grid/2), two good volumes, Vol Tolerated=1, expected outcome = BPService should NOT start\n\n11/05/19 05:04:06 INFO common.Storage: Storage directory /grid/1/testing/hadoop-logs/dfs/data is not formatted.\n11/05/19 05:04:06 INFO common.Storage: Formatting ...\n11/05/19 05:04:06 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data is not formatted.\n11/05/19 05:04:06 INFO common.Storage: Formatting ...\n11/05/19 05:04:06 WARN common.Storage: Invalid directory in: /grid/1/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822: File file:/grid/1/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:04:06 WARN common.Storage: Invalid directory in: /grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822: File file:/grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:04:06 INFO common.Storage: Locking is disabled\n11/05/19 05:04:06 INFO common.Storage: Storage directory /grid/1/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:04:06 INFO common.Storage: Storage directory /grid/1/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:04:06 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:04:06 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data/current/BP-1694914230-10.72.86.55-1305704227822 does not exist.\n11/05/19 05:04:06 INFO common.Storage: Locking is disabled\n11/05/19 05:04:06 INFO datanode.DataNode: setting up storage: nsid=0;bpid=BP-1694914230-10.72.86.55-1305704227822;lv=-35;nsInfo=lv=-35;cid=test;nsid=413952175;c=0;bpid=BP-1694914230-10.72.86.55-1305704227822\n11/05/19 05:04:06 FATAL datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=DS-340618566-10.72.86.55-50010-1305704313207, infoPort=50075, ipcPort=50020, storageInfo=lv=-35;cid=test;nsid=413952175;c=0) initialization failed for block pool BP-1694914230-10.72.86.55-1305704227822\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Invalid value for volumes required - validVolsRequired: 3, Current valid volumes: 2, volsConfigured: 4, volFailuresTolerated: 1\n\tat org.apache.hadoop.hdfs.server.datanode.FSDataset.<init>(FSDataset.java:1160)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initFsDataSet(DataNode.java:1420)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.access$1100(DataNode.java:169)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.setupBPStorage(DataNode.java:804)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.setupBP(DataNode.java:774)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.run(DataNode.java:1191)\n\tat java.lang.Thread.run(Thread.java:619)\n11/05/19 05:04:06 WARN datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=DS-340618566-10.72.86.55-50010-1305704313207, infoPort=50075, ipcPort=50020, storageInfo=lv=-35;cid=test;nsid=413952175;c=0) ending block pool service for: BP-1694914230-10.72.86.55-1305704227822\n\ncase 4: All failed volumes, Vol Tolerated=1, expected outcome = BPService should NOT start\n\n11/05/19 05:07:51 INFO common.Storage: Storage directory /grid/0/testing/hadoop-logs/dfs/data is not formatted.\n11/05/19 05:07:51 INFO common.Storage: Formatting ...\n11/05/19 05:07:51 INFO common.Storage: Storage directory /grid/1/testing/hadoop-logs/dfs/data is not formatted.\n11/05/19 05:07:51 INFO common.Storage: Formatting ...\n11/05/19 05:07:51 INFO common.Storage: Storage directory /grid/2/testing/hadoop-logs/dfs/data is not formatted.\n11/05/19 05:07:51 INFO common.Storage: Formatting ...\n11/05/19 05:07:51 INFO common.Storage: Storage directory /grid/3/testing/hadoop-logs/dfs/data is not formatted.\n11/05/19 05:07:51 INFO common.Storage: Formatting ...\n11/05/19 05:07:51 FATAL datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=, infoPort=50075, ipcPort=50020, storageInfo=lv=0;cid=;nsid=0;c=0) initialization failed for block pool BP-1694914230-10.72.86.55-1305704227822\njava.io.IOException: All specified directories are not accessible or do not exist.\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:182)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:217)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.setupBPStorage(DataNode.java:797)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.setupBP(DataNode.java:774)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.run(DataNode.java:1191)\n\tat java.lang.Thread.run(Thread.java:619)\n11/05/19 05:07:51 WARN datanode.DataNode: DatanodeRegistration(hadooplab40.yst.corp.yahoo.com:50010, storageID=, infoPort=50075, ipcPort=50020, storageInfo=lv=0;cid=;nsid=0;c=0) ending block pool service for: BP-1694914230-10.72.86.55-1305704227822\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-19T05:24:10.744+0000","updated":"2011-05-19T05:24:10.744+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13036024","id":"13036024","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12479721/HDFS-1592-3.patch\n  against trunk revision 1124459.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 5 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.hdfs.TestDFSRemove\n                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery\n                  org.apache.hadoop.hdfs.TestFileConcurrentReader\n                  org.apache.hadoop.tools.TestJMXGet\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/581//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/581//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/581//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-19T06:49:48.896+0000","updated":"2011-05-19T06:49:48.896+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13036714","id":"13036714","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Attaching a patch with more unit tests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-20T08:03:05.039+0000","updated":"2011-05-20T08:03:05.039+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13036716","id":"13036716","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Eli,\n\nI have added more unit tests as mentioned above. Also, note that, the case you pointed is a rare condition. In our tests, making file system readonly through mount or umounting disks or even setting permission one level above, we will not hit this issue. Only, when we set the permission on this particular directory then only we will hit this issue. Anyways, i have fixed the case you pointed also. \n\nThanks for spotting this though.\n\n\n ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-20T08:08:50.189+0000","updated":"2011-05-20T08:08:50.189+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13036731","id":"13036731","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12479883/HDFS-1592-4.patch\n  against trunk revision 1125217.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 5 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery\n                  org.apache.hadoop.hdfs.TestFileConcurrentReader\n                  org.apache.hadoop.hdfs.TestHDFSTrash\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/600//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/600//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/600//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-20T09:15:05.624+0000","updated":"2011-05-20T09:15:05.624+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13036929","id":"13036929","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"These failing tests are not related to this patch. \n\nEli, If you don't have any comments, we will commit this patch today.  \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-20T16:56:59.960+0000","updated":"2011-05-20T16:56:59.960+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13037180","id":"13037180","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"body":"The new testcases look good and cover most of the use-case scenarios. However, please see if these test cases can be refactored to re-use code for failure simulation and cluster startup etc.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-21T00:06:06.949+0000","updated":"2011-05-21T00:06:06.949+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13037726","id":"13037726","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Bharath,\n\nApologies for the slow response, I didn't see the jira updates for some reason.  Thanks for adding the tests, the new test cases look good. I think other types of faults can be handled in a separate jira.\n\nSome minor comments, otherwise looks great!\n\n* DataStorage#recoverTransitionRead should log the IOE now that it's swallowed instead of thrown\n* It would be more clear to users if the message in the exception thrown FSDataSet was something like \"Too many failed volumes\". Since \"volumes required\" is an internal variable rather than a config option users won't what \"Invalid value for volumes required\" means.\n* Nit: volsConfigured can be initialized in 1 line: {{int volsConfigured = (dataDirs == null) ? 0 : dataDirs.length}}\n* startCluster should probably be renamed restartCluster since it's always called on active clusters\n* Jitendra's comment wrt refactoring makes sense\n\nThanks,\nEli","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-05-23T04:37:04.477+0000","updated":"2011-05-23T04:37:04.477+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13038866","id":"13038866","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"body":"Thanks the review, Eli and Jitendra. I am attaching a patch which incorporates your comments. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=bharathm","name":"bharathm","key":"bharathm","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bharath Mundlapudi","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-24T22:32:31.316+0000","updated":"2011-05-24T22:32:31.316+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13038884","id":"13038884","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12480337/HDFS-1592-5.patch\n  against trunk revision 1127317.\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 5 new or modified tests.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these core unit tests:\n                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\n    +1 system test framework.  The patch passed system test framework compile.\n\nTest results: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/619//testReport/\nFindbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/619//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html\nConsole output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/619//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-24T23:54:32.551+0000","updated":"2011-05-24T23:54:32.551+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13039240","id":"13039240","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"body":"+1 for the patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-25T17:52:48.388+0000","updated":"2011-05-25T17:52:48.388+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13039473","id":"13039473","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"body":"Eli,\n   Do you have any more concerns? I intend to commit this patch by tomorrow.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-26T01:46:52.389+0000","updated":"2011-05-26T01:46:52.389+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13039509","id":"13039509","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 lgtm. Thanks Bharath\n\nNit: \"due an exception\" should \"due to exception\"","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-05-26T05:04:40.361+0000","updated":"2011-05-26T05:04:40.361+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13039810","id":"13039810","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"body":"I have committed this. Thanks to Bharath.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-26T17:36:29.057+0000","updated":"2011-05-26T17:36:29.057+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13039811","id":"13039811","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"body":"> Nit: \"due an exception\" should \"due to exception\"\nI fixed it before the commit.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jnp","name":"jnp","key":"jnp","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jitendra Nath Pandey","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-26T17:38:07.713+0000","updated":"2011-05-26T17:38:07.713+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13039827","id":"13039827","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Hdfs-trunk-Commit #689 (See [https://builds.apache.org/hudson/job/Hadoop-Hdfs-trunk-Commit/689/])\n    HDFS-1592. Datanode startup doesn't honor volumes.tolerated. Contributed by Bharath Mundlapudi.\n\njitendra : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1127995\nFiles : \n* /hadoop/hdfs/trunk/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java\n* /hadoop/hdfs/trunk/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java\n* /hadoop/hdfs/trunk/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\n* /hadoop/hdfs/trunk/src/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java\n* /hadoop/hdfs/trunk/CHANGES.txt\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-26T17:52:54.231+0000","updated":"2011-05-26T17:52:54.231+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13040212","id":"13040212","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Hdfs-trunk #679 (See [https://builds.apache.org/hudson/job/Hadoop-Hdfs-trunk/679/])\n    HDFS-1592. Datanode startup doesn't honor volumes.tolerated. Contributed by Bharath Mundlapudi.\n\njitendra : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1127995\nFiles : \n* /hadoop/hdfs/trunk/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java\n* /hadoop/hdfs/trunk/src/test/hdfs/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java\n* /hadoop/hdfs/trunk/src/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\n* /hadoop/hdfs/trunk/src/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java\n* /hadoop/hdfs/trunk/CHANGES.txt\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2011-05-27T12:46:22.973+0000","updated":"2011-05-27T12:46:22.973+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12496444/comment/13096417","id":"13096417","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"body":"Hadoop 0.20.204.0 was released.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=owen.omalley","name":"owen.omalley","key":"owen.omalley","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=owen.omalley&avatarId=29697","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=owen.omalley&avatarId=29697","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=owen.omalley&avatarId=29697","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=owen.omalley&avatarId=29697"},"displayName":"Owen O'Malley","active":true,"timeZone":"America/Los_Angeles"},"created":"2011-09-02T22:16:02.317+0000","updated":"2011-09-02T22:16:02.317+0000"}],"maxResults":35,"total":35,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-1592/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i02p4f:"}}