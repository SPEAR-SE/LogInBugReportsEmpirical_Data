{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12671445","self":"https://issues.apache.org/jira/rest/api/2/issue/12671445","key":"HDFS-5280","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":null,"customfield_12312322":null,"customfield_12310220":"2014-01-11T08:49:09.623+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Apr 27 03:45:14 UTC 2016","customfield_12310420":"351151","customfield_12312320":null,"customfield_12310222":null,"customfield_12312321":null,"resolutiondate":null,"workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-5280/watchers","watchCount":6,"isWatching":false},"created":"2013-09-30T21:23:04.611+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12321656","id":"12321656","description":"maintenance release on branch-1.1","name":"1.1.1","archived":false,"released":true,"releaseDate":"2012-11-27"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12324031","id":"12324031","description":"2.1.0-beta release","name":"2.1.0-beta","archived":false,"released":true,"releaseDate":"2013-08-22"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12324136","id":"12324136","description":"2.0.4-alpha bug-fix release","name":"2.0.4-alpha","archived":false,"released":true,"releaseDate":"2013-04-25"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12332790","id":"12332790","description":"2.7.2 release","name":"2.7.2","archived":false,"released":true,"releaseDate":"2016-01-25"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12335732","id":"12335732","description":"3.0.0-alpha1 release","name":"3.0.0-alpha1","archived":false,"released":true,"releaseDate":"2016-09-03"}],"issuelinks":[],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aaperezl","name":"aaperezl","key":"aaperezl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aaperezl&avatarId=29740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aaperezl&avatarId=29740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aaperezl&avatarId=29740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aaperezl&avatarId=29740"},"displayName":"Andres Perez","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2016-05-12T18:12:29.218+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"},{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"Meta files being corrupted causes the DFSClient not able to connect to the datanodes to access the blocks, so DFSClient never perform a read on the block, which is what throws the ChecksumException when file blocks are corrupted and report to the namenode to mark the block as corrupt.  Since the client never got to that far, thus the file status remain as healthy and so are all the blocks.\n\nTo replicate the error, put a file onto HDFS.\nrun hadoop fsck /tmp/bogus.csv -files -blocks -location will get that following output.\nFSCK started for path /tmp/bogus.csv at 11:33:29\n/tmp/bogus.csv 109 bytes, 1 block(s):  OK\n0. blk_-4255166695856420554_5292 len=109 repl=3\n\nfind the block/meta files for 4255166695856420554 by running \nssh datanode1.address find /hadoop/ -name \"*4255166695856420554*\" and it will get the following output:\n/hadoop/data1/hdfs/current/subdir2/blk_-4255166695856420554\n/hadoop/data1/hdfs/current/subdir2/blk_-4255166695856420554_5292.meta\n\nnow corrupt the meta file by running \nssh datanode1.address \"sed -i -e '1i 1234567891' /hadoop/data1/hdfs/current/subdir2/blk_-4255166695856420554_5292.meta\" \n\nnow run hadoop fs -cat /tmp/bogus.csv\nwill show the stack trace of DFSClient failing to connect to the data node with the corrupted meta file.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12799686","id":"12799686","filename":"HDFS-5280.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aaperezl","name":"aaperezl","key":"aaperezl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aaperezl&avatarId=29740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aaperezl&avatarId=29740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aaperezl&avatarId=29740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aaperezl&avatarId=29740"},"displayName":"Andres Perez","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-20T06:51:59.471+0000","size":5050,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12799686/HDFS-5280.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"351443","customfield_12312823":null,"summary":"Corrupted meta files on data nodes prevents DFClient from connecting to data nodes and updating corruption status to name node.","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jwang302","name":"jwang302","key":"jwang302","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jinghui Wang","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jwang302","name":"jwang302","key":"jwang302","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jinghui Wang","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"Red hat enterprise 6.4\nHadoop-2.1.0","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12671445/comment/13868711","id":"13868711","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jiangbinglover","name":"jiangbinglover","key":"jiangbinglover","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bing Jiang","active":true,"timeZone":"Asia/Hong_Kong"},"body":"Any updates about the issues? ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jiangbinglover","name":"jiangbinglover","key":"jiangbinglover","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Bing Jiang","active":true,"timeZone":"Asia/Hong_Kong"},"created":"2014-01-11T08:49:09.623+0000","updated":"2014-01-11T08:49:09.623+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12671445/comment/15218951","id":"15218951","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aaperezl","name":"aaperezl","key":"aaperezl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aaperezl&avatarId=29740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aaperezl&avatarId=29740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aaperezl&avatarId=29740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aaperezl&avatarId=29740"},"displayName":"Andres Perez","active":true,"timeZone":"America/Los_Angeles"},"body":"The problem is happening because the {{IllegalArgumentException}} is not being handled at any level. Maybe the solution might be throwing a {{ChecksumException}} also in this case, or create a new type of exception indicating the wrong format of the blk*.meta file.\nAt some point as well the affected block should be marked as corrupt.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aaperezl","name":"aaperezl","key":"aaperezl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aaperezl&avatarId=29740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aaperezl&avatarId=29740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aaperezl&avatarId=29740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aaperezl&avatarId=29740"},"displayName":"Andres Perez","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-03-30T21:57:00.450+0000","updated":"2016-03-30T21:57:00.450+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12671445/comment/15249388","id":"15249388","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aaperezl","name":"aaperezl","key":"aaperezl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aaperezl&avatarId=29740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aaperezl&avatarId=29740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aaperezl&avatarId=29740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aaperezl&avatarId=29740"},"displayName":"Andres Perez","active":true,"timeZone":"America/Los_Angeles"},"body":"The {{IllegalArgumentException}} is catched and a dummy {{DataChecksum}} object is created, that way the Checksum test fails later in the pipeline and marks the node as non existant in that node, instead of marking the entire node as dead becasue the client was supposedly unable to connect.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aaperezl","name":"aaperezl","key":"aaperezl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aaperezl&avatarId=29740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aaperezl&avatarId=29740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aaperezl&avatarId=29740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aaperezl&avatarId=29740"},"displayName":"Andres Perez","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-20T06:51:26.431+0000","updated":"2016-04-20T06:51:26.431+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12671445/comment/15249649","id":"15249649","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=walter.k.su","name":"walter.k.su","key":"walter.k.su","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Walter Su","active":true,"timeZone":"Asia/Shanghai"},"body":"+1 for catching the exception. The same exception will cause {{BlockScanner}} to shutdown.\nWe should be cautious to catch any {{RuntimeException}}. Instead of add {{catch}} to the outside try-finally clause, how about just catch the exactly exception at the place where it's been threw.  Like what we did in {{FSNamesystem.java}}\n{code}\n 744       try {\n 745          checksumType = DataChecksum.Type.valueOf(checksumTypeStr);\n 746       } catch (IllegalArgumentException iae) {\n 747          throw new IOException(\"Invalid checksum type in \"\n 748             + DFS_CHECKSUM_TYPE_KEY + \": \" + checksumTypeStr);\n 749       }\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=walter.k.su","name":"walter.k.su","key":"walter.k.su","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Walter Su","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-04-20T11:06:06.575+0000","updated":"2016-04-20T11:06:06.575+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12671445/comment/15249701","id":"15249701","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"| (x) *{color:red}-1 overall{color}* |\n\\\\\n\\\\\n|| Vote || Subsystem || Runtime || Comment ||\n| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 23s {color} | {color:blue} Docker mode activated. {color} |\n| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |\n| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 8m 47s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 55s {color} | {color:green} trunk passed with JDK v1.8.0_77 {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 47s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |\n| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 24s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 59s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 6s {color} | {color:green} trunk passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 23s {color} | {color:green} trunk passed with JDK v1.8.0_77 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 6s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |\n| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 12s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 10s {color} | {color:green} the patch passed with JDK v1.8.0_77 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 10s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 45s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |\n| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 45s {color} | {color:green} the patch passed {color} |\n| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 22s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs: patch generated 1 new + 42 unchanged - 1 fixed = 43 total (was 43) {color} |\n| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 0s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 13s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |\n| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 37s {color} | {color:green} the patch passed {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 16s {color} | {color:green} the patch passed with JDK v1.8.0_77 {color} |\n| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 47s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 135m 28s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_77. {color} |\n| {color:red}-1{color} | {color:red} unit {color} | {color:red} 125m 57s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_95. {color} |\n| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 41s {color} | {color:green} Patch does not generate ASF License warnings. {color} |\n| {color:black}{color} | {color:black} {color} | {color:black} 293m 20s {color} | {color:black} {color} |\n\\\\\n\\\\\n|| Reason || Tests ||\n| JDK v1.8.0_77 Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeUUID |\n|   | hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |\n|   | hadoop.hdfs.server.namenode.TestNamenodeRetryCache |\n|   | hadoop.hdfs.server.namenode.ha.TestEditLogTailer |\n|   | hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations |\n|   | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |\n|   | hadoop.hdfs.TestErasureCodeBenchmarkThroughput |\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |\n|   | hadoop.hdfs.server.namenode.TestEditLog |\n|   | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |\n|   | hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA |\n|   | hadoop.hdfs.server.namenode.TestNamenodeCapacityReport |\n|   | hadoop.hdfs.server.namenode.TestFileTruncate |\n|   | hadoop.hdfs.server.datanode.TestDirectoryScanner |\n| JDK v1.8.0_77 Timed out junit tests | org.apache.hadoop.hdfs.TestLeaseRecovery2 |\n| JDK v1.7.0_95 Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeUUID |\n|   | hadoop.hdfs.TestDFSUpgradeFromImage |\n|   | hadoop.hdfs.server.namenode.TestNamenodeRetryCache |\n|   | hadoop.hdfs.server.namenode.ha.TestEditLogTailer |\n|   | hadoop.hdfs.server.blockmanagement.TestBlockManager |\n|   | hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations |\n|   | hadoop.hdfs.TestDataTransferKeepalive |\n|   | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |\n|   | hadoop.hdfs.security.TestDelegationTokenForProxyUser |\n|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure160 |\n|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |\n|   | hadoop.hdfs.server.namenode.TestEditLog |\n|   | hadoop.hdfs.server.namenode.ha.TestHAAppend |\n|   | hadoop.hdfs.server.namenode.TestFileTruncate |\n|   | hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider |\n|   | hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot |\n|   | hadoop.hdfs.server.datanode.TestDirectoryScanner |\n\\\\\n\\\\\n|| Subsystem || Report/Notes ||\n| Docker |  Image:yetus/hadoop:fbe3e86 |\n| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12799686/HDFS-5280.patch |\n| JIRA Issue | HDFS-5280 |\n| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |\n| uname | Linux 7e78a377755e 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |\n| git revision | trunk / af9bdbe |\n| Default Java | 1.7.0_95 |\n| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_77 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |\n| findbugs | v3.0.0 |\n| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/15214/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15214/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_77.txt |\n| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15214/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |\n| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/15214/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_77.txt https://builds.apache.org/job/PreCommit-HDFS-Build/15214/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |\n| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/15214/testReport/ |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |\n| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/15214/console |\n| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2016-04-20T11:52:14.325+0000","updated":"2016-04-20T11:52:14.325+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12671445/comment/15258365","id":"15258365","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aaperezl","name":"aaperezl","key":"aaperezl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aaperezl&avatarId=29740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aaperezl&avatarId=29740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aaperezl&avatarId=29740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aaperezl&avatarId=29740"},"displayName":"Andres Perez","active":true,"timeZone":"America/Los_Angeles"},"body":"IF you catch the exception and throw a new {{IOException}} so close to where the exception happens which {{ChecksumException}} extends, it will report the node containing the corrupted block as a dead node.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aaperezl","name":"aaperezl","key":"aaperezl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aaperezl&avatarId=29740","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aaperezl&avatarId=29740","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aaperezl&avatarId=29740","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aaperezl&avatarId=29740"},"displayName":"Andres Perez","active":true,"timeZone":"America/Los_Angeles"},"created":"2016-04-26T16:22:00.061+0000","updated":"2016-04-26T16:22:00.061+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12671445/comment/15259490","id":"15259490","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=walter.k.su","name":"walter.k.su","key":"walter.k.su","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Walter Su","active":true,"timeZone":"Asia/Shanghai"},"body":"There's other IOExceptions will cause readBlock RPC call fails, then cause the dn marked as dead. We could fix them as well.\nIf I understand correctly, you approach is to use a fake checksum. When client reads data, the check failed, and client will mark block as corrupted instead of mark dn as dead. I think, can we let client not to read from this dn at first? If client fails to create blockreader, it can tell if the dn is dead or it's just the block is corrupted.\n\n{code}\n//DFSInputStream.java\n 652       try {\n 653         blockReader = getBlockReader(targetBlock, offsetIntoBlock,\n 654             targetBlock.getBlockSize() - offsetIntoBlock, targetAddr,\n 655             storageType, chosenNode);\n 656         if(connectFailedOnce) {\n 657           DFSClient.LOG.info(\"Successfully connected to \" + targetAddr +\n 658                              \" for \" + targetBlock.getBlock());\n 659         }\n 660         return chosenNode;\n 661       } catch (IOException ex) {\n 662         if (ex instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {\n...\n 672         } else {\n...\n 677           addToDeadNodes(chosenNode);\n 678         }\n 679       }\n 680     }\n 681   }\n{code}\nInstead of going to {{else}} clause, can we have another Exception like {{InvalidEncryptionKeyException}}, if we catch it, we skip the dn, and do not add it to dead nodes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=walter.k.su","name":"walter.k.su","key":"walter.k.su","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Walter Su","active":true,"timeZone":"Asia/Shanghai"},"created":"2016-04-27T03:45:14.640+0000","updated":"2016-04-27T03:45:14.640+0000"}],"maxResults":7,"total":7,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-5280/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1ojmv:"}}