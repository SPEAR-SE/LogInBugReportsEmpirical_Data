{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"13085940","self":"https://issues.apache.org/jira/rest/api/2/issue/13085940","key":"HDFS-12109","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/10004","id":"10004","description":"Not A Bug","name":"Not A Bug"},"customfield_12312322":null,"customfield_12310220":"2017-07-10T16:19:45.077+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Jul 12 06:38:32 UTC 2017","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_148448951_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2017-07-12T06:39:20.941+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-12109/watchers","watchCount":4,"isWatching":false},"created":"2017-07-10T13:25:12.025+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12329057","id":"12329057","description":"2.8.0 release","name":"2.8.0","archived":false,"released":true,"releaseDate":"2017-03-22"}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2017-07-12T06:42:37.797+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12327021","id":"12327021","name":"fs"}],"timeoriginalestimate":null,"description":"After setting up an HA NameNode configuration, the following invocation of \"fs\" fails:\n\n[hadoop@namenode01 ~]$ /usr/local/hadoop/bin/hdfs dfs -ls /\n-ls: java.net.UnknownHostException: saccluster\n\nIt works if properties are defined as per below:\n\n/usr/local/hadoop/bin/hdfs dfs -Ddfs.nameservices=saccluster -Ddfs.client.failover.proxy.provider.saccluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider -Ddfs.ha.namenodes.saccluster=namenode01,namenode02 -Ddfs.namenode.rpc-address.saccluster.namenode01=namenode01:8020 -Ddfs.namenode.rpc-address.saccluster.namenode02=namenode02:8020 -ls /\n\nThese properties are defined in /usr/local/hadoop/etc/hadoop/hdfs-site.xml as per below:\n\n    <property>\n        <name>dfs.nameservices</name>\n        <value>saccluster</value>\n    </property>\n    <property>\n        <name>dfs.ha.namenodes.saccluster</name>\n        <value>namenode01,namenode02</value>\n    </property>\n    <property>\n        <name>dfs.namenode.rpc-address.saccluster.namenode01</name>\n        <value>namenode01:8020</value>\n    </property>\n    <property>\n        <name>dfs.namenode.rpc-address.saccluster.namenode02</name>\n        <value>namenode02:8020</value>\n    </property>\n        <property>\n        <name>dfs.namenode.http-address.saccluster.namenode01</name>\n        <value>namenode01:50070</value>\n    </property>\n    <property>\n        <name>dfs.namenode.http-address.saccluster.namenode02</name>\n        <value>namenode02:50070</value>\n    </property>\n        <property>\n        <name>dfs.namenode.shared.edits.dir</name>\n        <value>qjournal://namenode01:8485;namenode02:8485;datanode01:8485/saccluster</value>\n    </property>\n    <property>\n        <name>dfs.client.failover.proxy.provider.mycluster</name>\n        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n    </property>\n\nIn /usr/local/hadoop/etc/hadoop/core-site.xml the default FS is defined as per below:\n\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://saccluster</value>\n    </property>\n\nIn /usr/local/hadoop/etc/hadoop/hadoop-env.sh the following export is defined:\n\nexport HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"\n\nIs \"fs\" trying to read these properties from somewhere else, such as a separate client configuration file?\n\nApologies if I am missing something obvious here.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"\"fs\" java.net.UnknownHostException when HA NameNode is used","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luigidifraia","name":"luigidifraia","key":"luigidifraia","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luigi Di Fraia","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luigidifraia","name":"luigidifraia","key":"luigidifraia","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luigi Di Fraia","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":"[hadoop@namenode01 ~]$ cat /etc/redhat-release\nCentOS Linux release 7.3.1611 (Core)\n[hadoop@namenode01 ~]$ uname -a\nLinux namenode01 3.10.0-514.10.2.el7.x86_64 #1 SMP Fri Mar 3 00:04:05 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n[hadoop@namenode01 ~]$ java -version\njava version \"1.8.0_131\"\nJava(TM) SE Runtime Environment (build 1.8.0_131-b11)\nJava HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)","customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/13085940/comment/16080591","id":"16080591","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"body":"The HADOOP_CONF_DIR environment variable is how the shell scripts find where hadoop-env.sh is located. Defining it inside hadoop-env.sh won't work. Given what I can imply from your description, hadoop 3.x would work fine because it can autodetermine where stuff is located based upon the executable location.  But hadoop 2.x has a lot of bugs, so it needs to have (minimally) HADOOP_PREFIX defined outside of the shell script code.  If that is defined, it should know where everything is located, including auto-defining HADOOP_CONF_DIR to be HADOOP_PREFIX/etc/hadoop.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=aw","name":"aw","key":"aw","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=aw&avatarId=23681","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=aw&avatarId=23681","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=aw&avatarId=23681","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=aw&avatarId=23681"},"displayName":"Allen Wittenauer","active":true,"timeZone":"America/Tijuana"},"created":"2017-07-10T16:19:45.077+0000","updated":"2017-07-10T16:20:11.921+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13085940/comment/16081740","id":"16081740","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luigidifraia","name":"luigidifraia","key":"luigidifraia","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luigi Di Fraia","active":true,"timeZone":"Etc/UTC"},"body":"Thanks for your reply [~aw]. I exported the variables as per below for testing purposes:\n\n[hadoop@namenode01 ~]$ export HADOOP_PREFIX=/usr/local/hadoop\n[hadoop@namenode01 ~]$ export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop\n\nHowever the issue persists. What I'd like to underline is that part of the configuration seems to be visible to file-system tools, based on the exception I get:\n\n[hadoop@namenode01 ~]$ /usr/local/hadoop/bin/hdfs dfs -ls /\n-ls: java.net.UnknownHostException: saccluster\n\nIndeed \"saccluster\" is the nameservice I had configured and the default FS.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luigidifraia","name":"luigidifraia","key":"luigidifraia","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luigi Di Fraia","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-11T06:18:03.325+0000","updated":"2017-07-11T06:18:03.325+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13085940/comment/16081872","id":"16081872","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luigidifraia","name":"luigidifraia","key":"luigidifraia","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luigi Di Fraia","active":true,"timeZone":"Etc/UTC"},"body":"It's also probably worth mentioning that I am trying to use the HA NameNode setup with Accumulo 1.8.1 and I am having the same problem there (namenode service being used as if it were a hostname in a non-HA NameNode setup) when I try to init Accumulo or show volumes, as per below:\n\naccumulo@namenode01 ~]$ /usr/local/accumulo/bin/accumulo admin volumes --list\n2017-07-11 09:24:52,380 [start.Main] ERROR: Problem initializing the class loader\njava.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.accumulo.start.Main.getClassLoader(Main.java:94)\n        at org.apache.accumulo.start.Main.main(Main.java:47)\nCaused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: saccluster\n        at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:417)\n        at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:130)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:343)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:287)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:156)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2811)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100)\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2848)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)\n        at org.apache.commons.vfs2.provider.hdfs.HdfsFileSystem.resolveFile(HdfsFileSystem.java:164)\n        at org.apache.commons.vfs2.provider.AbstractOriginatingFileProvider.findFile(AbstractOriginatingFileProvider.java:84)\n        at org.apache.commons.vfs2.provider.AbstractOriginatingFileProvider.findFile(AbstractOriginatingFileProvider.java:64)\n        at org.apache.commons.vfs2.impl.DefaultFileSystemManager.resolveFile(DefaultFileSystemManager.java:804)\n        at org.apache.commons.vfs2.impl.DefaultFileSystemManager.resolveFile(DefaultFileSystemManager.java:760)\n        at org.apache.commons.vfs2.impl.DefaultFileSystemManager.resolveFile(DefaultFileSystemManager.java:709)\n        at org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.resolve(AccumuloVFSClassLoader.java:141)\n        at org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.resolve(AccumuloVFSClassLoader.java:121)\n        at org.apache.accumulo.start.classloader.vfs.AccumuloVFSClassLoader.getClassLoader(AccumuloVFSClassLoader.java:211)\n\nIt was due to the above exception that I then went back one step and tried file-system commands for HDFS directly.\n\nThe Web UI for NameNodes on the active NameNode (http://namenode01:50070/dfshealth.html#tab-overview) is picking up the HA NameNode configuration just fine and showing the Namespace as expected, saccluster,\n\nAs a side note, without HA NameNode the setup has been working just fine for me for quite some time, including using Accumulo with HDFS. It seems like there's something missing in the way HA NameNode properties are exposed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luigidifraia","name":"luigidifraia","key":"luigidifraia","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luigi Di Fraia","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-11T08:35:26.052+0000","updated":"2017-07-11T08:35:26.052+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13085940/comment/16083409","id":"16083409","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=surendrasingh","name":"surendrasingh","key":"surendrasingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=surendrasingh&avatarId=30759","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=surendrasingh&avatarId=30759","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=surendrasingh&avatarId=30759","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=surendrasingh&avatarId=30759"},"displayName":"Surendra Singh Lilhore","active":true,"timeZone":"Etc/UTC"},"body":"[~luigidifraia] based on description, I think you configured one property wrongly.   \n\n{code}\n<property>\n<name>dfs.client.failover.proxy.provider.mycluster</name>\n<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n</property>\n{code}\n\nIn this property, you given wrong namespace name(*mycluster*). It should be *saccluster*.\nSo your configuration should be like this..\n{code}\n<property>\n<name>dfs.client.failover.proxy.provider.saccluster</name>\n<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n</property>\n{code}\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=surendrasingh","name":"surendrasingh","key":"surendrasingh","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=surendrasingh&avatarId=30759","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=surendrasingh&avatarId=30759","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=surendrasingh&avatarId=30759","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=surendrasingh&avatarId=30759"},"displayName":"Surendra Singh Lilhore","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-12T04:08:05.615+0000","updated":"2017-07-12T04:08:05.615+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/13085940/comment/16083520","id":"16083520","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luigidifraia","name":"luigidifraia","key":"luigidifraia","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luigi Di Fraia","active":true,"timeZone":"Etc/UTC"},"body":"Thanks [~surendrasingh]. Appreciate your help with this. Indeed it was the property name that was using the wrong namespace. Oddly enough, the property I was passing on the commandline was correctly defined, which somehow masked out the hdfs-site.xml configuration issue.\nI am resolving the issue as \"Not a bug\".\nThanks again.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=luigidifraia","name":"luigidifraia","key":"luigidifraia","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Luigi Di Fraia","active":true,"timeZone":"Etc/UTC"},"created":"2017-07-12T06:38:32.235+0000","updated":"2017-07-12T06:42:37.790+0000"}],"maxResults":5,"total":5,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-12109/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i3hbgv:"}}