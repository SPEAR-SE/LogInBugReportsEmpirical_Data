{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12439023","self":"https://issues.apache.org/jira/rest/api/2/issue/12439023","key":"HDFS-732","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12314204","id":"12314204","description":"","name":"0.20.2","archived":false,"released":true,"releaseDate":"2010-02-16"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2009-10-28T00:27:25.239+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sat Oct 31 00:38:22 UTC 2009","customfield_12310420":"16179","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_436608241_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2009-10-30T02:11:02.557+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-732/watchers","watchCount":16,"isWatching":false},"created":"2009-10-25T00:54:14.316+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"1.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12314204","id":"12314204","description":"","name":"0.20.2","archived":false,"released":true,"releaseDate":"2010-02-16"}],"issuelinks":[{"id":"12327552","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12327552","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12409727","key":"HADOOP-4760","self":"https://issues.apache.org/jira/rest/api/2/issue/12409727","fields":{"summary":"HDFS streams should not throw exceptions when closed twice","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12327554","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12327554","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12425640","key":"HADOOP-5859","self":"https://issues.apache.org/jira/rest/api/2/issue/12425640","fields":{"summary":"FindBugs : fix \"wait() or sleep() with locks held\" warnings in hdfs","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2009-10-31T00:38:22.299+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312928","id":"12312928","name":"hdfs-client"}],"timeoriginalestimate":null,"description":"We recently started to use hadoop-0.20.1 in our production environment (less than 2 weeks ago) and already had 3 instances of truncated files, more than we had for months using hadoop-0.18.3.\nWriting is done using libhdfs, although it rather seems to be a problem on the server side.\n\nI will post some relevant logs (they are too large to be put into the description)\n\n\n ","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12423497","id":"12423497","filename":"h732_20091028_0.20.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-28T22:02:05.902+0000","size":597,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12423497/h732_20091028_0.20.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"113074","customfield_12312823":null,"summary":"HDFS files are ending up truncated","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12769743","id":"12769743","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"body":"The file size of the file should have been 18654752, computed by adding up all the bytes successfully written. But it ended up having only 17825792. I verified that the missing data was at the end, i.e. the block got somehow recovered from an incomplete replica.\n\nHere are relevant log messages from the namenode:\n\ngrep blk_6703874482275767879 hadoop-user-namenode-host.log.2009-10-23\n\n2009-10-23 19:46:47,815 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock:\ndir/output/app/_temporary/_attempt_200910221954_0001_r_009110_0/9110/filename.\nblk_6703874482275767879_76799972\n2009-10-23 21:16:00,028 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem:\ncommitBlockSynchronization(lastblock=blk_6703874482275767879_76799972, newgenerationstamp=76840998, newlength=17825792,\nnewtargets=[xxx.yyy.zzz.56:uuu10, xxx.yyy.zzz.44:uuu10], closeFile=false, deleteBlock=false)\n2009-10-23 21:16:00,028 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem:\ncommitBlockSynchronization(blk_6703874482275767879_76840998) successful\n2009-10-23 21:16:00,397 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem:\ncommitBlockSynchronization(lastblock=blk_6703874482275767879_76840998, newgenerationstamp=76840999, newlength=17825792,\nnewtargets=[xxx.yyy.zzz.44:uuu10], closeFile=false, deleteBlock=false)\n2009-10-23 21:16:00,397 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem:\ncommitBlockSynchronization(blk_6703874482275767879_76840999) successful\n2009-10-23 22:16:02,159 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* blk_6703874482275767879_76840999 recovery\nstarted, primary=xxx.yyy.zzz.44:uuu10\n2009-10-23 22:16:02,925 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: addStoredBlock\nrequest received for blk_6703874482275767879_76888761 on xxx.yyy.zzz.44:uuu10 size 17825792 But it does not belong to any\nfile.\n2009-10-23 22:16:02,925 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem:\ncommitBlockSynchronization(lastblock=blk_6703874482275767879_76840999, newgenerationstamp=76888761, newlength=17825792,\nnewtargets=[xxx.yyy.zzz.44:uuu10], closeFile=true, deleteBlock=false)\n2009-10-23 22:16:02,931 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem:\ncommitBlockSynchronization(newblock=blk_6703874482275767879_76888761,\nfile=dir/output/app/9110/filename,\nnewgenerationstamp=76888761, newlength=17825792, newtargets=[xxx.yyy.zzz.44:uuu10]) successful\n2009-10-23 22:16:05,112 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask xxx.yyy.zzz.44:uuu10 to replicate\nblk_6703874482275767879_76888761 to datanode(s) xxx.yyy.zzz.51:uuu10 xxx.yyy.zzz.237:uuu10\n2009-10-23 22:16:06,310 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated:\nxxx.yyy.zzz.51:uuu10 is added to blk_6703874482275767879_76888761 size 17825792\n2009-10-23 22:16:06,312 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated:\nxxx.yyy.zzz.237:uuu10 is added to blk_6703874482275767879_76888761 size 17825792\n\n\nRelevant log messages of the task (the block went through recovery):\n\n09/10/23 21:15:59 WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block\nblk_6703874482275767879_76799972java.io.IOException: Connection reset by peer\n09/10/23 21:15:59 WARN hdfs.DFSClient: Error Recovery for block blk_6703874482275767879_76799972 bad datanode[0]\nxxx.yyy.zzz.43:uuu10\n09/10/23 21:15:59 WARN hdfs.DFSClient: Error Recovery for block blk_6703874482275767879_76799972 in pipeline\nxxx.yyy.zzz.43:uuu10, xxx.yyy.zzz.56:uuu10, xxx.yyy.zzz.44:uuu10: bad datanode xxx.yyy.zzz.43:uuu10\n09/10/23 21:16:00 WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Connection reset by peer\n09/10/23 21:16:00 WARN hdfs.DFSClient: Error Recovery for block blk_6703874482275767879_76840998 bad datanode[0]\nxxx.yyy.zzz.56:uuu10\n09/10/23 21:16:00 WARN hdfs.DFSClient: Error Recovery for block blk_6703874482275767879_76840998 in pipeline\nxxx.yyy.zzz.56:uuu10, xxx.yyy.zzz.44:uuu10: bad datanode xxx.yyy.zzz.56:uuu10\n09/10/23 21:16:00 WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Connection reset by peer\n09/10/23 21:16:00 WARN hdfs.DFSClient: Error Recovery for block blk_6703874482275767879_76840999 bad datanode[0]\nxxx.yyy.zzz.44:uuu10\nException in thread \"main\" java.io.IOException: All datanodes xxx.yyy.zzz.44:uuu10 are bad. Aborting..\n\nFirst attempt to close the file was unsuccessful, but second attempt was successful (but with truncated size).\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-25T01:16:55.755+0000","updated":"2009-10-25T01:16:55.755+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12769746","id":"12769746","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"body":"Here I post a message I sent to common-dev on September 4, about an incidence where\ncommitBlockSynchronization replaced a block with a block of smaller size.\nThis happened on hadoop-0.18.3, but it seemed to be very rare. Question is whether this is the same problem as the one in the previous comment, and if so, whether something changed in hadoop-0.20.1 to increase the rate of such incidences.\n\nAnyhow, are there situations where block sizes can shrink? If not, this should <b>absolutely</b> not be allowed.\n\nHere are the relevant log messages of the namenode, showing the block size went from 273592 to 262144.\n\nlog.2009-09-01:2009-09-01 20:52:45,451 INFO org.apache.hadoop.dfs.StateChange: BLOCK*\nNameSystem.allocateBlock:\nfileName.blk_2101888387690035515_63606967\n\nlog.2009-09-01:2009-09-01 20:57:43,535 INFO org.apache.hadoop.dfs.StateChange: BLOCK*\nNameSystem.addStoredBlock: blockMap updated: xxx.yyy.zzz.234:54610 is added to blk_2101888387690035515_63606967 size 273592\n\nlog.2009-09-01:2009-09-01 20:57:47,847 INFO org.apache.hadoop.dfs.StateChange: BLOCK*\nNameSystem.addStoredBlock: blockMap updated: xxx.yyy.zzz.250:54610 is added to blk_2101888387690035515_63606967 size 273592\n\nlog.2009-09-01:2009-09-01 20:58:01,480 INFO org.apache.hadoop.fs.FSNamesystem:\ncommitBlockSynchronization(lastblock=blk_2101888387690035515_63606967, newgenerationstamp=63637360, newlength=262144,\nnewtargets=[xxx.yyy.zzz.234:54610])\n\nlog.2009-09-01:2009-09-01 20:58:01,482 INFO org.apache.hadoop.fs.FSNamesystem:\ncommitBlockSynchronization(blk_2101888387690035515_63637360) successful\n\nlog.2009-09-01:2009-09-01 21:01:07,508 INFO org.apache.hadoop.fs.FSNamesystem:\ncommitBlockSynchronization(lastblock=blk_2101888387690035515_63637360, newgenerationstamp=63639267, newlength=262144,\nnewtargets=[xxx.yyy.zzz.234:54610])\n\nlog.2009-09-01:2009-09-01 21:01:07,512 INFO org.apache.hadoop.fs.FSNamesystem:\ncommitBlockSynchronization(blk_2101888387690035515_63639267) successful\n\nlog.2009-09-01:2009-09-01 21:01:08,946 INFO org.apache.hadoop.dfs.StateChange: BLOCK*\nask xxx.yyy.zzz.234:54610 to replicate blk_2101888387690035515_63639267 to datanode(s) xxx.yyy.zzz.230:54610 xxx.yyy.zzz.115:54610\n\nlog.2009-09-01:2009-09-01 21:01:12,061 INFO org.apache.hadoop.dfs.StateChange: BLOCK*\nNameSystem.addStoredBlock: blockMap updated: xxx.yyy.zzz.230:54610 is added to blk_2101888387690035515_63639267 size 262144\n\nlog.2009-09-01:2009-09-01 21:01:12,097 INFO org.apache.hadoop.dfs.StateChange: BLOCK*\nNameSystem.addStoredBlock: blockMap updated: xxx.yyy.zzz.115:54610 is added to blk_2101888387690035515_63639267 size 262144\n\nlog.2009-09-01:2009-09-01 21:35:03,362 INFO org.apache.hadoop.dfs.StateChange: BLOCK*\nNameSystem.processReport: block blk_2101888387690035515_63637360 on xxx.yyy.zzz.250:54610 size 273592 does not belong to any file.\n\nlog.2009-09-01:2009-09-01 21:35:06,344 INFO org.apache.hadoop.dfs.StateChange: BLOCK*\nask xxx.yyy.zzz.250:54610 to delete blk_2101888387690035515_63637360","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-25T01:29:41.579+0000","updated":"2009-10-25T01:29:41.579+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12769833","id":"12769833","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"body":"Concerning the file in the first comment I found logs of 2 datanodes showing that indeed the block size shrunk from 18153472 to 17825792. 18153472 is not the correct size, but it is larger than 17825792, and I would argue, that a block should never be recovered by a block of smaller size.\n\nLogs from datanode receiving original block:\n\n2009-10-23 19:46:47,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block blk_6703874482275767879_76799972 src: /xxx.yyy.zzz.43:34608 dest: /xxx.yyy.zzz.43:uuu10\n2009-10-23 21:15:59,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(xxx.yyy.zzz.43:uuu10, storageID=DS-243564233-xxx.yyy.zzz.43-uuu10-1254870555871, infoPort=50075, ipcPort=8020):Exception writing block blk_6703874482275767879_76799972 to mirror xxx.yyy.zzz.56:uuu10\n2009-10-23 21:15:59,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in receiveBlock for block blk_6703874482275767879_76799972 java.io.IOException: Connection reset by peer\n2009-10-23 21:15:59,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder blk_6703874482275767879_76799972 2 Exception java.io.EOFException\n2009-10-23 21:15:59,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder blk_6703874482275767879_76799972 2 : Thread is interrupted.\n2009-10-23 21:15:59,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder 2 for block blk_6703874482275767879_76799972 terminating\n2009-10-23 21:15:59,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: writeBlock blk_6703874482275767879_76799972 received exception java.io.IOException: Connection reset by peer\n\nLogs from datanode next in the pipeline:\n\n2009-10-23 19:46:48,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block blk_6703874482275767879_76799972 src: /xxx.yyy.zzz.43:34609 dest: /xxx.yyy.zzz.56:uuu10\n2009-10-23 21:15:59,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(xxx.yyy.zzz.56:uuu10, storageID=DS-807595239-72.30.217.12-50010-1203107050520, infoPort=50075, ipcPort=8020):Exception writing block blk_6703874482275767879_76799972 to mirror xxx.yyy.zzz.44:uuu10\n2009-10-23 21:15:59,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in receiveBlock for block blk_6703874482275767879_76799972 java.io.IOException: Connection reset by peer\n2009-10-23 21:15:59,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder blk_6703874482275767879_76799972 1 Exception java.io.EOFException\n2009-10-23 21:15:59,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder blk_6703874482275767879_76799972 1 : Thread is interrupted.\n2009-10-23 21:15:59,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder 1 for block blk_6703874482275767879_76799972 terminating\n2009-10-23 21:15:59,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: writeBlock blk_6703874482275767879_76799972 received exception java.io.IOException: Connection reset by peer\n2009-10-23 21:16:00,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: oldblock=blk_6703874482275767879_76799972(length=18153472), newblock=blk_6703874482275767879_76840998(length=17825792), datanode=xxx.yyy.zzz.56:uuu10\n2009-10-23 21:16:00,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block blk_6703874482275767879_76840998 src: /xxx.yyy.zzz.43:36067 dest: /xxx.yyy.zzz.56:uuu10\n2009-10-23 21:16:00,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Reopen already-open Block for append blk_6703874482275767879_76840998\n2009-10-23 21:16:00,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Changing block file offset of block blk_6703874482275767879_76840998 from 0 to 17825792 meta file offset to 17415\n2009-10-23 21:16:00,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(xxx.yyy.zzz.56:uuu10, storageID=DS-807595239-72.30.217.12-50010-1203107050520, infoPort=50075, ipcPort=8020):Exception writing block blk_6703874482275767879_76840998 to mirror xxx.yyy.zzz.44:uuu10\n2009-10-23 21:16:00,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in receiveBlock for block blk_6703874482275767879_76840998 java.io.IOException: Connection reset by peer\n2009-10-23 21:16:00,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder blk_6703874482275767879_76840998 1 Exception java.nio.channels.ClosedByInterruptException\n2009-10-23 21:16:00,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder blk_6703874482275767879_76840998 1 : Thread is interrupted.\n2009-10-23 21:16:00,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder 1 for block blk_6703874482275767879_76840998 terminating\n2009-10-23 21:16:00,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: writeBlock blk_6703874482275767879_76840998 received exception java.io.IOException: Connection reset by peer\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-25T16:52:08.251+0000","updated":"2009-10-25T16:52:08.251+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12770722","id":"12770722","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"First of all, dfs supports non-blocking writes. Although an application may successful writes 18654752 byte and returns, the bytes may still be buffered at the client side and have not been pushed the datanodes in the pipeline yet. \n\nFrom the logs that you provided, it seemed to me that the packet that starts with the byte  17825793 was not successfully pushed to all datanodes. For some reason, three datanodes failed in a row. The dfs client tried to resend the packet twice as a result the generation stamp of the block was bumped from 76799972 to 76840998, then to 76840999 and replica length was truncated to  17825792. Eventually the dfs client failed with an error: \"All datanodes xxx.yyy.zzz.44:uuu10 are bad. Aborting..\". Afterwards NN tried to recover this un-closed file. Since the only valid replica at xxx.yyy.zzz.44:uuu10  had 17825792 bytes, that's why  the block ended up with 17825792 bytes.\n\nBasically dfs does not provide any guarantee on the file length if a dfs client goes away and the file is left unclosed. But in 0.21, if an application calls hflush(), then dfs guarantee\nthat hflush bytes will not be truncated on error recovery.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-28T00:27:25.239+0000","updated":"2009-10-28T00:27:25.239+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12770785","id":"12770785","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"body":"I am still not convinced that everything is okay. Even when the close call fails the DFS client does not go by itself and has to continue to provide consistent results.\n\nOur client application called hdfsCloseFile of libhdfs in 2 attempts, the second one was successful, as mentioned at the end of the first comment.\n\nWhen looking at source code of hadoop-0.18.3, hadoop-0.20.1, and trunk, I see different behaviors of the close function in DFSOutputStream:\n\nhadoop-0.18.3:\nclose() calls closeInternal() which throws an exception when aborted previously.\n\nhadoop-0.20.1:\nif(closed) return;\nalways returns okay when closed, even when aborted previously.\n\ntrunk:\nif (closed) { IOException e = lastException; if (e == null) return; else throw e;}\n\nhadoop-0.18.3 and trunk are acceptable, but in hadoop-0.20.1, when a client tries to close a file twice, it will always be successful on the second attempt, even when aborted previously. This is inconsistent.\n\nBesides that, what is the purpose of recovering a file aborted during close? What is a use case for that?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-28T04:57:04.030+0000","updated":"2009-10-28T04:57:04.030+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12770794","id":"12770794","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"hi christian, the 0.20 code looks correct. DFSOutputStream.closed is initialized to false and is set to true if and only if a call to DFSOutputStream.close() successfully completed. In this case, if the app invoked DFSOutputStream.close() again, this method will return success. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-10-28T05:22:15.918+0000","updated":"2009-10-28T05:22:15.918+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771012","id":"12771012","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"body":"\n0.20 seems to be setting 'closed' to true inside a finally. It would be better to fix the behaviour to be equivalent to 0.21.\n\nThat said, I think contract of close is not the real issue here. Why isn't error from first close() not treated as hard error? \n\nbq. Even when the close call fails, DFS client does not go by itself and has to continue to provide consistent results. \nDo you mean DFS client does not serve other streams properly after this error?\n\nbq. Besides that, what is the purpose of recovering a file aborted during close? What is a use case for that?\nThis changed quite some time back. This is the normal expected behaviour of most filesystems. A user's process or machine might die in the middle of writing and there is no use of throwing the data that is already written away.\n\nChristian, do you expect the actual error on datanodes while writing? I would be concerned about pipeline error detection whenever I see failure on all the three datanodes. Multiple bugs were fixed in this area. Please include any stacktrace around the messages in datanode logs (third datanode log would be very useful, but looks like you were not able to recover it).\n\npartial data recovered after such a failure is as expected. I agree, it would be better to make second invocation of close() return error as well and it would be good practice for app not to ignore error from the first close().\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rangadi","name":"rangadi","key":"rangadi","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Raghu Angadi","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-28T17:37:58.794+0000","updated":"2009-10-28T17:37:58.794+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771082","id":"12771082","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"body":"From what I see in the code, 'closed = true' is set in processDatanodeError after any exception, and in the finally clause of closeInternal.\n\nSeemingly multiple tries of close must have been introduced in the past for some early version of hadoop (maybe hadoop-0.15 or earlier) when it reduced the number of task failures, and since then we did not have problems with it (although without any beneficial effects) till hadoop-0.20.1.\n\nOkay, we will change our applications to not retry a close, assuming that the output stream should not be used after a call to close, whether successful or not.\n\nMaybe, hdfsCloseFile in libhdfs should be changed as well to deallocate resources not only when it is successful.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-28T19:50:04.387+0000","updated":"2009-10-28T19:50:04.387+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771092","id":"12771092","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"> First attempt to close the file was unsuccessful, but second attempt was successful (but with truncated size).\nI do not think this is true. In most of cases, a dfs client is not able to close a file if it failed to push data to datanodes because all replicas are left in under construction state.\n\nAs I said in my yesterday's comment, it is the NameNode that closed the file. The following NameNode log\n{noformat}\n2009-10-23 21:16:00,397 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem:\ncommitBlockSynchronization(blk_6703874482275767879_76840999) successful\n2009-10-23 22:16:02,159 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* blk_6703874482275767879_76840999 recovery\nstarted, primary=xxx.yyy.zzz.44:uuu10\n2009-10-23 22:16:02,925 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem:\ncommitBlockSynchronization(lastblock=blk_6703874482275767879_76840999, newgenerationstamp=76888761, newlength=17825792,\nnewtargets=[xxx.yyy.zzz.44:uuu10], closeFile=true, deleteBlock=false)\n{noformat}\nshows that after the dfs client died (around 21:16), its lease expired after 1 hour (around 22:16). So NameNode initiated recovery and then closed the file. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-28T20:16:24.956+0000","updated":"2009-10-28T20:16:24.956+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771097","id":"12771097","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"body":"Hairong, of course, you are right in technical terms.\nBut from our client application's point of view, when it called hdfsCloseFile in libhdfs again after a first failure of close, the call was successful. This behavior of close in DFSOutputStream changed in hadoop-0.20.1 compared to hadoop-0.18 and made the application think that the files was successfully closed.\nAs mentioned in the previous comment, we are changing our application, assuming the OutputStream should not be used after a call to close.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ckunz","name":"ckunz","key":"ckunz","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Christian Kunz","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-28T20:31:38.743+0000","updated":"2009-10-28T20:31:38.743+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771100","id":"12771100","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"> But from our client application's point of view, when it called hdfsCloseFile in libhdfs again after a first failure of close, the call was successful. ...\n\nThis is definitely a bug if the second call of close() returns successfully, given that the first call failed.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-28T20:38:01.667+0000","updated":"2009-10-28T20:38:01.667+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771113","id":"12771113","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"If a DFSOutputStraem.close() returned error, then the next call to DFSOutputStream.close() should also return an error. Christian has pointed out that this the code is such that this is not the case. It looks like a bug.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-10-28T20:57:38.609+0000","updated":"2009-10-28T20:57:38.609+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771117","id":"12771117","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"The behavior of DFSOutputStream.close() was changed by HADOOP-4760.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-28T21:06:58.321+0000","updated":"2009-10-28T21:06:58.321+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771118","id":"12771118","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Since HADOOP-4760 was committed to 0.19 and above, so 0.18 does not have such problem.\n\nUpdated component to \"hdfs client\".","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-28T21:08:57.882+0000","updated":"2009-10-28T21:08:57.882+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771120","id":"12771120","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"I do not think the change made in HADOOP-4760 was right.\n\n> If the stream is already closed then invoking this  method has no effect.\nshould be interpreted as: if the stream is successfully closed then invoking this method has no effect.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-28T21:14:19.654+0000","updated":"2009-10-28T21:14:19.654+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771129","id":"12771129","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"> If a DFSOutputStraem.close() returned error, then the next call to DFSOutputStream.close() should also return an error. ...\n+1\n\nThis idea actually was implemented by HADOOP-5859, which was committed to 0.20.  So this bug does not exist in 0.20 and above.  Only 0.19 and 0.20 have such problem.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-28T21:49:54.941+0000","updated":"2009-10-28T21:49:54.941+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771133","id":"12771133","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"> This idea actually was implemented by HADOOP-5859, which was committed to 0.20. So this bug does not exist in 0.20 and above. Only 0.19 and 0.20 have such problem.\n\nTYPO: I mean HADOOP-5859 was committed to *0.21*. So this bug does not exist in *0.21* and above.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-28T21:53:55.851+0000","updated":"2009-10-28T21:53:55.851+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771139","id":"12771139","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"h732_20091028_0.20.patch: fix 0.20 by the codes in trunk.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-28T22:02:05.927+0000","updated":"2009-10-28T22:02:05.927+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771218","id":"12771218","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"body":"+1. The patch looks good.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hairong","name":"hairong","key":"hairong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hairong Kuang","active":true,"timeZone":"Etc/UTC"},"created":"2009-10-29T00:25:24.668+0000","updated":"2009-10-29T00:25:24.668+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771221","id":"12771221","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"Hi nicholas, can we put this patch in 0.20.3 release? Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-10-29T00:36:50.543+0000","updated":"2009-10-29T00:36:50.543+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771599","id":"12771599","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"> Hi nicholas, can we put this patch in 0.20.3 release? Thanks. \n\nI believe this is qualified to be a blocker of 0.20.\n\nShould we brother fixing 0.19?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-29T20:45:42.543+0000","updated":"2009-10-29T20:45:42.543+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771675","id":"12771675","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"body":"> Should we brother fixing 0.19?\n\nEither way is fine. I will have to pull it into pur 0.19 hadoop cluster as well.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dhruba","name":"dhruba","key":"dhruba","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=dhruba&avatarId=30636","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=dhruba&avatarId=30636","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=dhruba&avatarId=30636","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=dhruba&avatarId=30636"},"displayName":"dhruba borthakur","active":true,"timeZone":"America/Tijuana"},"created":"2009-10-29T22:56:13.928+0000","updated":"2009-10-29T22:56:13.928+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771739","id":"12771739","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Tested on 0.20:\n{noformat}\n     [exec] -1 overall.  \n     [exec] \n     [exec]     +1 @author.  The patch does not contain any @author tags.\n     [exec] \n     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.\n     [exec]                         Please justify why no tests are needed for this patch.\n     [exec] \n     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.\n     [exec] \n     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n     [exec] \n     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.\n     [exec] \n     [exec]     +1 Eclipse classpath. The patch retains Eclipse classpath integrity.\n{noformat}\nAll unit tests passed except TestDatanodeBlockScanner, TestFsck and TestReduceFetch.  The failures are not related to the patch.  These three tests also failed on a clean 0.20 trunk in my machine.  See HDFS-734 for TestDatanodeBlockScanner.  I will file new issues for TestFsck and TestReduceFetch.\n\nNo new test is added since the change is very simple.\n\nI am going to commit the patch to 0.20 only since we don't have plan to release 0.19.3.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-30T01:57:34.513+0000","updated":"2009-10-30T01:57:34.513+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12771741","id":"12771741","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"I have committed this to 0.20 only.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-30T02:11:02.523+0000","updated":"2009-10-30T02:11:02.523+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12439023/comment/12772191","id":"12772191","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"> ... I will have to pull it into pur 0.19 hadoop cluster as well.\n\nHi Dhruba, h732_20091028_0.20.patch is also applied to 0.19.  You may want to use it.\n\n> ... I will file new issues for TestFsck and TestReduceFetch.\n\nFiled HDFS-745 and MAPREDUCE-1172 for TestFsck and TestReduceFetch, respectively.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-10-31T00:38:22.281+0000","updated":"2009-10-31T00:38:22.281+0000"}],"maxResults":25,"total":25,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-732/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0jpkv:"}}