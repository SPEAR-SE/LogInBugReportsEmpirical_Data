{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12471144","self":"https://issues.apache.org/jira/rest/api/2/issue/12471144","key":"HDFS-1336","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/8","id":"8","description":"The described issue is not actually a problem - it is as designed.","name":"Not A Problem"},"customfield_12312322":null,"customfield_12310220":"2014-04-03T02:39:35.038+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Apr 03 02:39:35 UTC 2014","customfield_12310420":"15664","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_115104598964_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-04-03T02:39:35.005+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-1336/watchers","watchCount":3,"isWatching":false},"created":"2010-08-09T21:09:36.083+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12315103","id":"12315103","description":"Append/sync support for Hadoop 0.20","name":"0.20-append","archived":true,"released":false}],"issuelinks":[],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-04-03T02:39:35.043+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312927","id":"12312927","name":"datanode"}],"timeoriginalestimate":null,"description":"- Component: data node\n \n- Version: 0.20-append\n \n- Summary: we found a case that when a block is truncated during updateBlock,\nthe length on the ongoingCreates is not updated, hence leading to failed append.\n\n \n- Setup:\n# disks / datanode = 3\n# failures = 2\nfailure type = crash\nWhen/where failure happens = (see below)\n \n- Details:\n1) Client writes to dn1-dn2-dn3. Write successes.\n2) Now client tried to append. It first call dn1.recoverBlock().This recoverBlock succeeds.\n3) Suppose the pipeline is dn3-dn2-dn1. Client sends packet to dn3. \ndn3 forwards the packet to dn2 and writes to its disk (i.e dn3's disk).\nNow, *dn2 crashes*, so that dn1 has not received this packet yet.\n4) Client calls dn1.recoverBlock() again, this time with dn3-dn1 in the pipeline.\ndn1 then calls dn3.startBlockRecovery() to terminate the writer thread in dn3.\nget the *in memory* metadata info of the block, and verify that info with\nthe real file on disk.\ndn3 maintains an in-memory data structure call *ongoingCreates* to record\ninformation about currently-being-created block. If a block is finalized, then\nits info is removed from *ongoingCreates*.\n \nNow suppose that at the time dn3 receives startBlockRecovery() request from dn1, \nit has:\n     + finished writing data to disk (hence, the block length on disk is 1024)\n     + set visible in memory length (hence, in memory length is also 1024)\nbut it *has not* finalized the block, hence the block info is still in the *ongoingCreates*.\n(Note: the interruption of writer thread makes the finalization never happens)\n \nBecause of all above stuff, dn3 gives dn1 info about the block with length 1024.\n \n5. Now dn1 calls its own startBlockRecovery() successfully (because the on-disk\nfile length and memory file length match, both are 512 byte).\n \n6. Now, dn1 has a sync list (block_X_GS1 at dn1 with length 512, block_X_GS1 at dn3 with length 1024).\nit needs to make sure all dn in the pipeline agree on new GS and length.\ndn1 calls NN.nextGS() to get new GS2. It form new block_X_GS2 with length 512, and\ncall updateBlock on dn3 and itself.\n \n7. dn3, receiving updateBlock request from dn1, does:\n     + rename the block from block_X_GS1 ==> block_X_GS2\n     + truncate the block file length from 1024 to 512\n     But, here is the key, it *does not update the length of the block kept in ongoingCreates*\n     + return to dn1 successfully\n \n8. Now, dn1 call its own updateBlock and *crashes*.\n \n9. From client point of view, dn1.recoverBlock fails. \nIt retries call dn1.recoverBlock six times, and declare dn1 as bad.\n \n10. Client now calls dn3.recoverBlock()\n \n11. Dn3 in turns calls its startBlockRecovery() to\n     + interrupt block writer threads if any\n     + getBlockMetadataInfo (as part of forming the syncList, and updateBlock later)\n          > it first look into ongoingCreates to see the block info is there,\n          and found it (because the block is not finalized).\n          Hence, in-memory length is 1024 (even though truncateBlock is called before) \n          > verify if the in-memory length (1024) with on-disk length (512)\n          Hence, the *un-matched file length exception*\n \n12. From client point of view, recoverBlock fails, because *All data nodes are bad*\nClient retries calling dn3.recoverBlock five more times and gets the same exception,\nHence, append fails.\n \nNote:\n- to fix it, i think when truncating the file, we need to update the ongoingCreates too\n(but i am not sure, if we fix thing like this, is there any other workload may affect)\n- interestingly, NN.leaseRecovery fails because of the exact exception at dn3.\n- until dead node restarts and NN.leaseRecovery is triggered again, NN is still the lease holder of the file\n\nThis bug was found by our Failure Testing Service framework:\nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html\nFor questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and \nHaryadi Gunawi (haryadi@eecs.berkeley.edu","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"113468","customfield_12312823":null,"summary":"TruncateBlock does not update in-memory information correctly","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thanhdo","name":"thanhdo","key":"thanhdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thanhdo&avatarId=22565","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thanhdo&avatarId=22565","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thanhdo&avatarId=22565","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thanhdo&avatarId=22565"},"displayName":"Thanh Do","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thanhdo","name":"thanhdo","key":"thanhdo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thanhdo&avatarId=22565","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thanhdo&avatarId=22565","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thanhdo&avatarId=22565","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thanhdo&avatarId=22565"},"displayName":"Thanh Do","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12471144/comment/13958465","id":"13958465","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"I guess that this is not a problem anymore. Please feel free to reopen this if I am wrong. Resolving ...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-04-03T02:39:35.038+0000","updated":"2014-04-03T02:39:35.038+0000"}],"maxResults":1,"total":1,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-1336/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0js0f:"}}