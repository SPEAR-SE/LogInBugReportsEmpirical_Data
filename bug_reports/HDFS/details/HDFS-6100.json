{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12701132","self":"https://issues.apache.org/jira/rest/api/2/issue/12701132","key":"HDFS-6100","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326143","id":"12326143","description":"2.4.0 release","name":"2.4.0","archived":false,"released":true,"releaseDate":"2014-04-07"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2014-03-14T00:38:30.291+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Mar 20 15:00:23 UTC 2014","customfield_12310420":"379478","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_494594450_*|*_1_*:*_1_*:*_86384135_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2014-03-19T17:36:28.216+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6100/watchers","watchCount":5,"isWatching":false},"created":"2014-03-13T00:13:29.680+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326143","id":"12326143","description":"2.4.0 release","name":"2.4.0","archived":false,"released":true,"releaseDate":"2014-04-07"}],"issuelinks":[{"id":"12385140","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12385140","type":{"id":"12310050","name":"Regression","inward":"is broken by","outward":"breaks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310050"},"outwardIssue":{"id":"12702976","key":"HDFS-6140","self":"https://issues.apache.org/jira/rest/api/2/issue/12702976","fields":{"summary":"WebHDFS cannot create a file with spaces in the name after HA failover changes.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wheat9","name":"wheat9","key":"wheat9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haohui Mai","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2014-04-10T13:11:25.037+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12316609","id":"12316609","name":"ha","description":"High Availability"}],"timeoriginalestimate":null,"description":"In {{DataNodeWebHdfsMethods}}, the code creates a {{DFSClient}} to connect to the NN, so that it can access the files in the cluster. {{DataNodeWebHdfsMethods}} relies on the address passed in the URL to locate the NN. This implementation has two problems:\n\n# The {{DFSClient}} only knows about the current active NN, thus it does not support failover.\n# The delegation token is based on the active NN, therefore the {{DFSClient}} will fail to authenticate of the standby NN in secure HA setup.\n\nCurrently the parameter {{namenoderpcaddress}} in the URL stores the host-ip pair that corresponds to the active NN. To fix this bug, this jira proposes to store the name service id in the parameter in HA setup (yet the parameter stays the same in non-HA setup).","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326143","id":"12326143","description":"2.4.0 release","name":"2.4.0","archived":false,"released":true,"releaseDate":"2014-04-07"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12634593","id":"12634593","filename":"HDFS-6100.000.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wheat9","name":"wheat9","key":"wheat9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haohui Mai","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-14T00:13:09.598+0000","size":34216,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12634593/HDFS-6100.000.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12635471","id":"12635471","filename":"HDFS-6100.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wheat9","name":"wheat9","key":"wheat9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haohui Mai","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-19T03:03:21.874+0000","size":37928,"content":"https://issues.apache.org/jira/secure/attachment/12635471/HDFS-6100.001.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"379769","customfield_12312823":null,"summary":"DataNodeWebHdfsMethods does not failover in HA mode","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitgupta","name":"arpitgupta","key":"arpitgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Gupta","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitgupta","name":"arpitgupta","key":"arpitgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Gupta","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13932691","id":"13932691","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitgupta","name":"arpitgupta","key":"arpitgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Gupta","active":true,"timeZone":"America/Los_Angeles"},"body":"{code}\n/usr/lib/hadoop/bin/hadoop org.apache.hadoop.fs.slive.SliveTest -rename 14,uniform -packetSize 65536 -baseDir webhdfs://ha-2-unsecure/user/hrt_qa/ha-slive -seed 12345678 -sleep 100,1000 -duration 600 -append 14,uniform -blockSize 16777216,33554432 -create 16,uniform -mkdir 14,uniform -maps 15 -ls 14,uniform -writeSize 1,134217728 -files 1024 -ops 10000 -read 14,uniform -replication 1,3 -appendSize 1,134217728 -reduces 8 -resFile /grid/0/tmp/hwqe/artifacts/ha-slive-00004-namenode2-1394608134.out -readSize 1,4294967295 -dirSize 16 -delete 14,uniform\n14/03/12 07:08:54 INFO slive.SliveTest: Running with option list -rename 14,uniform -packetSize 65536 -baseDir webhdfs://ha-2-unsecure/user/hrt_qa/ha-slive -seed 12345678 -sleep 100,1000 -duration 600 -append 14,uniform -blockSize 16777216,33554432 -create 16,uniform -mkdir 14,uniform -maps 15 -ls 14,uniform -writeSize 1,134217728 -files 1024 -ops 10000 -read 14,uniform -replication 1,3 -appendSize 1,134217728 -reduces 8 -resFile /grid/0/tmp/hwqe/artifacts/ha-slive-00004-namenode2-1394608134.out -readSize 1,4294967295 -dirSize 16 -delete 14,uniform\n14/03/12 07:08:54 INFO slive.SliveTest: Options are:\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Base directory = webhdfs://ha-2-unsecure/user/hrt_qa/ha-slive/slive\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Data directory = webhdfs://ha-2-unsecure/user/hrt_qa/ha-slive/slive/data\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Output directory = webhdfs://ha-2-unsecure/user/hrt_qa/ha-slive/slive/output\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Result file = /grid/0/tmp/hwqe/artifacts/ha-slive-00004-namenode2-1394608134.out\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Grid queue = default\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Should exit on first error = false\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Duration = 600000 milliseconds\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Map amount = 15\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Reducer amount = 8\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Operation amount = 10000\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Total file limit = 1024\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Total dir file limit = 16\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Read size = 1,4294967295 bytes\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Write size = 1,134217728 bytes\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Append size = 1,134217728 bytes\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Block size = 16777216,33554432 bytes\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Random seed = 12345678\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Sleep range = 100,1000 milliseconds\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Replication amount = 1,3\n14/03/12 07:08:54 INFO slive.ConfigExtractor: Operations are:\n14/03/12 07:08:54 INFO slive.ConfigExtractor: MKDIR\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  UNIFORM\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  14%\n14/03/12 07:08:54 INFO slive.ConfigExtractor: APPEND\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  UNIFORM\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  14%\n14/03/12 07:08:54 INFO slive.ConfigExtractor: RENAME\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  UNIFORM\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  14%\n14/03/12 07:08:54 INFO slive.ConfigExtractor: LS\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  UNIFORM\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  14%\n14/03/12 07:08:54 INFO slive.ConfigExtractor: READ\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  UNIFORM\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  14%\n14/03/12 07:08:54 INFO slive.ConfigExtractor: CREATE\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  UNIFORM\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  16%\n14/03/12 07:08:54 INFO slive.ConfigExtractor: DELETE\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  UNIFORM\n14/03/12 07:08:54 INFO slive.ConfigExtractor:  14%\n14/03/12 07:08:54 INFO slive.SliveTest: Running job:\n14/03/12 07:08:55 WARN hdfs.DFSClient: dfs.client.test.drop.namenode.response.number is set to 1, this hacked client will proactively drop responses\n14/03/12 07:08:55 WARN hdfs.DFSClient: dfs.client.test.drop.namenode.response.number is set to 1, this hacked client will proactively drop responses\n14/03/12 07:08:55 WARN hdfs.DFSClient: dfs.client.test.drop.namenode.response.number is set to 1, this hacked client will proactively drop responses\n14/03/12 07:08:57 INFO web.WebHdfsFileSystem: Retrying connect to namenode: hostname/ip:50070. Already tried 0 time(s); retry policy is org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry@1959e6e8, delay 0ms.\n14/03/12 07:09:00 INFO mapreduce.JobSubmitter: number of splits:15\n14/03/12 07:09:01 INFO Configuration.deprecation: mapred.job.queue.name is deprecated. Instead, use mapreduce.job.queuename\n14/03/12 07:09:01 INFO Configuration.deprecation: dfs.write.packet.size is deprecated. Instead, use dfs.client-write-packet-size\n14/03/12 07:09:01 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n14/03/12 07:09:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1394605353097_0010\n14/03/12 07:09:03 INFO impl.YarnClientImpl: Submitted application application_1394605353097_0010\n14/03/12 07:09:03 INFO mapreduce.Job: The url to track the job: http://hostname:8088/proxy/application_1394605353097_0010/\n14/03/12 07:09:03 INFO mapreduce.Job: Running job: job_1394605353097_0010\n14/03/12 07:09:13 INFO mapreduce.Job: Job job_1394605353097_0010 running in uber mode : false\n14/03/12 07:09:13 INFO mapreduce.Job:  map 0% reduce 0%\n14/03/12 07:09:23 INFO mapreduce.Job:  map 27% reduce 0%\n14/03/12 07:09:24 INFO mapreduce.Job:  map 67% reduce 0%\n14/03/12 07:09:25 INFO mapreduce.Job:  map 100% reduce 0%\n14/03/12 07:09:27 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000002_0, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:27 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000004_0, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:27 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000006_0, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:27 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000001_0, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:27 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000003_0, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:27 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000000_0, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:27 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000005_0, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:28 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000007_0, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:32 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000001_1, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:32 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000003_1, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:32 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000000_1, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:32 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000006_1, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:32 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000005_1, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:33 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000004_1, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:33 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000007_1, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:33 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000002_1, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:37 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000001_2, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:37 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000000_2, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:37 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000005_2, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:37 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000006_2, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:37 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000003_2, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:38 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000004_2, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:38 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000007_2, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:38 INFO mapreduce.Job: Task Id : attempt_1394605353097_0010_r_000002_2, Status : FAILED\nError: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\nat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:348)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:326)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:107)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:855)\nat org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.close(TextOutputFormat.java:108)\nat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby\nat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)\nat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1563)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1174)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2204)\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2171)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:517)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:354)\nat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n\nat org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:159)\nat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:325)\n... 11 more\n\n14/03/12 07:09:43 INFO mapreduce.Job:  map 100% reduce 100%\n14/03/12 07:09:43 INFO mapreduce.Job: Job job_1394605353097_0010 failed with state FAILED due to: Task failed task_1394605353097_0010_r_000000\nJob failed as tasks failed. failedMaps:0 failedReduces:1\n\n14/03/12 07:09:43 INFO mapreduce.Job: Counters: 43\nFile System Counters\nFILE: Number of bytes read=0\nFILE: Number of bytes written=1672250\nFILE: Number of read operations=0\nFILE: Number of large read operations=0\nFILE: Number of write operations=0\nHDFS: Number of bytes read=825\nHDFS: Number of bytes written=0\nHDFS: Number of read operations=21015\nHDFS: Number of large read operations=0\nHDFS: Number of write operations=108000\nWEBHDFS: Number of bytes read=0\nWEBHDFS: Number of bytes written=0\nWEBHDFS: Number of read operations=15\nWEBHDFS: Number of large read operations=0\nWEBHDFS: Number of write operations=0\nJob Counters\nFailed reduce tasks=25\nKilled reduce tasks=7\nLaunched map tasks=15\nLaunched reduce tasks=32\nOther local map tasks=15\nTotal time spent by all maps in occupied slots (ms)=254062\nTotal time spent by all reduces in occupied slots (ms)=145824\nTotal time spent by all map tasks (ms)=127031\nTotal time spent by all reduce tasks (ms)=72912\nTotal vcore-seconds taken by all map tasks=127031\nTotal vcore-seconds taken by all reduce tasks=72912\nTotal megabyte-seconds taken by all map tasks=195119616\nTotal megabyte-seconds taken by all reduce tasks=149323776\nMap-Reduce Framework\nMap input records=15\nMap output records=30\nMap output bytes=1095\nMap output materialized bytes=2895\nInput split bytes=825\nCombine input records=0\nSpilled Records=30\nFailed Shuffles=0\nMerged Map outputs=0\nGC time elapsed (ms)=1629\nCPU time spent (ms)=101350\nPhysical memory (bytes) snapshot=7025311744\nVirtual memory (bytes) snapshot=26846732288\nTotal committed heap usage (bytes)=6805389312\nFile Input Format Counters\nBytes Read=0\n14/03/12 07:09:43 ERROR slive.SliveTest: Unable to run job due to error:\njava.io.IOException: Job failed!\nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\nat org.apache.hadoop.fs.slive.SliveTest.runJob(SliveTest.java:204)\nat org.apache.hadoop.fs.slive.SliveTest.run(SliveTest.java:117)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\nat org.apache.hadoop.fs.slive.SliveTest.main(SliveTest.java:314)\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=arpitgupta","name":"arpitgupta","key":"arpitgupta","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Arpit Gupta","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-13T00:17:39.300+0000","updated":"2014-03-13T00:17:39.300+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13934360","id":"13934360","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wheat9","name":"wheat9","key":"wheat9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haohui Mai","active":true,"timeZone":"America/Los_Angeles"},"body":"The v0 patch overloads the meaning of the URL parameter {{namenoderpcaddress}}. It is the host-port pair of the NN in non-HA mode, but it becomes the nameservice id in HA mode.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wheat9","name":"wheat9","key":"wheat9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haohui Mai","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-14T00:38:30.291+0000","updated":"2014-03-14T00:38:30.291+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13934541","id":"13934541","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12634593/HDFS-6100.000.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6402//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6402//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-14T03:59:49.231+0000","updated":"2014-03-14T03:59:49.231+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13935411","id":"13935411","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"The patch looks pretty good to me. Some minor comments:\n# In DatanodeWebHdfsMethods, the current patch has some inconsistent field name for the NamenodeAddressParam parameter (nnId, namenodeId, and namenodeRpcAddress). How about just calling them \"namenode\" since it can be either NameService ID or NameNode RPC address?\n# Nit: the following code needs some reformat:\n{code}\n    tokenServiceName = HAUtil.isHAEnabled(conf,\n            nsId) ? nsId : NetUtils.getHostPortString\n            (rpcServer.getRpcAddress());\n{code}\n# In the new unit test, we can add some extra check about the content of the new created file. Also, maybe we can try to transition the second NN to active first so that the first create call can also hit a failover.\n# Looks like the patch also fixes the token service name in HA setup for webhdfs. Please update the description of the jira.\n# Could you also post your system test results (HA, non-HA, secured, insecure setup etc.)?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-14T18:28:39.423+0000","updated":"2014-03-14T18:28:39.423+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13940234","id":"13940234","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12635471/HDFS-6100.001.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:\n\norg.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6435//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6435//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-19T06:28:49.763+0000","updated":"2014-03-19T06:28:49.763+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13940245","id":"13940245","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wheat9","name":"wheat9","key":"wheat9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haohui Mai","active":true,"timeZone":"America/Los_Angeles"},"body":"The v1 patch addresses Jing's comments.\n\nI've tested the patch in all four combinations of secure and HA setups. I've verified this patch by using WebHDFS to retrieve the content of a file.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wheat9","name":"wheat9","key":"wheat9","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Haohui Mai","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-19T06:42:02.854+0000","updated":"2014-03-19T06:42:02.854+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13940351","id":"13940351","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:green}+1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12635471/HDFS-6100.001.patch\n  against trunk revision .\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/6439//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/6439//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-19T09:58:10.164+0000","updated":"2014-03-19T09:58:10.164+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13940716","id":"13940716","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"+1 for the latest patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-19T17:22:46.100+0000","updated":"2014-03-19T17:22:46.100+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13940727","id":"13940727","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Hadoop-trunk-Commit #5358 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5358/])\nHDFS-6100. DataNodeWebHdfsMethods does not failover in HA mode. Contributed by Haohui Mai. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1579301)\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeRpcAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFSForHA.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-19T17:36:01.897+0000","updated":"2014-03-19T17:36:01.897+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13940729","id":"13940729","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"I've committed the patch to trunk, branch-2, and branch-2.4. Thanks [~wheat9] for the contribution.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2014-03-19T17:36:28.249+0000","updated":"2014-03-19T17:36:28.249+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13941631","id":"13941631","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Yarn-trunk #515 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/515/])\nHDFS-6100. DataNodeWebHdfsMethods does not failover in HA mode. Contributed by Haohui Mai. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1579301)\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeRpcAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFSForHA.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-20T11:14:25.116+0000","updated":"2014-03-20T11:14:25.116+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13941734","id":"13941734","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Hdfs-trunk #1707 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1707/])\nHDFS-6100. DataNodeWebHdfsMethods does not failover in HA mode. Contributed by Haohui Mai. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1579301)\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeRpcAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFSForHA.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-20T13:44:35.235+0000","updated":"2014-03-20T13:44:35.235+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12701132/comment/13941806","id":"13941806","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1732 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1732/])\nHDFS-6100. DataNodeWebHdfsMethods does not failover in HA mode. Contributed by Haohui Mai. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1579301)\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/NamenodeRpcAddressParam.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFSForHA.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2014-03-20T15:00:23.302+0000","updated":"2014-03-20T15:00:23.302+0000"}],"maxResults":13,"total":13,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-6100/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1tdxb:"}}