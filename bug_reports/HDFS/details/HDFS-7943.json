{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12782644","self":"https://issues.apache.org/jira/rest/api/2/issue/12782644","key":"HDFS-7943","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327584","id":"12327584","description":"2.7.0 release","name":"2.7.0","archived":false,"released":true,"releaseDate":"2015-04-20"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2015-03-17T22:38:04.090+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Thu Mar 19 15:23:07 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_112750599_*|*_1_*:*_1_*:*_596138_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-03-19T01:49:00.675+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-7943/watchers","watchCount":6,"isWatching":false},"created":"2015-03-17T18:19:53.999+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327584","id":"12327584","description":"2.7.0 release","name":"2.7.0","archived":false,"released":true,"releaseDate":"2015-04-20"}],"issuelinks":[{"id":"12410911","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12410911","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12599539","key":"HDFS-3689","self":"https://issues.apache.org/jira/rest/api/2/issue/12599539","fields":{"summary":"Add support for variable length block","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/2","id":"2","description":"A new feature of the product, which has yet to be developed.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype","name":"New Feature","subtask":false,"avatarId":21141}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-04-10T20:29:39.871+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":"In HDFS-3689, we remove the restriction from concat that all the source files should have the same preferred block size with the target file. This can cause a file to contain blocks with size larger than its preferred block size.\n\nIf such block happens to be the last block of a file, and later we append data to the file without the {{CreateFlag.NEW_BLOCK}} flag (i.e., appending data to the last block), looks like the current client code will keep writing to the last block and never allocate a new block.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12327584","id":"12327584","description":"2.7.0 release","name":"2.7.0","archived":false,"released":true,"releaseDate":"2015-04-20"}],"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12705144","id":"12705144","filename":"HDFS-7943.000.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-17T18:29:39.968+0000","size":4709,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12705144/HDFS-7943.000.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12705443","id":"12705443","filename":"HDFS-7943.001.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-18T21:32:06.465+0000","size":6504,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12705443/HDFS-7943.001.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Append cannot handle the last block with length greater than the preferred block size","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14365720","id":"14365720","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"We have two options to fix:\n# Change append to support blocks with size greater than the preferred block size. Specifically, we can update the logic of {{BlockManager#convertLastBlockToUnderConstruction}}: if the last block size is greater than or equal to the preferred size, the client should write to a new block. Since the same function is also used by truncate, the new logic should also support the scenario that data is truncated from such block.\n# Or we can add a new restriction to concat: the source file's preferred block size cannot be greater than the target. This fix is simpler and keeps the semantic that the preferred block size is the upper limit of a file's blocks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-17T18:27:57.975+0000","updated":"2015-03-17T18:27:57.975+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14365724","id":"14365724","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Upload a patch implementing the above #2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-17T18:29:39.975+0000","updated":"2015-03-17T18:29:39.975+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14366221","id":"14366221","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12705144/HDFS-7943.000.patch\n  against trunk revision d884670.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.TestDFSClientRetries\n\n                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:\n\norg.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9940//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9940//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-17T22:38:04.090+0000","updated":"2015-03-17T22:38:04.090+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14366303","id":"14366303","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"The test failures should be unrelated and they all passed in my local run.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-17T23:31:10.133+0000","updated":"2015-03-17T23:31:10.133+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14366496","id":"14366496","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"- In verifyQuotaForUCBlock,\n-* let's add a precondition check to make sure delta >= 0 (for append).\n-* It should check if (!fsd.getFSNamesystem().isImageLoaded() || fsd.shouldSkipQuotaChecks()); see FSDirConcatOp.verifyQuota and FSDirRenameOp.verifyQuotaForRename.\n\n- We should also verity quota by storage types; see FSDirConcatOp.computeQuotaDeltas.\n\n- For truncate, the quota should be updated differently as following:\n-* Copy-on-truncate for snapshot: need quota check for creating the new block.  Quota usage count is increased.\n-* Non-copy-on-truncate OR Copy-on-truncate for upgrade but not snapshot: Quota usage count is decreased.  No quota check is needed.\n\nDo you want to fix truncate in a different JIRA?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-18T02:16:37.421+0000","updated":"2015-03-18T02:16:37.421+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14366502","id":"14366502","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Oops, my previous comment was for HDFS-7587.  Please ignore it here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-18T02:21:15.017+0000","updated":"2015-03-18T02:21:15.017+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14366513","id":"14366513","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Is it possible to change append so that it always appends to a new block when the last block is great than or equal to the preferred block size?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-18T02:29:27.097+0000","updated":"2015-03-18T02:29:27.097+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14367807","id":"14367807","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Yes, that's fix option #1. Then the code change on {{convertLastBlockToUnderConstruction}} also affects truncate. It's a little bit more complicated than option #2. I will upload a patch doing #1 later.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-18T20:15:52.589+0000","updated":"2015-03-18T20:15:52.589+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14367953","id":"14367953","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"I have an initial patch fixing the issue following #1. Then I find that more places have the assumption that the block's size upper limit is the preferred size. E.g., when conversion between UC block and complete block happens, we always follow this assumption to update quota usage. Also, when computing storage usage of a UC file, we use the preferred size as the size of the last UC block. These places require fixes as well.\n\nCurrently I prefer #2 because of its simplicity. Also I think it is good to have the preferred block size as the upper limit. This upper limit is also kind of respected by the client while writing data.\n\nWhat do you think, [~szetszwo]?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-18T21:24:40.528+0000","updated":"2015-03-18T21:29:12.554+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14367964","id":"14367964","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"001 patch tries to fix the append. But still need to check/fix the quota usage code.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-18T21:32:06.472+0000","updated":"2015-03-18T21:32:06.472+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14368252","id":"14368252","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"{color:red}-1 overall{color}.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12705443/HDFS-7943.001.patch\n  against trunk revision c239b6d.\n\n    {color:green}+1 @author{color}.  The patch does not contain any @author tags.\n\n    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.\n                        Please justify why no new tests are needed for this patch.\n                        Also please list what manual steps were performed to verify this patch.\n\n    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.\n\n    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.\n\n    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.\n\n    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.\n\n    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.\n\n    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.TestDFSClientRetries\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/9965//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/9965//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-19T00:54:48.459+0000","updated":"2015-03-19T00:54:48.459+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14368315","id":"14368315","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"> Currently I prefer #2 because of its simplicity. Also I think it is good to have the preferred block size as the upper limit. This upper limit is also kind of respected by the client while writing data.\n\nSure, let's do it for now.  We may change convertLastBlockToUnderConstruction in a separated JIRA.\n\n+1 on HDFS-7943.000.patch\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-19T01:37:16.067+0000","updated":"2015-03-19T01:37:16.067+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14368330","id":"14368330","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks again for the review, Nicholas! I've committed this to trunk, branch-2 and branch-2.7.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jingzhao","name":"jingzhao","key":"jingzhao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jing Zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-19T01:49:00.712+0000","updated":"2015-03-19T01:49:00.712+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14368335","id":"14368335","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-trunk-Commit #7367 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7367/])\nHDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-19T01:52:39.467+0000","updated":"2015-03-19T01:52:39.467+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14368907","id":"14368907","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #137 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/137/])\nHDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-19T11:35:33.257+0000","updated":"2015-03-19T11:35:33.257+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14368929","id":"14368929","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Yarn-trunk #871 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/871/])\nHDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-19T11:37:13.228+0000","updated":"2015-03-19T11:37:13.228+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14369398","id":"14369398","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Hdfs-trunk #2069 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2069/])\nHDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-19T14:37:45.844+0000","updated":"2015-03-19T14:37:45.844+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14369419","id":"14369419","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #128 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/128/])\nHDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-19T14:39:43.672+0000","updated":"2015-03-19T14:39:43.672+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14369487","id":"14369487","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #137 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/137/])\nHDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-19T15:12:05.693+0000","updated":"2015-03-19T15:12:05.693+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12782644/comment/14369525","id":"14369525","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hadoop-Mapreduce-trunk #2087 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2087/])\nHDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)\n* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java\n* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2015-03-19T15:23:07.788+0000","updated":"2015-03-19T15:23:07.788+0000"}],"maxResults":20,"total":20,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-7943/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i26vpb:"}}