{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12596327","self":"https://issues.apache.org/jira/rest/api/2/issue/12596327","key":"HDFS-3577","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320052","id":"12320052","description":"0.23.3","name":"0.23.3","archived":false,"released":true,"releaseDate":"2012-09-20"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12322472","id":"12322472","description":"2.0.2-alpha release","name":"2.0.2-alpha","archived":false,"released":true,"releaseDate":"2012-10-09"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2012-06-28T22:07:32.398+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Sun Aug 12 21:52:12 UTC 2012","customfield_12310420":"241008","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_1076088936_*|*_1_*:*_1_*:*_631165974_*|*_5_*:*_2_*:*_9901975_*|*_4_*:*_1_*:*_109221943","customfield_12312321":null,"resolutiondate":"2012-07-20T00:37:46.201+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-3577/watchers","watchCount":17,"isWatching":false},"created":"2012-06-28T21:18:07.494+0000","customfield_12310192":null,"customfield_12310191":[{"self":"https://issues.apache.org/jira/rest/api/2/customFieldOption/10343","value":"Reviewed","id":"10343"}],"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"5.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12320052","id":"12320052","description":"0.23.3","name":"0.23.3","archived":false,"released":true,"releaseDate":"2012-09-20"},{"self":"https://issues.apache.org/jira/rest/api/2/version/12320353","id":"12320353","description":"hadoop-2.0.0-alpha release","name":"2.0.0-alpha","archived":false,"released":true,"releaseDate":"2012-05-23"}],"issuelinks":[{"id":"12356585","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12356585","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12603151","key":"HDFS-3788","self":"https://issues.apache.org/jira/rest/api/2/issue/12603151","fields":{"summary":"distcp can't copy large files using webhdfs due to missing Content-Length header","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12355252","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12355252","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12599005","key":"HDFS-3671","self":"https://issues.apache.org/jira/rest/api/2/issue/12599005","fields":{"summary":"ByteRangeInputStream shouldn't require the content length header be present","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/10002","description":"A patch for this issue has been uploaded to JIRA by a contributor.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/document.png","name":"Patch Available","id":"10002","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/4","id":4,"key":"indeterminate","colorName":"yellow","name":"In Progress"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12367684","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12367684","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12643423","key":"HDFS-4715","self":"https://issues.apache.org/jira/rest/api/2/issue/12643423","fields":{"summary":"Backport HDFS-3577 and other related WebHDFS issue to branch-1","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12355251","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12355251","type":{"id":"12310050","name":"Regression","inward":"is broken by","outward":"breaks","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310050"},"inwardIssue":{"id":"12552440","key":"HDFS-3318","self":"https://issues.apache.org/jira/rest/api/2/issue/12552440","fields":{"summary":"Hftp hangs on transfers >2GB","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2013-04-18T21:29:52.425+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12319200","id":"12319200","name":"webhdfs","description":"WebHDFS: HTTP REST API for HDFS"}],"timeoriginalestimate":null,"description":"If reading a file large enough for which the httpserver running webhdfs/httpfs uses chunked transfer encoding (more than 24K in the case of webhdfs), then the WebHdfsFileSystem client fails with an IOException with message *Content-Length header is missing*.\n\nIt looks like WebHdfsFileSystem is delegating opening of the inputstream to *ByteRangeInputStream.URLOpener* class, which checks for the *Content-Length* header, but when using chunked transfer encoding the *Content-Length* header is not present and  the *URLOpener.openInputStream()* method thrown an exception.\n","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12535316","id":"12535316","filename":"h3577_20120705.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-06T04:37:22.831+0000","size":3746,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12535316/h3577_20120705.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12535614","id":"12535614","filename":"h3577_20120708.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-09T04:00:32.327+0000","size":3935,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12535614/h3577_20120708.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12536511","id":"12536511","filename":"h3577_20120714.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-14T18:35:56.285+0000","size":9336,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12536511/h3577_20120714.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12536761","id":"12536761","filename":"h3577_20120716.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-17T01:56:34.832+0000","size":9474,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12536761/h3577_20120716.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12536946","id":"12536946","filename":"h3577_20120717.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-18T03:08:51.706+0000","size":7591,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12536946/h3577_20120717.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"5027","customfield_12312823":null,"summary":"WebHdfsFileSystem can not read files larger than 24KB","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13403515","id":"13403515","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Is this true on branch-1 as well?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-06-28T22:07:32.398+0000","updated":"2012-06-28T22:07:32.398+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13403521","id":"13403521","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"body":"@Eli, it doesn't look like, doing a quick check to the *ByteRangeInputStream.getInputStream()* method the impl is different, no exception begin thrown:\n\n{code}\n      final String cl = connection.getHeaderField(StreamFile.CONTENT_LENGTH);\n      filelength = (cl == null) ? -1 : Long.parseLong(cl);\n      in = connection.getInputStream();\n{code}\n\nThe *ByteRangeInputClass* seems that had some significant rewriting in Hadoop2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"created":"2012-06-28T22:16:52.859+0000","updated":"2012-06-28T22:16:52.859+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13406232","id":"13406232","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"I verfied this is an issue on branch-1 as well:\n\n{noformat}\n2012-07-03 18:43:23,150 INFO org.apache.hadoop.tools.DistCp: FAIL CentOS-5.8-x86_64-netinstall.iso : java.io.IOException: Content-Length header is missing\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:125)\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:103)\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.read(ByteRangeInputStream.java:158)\n\tat java.io.DataInputStream.read(DataInputStream.java:83)\n\tat org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:424)\n\tat org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:547)\n\tat org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:314)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:266)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1178)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:260)\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-04T01:49:55.288+0000","updated":"2012-07-04T01:49:55.288+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13406233","id":"13406233","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Marking as a blocker since this prevents any reasonable distcp using webhdfs from working.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-04T01:50:35.019+0000","updated":"2012-07-04T01:50:35.019+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13406257","id":"13406257","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Two potential fixes:\n\n# change WebHdfsFileSystem to support chunked transfer encoding;\n# change server to set content length.\n\nWhich one is better?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-04T03:47:17.546+0000","updated":"2012-07-04T03:47:17.546+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13406259","id":"13406259","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"body":"AFAIK you don't want to do #2 as that triggers in-memory client full buffering of the response. Note that to do #1 you don't have to do much, in Hadoop 2 just don't throw an exception if content length is not set.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"created":"2012-07-04T04:08:58.532+0000","updated":"2012-07-04T04:08:58.532+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13406265","id":"13406265","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Alejandro, thanks for the hint.  I will try #1 then.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-04T04:23:38.866+0000","updated":"2012-07-04T04:23:38.866+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13407699","id":"13407699","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"h3577_20120705.patch: do not throw exceptions when Content-Length is missing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-06T04:37:23.079+0000","updated":"2012-07-06T04:37:23.079+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13407740","id":"13407740","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12535316/h3577_20120705.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.TestHDFSTrash\n                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2746//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2746//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-06T06:34:02.094+0000","updated":"2012-07-06T06:34:02.094+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13407742","id":"13407742","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12535316/h3577_20120705.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract\n                  org.apache.hadoop.hdfs.TestHDFSTrash\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2747//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2747//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-06T06:40:35.038+0000","updated":"2012-07-06T06:40:35.038+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13409153","id":"13409153","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"The file size was 1MB in the test but the block size was only 1kB.  Therefore, it created a lot of local files and failed with \"java.net.SocketException: Too many open files\".","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-09T03:58:33.327+0000","updated":"2012-07-09T03:58:33.327+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13409155","id":"13409155","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"h3577_20120708.patch: use a larger block size.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-09T04:00:32.414+0000","updated":"2012-07-09T04:00:32.414+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13409186","id":"13409186","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12535614/h3577_20120708.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    -1 findbugs.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract\n                  org.apache.hadoop.hdfs.TestDFSClientRetries\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2761//testReport/\nFindbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/2761//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2761//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-09T05:26:41.608+0000","updated":"2012-07-09T05:26:41.608+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13409506","id":"13409506","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"No, no, no!  This is reverting a fix for > 32-bit file transfers.  I think the correct fix is to require content-length unless chunked encoding is being used.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2012-07-09T14:19:01.551+0000","updated":"2012-07-09T14:19:01.551+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13409508","id":"13409508","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"Sorry, I freaked out before studying the whole patch.  I still think a chunked encoding check should be present unless I'm misunderstanding something.  There's also not much use in instantiating a {{BoundedInputStream}} w/o a limit.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2012-07-09T14:22:45.826+0000","updated":"2012-07-09T14:22:45.826+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13409701","id":"13409701","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"bq. The file size was 1MB in the test but the block size was only 1kB. Therefore, it created a lot of local files and failed with \"java.net.SocketException: Too many open files\".\n\nDoes this mean there's a fd leak?  Or at least a leak during the create request?  If so, is the test at fault?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2012-07-09T18:21:55.846+0000","updated":"2012-07-09T18:21:55.846+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13414469","id":"13414469","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"It turns that the size of each chunk is very small (tens of bytes) when it uses chunked transfer encoding.  The client keeps opening new socket for each chunk and then it leads to SocketException.  The problem disappear if StreamingOutput is replaced with MessageBodyWriter since it can specify the response size.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-14T18:34:45.832+0000","updated":"2012-07-14T18:34:45.832+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13414470","id":"13414470","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"h3577_20120714.patch: uses MessageBodyWriter in DatanodeWebHdfsMethods.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-14T18:35:56.308+0000","updated":"2012-07-14T18:35:56.308+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13414488","id":"13414488","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12536511/h3577_20120714.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.namenode.TestBackupNode\n                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2824//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2824//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-14T20:12:13.004+0000","updated":"2012-07-14T20:12:13.004+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13414718","id":"13414718","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"How about a test that copies > 1 block of data so we can detect regressions?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-15T17:39:50.556+0000","updated":"2012-07-15T17:39:50.556+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13414721","id":"13414721","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Tucu, do we have an HttpFs test that checks that it can read files big enough to require chunked transfer encoding?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-15T17:44:56.528+0000","updated":"2012-07-15T17:44:56.528+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13414838","id":"13414838","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"The failed TestBlocksWithNotEnoughRacks is not related to this.\n\n> How about a test that copies > 1 block of data so we can detect regressions?\n\nHi Eli, thanks for taking a look.  The modified test in the patch already has writing and reading > 1 blocks of data.  It will fail if the fix is not applied.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-16T04:08:40.209+0000","updated":"2012-07-16T04:08:40.209+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13414859","id":"13414859","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12536511/h3577_20120714.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.namenode.TestBackupNode\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2830//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2830//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-16T05:42:19.096+0000","updated":"2012-07-16T05:42:19.096+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415162","id":"13415162","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"The {{BoundedInputStream}} is a no-op when the ctor w/o a length is used, so I think this:\n{code}final InputStream is = cl == null? new BoundedInputStream(in)\n    : new BoundedInputStream(in, Long.parseLong(cl));{code}\ncan be:\n{code}final InputStream is = cl == null? in\n    : new BoundedInputStream(in, Long.parseLong(cl));{code}\n\nA chunk size can be specified for a {{HttpURLConnection}} and we should be able to enable keep-alive on the socket (I thought it was the default?) to avoid new connections for every chunk.  I don't know anything about {{MessageBodyWriter}} et al, so if my suggestion isn't feasible and someone else oks the {{MessageBodyWriter}}, I'm fine with it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2012-07-16T14:23:17.323+0000","updated":"2012-07-16T14:23:17.323+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415332","id":"13415332","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"body":"This impacts branch-0.23 as well.  I really would like to see whatever fix happens go into branch-0.23 as well.  I applied the latest patch and it looks to apply fairly cleanly.  If it does not apply cleanly when checking in the file fix I will be happy to port it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=revans2","name":"revans2","key":"revans2","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Robert Joseph Evans","active":true,"timeZone":"America/Chicago"},"created":"2012-07-16T15:58:43.372+0000","updated":"2012-07-16T15:58:43.372+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415403","id":"13415403","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"body":"@Eli, chunked encoding is enabled by default in modern servlet containers, it normally kicks in after a threshold (I believe in Tomcat this threshold is 8K), don't know what is the threshold of jetty.\n\nAnother thing to keep in mind, is that the HttpURLConnection fully caches in memory the outputstream (uploading data) and this seems hardcoded in the JDK. This would pose a limitation in writing files through webhdfs/httpfs. A solution may be to refactor WebHdfsFileSystem to use HttpClient (arghh, we may need to tweak hadoop-auth to work with HttpClient).","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"created":"2012-07-16T17:13:57.526+0000","updated":"2012-07-16T17:13:57.526+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415419","id":"13415419","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rjain7","name":"rjain7","key":"rjain7","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rahul Jain","active":true,"timeZone":"America/Los_Angeles"},"body":"Added link to HDFS-3318: fix of that issue led to the issue described in HDFS-3577","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rjain7","name":"rjain7","key":"rjain7","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Rahul Jain","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-16T17:31:02.170+0000","updated":"2012-07-16T17:31:02.170+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415448","id":"13415448","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"bq. Another thing to keep in mind, is that the HttpURLConnection fully caches in memory the outputstream\n\nIsn't that only the case for non-chunked responses since it has to compute the content-length?  But to avoid that, I think you can use {{setFixedLengthStreamingMode(length)}}.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2012-07-16T17:55:18.174+0000","updated":"2012-07-16T17:55:18.174+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415840","id":"13415840","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"h3577_20120716.patch: do not use BoundedInputStream when content-length is not found.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-17T01:56:34.871+0000","updated":"2012-07-17T01:56:34.871+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415865","id":"13415865","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"+1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12536761/h3577_20120716.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2838//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2838//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-17T03:35:31.616+0000","updated":"2012-07-17T03:35:31.616+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415867","id":"13415867","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"body":"@daryn, http://docs.oracle.com/javase/6/docs/api/java/net/HttpURLConnection.html#setFixedLengthStreamingMode(int), works only if you know the length in advance, when doing a FS create() you don't know how many bytes will be written.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"created":"2012-07-17T03:41:13.314+0000","updated":"2012-07-17T03:41:13.314+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13415869","id":"13415869","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"body":"It seems that using http://docs.oracle.com/javase/6/docs/api/java/net/HttpURLConnection.html#setChunkedStreamingMode(int) would do the trick, if the server side (jetty & tomcat) support it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=tucu00","name":"tucu00","key":"tucu00","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alejandro Abdelnur","active":true,"timeZone":"Europe/Madrid"},"created":"2012-07-17T03:42:19.703+0000","updated":"2012-07-17T03:42:19.703+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13416186","id":"13416186","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"If the content-length check is removed and content-length is not present, transfers >2GB will fail unless the client's read timeout is greater than the server's connection idle timeout (200s), and partial downloads will be considered successful.\n\nI'd suggest the check be left in place on this jira, but it only generates an exception if it's not a chunked download.  We can then decide what to do on HDFS-3671. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2012-07-17T13:46:27.791+0000","updated":"2012-07-17T13:46:27.791+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13416835","id":"13416835","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Okay, let's revert the changes in ByteRangeInputStream.  Here is a new patch.\n\nh3577_20120717.patch","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-18T03:08:51.922+0000","updated":"2012-07-18T03:08:51.922+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13416895","id":"13416895","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"body":"-1 overall.  Here are the results of testing the latest attachment \n  http://issues.apache.org/jira/secure/attachment/12536946/h3577_20120717.patch\n  against trunk revision .\n\n    +1 @author.  The patch does not contain any @author tags.\n\n    +1 tests included.  The patch appears to include 1 new or modified test files.\n\n    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.\n\n    +1 javadoc.  The javadoc tool did not generate any warning messages.\n\n    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.\n\n    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.\n\n    +1 release audit.  The applied patch does not increase the total number of release audit warnings.\n\n    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:\n\n                  org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics\n                  org.apache.hadoop.hdfs.TestPersistBlocks\n\n    +1 contrib tests.  The patch passed contrib unit tests.\n\nTest results: https://builds.apache.org/job/PreCommit-HDFS-Build/2851//testReport/\nConsole output: https://builds.apache.org/job/PreCommit-HDFS-Build/2851//console\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hadoopqa","name":"hadoopqa","key":"hadoopqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hadoopqa&avatarId=10393","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hadoopqa&avatarId=10393","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hadoopqa&avatarId=10393","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hadoopqa&avatarId=10393"},"displayName":"Hadoop QA","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-18T06:21:19.343+0000","updated":"2012-07-18T06:21:19.343+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13417071","id":"13417071","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"+1 Looks good to me!  Might want to alter the description to indicate it's fixing the problem of making new connections for every chunk and exhausting fds.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2012-07-18T13:25:58.827+0000","updated":"2012-07-18T13:25:58.827+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13417167","id":"13417167","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Common-trunk-Commit #2495 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2495/])\n    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)\n\n     Result = SUCCESS\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-18T15:25:48.532+0000","updated":"2012-07-18T15:25:48.532+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13417170","id":"13417170","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Hdfs-trunk-Commit #2560 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2560/])\n    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)\n\n     Result = SUCCESS\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-18T15:27:47.669+0000","updated":"2012-07-18T15:27:47.669+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13417175","id":"13417175","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Daryn for the review.\n\nI have committed this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-18T15:32:22.076+0000","updated":"2012-07-18T15:32:22.076+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13417206","id":"13417206","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Mapreduce-trunk-Commit #2517 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2517/])\n    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)\n\n     Result = FAILURE\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-18T16:15:35.854+0000","updated":"2012-07-18T16:15:35.854+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13417334","id":"13417334","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Nicholas.  This needs to be fixed in branch-1 as well, mind posting a patch?  IMO this is a 1.1 blocker as w/o it we've broken compat with earlier 20 releases and you can't distcp with webdhfs.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-18T18:17:24.376+0000","updated":"2012-07-18T18:17:24.376+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13417933","id":"13417933","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi Eli, the Content-Length exception was introduced by HDFS-3318 which was committed to 0.23 but not branch-1.  So branch-1 does not have the problem on transferring files > 24kB.  However, it may have problem on transferring files > 2GB since it does not have HDFS-3318.  If it is the case, we should backport both HDFS-3318 and this.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-19T01:18:27.169+0000","updated":"2012-07-19T01:18:27.169+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13417936","id":"13417936","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"body":"I will test > 2GB files in branch-1.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=szetszwo","name":"szetszwo","key":"szetszwo","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=szetszwo&avatarId=23156","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=szetszwo&avatarId=23156","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=szetszwo&avatarId=23156","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=szetszwo&avatarId=23156"},"displayName":"Tsz Wo Nicholas Sze","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-19T01:20:24.332+0000","updated":"2012-07-19T01:20:24.332+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13418231","id":"13418231","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Hdfs-trunk #1108 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1108/])\n    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)\n\n     Result = FAILURE\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-19T11:40:47.073+0000","updated":"2012-07-19T11:40:47.073+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13418253","id":"13418253","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Hdfs-0.23-Build #318 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/318/])\n    svn merge -c 1362976 from trunk for HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362981)\n\n     Result = SUCCESS\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362981\nFiles : \n* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs\n* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java\n* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java\n* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-19T12:39:51.269+0000","updated":"2012-07-19T12:39:51.269+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13418301","id":"13418301","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"Integrated in Hadoop-Mapreduce-trunk #1141 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1141/])\n    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)\n\n     Result = FAILURE\nszetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976\nFiles : \n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java\n* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2012-07-19T14:02:53.033+0000","updated":"2012-07-19T14:02:53.033+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13418521","id":"13418521","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Nicholas, I forgot 3318 was not in branch-1. I pinged HDFS-3671 since we're not removing the Content-Length check here.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-19T18:12:29.331+0000","updated":"2012-07-19T18:12:29.331+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13418831","id":"13418831","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"I merged this to branch-2.1.0-alpha.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-20T00:37:46.233+0000","updated":"2012-07-20T00:37:46.233+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13418974","id":"13418974","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Hey Nicholas,\n\nDid you test this with distcp? Trying to distcp from a recent trunk build with this change still fails with *Content-Length header is missing*. Hadoop fs -get using webhdfs with the same file works.\n\n{noformat}\n12/07/19 23:56:43 INFO mapreduce.Job: Task Id : attempt_1342766959778_0002_m_000000_0, Status : FAILED\nError: java.io.IOException: File copy failed: webhdfs://eli-thinkpad:50070/user/eli/data1/big.iso --> hdfs://localhost:8020/user/eli/data4/data1/big.iso\n\tat org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:262)\n\tat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:229)\n\tat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:45)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:726)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:333)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:154)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:149)\nCaused by: java.io.IOException: Couldn't run retriable-command: Copying webhdfs://eli-thinkpad:50070/user/eli/data1/big.iso to hdfs://localhost:8020/user/eli/data4/data1/big.iso\n\tat org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)\n\tat org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:258)\n\t... 10 more\nCaused by: org.apache.hadoop.tools.mapred.RetriableFileCopyCommand$CopyReadException: java.io.IOException: Content-Length header is missing\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.readBytes(RetriableFileCopyCommand.java:201)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:167)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToTmpFile(RetriableFileCopyCommand.java:112)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:90)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:71)\n\tat org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\n\t... 11 more\nCaused by: java.io.IOException: Content-Length header is missing\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:125)\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:103)\n\tat org.apache.hadoop.hdfs.ByteRangeInputStream.read(ByteRangeInputStream.java:158)\n\tat java.io.DataInputStream.read(DataInputStream.java:132)\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:317)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:90)\n\tat org.apache.hadoop.tools.util.ThrottledInputStream.read(ThrottledInputStream.java:70)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.readBytes(RetriableFileCopyCommand.java:198)\n\t... 16 more\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-20T07:01:16.449+0000","updated":"2012-07-20T07:01:16.449+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13419396","id":"13419396","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Per offline conversation with Daryn the reason this isn't an issue is that HDFS-3166 (Add timeout to Hftp connections) isn't in branch-1 (w/o the timeout we just get a 200s lag, not a failure) and 3166 exposed HDFS-3318 (Hftp hangs on transfers >2GB) which introduces the check that will fail if the content length header is not set (ie 20.x).\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-20T18:05:18.679+0000","updated":"2012-07-20T18:05:18.679+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13419410","id":"13419410","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"body":"Yes, HDFS-3166 (add timeouts) exposed the 200s tail on >2GB files caused by a java bug.  The content-length has to be known in order to workaround the java bug, and thus avoid the read timeout.\n\nWhat I think can be done:\n* If chunked, eliminate the content-length requirement\n* If not chunked, and no content-length, obtain the length from a file stat or a HEAD, etc","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=daryn","name":"daryn","key":"daryn","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Daryn Sharp","active":true,"timeZone":"America/Chicago"},"created":"2012-07-20T18:23:07.264+0000","updated":"2012-07-20T18:23:07.264+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13419412","id":"13419412","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Shouldn't be an issue in branch-1 I mean. I verified that on a branch-1 build from today distcp using webhdfs of a directory that contains a 3gb file works.  So looks like this is just an issue on trunk/2.x, though per my comment above the same distcp does not work even with 3577 applied.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-07-20T18:25:27.737+0000","updated":"2012-07-20T18:25:27.737+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12596327/comment/13432846","id":"13432846","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"body":"Btw webhdfs is still broken for large files when using distcp. I confirmed on the latest trunk build and filed HDFS-3788.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=eli","name":"eli","key":"eli","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Eli Collins","active":true,"timeZone":"America/Los_Angeles"},"created":"2012-08-12T21:52:12.240+0000","updated":"2012-08-12T21:52:12.240+0000"}],"maxResults":53,"total":53,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-3577/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i017rj:"}}