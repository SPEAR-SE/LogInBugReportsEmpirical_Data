{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12758249","self":"https://issues.apache.org/jira/rest/api/2/issue/12758249","key":"HDFS-7453","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310942","id":"12310942","key":"HDFS","name":"Hadoop HDFS","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2015-01-07T18:31:07.217+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Jun 16 22:09:05 UTC 2015","customfield_12310420":"9223372036854775807","customfield_12312320":null,"customfield_12310222":"1_*:*_1_*:*_17389926605_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2015-06-16T22:09:04.962+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-7453/watchers","watchCount":10,"isWatching":false},"created":"2014-11-27T15:36:58.470+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"labels":[],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"0.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12326696","id":"12326696","description":"2.4.1 bug-fix release","name":"2.4.1","archived":false,"released":true,"releaseDate":"2014-06-30"}],"issuelinks":[{"id":"12428099","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12428099","type":{"id":"12310000","name":"Duplicate","inward":"is duplicated by","outward":"duplicates","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"},"outwardIssue":{"id":"12746626","key":"HDFS-7208","self":"https://issues.apache.org/jira/rest/api/2/issue/12746626","fields":{"summary":"NN doesn't schedule replication when a DN storage fails","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}}],"customfield_12312339":null,"assignee":null,"customfield_12312337":null,"customfield_12312338":null,"updated":"2015-06-16T22:09:05.059+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312926","id":"12312926","name":"namenode"}],"timeoriginalestimate":null,"description":"We got a severe datalose due to the fact that namenode didn't recognized block it thinks exist are not actually exist on the datanodes. The senario is something as follows:\n\n# A disk fails on one of the datanodes\n# The datanode is forced to decommiss and shut down\n# The disk is replaced and datanode is back again\n# fsck shows everything is fine\n# Repeat 1-4 for few weeks\n# Restart the namenode\n# It suddenly sees tens of thousands under-replicated blocks and hundreds missing blocks\n\nDuring the next disk failure we analysed situation a bit more and found particular block on particular datanode that is missing: there is no file for block and if we try to read it, we got\n{code}\njava.io.IOException: Got error for OP_READ_BLOCK, self=/XXX:33817, remote=XXX/X.X.X.X:50010, for file XXX/X.X.X.X:50010:BP-879324367-YYY-1404837025894:1083356878, for pool BP-879324367-YYY-1404837025894 block 1083356878_9644290\n{code}\n\nWe restarted the datanode and in the log we can see that it did scan all the directories and send the report to namenode:\n\n{code}\n2014-11-27 17:06:34,174  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] FsDatasetImpl - Adding block pool BP-879324367-YYY-1404837025894\n2014-11-27 17:06:34,175  INFO [Thread-41] FsDatasetImpl - Scanning block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/0/dfs/data/current...\n2014-11-27 17:06:34,176  INFO [Thread-43] FsDatasetImpl - Scanning block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/2/dfs/data/current...\n2014-11-27 17:06:34,176  INFO [Thread-42] FsDatasetImpl - Scanning block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/1/dfs/data/current...\n2014-11-27 17:06:34,279  INFO [Thread-42] FsDatasetImpl - Cached dfsUsed found for /mnt/hadoop/1/dfs/data/current/BP-879324367-YYY-1404837025894/current: 62677866794\n2014-11-27 17:06:34,282  INFO [Thread-42] FsDatasetImpl - Time taken to scan block pool BP-879324367-YYY-1404837025894 on /mnt/hadoop/1/dfs/data/current: 105ms\n\n2014-11-27 17:06:34,744  INFO [Thread-41] FsDatasetImpl - Cached dfsUsed found for /mnt/hadoop/0/dfs/data/current/BP-879324367-YYY-1404837025894/current: 2465590681232\n2014-11-27 17:06:34,744  INFO [Thread-41] FsDatasetImpl - Time taken to scan block pool BP-879324367-YYY-1404837025894 on /mnt/hadoop/0/dfs/data/current: 568ms\n2014-11-27 17:06:34,856  INFO [Thread-43] FsDatasetImpl - Cached dfsUsed found for /mnt/hadoop/2/dfs/data/current/BP-879324367-YYY-1404837025894/current: 2475580099468\n2014-11-27 17:06:34,857  INFO [Thread-43] FsDatasetImpl - Time taken to scan block pool BP-879324367-YYY-1404837025894 on /mnt/hadoop/2/dfs/data/current: 680ms\n2014-11-27 17:06:34,857  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] \n2014-11-27 17:06:34,858  INFO [Thread-44] FsDatasetImpl - Adding replicas to map for block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/0/dfs/data/current...\n2014-11-27 17:06:34,890  INFO [Thread-46] FsDatasetImpl - Adding replicas to map for block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/2/dfs/data/current...\n2014-11-27 17:06:34,890  INFO [Thread-45] FsDatasetImpl - Adding replicas to map for block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/1/dfs/data/current...\n2014-11-27 17:06:34,961  INFO [Thread-45] FsDatasetImpl - Time to add replicas to map for block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/1/dfs/data/current: 70ms\n2014-11-27 17:06:36,083  INFO [Thread-44] FsDatasetImpl - Time to add replicas to map for block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/0/dfs/data/current: 1193ms\n2014-11-27 17:06:36,162  INFO [Thread-46] FsDatasetImpl - Time to add replicas to map for block pool BP-879324367-YYY-1404837025894 on volume /mnt/hadoop/2/dfs/data/current: 1271ms\n2014-11-27 17:06:36,162  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] FsDatasetImpl - Total time to add all replicas to map: 1304ms\n2014-11-27 17:06:36,167  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] DataNode - Block pool BP-879324367-YYY-1404837025894 (Datanode Uuid null) service to /YYY:8020 beginning handshake with NN\n2014-11-27 17:06:36,167  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] DataNode - Block pool BP-879324367-YYY-1404837025894 (Datanode Uuid null) service to /XXX:8020 beginning handshake with NN\n2014-11-27 17:06:36,185  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] DataNode - Block pool Block pool BP-879324367-YYY-1404837025894 (Datanode Uuid null) service to /YYY:8020 successfully registered with NN\n2014-11-27 17:06:36,185  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] DataNode - Block pool Block pool BP-879324367-YYY-1404837025894 (Datanode Uuid null) service to /XXX:8020 successfully registered with NN\n2014-11-27 17:06:36,186  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] DataNode - For namenode /YYY:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\n2014-11-27 17:06:36,186  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] DataNode - For namenode /XXX:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\n2014-11-27 17:06:36,264  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] DataNode - Namenode Block pool BP-879324367-YYY-1404837025894 (Datanode Uuid 91e8deeb-7030-4e7b-af14-a3f35cf65bd2) service to /YYY:8020 trying to claim ACTIVE state with txid=114828921\n2014-11-27 17:06:36,265  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] DataNode - Acknowledging ACTIVE Namenode Block pool BP-879324367-YYY-1404837025894 (Datanode Uuid 91e8deeb-7030-4e7b-af14-a3f35cf65bd2) service to /YYY:8020\n2014-11-27 17:06:36,511  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] DataNode - Sent 1 blockreports 49708 blocks total. Took 72 msec to generate and 174 msecs for RPC and NN processing.  Got back commands none\n2014-11-27 17:06:36,511  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] DataNode - Sent 1 blockreports 49708 blocks total. Took 76 msec to generate and 170 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@464c42e6\n2014-11-27 17:06:36,512  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /YYY:8020] DataNode - Got finalize command for block pool BP-879324367-YYY-1404837025894\n2014-11-27 17:06:36,519  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] GSet - Computing capacity for map BlockMap\n2014-11-27 17:06:36,523  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] GSet - capacity      = 2^20 = 1048576 entries\n2014-11-27 17:06:36,520  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] GSet - VM type       = 64-bit\n2014-11-27 17:06:36,522  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] GSet - 0.5% max memory 1.9 GB = 9.9 MB\n2014-11-27 17:06:36,524  INFO [DataNode: [[[DISK]file:/mnt/hadoop/0/dfs/data/, [DISK]file:/mnt/hadoop/1/dfs/data/, [DISK]file:/mnt/hadoop/2/dfs/data/]]  heartbeating to /XXX:8020] BlockPoolSliceScanner - Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-879324367-YYY-1404837025894\n{code}\n\nBut block is sitll reported with two replicas, one of those is assigned to datanode which hasn't it:\n{code}\n<block_info>\n<block_id>1083356878</block_id>\n<block_name>blk_1083356878</block_name>\n<file>\n<local_name>commons-lang-2.6.jar</local_name>\n<local_directory>/user/user/analysis</local_directory>\n<user_name>user</user_name>\n<group_name>supergroup</group_name>\n<is_directory>false</is_directory>\n<access_time>1417094143429</access_time>\n<is_under_construction>false</is_under_construction>\n<ds_quota>-1</ds_quota>\n<permission_status>user:supergroup:rw-r--r--</permission_status>\n<replication>2</replication>\n<disk_space_consumed>568440</disk_space_consumed>\n<preferred_block_size>134217728</preferred_block_size>\n</file>\n<replicas>\n<replica>\n<host_name>bad_node</host_name>\n<is_corrupt>false</is_corrupt>\n</replica>\n<replica>\n<host_name>good_node</host_name>\n<is_corrupt>false</is_corrupt>\n</replica>\n</replicas>\n</block_info>\n{code}\n\nfsck does not change the situation, ony the namenode restart force it to recognize that block replica as missing.\n\nAs a result it is required to restart namenode after each disk failure which is not very practical...","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"9223372036854775807","customfield_12312823":null,"summary":"Namenode does not recognize block is missing on a datanode","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14267992","id":"14267992","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"Hi [~dmitrybugaychenko].  Thank you for the report.\n\nI noticed that the replication factor is set to 2 in the example that you gave.  Is this the replication factor for all files that experienced this problem?\n\nI haven't seen anything like this.  When we lose a replica from a DataNode going down, we expect the NameNode to schedule a re-replication to another healthy DataNode.  However, replication factor is most often set to 3.  I'm wondering if there might be a bug that only gets triggered with replication factor set to 2.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-01-07T18:31:07.217+0000","updated":"2015-01-07T18:31:07.217+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14267999","id":"14267999","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jasperh%40hafcom.nl","name":"jasperh@hafcom.nl","key":"jasperh@hafcom.nl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jasper Hafkenscheid","active":true,"timeZone":"Etc/UTC"},"body":"Hi,\n\nWe have a cluster with bad disk(controler)s, and see this problem often.\nOur replication facor is 3.\nLuckely most disks recover after a powercycle.\n\nRegards Jasper\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jasperh%40hafcom.nl","name":"jasperh@hafcom.nl","key":"jasperh@hafcom.nl","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Jasper Hafkenscheid","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-07T18:37:41.121+0000","updated":"2015-01-07T18:37:41.121+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14271809","id":"14271809","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilsoncraft","name":"wilsoncraft","key":"wilsoncraft","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Allan Wilson","active":true,"timeZone":"America/New_York"},"body":"I see this same behavior in our cluster (Hadoop 2.0.0) CDH4.7.  These are for files with a replica count of 3.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=wilsoncraft","name":"wilsoncraft","key":"wilsoncraft","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Allan Wilson","active":true,"timeZone":"America/New_York"},"created":"2015-01-09T20:11:45.516+0000","updated":"2015-01-09T20:11:45.516+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14273278","id":"14273278","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"body":"We have different RF configured for different files, including 1, 2 and 3. Replicas might get lost for files with any of them. It looks like that there is a workaround that forces namenode to handle recovery properly - after adding this piece to core-site.xml it seems to work:\n\n{code}\n  <property>\n    <name>net.topology.script.file.name</name>\n    <value>/this/enables/BlockManager/shouldCheckForEnoughRacks</value>\n  </property>\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-12T07:23:19.461+0000","updated":"2015-01-12T07:23:19.461+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14273279","id":"14273279","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"body":"We have different RF configured for different files, including 1, 2 and 3. Replicas might get lost for files with any of them. It looks like that there is a workaround that forces namenode to handle recovery properly - after adding this piece to core-site.xml it seems to work:\n\n{code}\n  <property>\n    <name>net.topology.script.file.name</name>\n    <value>/this/enables/BlockManager/shouldCheckForEnoughRacks</value>\n  </property>\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-12T07:23:20.577+0000","updated":"2015-01-12T07:23:20.577+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14273280","id":"14273280","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"body":"We have different RF configured for different files, including 1, 2 and 3. Replicas might get lost for files with any of them. It looks like that there is a workaround that forces namenode to handle recovery properly - after adding this piece to core-site.xml it seems to work:\n\n{code}\n  <property>\n    <name>net.topology.script.file.name</name>\n    <value>/this/enables/BlockManager/shouldCheckForEnoughRacks</value>\n  </property>\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-12T07:23:22.713+0000","updated":"2015-01-12T07:23:22.713+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14273281","id":"14273281","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"body":"We have different RF configured for different files, including 1, 2 and 3. Replicas might get lost for files with any of them. It looks like that there is a workaround that forces namenode to handle recovery properly - after adding this piece to core-site.xml it seems to work:\n\n{code}\n  <property>\n    <name>net.topology.script.file.name</name>\n    <value>/this/enables/BlockManager/shouldCheckForEnoughRacks</value>\n  </property>\n{code}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=dmitrybugaychenko","name":"dmitrybugaychenko","key":"dmitrybugaychenko","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Dmitry Bugaychenko","active":true,"timeZone":"Etc/UTC"},"created":"2015-01-12T07:23:27.334+0000","updated":"2015-01-12T07:23:27.334+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14368446","id":"14368446","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"body":"Our cluster also have this issue.\nAfter I checked the trunk code, I found that HDFS-7208 already fixed the issue.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zhaoyunjiong","name":"zhaoyunjiong","key":"zhaoyunjiong","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"yunjiong zhao","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-19T03:31:17.940+0000","updated":"2015-03-19T03:31:17.940+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14369864","id":"14369864","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"HDFS-7208 does look like a plausible explanation.  I propose that we resolve this as a duplicate unless there are objections.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-03-19T18:34:26.148+0000","updated":"2015-03-19T18:34:26.148+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12758249/comment/14588885","id":"14588885","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"body":"As per prior comments, I am resolving this as a duplicate of HDFS-7208.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cnauroth","name":"cnauroth","key":"cnauroth","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=cnauroth&avatarId=11432","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=cnauroth&avatarId=11432","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=cnauroth&avatarId=11432","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=cnauroth&avatarId=11432"},"displayName":"Chris Nauroth","active":true,"timeZone":"America/Los_Angeles"},"created":"2015-06-16T22:09:05.010+0000","updated":"2015-06-16T22:09:05.010+0000"}],"maxResults":10,"total":10,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HDFS-7453/votes","votes":2,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i22utj:"}}