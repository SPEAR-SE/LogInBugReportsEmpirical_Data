[Here's a test program Harsh wrote which makes two threads that race on the same filename., I don't think there is problem for concurrent creating the same file. because there is file lease in the namenode.

only one client can own the lease. so other "concurrent clients" would be failure in this scenario., [~azuryy] - "Concurrent clients" as in a different DFSClient instance will not be affected by this, you are correct. This problem occurs where the same DFSClient does the overwrite and ends up with a dual-alloc despite that. Please also see the attached program to see what we mean., Hi Harsh,

I do misunderstood. the attached program illustrated everything now. 

so I attached a minor patch for this issue., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578352/HDFS-4688.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4230//console

This message is automatically generated., updated the patch.


, I will add some test after a while., [~azuryy] it's bad form to poach a JIRA when it's already been assigned and is still being actively updated. I was going to get to this one today or tomorrow. Just an FYI for the future. You can have this one if you're really interested, else I'll take it on., oh, sorry, I am not noticed this has been assigned. you can take it as always., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578263/TestBadFileMaker.java
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4233//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578355/HDFS-4688.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestLeaseRecovery2
                  org.apache.hadoop.hdfs.TestQuota
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.server.namenode.TestINodeFile

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4232//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4232//console

This message is automatically generated., I ended up with a patch similar to [~azuryy] but looking at the test failures, we need to fix the close semantics. Right now, if close fails to write out the data, it'll leak the file lease (preventing future re-opens).

This is being tracked in HDFS-4504, so we'll need that first.]