[Hi Ding, thanks a lot for the report. Usually we try to keep the description of a JIRA short since it's reproduced in every email update about the JIRA. You are encouraged to put detailed descriptions in the first comment of the JIRA. I'm reproducing them here for you:

Description of issues from Ding Yuan:

==========================
Case 1:
org/apache/hadoop/hdfs/server/namenode/FSEditLog.java

In recoverUnclosedStreams, purgeLogsOlderThan, and endCurrentLogSegment,
when not enough journals are found, mapJournalsAndReportErrors throws IOException.
While in some other cases (such as in logEdit()), this case would be handled by a
later call to logSync(), which will terminate, but in these three methods these exceptions
might be swallowed since logSync() won't be called in anytime soon.

Propose to immediately handle such cases as in logSync().

{noformat}
  synchronized void recoverUnclosedStreams() {
    Preconditions.checkState(
        state == State.BETWEEN_LOG_SEGMENTS,
        "May not recover segments - wrong state: %s", state);
     try {
       journalSet.recoverUnfinalizedSegments();
     } catch (IOException ex) {
-      // All journals have failed, it is handled in logSync.
-      // TODO: are we sure this is OK?
+        //All journals have failed, handle it here.
+        final String msg =
+                "Could not find enough journals. "
+            LOG.fatal(msg, new Exception());
+            IOUtils.cleanup(LOG, journalSet);
+            terminate(1, msg);
     }
   }
{noformat}
==========================================

==========================
Case 2:
File: "org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java"

{noformat}
@@ -446,6 +446,8 @@
           deleteDir(tmpDir);
         } catch (IOException ex) {
           LOG.error("Finalize upgrade for " + dataDirPath + " failed.", ex);
+          return; // return so users will not see confusing log messages where
+                  // "failed" is immediately followed by "complete"
         }
         LOG.info("Finalize upgrade for " + dataDirPath + " is complete.");
       }
{noformat}

 In this case, the log can be confusing if deleteDir failed:
"Finalize upgrade failed" message will be immediately followed by "Finalize upgrade is complete".

Same pattern can be seen at:
  Line: 600, File: "org/apache/hadoop/hdfs/server/datanode/DataStorage.java"
  Line: 1250, File: "org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
==========================================

==========================
Case 3:
File: "org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java"

The log message doesn't include the exception cause.

{noformat}
@@ -167,9 +167,9 @@
         LOG.debug(this + " received versionRequest response: " + nsInfo);
         break;
       } catch(SocketTimeoutException e) {  // namenode is busy
-        LOG.warn("Problem connecting to server: " + nnAddr);
+        LOG.warn("Problem connecting to server: " + nnAddr, e);
       } catch(IOException e ) {  // namenode is not available
-        LOG.warn("Problem connecting to server: " + nnAddr);
+        LOG.warn("Problem connecting to server: " + nnAddr, e);
       }
{noformat}
==========================================

==========================
Case 4:
File: "org/apache/hadoop/hdfs/server/datanode/BlockSender.java"

{noformat}
@@ -371,6 +371,7 @@
    IOException ioe = null;
    if (blockInFd != null &&
        ((dropCacheBehindAllReads) ||
         (dropCacheBehindLargeReads && isLongRead()))) {
      try {
        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(
            block.getBlockName(), blockInFd, lastCacheDropOffset,
            offset - lastCacheDropOffset,
            NativeIO.POSIX.POSIX_FADV_DONTNEED);
      } catch (IOException e) {
        LOG.warn("Unable to drop cache on file close", e);
+       ioe = e;
      }
    }

    if(checksumIn!=null) {
      try {
        checksumIn.close(); // close checksum file
      } catch (IOException e) {
        ioe = e;
      }
      checksumIn = null;

 .. ..
    if (ioe!=null) {
      throw ioe;
    }
{noformat}

It's unclear why exception from posixFadviceIfPossible is not returned to the caller.
==========================================

==========================
Case 5:
File: org/apache/hadoop/hdfs/server/datanode/DataXceiver.java

{noformat}
    } finally {
      dataXceiverServer.balanceThrottler.release();
      if (isOpSuccess) {
@@ -751,7 +751,8 @@
         try {
           // send one last byte to indicate that the resource is cleaned.
           reply.writeChar('d');
-        } catch (IOException ignored) {
+        } catch (IOException e) {
+          LOG.warn("Exception caught after copy block succeeded " + e);
         }
       }
       IOUtils.closeStream(reply);
{noformat}

It is unclear why the IOException is not logged.

Similar case:
  Line: 876, File: "org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
==========================================
, Thanks Aaron!, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12628321/hdfs-5931.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6114//console

This message is automatically generated., Don't know why the previous patch cannot be applied by Hadoop QA. Attaching -v2 that is applied to the newly checked out trunk., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12628390/hdfs-5931-v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.qjournal.TestNNWithQJM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6117//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6117//console

This message is automatically generated., A new patch fixing the test error. The reason hdfs-5931-v2.patch broke the test is that TestNNWithQJM#testMismatchedNNIsRejected() expects the exception message "Unable to start log segment 1: too few journals" thrown by FSEditLog.startLogSegment. Now this "too few journals" will be detected earlier by recoverUnclosedStreams, and therefore aborts earlier. This seems to be more reasonable as if we cannot even find enough journal in recoverUnclosedStreams, we should not proceed to startLogSegment and wait until then to reject this case., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12628500/hdfs-5931-v3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestClientReportBadBlock

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6123//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6123//console

This message is automatically generated., Sorry but I am confused by the Hadoop QA's report of failed test above. I ran TestClientReportBadBlock locally against my patch and all three tests seem to have passed. By looking at the test code I could not see why it can fail on the patch...  Could someone please advise?, Hi [~atm], I took another close look at the test output. It seems the SocketTimeoutException might not be caused by my patch (I ran the test on my machine and it passed). Is there any chance to comment on this patch, and if indeed the test was broke by the patch or there is any other problems with my patch I can further fix it?

Thanks,, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  15m  3s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 45s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 49s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   2m 16s | The applied patch generated  499 new checkstyle issues (total was , now 499). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   3m  7s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | native |   3m 23s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests | 165m 58s | Tests failed in hadoop-hdfs. |
| | | 209m 58s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.hdfs.qjournal.TestNNWithQJM |
|   | hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics |
| Timed out tests | org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistFiles |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12628500/hdfs-5931-v3.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / a319771 |
| checkstyle |  https://builds.apache.org/job/PreCommit-HDFS-Build/10724/artifact/patchprocess/diffcheckstylehadoop-hdfs.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/10724/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/10724/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10724/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 33s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 27s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 33s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   2m 14s | The applied patch generated  1 new checkstyle issues (total was 501, now 500). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 32s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   3m  3s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | native |   3m 12s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests | 165m 46s | Tests failed in hadoop-hdfs. |
| | | 208m 20s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics |
|   | hadoop.hdfs.qjournal.TestNNWithQJM |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12628500/hdfs-5931-v3.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / a319771 |
| checkstyle |  https://builds.apache.org/job/PreCommit-HDFS-Build/10749/artifact/patchprocess/diffcheckstylehadoop-hdfs.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/10749/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/10749/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10749/console |


This message was automatically generated., Will update the patch for QA reported errors. [~d.yuan], please free to assign to you if you want to work on this JIRA, Please add tests for the FSEditLog changes., BTW, what are the "potential bugs"?  Could you explain?]