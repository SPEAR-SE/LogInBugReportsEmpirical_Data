[Moved to HDFS, 
bq. Then i tried to put a file of 100 Mb . It should since it will just consume 200 Mb of space with replication. But when i put that i got an error
HDFS doesnt know the complete size of the file ahead. It considers default blocksize ( in your case 256MB) for calculation while adding the new block.
So with RF 2, always quota should be more than 512 MB in your case to upload the file successfully.

With this, I am surprised how you are able to upload first 2 files.?

Did you uploaded to some other directory and moved to this dir? 
or set the quota after uploading these 2 files.?, [~putta_jammy], I'm working on a quota related feature recently. So I took a quick look but can't repro this with a dfs.block.size=128MB and replication factor of 2. 

Based on the information you posted, the intended quota usage for the first block of the last file is 632 MB - 140 MB ~= 492 MB. Consider the replication factor of 2, the first block allocated for the last file should be at around ~250 MB. You could get quota exceeded exception if the dfs.block.size get changed (e.g., from 128MB to 256MB) for the last file OR your last file size is greater than 128MB?

 , bq. 
HDFS doesnt know the complete size of the file ahead. It considers default blocksize ( in your case 256MB) for calculation while adding the new block.

His default blocksize should be 128 MB without modifying dfs.blocksize in hdfs-site.xml. If it is a 256MB block size, copy the first 10MB file will fail with RF=2 and a 500MB space quota as you mentioned., bq. 
HDFS doesnt know the complete size of the file ahead. It considers default blocksize ( in your case 256MB) for calculation while adding the new block.

His default blocksize should be 128 MB without modifying dfs.blocksize in hdfs-site.xml. If it is a 256MB block size, copy the first 10MB file will fail with RF=2 and a 500MB space quota as you mentioned., bq. 
HDFS doesnt know the complete size of the file ahead. It considers default blocksize ( in your case 256MB) for calculation while adding the new block.

His default blocksize should be 128 MB without modifying dfs.blocksize in hdfs-site.xml. If it is a 256MB block size, copy the first 10MB file will fail with RF=2 and a 500MB space quota as you mentioned., Hi, I tried and cannot reproduce.
{code:none}
root@ds-34:/home/skh/hadoop-2.6.0 # bin/hdfs dfs -put 10mb.txt /testdir
root@ds-34:/home/skh/hadoop-2.6.0 # bin/hdfs dfs -count -q /testdir
none             inf       524288000       504288000            1            1           10000000 /testdir
root@ds-34:/home/skh/hadoop-2.6.0 # bin/hdfs dfs -put 50mb.txt /testdir 
root@ds-34:/home/skh/hadoop-2.6.0 # bin/hdfs dfs -count -q /testdir
none             inf       524288000       404288000            1            2           60000000 /testdir
root@ds-34:/home/skh/hadoop-2.6.0 # bin/hdfs dfs -put 100mb.txt /testdir
root@ds-34:/home/skh/hadoop-2.6.0 # bin/hdfs dfs -count -q /testdir
none             inf       524288000       204288000            1            3          160000000 /testdir
{code}, Closing this jira..If anyone disagree with me, Feel free to reopen this jira..Thanks]