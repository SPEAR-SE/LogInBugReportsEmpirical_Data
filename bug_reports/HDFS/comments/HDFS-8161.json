[ *{color:blue}ACTIVE NAMENODE ZKFC :{color}* 
======================

{noformat}
2015-04-16 11:32:36,872 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-SendThread(ZKHOST-212:24002) | Client session timed out, have not heard from server in 30015ms for sessionid 0x154cb2b3e4746ace, closing socket connection and attempting reconnect | org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1120)
2015-04-16 11:32:36,974 | INFO  | Health Monitor for NameNode at IP-114/IP.114:25000-EventThread | Session disconnected. Entering neutral mode... | org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:558)
2015-04-16 11:32:37,632 | INFO  | Health Monitor for NameNode at IP-114/IP.114:25000-SendThread(HOST-114:24002) | Client will use GSSAPI as SASL mechanism. | org.apache.zookeeper.client.ZooKeeperSaslClient$1.run(ZooKeeperSaslClient.java:285)
2015-04-16 11:32:37,633 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-SendThread(HOST-114:24002) | Opening socket connection to server HOST-114/IP.114:24002. Will attempt to SASL-authenticate using Login Context section 'Client' | org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:999)
2015-04-16 11:32:37,634 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-SendThread(HOST-114:24002) | Socket connection established to HOST-114/IP.114:24002, initiating session | org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:854)
2015-04-16 11:32:37,635 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-SendThread(HOST-114:24002) | Unable to reconnect to ZooKeeper service, session 0x154cb2b3e4746ace has expired, closing socket connection | org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1118)
2015-04-16 11:32:37,636 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-EventThread | Session expired. Entering neutral mode and rejoining... | org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:568)
2015-04-16 11:32:37,636 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-EventThread | Trying to re-establish ZK session | org.apache.hadoop.ha.ActiveStandbyElector.reJoinElection(ActiveStandbyElector.java:670)
2015-04-16 11:32:37,639 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-EventThread | Initiating client connection, connectString=ZKHOST-212:24002,HOST-114:24002,HOST-117:24002 sessionTimeout=45000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@2127d120 | org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438)
2015-04-16 11:32:37,641 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-SendThread(HOST-117:24002) | Client will use GSSAPI as SASL mechanism. | org.apache.zookeeper.client.ZooKeeperSaslClient$1.run(ZooKeeperSaslClient.java:285)
2015-04-16 11:32:37,642 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-SendThread(HOST-117:24002) | Opening socket connection to server HOST-117/IP.117:24002. Will attempt to SASL-authenticate using Login Context section 'Client' | org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:999)
2015-04-16 11:32:37,642 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-SendThread(HOST-117:24002) | Socket connection established to HOST-117/IP.117:24002, initiating session | org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:854)
2015-04-16 11:32:37,661 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-SendThread(HOST-117:24002) | Session establishment complete on server HOST-117/IP.117:24002, sessionid = 0x174cbd419924047a, negotiated timeout = 45000 | org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1259)
2015-04-16 11:32:37,664 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-EventThread | EventThread shut down | org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:512)
2015-04-16 11:32:37,666 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-EventThread | Session connected. | org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:547)
2015-04-16 11:32:37,672 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-EventThread | Successfully authenticated to ZooKeeper using SASL. | org.apache.hadoop.ha.ActiveStandbyElector.processWatchEvent(ActiveStandbyElector.java:573)
2015-04-16 11:32:37,699 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-EventThread | ZK Election indicated that NameNode at HOST-114/IP.114:25000 should become standby | org.apache.hadoop.ha.ZKFailoverController.becomeStandby(ZKFailoverController.java:488)
2015-04-16 11:32:38,158 | INFO  | Health Monitor for NameNode at HOST-114/IP.114:25000-EventThread | Successfully transitioned NameNode at HOST-114/IP.114:25000 to standby state | org.apache.hadoop.ha.ZKFailoverController.becomeStandby(ZKFailoverController.java:493)
{noformat}

 *{color:blue}STANDBY NAMENODE ZKFC:{color}* 
=========================
2015-04-16 11:32:54,031 | INFO  | main-EventThread | Checking for any old active which needs to be fenced... | org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:877)
2015-04-16 11:32:54,045 | INFO  | main-EventThread | Old node exists: 0a096861636c7573746572120232361a0d3136302d3134392d302d31313420a8c30128b7c301 | org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:898)
2015-04-16 11:32:54,046 | INFO  | main-EventThread | Should fence: NameNode at HOST-114/IP.114:25000 | org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:517)
2015-04-16 11:32:54,060 | INFO  | main-EventThread | Successfully transitioned NameNode at HOST-114/IP.114:25000 to standby state without fencing | org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:523)
{color:red}
2015-04-16 11:32:54,060 | INFO  | main-EventThread | Writing znode /hadoop-ha/hacluster/ActiveBreadCrumb to indicate that the local node is the most recent active... | org.apache.hadoop.ha.ActiveStandbyElector.writeBreadCrumbNode(ActiveStandbyElector.java:824)
2015-04-16 11:32:54,073 | INFO  | main-EventThread | Trying to make NameNode at HOST-117/IP.117:25000 active... | org.apache.hadoop.ha.ZKFailoverController.becomeActive(ZKFailoverController.java:384)
2015-04-16 11:32:56,066 | INFO  | main-EventThread | Successfully transitioned NameNode at HOST-117/IP.117:25000 to active state | org.apache.hadoop.ha.ZKFailoverController.becomeActive(ZKFailoverController.java:391)
2015-04-16 11:32:56,070 | INFO  | main-EventThread | ZK Election indicated that NameNode at HOST-117/IP.117:25000 should become standby | org.apache.hadoop.ha.ZKFailoverController.becomeStandby(ZKFailoverController.java:488)
2015-04-16 11:32:56,637 | INFO  | main-EventThread | Successfully transitioned NameNode at HOST-117/IP.117:25000 to standby state | org.apache.hadoop.ha.ZKFailoverController.becomeStandby(ZKFailoverController.java:493) {color}


 *Marked in red color which I had doubt..why immediately state changes to active to standby..?* , Attached the Active Breadcumb details..


I am thinking , following code needs to be modify...

{code}
 Code code = Code.get(rc);
    if (isSuccess(code)) {
      // we successfully created the znode. we are the leader. start monitoring
      if (becomeActive()) {
        monitorActiveStatus();
      } else {
        reJoinElectionAfterFailureToBecomeActive();
      }
      return;
    }

    if (isNodeExists(code)) {
      if (createRetryCount == 0) {
        // znode exists and we did not retry the operation. so a different
        // instance has created it. become standby and monitor lock.
        becomeStandby();
      }
      // if we had retried then the znode could have been created by our first
      // attempt to the server (that we lost) and this node exists response is
      // for the second attempt. verify this case via ephemeral node owner. this
      // will happen on the callback for monitoring the lock.
      monitorActiveStatus();
      return;
    }
{code}


Any pointers to this issue..?, AS there is already {{/hadoop-ha/hacluster/ActiveStandbyElectorLock}} exists  with different session id ( It might be created by another ZKFC in same machine or ..)..Hence ZKFC immediately called become standby once after it's written {{/hadoop-ha/hacluster/ActiveBreadCrumb}}...
, As session id got corrupted and ZKFC received different session id ( described in  ZOOKEEPER-2175 )..It's not able to get the lock on {{/hadoop-ha/hacluster/ActiveStandbyElectorLock}} .. Hence both ZKFC's are not able got lock, both NN's are in standby..

Raised issue in ZK community for checksum validation., [~vinayrpet] , [~jnp] and [~arpitagarwal] any thoughts on this..? 
As there is no checksum verification from the ZK Side and seems to be no one interested in checksum feature in ZK side (since I  did not seen any comment in ZOOKEEPER-2175 ),Can we have some mechanism here..?, [~brahmareddy], this is excellent debugging.  Thank you for posting the information!

I have commented on ZOOKEEPER-2175.  They suspect their new wire encryption feature would catch a packet corruption issue like this.  I've also proposed a ZooKeeper feature for checksum validation even when wire encryption is not used.  (I think we'd have little motivation to use wire encryption for the HDFS HA use case, since the data we store in the znode isn't a secret.)

Meanwhile, I'm wondering if there is something we can change in Hadoop code to make ourselves more resilient to this.  The HDFS logic in this area is driven by ZooKeeper status code checks like the following in {{ActiveStandbyElector}}:

{code}
  private static boolean isSuccess(Code code) {
    return (code == Code.OK);
  }
{code}

I'm wondering if we can check for a specific ZooKeeper client status code, and then reconnect our session and retry taking the lock instead of transitioning to standby.  Do you know if there was a particular ZooKeeper status code that you saw when this happened?  Do you have the capability to repro consistently?
, Thanks a lot [~cnauroth] looking into this issue. 

Yes, we should make ourselves more resilient on this..

{quote}Do you know if there was a particular ZooKeeper status code that you saw when this happened?{quote}
I am not sure,Status might be "OK", same I will confirm..
{quote}
Do you have the capability to repro consistently?
{quote}
It's hard to reproduce but I can,I will try on next monday and post..
, Any progress? I saw the same issue on 2.7.1. Or any suggestion to fix this issue?, Hi [~johnjianfang] 

This particular case was caused by ZOOKEEPER-2175.
If your case also similar to this, then have to wait till ZOOKEEPER-2175 makes some progress, [~brahmareddy] - was this encountered on virtual machine hosts, or physical ones? Asking because https://tech.vijayp.ca/linux-kernel-bug-delivers-corrupt-tcp-ip-data-to-mesos-kubernetes-docker-containers-4986f88f7a19#.v3hx212ne (H/T [~daisuke.kobayashi]), Its happened in physical machines., Hi,
Every time I restart NameNode due to the config change, I hit two standby state. A solution is to delete /hadoop-ha/ha/ActiveBreadCrumb and /hadoop-ha/ha/ActiveStandbyElectorLock in Zookeeper after restarting NameNode.

Maybe you can consider to add the specific zk node deleting in the restarting procedure., bq. Every time I restart NameNode due to the config change, I hit two standby state.

Ideally this should not happen.. can you look at ZKFC logs..? might be some config missed...Please have look at [here | https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html] for detailed configurations.., Sorry, I forgot to mention the precondition. I had moved one NameNode and zookeeper to the other node (original node is dead) and then restart both NameNode.]