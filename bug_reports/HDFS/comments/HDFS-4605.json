[From MAPREDUCE-5065,

bq. Most hashing is incremental, so if DFSClient feeds the last state of hash into the next datanode and let it continue updating it, the result will be independent of block size. The current way of doing file checksum allows calculating individual block checksums in parallel, but we are not taking advantage of it in DFSClient anyway. So I don't think there will be any significant changes in performance or overhead.

I think this will work as long as 
* no partial blocks in the middle.
* block size is multiple of crc chunk/block size.
As far as I know these are enforced in HDFS.

Assuming this can be done, what will be the best way to add this feature? , If we do this, there will be a restriction that the summing hash function should consume 32 bits (CRC32 or CRC32C) at a time., bq. no partial blocks in the middle.
I assume that this means the block in the middle should be of size block length. However, one of the things I have been thinking of is variable length block sizes - see HDFS-3689. It would also be good not to have block size as multiple of crc chunk size. Though these are currently enforced, there are advantages to not enforce this., The goal here is just to make checksums independent of the file's block size.  They'd still be dependent on bytesPerChecksum, partial blocks, bitsPerChecksum, etc.  This change would permit efficient verification of copies that differ in blockSize but not these other aspects, a presumably common case.  That's a net improvement, no?, [~cutting] Makes sense. I had not read MAPREDUCE-5065, when I posted my comments., bq. If we do this, there will be a restriction that the summing hash function should consume 32 bits (CRC32 or CRC32C) at a time.

If this restriction leads to unacceptable collision rate, we could have datanode send back remaining input to be prepended on the next node. But stock md5 (input size 512 bits) will not work even with this, because it internally pads the end of input with the input length. We need either a version of md5 that allows user to control the padding, or pick a different hash algorithm., > Most hashing is incremental, so if DFSClient feeds the last state of hash into the next datanode and let it continue updating it, the result will be independent of block size. ...

However, the above computation is a sequential computation.  I think it would take a very long time for large files.

How about we make an assumption that block size is a multiple of a small number, say 1 MB?  Then, each datanode computes 1-MB checksums (over CRC32s) in parallel.  At last, DFSClient combines all the 1-MB checksums.]