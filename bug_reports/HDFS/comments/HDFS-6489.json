[This sounds like correct behavior to me. We don't know how many bytes a client is going to write to a block until it either asks for a new block or closes the file, so we conservatively say that the client might write the full block size. This makes sense to avoid writing incomplete blocks due to running out of disk space.

If you close the open streams, does the disk usage go back down? If so, I'm inclined to close as "Not A Problem"., No, the disk usage will not go back down untill after 10 minutes (the next round of "du -sk" is called). 

I printed the stack trace of the BlockPoolSlice.addBlock, which does the actual increase of the disk usage
{quote}at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.addBlock(BlockPoolSlice.ja
va:157)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlock(FsVolumeImpl.java:1
67)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeReplica(FsDatasetIm
pl.java:961)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl
.java:942)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:727)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)
        at java.lang.Thread.run(Thread.java:722){quote}
we can see it is called at "finalizeBlock" phase. at this phase, we should already know how much data we wrote, then increasing the dfs usage with the total blocksize doesn't make sense.
, Take example,

existing behavior:

1. create file 60M with prefer block size 64M.
2. append 10 bytes  (disk utilization is increased by 60M + 10 bytes, totally 120M + 10 bytes)
3. append 10 bytes  (disk utilization is increased by  60M + 20 bytes, totally 120M + 30 bytes)
4. append 10 bytes (disk utilization is increased by 60M + 30 bytes, totally 180M + 60bytes)

expected behavior:

1. create file 60M with prefer block size 64M.
2. append 10 bytes  (disk utilization is increased 10 bytes, totally 60M + 10 bytes)
3. append 10 bytes  (disk utilization is increased 10 bytes, totally 60M + 20 bytes)
4. append 10 bytes (disk utilization is increased 10 bytes, totally 60M + 30 bytes), Are there any more infomations on this issue? I am trying to solve this problem also., I've recently hit this bug in 2.7.1. I attached repro code. The repro should fail with 'all datanodes are bad' exception while the datanode log will show the "insufficient disk space" exception.
While the program is running you can see the reported "Block pool used" increase by a lot. A minute or two after the failure the "Block pool used" goes down to normal., This issue happened to us as well. We have some clients continually append data to HDFS, this causes the dfs usage jump up to a very high value and gives insufficient disk space issue just like this. It looks like HDFS refreshes the space usage in a time interval (with property fs.du.interval), default value is 600000, 10 minutes ... that means the dfs usage will be inaccurate and causing a lot of operations fail in 10 minutes (even client did close the streams). We really should have this fixed., I have a patch to fix this issue. The idea is during append, when datanode detects the space of the volume is insufficient,  instead of failing directly and trying a new node,  interrupts DURefreshThread and then evaluates the space again. This will let DU updates "used" variable to up-to-date, other than waiting for the long interval. 

I also wrote a test case, simulate a client  appends 10 bytes of data to a file continually. Before applying the patch, it fails even file system has plenty of spaces with following error 

bq. java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:58613,DS-a68ddbc7-2e49-428a-839b-d16bf58106fe,DISK] are bad. Aborting... , | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 23s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 1m 11s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 4s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 6m 4s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 6m 54s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 1m 7s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 53s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 29s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 46s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 4s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 57s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 14s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 28s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 5m 56s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 5m 56s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 6m 49s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 6m 49s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 1m 8s {color} | {color:red} root: patch generated 1 new + 151 unchanged - 0 fixed = 152 total (was 151) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 53s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 2m 20s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs generated 1 new + 0 unchanged - 0 fixed = 1 total (was 0) {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 6s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 59s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 21m 39s {color} | {color:red} hadoop-common in the patch failed with JDK v1.8.0_74. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 72m 2s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_74. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 7m 3s {color} | {color:red} hadoop-common in the patch failed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 75m 25s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 25s {color} | {color:red} Patch generated 2 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 239m 23s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-hdfs-project/hadoop-hdfs |
|  |  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(String, FinalizedReplica, long, long) calls Thread.sleep() with a lock held  At FsDatasetImpl.java:Thread.sleep() with a lock held  At FsDatasetImpl.java:[line 1176] |
| JDK v1.8.0_74 Failed junit tests | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure180 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure020 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure170 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure200 |
|   | hadoop.hdfs.TestDFSClientRetries |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure120 |
|   | hadoop.hdfs.TestLeaseRecoveryStriped |
|   | hadoop.hdfs.TestDFSStripedOutputStream |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure030 |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure060 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure190 |
|   | hadoop.hdfs.TestEncryptedTransfer |
| JDK v1.8.0_74 Timed out junit tests | org.apache.hadoop.util.TestNativeLibraryChecker |
|   | org.apache.hadoop.http.TestHttpServerLifecycle |
| JDK v1.7.0_95 Failed junit tests | hadoop.net.TestClusterTopology |
|   | hadoop.hdfs.TestHFlush |
|   | hadoop.hdfs.TestReplication |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure090 |
|   | hadoop.hdfs.TestReadStripedFileWithDecoding |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure200 |
|   | hadoop.hdfs.server.namenode.TestReconstructStripedBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure160 |
|   | hadoop.hdfs.server.namenode.TestProcessCorruptBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure110 |
|   | hadoop.hdfs.TestEncryptionZonesWithKMS |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure120 |
|   | hadoop.hdfs.TestDFSStripedOutputStream |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure060 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure210 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure080 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure070 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure190 |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure040 |
| JDK v1.7.0_95 Timed out junit tests | org.apache.hadoop.util.TestNativeLibraryChecker |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:fbe3e86 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12795008/HDFS-6489.001.patch |
| JIRA Issue | HDFS-6489 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 77ab4c0dff4d 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / af1d125 |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_74 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/diff-checkstyle-root.txt |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/new-findbugs-hadoop-hdfs-project_hadoop-hdfs.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_74.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_74.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_95.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_74.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_74.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_95.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C:  hadoop-common-project/hadoop-common   hadoop-hdfs-project/hadoop-hdfs  U: . |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/14909/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 20s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 23s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 11m 13s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 20s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 43s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 1m 41s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 2m 56s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 48s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 5m 20s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 3m 40s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 4m 53s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 23s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 2m 24s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 15m 26s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 15m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 42s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 42s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 1m 40s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 2m 54s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 6m 2s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 3m 45s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 4m 52s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 13m 39s {color} | {color:red} hadoop-common in the patch failed with JDK v1.8.0_74. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 81m 3s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_74. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 12m 22s {color} | {color:red} hadoop-common in the patch failed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 109m 7s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 55s {color} | {color:red} Patch generated 2 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 327m 45s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_74 Failed junit tests | hadoop.ipc.TestRPCWaitForProxy |
|   | hadoop.fs.shell.find.TestIname |
|   | hadoop.fs.shell.find.TestPrint0 |
|   | hadoop.fs.shell.find.TestPrint |
|   | hadoop.fs.shell.find.TestName |
|   | hadoop.hdfs.server.namenode.ha.TestBootstrapStandby |
|   | hadoop.hdfs.server.namenode.TestDeleteRace |
|   | hadoop.hdfs.server.namenode.TestEditLog |
|   | hadoop.hdfs.qjournal.TestSecureNNWithQJM |
|   | hadoop.hdfs.server.namenode.ha.TestEditLogTailer |
|   | hadoop.hdfs.TestDistributedFileSystem |
|   | hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot |
|   | hadoop.hdfs.security.TestDelegationTokenForProxyUser |
|   | hadoop.hdfs.TestFileAppend |
|   | hadoop.hdfs.TestSafeModeWithStripedFile |
| JDK v1.8.0_74 Timed out junit tests | org.apache.hadoop.util.TestNativeLibraryChecker |
|   | org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage |
|   | org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport |
|   | org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
| JDK v1.7.0_95 Failed junit tests | hadoop.ipc.TestRPCWaitForProxy |
|   | hadoop.fs.shell.find.TestIname |
|   | hadoop.fs.shell.find.TestPrint0 |
|   | hadoop.fs.shell.find.TestPrint |
|   | hadoop.fs.shell.find.TestName |
|   | hadoop.hdfs.server.datanode.TestDirectoryScanner |
|   | hadoop.hdfs.server.namenode.ha.TestEditLogTailer |
|   | hadoop.hdfs.server.datanode.TestDataNodeUUID |
|   | hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations |
|   | hadoop.hdfs.security.TestDelegationTokenForProxyUser |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
| JDK v1.7.0_95 Timed out junit tests | org.apache.hadoop.util.TestNativeLibraryChecker |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:fbe3e86 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12795261/HDFS-6489.002.patch |
| JIRA Issue | HDFS-6489 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux ceeefd54ffd7 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2e1d0ff |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_74 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_74.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_74.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_95.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_74.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_74.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_95.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/14929/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/14929/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C:  hadoop-common-project/hadoop-common   hadoop-hdfs-project/hadoop-hdfs  U: . |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/14929/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

, Most of test failures are timeout, I can run them successfully on my laptop, the only failure was org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica

{quote}
Running org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.861 sec - in org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser
Running org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.426 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogTailer
Running org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica
Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 16.789 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica
testAppend(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica)  Time elapsed: 2.146 sec  <<< FAILURE!
java.lang.AssertionError: Should not have space to append to an RWR replicaBP-228064733-9.181.90.49-1459147940642:blk_4_2004
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica.testAppend(TestWriteToReplica.java:181)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica.testAppend(TestWriteToReplica.java:95)
Running org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 255.421 sec - in org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 69.761 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.381 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID
Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 36.823 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure

Tests run: 35, Failures: 1, Errors: 0, Skipped: 0
{quote}

this wasn't a code issue, this test was based on the old logic, I will upload v3 patch shortly to resolve this., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 8s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 4 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 13s {color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 31s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 6m 1s {color} | {color:green} trunk passed with JDK v1.8.0_77 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 6m 39s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 1m 6s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 45s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 28s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 25s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 1s {color} | {color:green} trunk passed with JDK v1.8.0_77 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 48s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue} 0m 15s {color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 5m 45s {color} | {color:green} the patch passed with JDK v1.8.0_77 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 5m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 6m 31s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 6m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 1m 4s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 47s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 58s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 58s {color} | {color:green} the patch passed with JDK v1.8.0_77 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 46s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 16m 49s {color} | {color:red} hadoop-common in the patch failed with JDK v1.8.0_77. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 67m 20s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_77. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 8m 58s {color} | {color:green} hadoop-common in the patch passed with JDK v1.7.0_95. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 55m 8s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 24s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 207m 6s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_77 Failed junit tests | hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM |
|   | hadoop.hdfs.server.namenode.TestReconstructStripedBlocks |
|   | hadoop.hdfs.TestEncryptionZones |
| JDK v1.8.0_77 Timed out junit tests | org.apache.hadoop.http.TestHttpServerLifecycle |
| JDK v1.7.0_95 Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:fbe3e86 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12797237/HDFS-6489.003.patch |
| JIRA Issue | HDFS-6489 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 6a8c340fa461 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 21eb428 |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_77 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15084/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_77.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15084/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_77.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15084/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/15084/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.8.0_77.txt https://builds.apache.org/job/PreCommit-HDFS-Build/15084/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_77.txt https://builds.apache.org/job/PreCommit-HDFS-Build/15084/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/15084/testReport/ |
| modules | C:  hadoop-common-project/hadoop-common   hadoop-hdfs-project/hadoop-hdfs  U: . |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/15084/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

, These tests can run successfully on my local environment, attach the output 
{quote}
Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 86.709 sec - in org.apache.hadoop.hdfs.TestEncryptionZones
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.741 sec - in org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 54.997 sec - in org.apache.hadoop.hdfs.server.namenode.TestReconstructStripedBlocks
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.338 sec - in org.apache.hadoop.http.TestHttpServerLifecycle
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 77.343 sec - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes
{quote}
Can someone help to review this patch? Thanks a lot., Thanks for the patch [~cheersyang]! Could you please let me know which branch I should apply it against and how? I am seeing conflicts. Usually we start with patches towards trunk, and then after it gets committed to trunk, backport it to branch-2, Unfortunately I don't think your approach will work Weiwei! The du thread takes really long times. On a single datanode there may be millions of files and thus the du takes a while to complete.
I would concur with Andrew's opinion *unless* the following is happening
1. Appending to the same file multiple times without closing is causing DFS usage to jump by block size every time. It seems [~andrew.wang] that users are reporting such behavior. It may be worth a look
2. We are rejecting writes on the datanodes based on (possibly outdated) information from the DU thread. If we are going to wait for the DU thread to update available space before writing, we may be rejecting for a long time.
Please let me know if I somehow misunderstand the issue / symptoms., From Bogdan's code (thanks for that :-) ) I see its not #1 in my comment above, but #2 that is causing the problem. I'll check to see if there is a workaround, [~raviprak] Thanks for looking at this. 

#1 Yes, this issue can be reproduced with appending same file lots of times (each time we close the stream), and also appending different files. The real thing matters is you use append API quite some times in a short time window.

#2 I was proposing to wait DU thread to refresh only when on a datanode, it is found the space is not enough for an append operation, this only happens at the time when that wait benefits (rather than fail). And once the space usage is updated, you would not need to wait for sometime until the problem comes up again. I'd love to know if you have any alternative approach.

I'll upload a patch that can apply to latest trunk shortly. Thanks for looking into this., The conflict was caused by some changes from HADOOP-12973, I'll consolidate a patch based on that. Appreciate any comments, thanks., The problem is here: https://github.com/apache/hadoop/blob/f16722d2ef31338a57a13e2c8d18c1c62d58bbaf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java#L323 . Even though this is an append, {{dfsUsage}} is incremented by the total block size every time. This can be easily seen by running the {{testFrequentAppend}} (included in Weiwei's patch) and adding a log line after line 323.
As far as I can see, this problem existed since 2012, but only recently did this become problematic because we started considering dfsUsed space in deciding whether to write a block or not., Revised the patch so it can be applied to trunk. Not sure if there is better approach to resolve this, I attach the patch and appreciate any comments., Here's a patch which reduces the dfsUsed space when a finalized replica is converted to an RBW (replica-being-written) on an append so that on finalization, the counts are correct. This lets {{testFrequentAppend}} pass.

I'll see if it doesn't make sense to add to dfsUsed only the delta during finalize., It *did* make more sense to change only by the additional bytes appended. Here's a patch to do so. Could someone please review? , Hi Ravi, thanks for working on this. Some comments:

* Unused imports in new test
* metaFile is unused now that the code is removed from addFinalizedBlock
* There are multiple callers of FSDatasetImpl#finalizeReplica, and now we only increment dfsUsed when ReplicaState is RBW. Is this intended behavior?
* If possible, I'd prefer to do this logic in FsVolumeImpl#addFinalizedBlock, since that's where other space tracking is happening., Andrew! Thanks for the review. Here's a patch with the changes you suggest. I am sticking with conditional for RBW for now because 1. Its the common case, 2. RUR doesn't have {{originalBytesReserved}}.

Even with this, {{dfsUsed}} and {{numblocks}} counting is all messed up. e.g. [FsDatasetImpl.removeOldBlock|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java#L2886] calls {{decDfsUsedAndNumBlocks}} twice (so even though {{dfsUsed}} is correctly decremented, {{numBlocks}} is not) . [~brahmareddy], [~vinayrpet] am I reading this right?

To really get this sorted out, we should probably have a unit test framework that tests DN side accounting is proper for several different operations (block creation, appends, block transfer (e.g. from 1 storage to another), etc.). Unfortunately, I don't think I'll have the cycles for that. Sorry :(
Arpit has seemed amenable to [removing reservations|https://issues.apache.org/jira/browse/HDFS-9530?focusedCommentId=15248968&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15248968] if there is some other alternative. I think we should disable the rejection of writes by DNs based on reservations until we can be sure that our accounting is correct. Just my $0.02, I guess this means that the available space block placement policy is also broken then :( Agree that we need a methodical rework.

I'm a +0.5 on this patch, it looks good to me, but would need to spend more time understanding the existing accounting before I could +1. If another reviewer more familiar with this area can also review, that'd be appreciated., Hi [~brahmareddy]! Could you please update us on this issue? Also, what do you think about a robust unit-test framework to find out all these issues?, bq.Even with this, dfsUsed and numblocks counting is all messed up. e.g. FsDatasetImpl.removeOldBlock calls decDfsUsedAndNumBlocks twice (so even though dfsUsed is correctly decremented, numBlocks is not)
Nope. I believe you read this wrong.
{{FsDatasetImpl#removeOldReplica}} call two seperate calls {{onBlockFileDeletion(..)}} and {{onMetaFileDeletion(...)}} for {{blockfile}} and {{metafile}} respectively.
{code}
    // Remove the old replicas
    if (replicaInfo.deleteBlockData() || !replicaInfo.blockDataExists()) {
      FsVolumeImpl volume = (FsVolumeImpl) replicaInfo.getVolume();
      volume.onBlockFileDeletion(bpid, replicaInfo.getBytesOnDisk());
      if (replicaInfo.deleteMetadata() || !replicaInfo.metadataExists()) {
        volume.onMetaFileDeletion(bpid, replicaInfo.getMetadataLength());
      }
{code}

 *Code from {{FsVolumeImpl.java}}* 
{code}
 void onBlockFileDeletion(String bpid, long value) {
    decDfsUsedAndNumBlocks(bpid, value, true);
    if (isTransientStorage()) {
      dataset.releaseLockedMemory(value, true);
    }
  }

  void onMetaFileDeletion(String bpid, long value) {
    decDfsUsedAndNumBlocks(bpid, value, false);
  }

  private void decDfsUsedAndNumBlocks(String bpid, long value,
                                      boolean blockFileDeleted) {
    try(AutoCloseableLock lock = dataset.acquireDatasetLock()) {
      BlockPoolSlice bp = bpSlices.get(bpid);
      if (bp != null) {
        bp.decDfsUsed(value);
        if (blockFileDeleted) {
          bp.decrNumBlocks();
        }
      }
    }
  }
{code}

{{onBlockFileDeletion(..)}} calls {{decDfsUsedAndNumBlocks(bpid, value, true);}} with {{blockFileDeleted}} flag as {{true}} to drement the {{numblocks}},where as   {{onMetaFileDeletion(...)}} calls {{decDfsUsedAndNumBlocks(bpid, value, false);}} with {{blockFileDeleted}} flag as {{false}}.Because,no need to decrement the {{numblocks}} for metafile deletion.


bq.Also, what do you think about a robust unit-test framework to find out all these issues?

Only way is to list all write/delete cases and write tests for that

 *Comments for this Jira* 

1) only {{incDfsUsed()}} should be used, as {{numBlocks}} will be updated during {{createRbw()}} for new blocks. For {{append}} incrementing {{numBlocks}} not required.
2) Previous metadata length also should be deducted.
{code}
860	    if(b instanceof ReplicaInfo) {
861	      ReplicaInfo replicaInfo  = ((ReplicaInfo) b);
862	      if(replicaInfo.getState() == ReplicaState.RBW) {
863	        ReplicaInPipeline rip = (ReplicaInPipeline) replicaInfo;
864	        // rip.getOriginalBytesReserved() - rip.getBytesReserved()
865	        // is the amount of data that was written to the replica
866	        long bytesAdded = rip.getOriginalBytesReserved() -
867	        rip.getBytesReserved() + replicaInfo.getMetaFile().length();
868	        incDfsUsedAndNumBlocks(bpid, bytesAdded);
869	      }
870	    }
{code}

Sorry for late reply., I am un-assigning myself from this ticket, [~raviprak] do you want to take this over?, Thanks for your reply [~brahmareddy]! Sorry about the tangent on {{FsDatasetImpl#removeOldReplica}} . I'm afraid I'm also not sure you are the point person on this. Could you please redirect me to the right person if you're not?

Let's focus on the {{HDFS6489.java}} test in written and reported by Bogdan. I see that it still fails on trunk. Here's the output
{code}
$ java HDFS6489
doing small appends...
17/09/06 13:20:25 INFO hdfs.DataStreamer: Exception in createBlockOutputStream blk_1073741835_1057
java.io.EOFException: Unexpected EOF while trying to read response from server
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:444)
	at org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1750)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1495)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:737)
Exception in thread "main" java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-af60f3f1-eb86-46c2-821a-8d2f1dcb339d,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1549)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1483)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:737)
{code}
Why do you think that is?

Where is the code you posted last? I wasn't able to find it in trunk or branch-2, bq.Where is the code you posted last? I wasn't able to find it in trunk or branch-2
it's from the patch [HDFS-6489.007.patch|https://issues.apache.org/jira/secure/attachment/12803016/HDFS-6489.007.patch] you uploaded., Any update about this issue?   I think frequent truncate will have the same issue., HI Guys,

Thanks for working on this issue. I have gone through the discussions, would like to share my thoughts:

- The append operation tend to happen many times on the same block, this makes it easy to deny write due to incorrect DU estimation of append.
- [~cheersyang]'s approach tries to remedy the situation by "interrupts DURefreshThread and then evaluates the space again", however, it would be too slow to help ongoing writes.
- [~raviprak]'s approach tries not to include block size when incr DU when converting complete block to RBW (append). This might under estimate the disk usage, and cause over subscription of a DN capacity, and cause write to fail. The fs.du.interval is 10 minutes by default, the oversubscription (if any) can be corrected in 10 minutes (right Ravi?). So the chance of this failure is relatively low. 

Sounds like we can go with Ravi's solution if the accounting is corrected every 10 minutes. Given that we don't have a perfect solution for this problem, and DU is an estimation anyways.

What do you guys think?

Thanks.








]