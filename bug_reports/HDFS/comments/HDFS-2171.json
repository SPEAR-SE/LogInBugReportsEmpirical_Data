[Just thinking out loud - another option would be to amend the DN to use the {{Reconfigurable}} interface to allow this setting to be changed in the DNs at run time.

Is this not also an issue on trunk? If so, this work should probably be done there and then back-ported to 0.20.205., On trunk I think we should do the protocol change - we use protobufs for the ops there, so we can make it backwards-compatible without any trouble., Thanks Aaron and Todd.

If the solutions between 0.20.205.0 and trunk turn out to be too different, I would like to open a separate Jira for 0.20.205.0 and trunk.

One problem with the Reconfigurable class is that it isn't defined in branches outside of trunk.

Even with the Reconfigurable implementation, wouldn't there still need to be some interface defined (maybe via dfsadmin) that would trigger each datanode to reset that value? This seems to be somewhat of a maintenance problem because if the request happens when a node is down, that node won't have the new value.

, bq. If the solutions between 0.20.205.0 and trunk turn out to be too different, I would like to open a separate Jira for 0.20.205.0 and trunk.

Sure, that's totally fine. My goal is just to make sure there are no behavioral regressions from previous releases to trunk.

bq. One problem with the Reconfigurable class...

Yup, makes sense. Just wanted to make sure you were aware of it.

bq. This seems to be somewhat of a maintenance problem because if the request happens when a node is down, that node won't have the new value.

I think the intended use of {{Reconfigurable}} interface was to change the config files on all the nodes, and then trigger a selective run-time reconfiguration. Doing this would allow for nodes which were down at the time to pick up the changes from the config files whenever they come back up., There are some concerns surrounding the solution proposed in the description of this ticket.

Namely, adding the {{-bandwidth}} parameter to the _balancer_ command would cause a version change of the {{DataTransferProtocol}}. This, in turn, would make it more complicated for a client to access multiple clusters from a single process/thread.

So, I'd like to outline the requirements as I see them and the proposed solutions:

+*Requirements*+:
   1. The _balancer_'s {{bandwidthPerSec}} parameter *must* be modifiable at runtime.
   2. The solution between 0.20 and trunk *must* look the same from a user's perspective.
   3. The {{DataTransferProtocol}} version *should not* be modified in a dot release.

+*Proposed Solutions*+:
   1. Add the {{bandwidth}} parameter to the _balancer_ CLI and pass it via the {{OP_REPLACE_BLOCK}} and {{OP_COPY_BLOCK}} protocol packtes.
   1.1. Change the {{DataTransferProtocol}} version to indicate to the DN that these packets contain the {{bandwidth}} parameter.
   2. Add the {{bandwidth}} parameter to the _balancer_ CLI and pass it via the {{OP_REPLACE_BLOCK}} and {{OP_COPY_BLOCK}} protocol packtes.
   2.1. Instead of changing the {{DataTransferProtocol}} add a magic number to the {{OP_REPLACE_BLOCK}} and {{OP_COPY_BLOCK}} protocol packets before adding the {{bandwidth}} parameter to the packet. The DN could then check for the existence of the magic number and, if it exists, read the new bandwidth. This would alert the DN that the {{bandwidth}} parameter is there without the need of changing the version number.
   3. Add a new _dfsadmin_ command that sets the {{bandwidth}} and passes it to eadh DN via the heartbeat ack from the NN.

I have implemented option 1 and it works. Likewise, option 2 is a minor modification to option 1, but with the advantage of not having to change the version number.

I will investigate option 3.
, +*Proposed Solutions*+:
   4. Add a new opcode that is used just by _balancer_ to change the {{bandwidth}}.
   4.1. This would not necessarily require a change to the protocol version, since this new opcode will be _balancer_-specific. If a newer release of the _balancer_ jars is used with an older release of the DN daemons, the DN will throw an IOException because it doesn't recognize the opcode, but I don't think the DN will crash. I think the IOExeption in that case is logged and the DN continues.

I'll have to investigate this further as well., In my tests, the dfsadmin solution (option 3) seems to work well.

After thinking about it, I like the dfsadmin solution best because the balancer utility can keep running while the bandwidth is adjusted.

I'll create a unit test and post a patch Monday.

-Eric, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12487865/HDFS-2171.patch
  against trunk revision 1150960.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/1023//console

This message is automatically generated., Hadoop QA PreCommitHDFS-Build is trying to apply patch to trunk. That is why the auto build failed.

Here is the output of the test-patch process which I ran in my build environment:

     [exec] BUILD SUCCESSFUL
     [exec] Total time: 5 minutes 31 seconds
     [exec] 
     [exec] 
     [exec] 
     [exec] 
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 6 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.
     [exec] 
     [exec] 
     [exec] 
     [exec] 
     [exec] ======================================================================
     [exec] ======================================================================
     [exec]     Finished build.
     [exec] ======================================================================
     [exec] ======================================================================
     [exec] 
     [exec] 

BUILD SUCCESSFUL
Total time: 14 minutes 33 seconds
, Unit Tests: I added ./src/test/org/apache/hadoop/hdfs/TestBalancerBandwidth.java. Unit tests passed.
1) Prior to starting daemons, set dfs.balance.bandwidthPerSec to 1M (1048576)

Manual Tests: The following tests passed:
2) Set up a cluster as follows:
  On different hosts, start the following daemons:
    1 JT
    1 NN
    1 SNN
    1 DN/TT

3) On client, run randomwriter utility to fill up the DN
$  $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar $HADOOP_HOME/hadoop-examples.jar randomwriter input_$(date +%s)

4) Stat 3 more DN/TT daemons on separate hosts

5) On NN, run balancer:
$ sudo -u hdfs bash -c "export HADOOP_HOME=$HADOOP_HOME; export HADOOP_CONF_DIR=$HADOOP_CONF_DIR; $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR balancer -threshold 10"

6) Watch network bandwidth on DN that was started in step 2). TX bandwidth should not go much above 1M bps. Note that it will be a little higher, but only because there are additional communications happening on that host besides balancing.

7) Watch network bandwidth on other 3 DN hosts. RX bandwidth should not exceed 1M bps (by much).

8) On client as hdfs user, change balancer bandwidth value to 20M:
$ $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR dfsadmin -setBalancerBandwidth 20971520

9) Watch network bandwidth on nodes. TX and RX network bandwidth should not exceed 20M bps (by much).
, This needs to be fixed for trunk before it can be put in the maintenance release. It's OK if the feature has a different implementation, but I'm not interested in reviewing a patch against 0.20 before the issue is addressed in trunk., OK. The changes were slightly different on trunk, but largely the same.

I have submitted a patch to HDFS-2202 for the changes to trunk. I have run the new unit test (TestBalancerBandwidth), and it was successful.
, Hey Todd,

Is it ever appropriate to merge a patch to a branch first and then merge it in trunk? My concern is that this patch will get stale while I try to resolve HDFS-2202, which is the patch for trunk.

I have added a patch to trunk for this feature in HDFS-2202, but there are a couple of issues with it:
1) there are some seemingly unrelated test failures in areas I didn't touch: hdfsCLI, Append, and HFlush. These DO NOT show up when I run test-patch in my build environment. I'm looking into those.
2) In HDFS-2106, Nickolas has refactored the FSNameSystem class (and others), so the trunk patch will also need to be redone.

I am working on these, and the patch for trunk will be updated in the next couple of days. Is it possible that the branch patch could go in first to 205 and then the trunk patch goes in a couple of days afterwards?

Thanks,
-Eric, In my opinion, the work should always be done on trunk first. Otherwise it's too easy for higher priority things to surface before the trunk patch gets done, and you end up with a trunk which is a regression against the "stable" release in some aspects. This makes trunk difficult to release., > there are some seemingly unrelated test failures in areas I didn't touch: hdfsCLI, Append, and HFlush. These DO NOT show up when I run test-patch in my build environment. I'm looking into those.
If tests failures are unrelated to your patch, please do comment so. That should not hold your patch from going in., The patch here and the one in HDFS-2202 are quite similar.  How about closing one of the JIRAs as duplicated?, Putting patches for both 0.20.205.0 and 0.23.0 on HDFS-2202., Closed upon release of 0.20.205.0]