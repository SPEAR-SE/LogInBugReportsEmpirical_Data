[I was able to  recreate this error on fresh machine with a single node setup running Linux Mint 17.2. The error occured in the same was as before even with a different avro file (took this:
https://cwiki.apache.org/confluence/display/AVRO/Interoperability+Testing but wrote a 100MB file by looping over append DATUM).
The error occured with pydoop as well as with the normal streaming command.

Marius, Hi [~butzy92], thanks for reporting this issue.
{code}
2015-07-17 16:33:45,671 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: BlockSender.sendChunks() exception:
java.io.IOException: Die Verbindung wurde vom Kommunikationspartner zurückgesetzt
        at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
        at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:443)
        at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:575)
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:559)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:728)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:496)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:235)
        at java.lang.Thread.run(Thread.java:745)
{code}
Base on above stack trace, This looks like a HDFS issue, updated the component to HDFS., Thanks, i found that this issue appears for longer txt files, too. These are my settings in hdfs-site.xml:
One the SingleNode:
http://pastebin.com/WNc93zWW

One the Multinode:
http://pastebin.com/faHYKLFv

Note that the address on the multinode was added later in order to check if there are some port issues, however this was the only port i tested.

Marius, I found the same error on a fresh installed OS, these are my full setting (running on Linux Mint 17.2):
http://pastebin.com/hr6jygbd (its an XML so if you import and reformat it it should look fine), > ... java.io.IOException: Die Verbindung wurde vom Kommunikationspartner zurückgesetzt

This is a connection reset exception.  The other side somehow dropped the connection; please check.  It does not look like a bug., I meet the same exception.
But I'm running a SparkSql.
Each datanode is not stopped printing the error log.
I set the log to debug log.
2016-01-15 17:23:14,761 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: block=BP-791930769-192.168.50.65-1452847035962:blk_1073742591_1767, replica=FinalizedReplica, blk_1073742591_1767, FINALIZED
  getNumBytes()     = 10060056
  getBytesOnDisk()  = 10060056
  getVisibleLength()= 10060056
  getVolume()       = /home/hadoop/tmp/dfs/data/current
  getBlockFile()    = /home/hadoop/tmp/dfs/data/current/BP-791930769-192.168.50.65-1452847035962/current/finalized/subdir0/subdir2/blk_1073742591
  unlinked          =false
2016-01-15 17:23:14,761 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: replica=FinalizedReplica, blk_1073742591_1767, FINALIZED
  getNumBytes()     = 10060056
  getBytesOnDisk()  = 10060056
  getVisibleLength()= 10060056
  getVolume()       = /home/hadoop/tmp/dfs/data/current
  getBlockFile()    = /home/hadoop/tmp/dfs/data/current/BP-791930769-192.168.50.65-1452847035962/current/finalized/subdir0/subdir2/blk_1073742591
  unlinked          =false
2016-01-15 17:23:14,764 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: BlockSender.sendChunks() exception: 
java.io.IOException: 连接被对端重置
	at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
	at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:433)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:565)
	at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:735)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:527)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:237)
	at java.lang.Thread.run(Thread.java:745)

There's a lot of log!!!!!!
And I found 3-4 java process is undering CLOSE_WAIT.
Please save my disk , it has 50g logs now.

, PS: what is strange!!
    my sparksql runs well;
    and not all the process stay in close wait;
    half of them is established




, > java.io.IOException: 连接被对端重置

This again is a connection reset exception.  The other side somehow dies or drops the connection.  Please check the other side.

> There's a lot of log!!!!!! ...

It is because the debug log was turned on.

> PS: what is strange!!
> my sparksql runs well;

It is possible that some of the (redundant) tasks were killed so that they generated the connection reset exceptions., I have the same IOException in unit-tests with miniDFS cluster]