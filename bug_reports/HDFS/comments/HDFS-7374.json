[This use case has a (slightly) conflicting objective as that of HDFS-6791. I can think of 2 options to accommodate both scenarios:

# Set a timeout (e.g., 10 minutes) limiting the time that a dead DN can stay in DECOMMISSION_INPROGRESS state.
# If a DN is already dead when decomm starts, indicating that a user is intentionally decommissioning a dead node, we should allow it to enter decommission complete state.

[~mingma] and [~jingzhao] please advise if they look reasonable to you, and whether you prefer one over the other. Or any other approaches. Thanks!, Zhe, thanks for reporting this.

At the high level, there is a state machine for DN with total of 6 possible states, {{Live, NORMAL}}, {{Live, DECOMMISSION_INPROGRESS}}, {{Live, DECOMMISSIONED}}, {{Dead, NORMAL}}, {{Dead, DECOMMISSION_INPROGRESS}}, {{Dead, DECOMMISSIONED}}. Events such as node membership change and decommission management will cause the state to change.

Your #1 suggestion is to have {{Dead, DECOMMISSION_INPROGRESS}} transition to {{Dead, DECOMMISSIONED}} upon timeout. Not sure if that is the best approach. Your #2 suggestion have {{Dead, NORMAL}} transition directly to {{Dead, DECOMMISSIONED}} upon decomm event. That sounds like like a good idea to address your situation.

However, we still have the situation regarding which state {{Live, DECOMMISSION_INPROGRESS}} should be transitioned to when DN becomes dead. HDFS-6791 makes it transition to {{Dead, DECOMMISSION_INPROGRESS}}. It seems you want to make sure it eventually gets to {{Dead, DECOMMISSIONED}} state.

Some more ideas on this.

1. If the node stays in {{Dead, DECOMMISSION_INPROGRESS}} for too long, have the higher layer application remove the node from exclude file and thus abort the decommission process. This will transition the node to {{Dead, NORMAL}}.
2. HDFS-6791 mentioned another way to address the original issue. When nodes become dead, mark them DECOMMISSIONED and fix the replication to handle this case. In other words, get rid of {{Dead, DECOMMISSION_INPROGRESS}} state.

Initially I plan to refactor the code to have more explicit state transition. But didn't find it worthwhile.


, [~mingma] Thanks much for clarifying the state machine. I agree my option #2 is cleaner and makes the decommissioning of dead nodes much faster. I'll go ahead with that approach now. 

bq. If the node stays in Dead, DECOMMISSION_INPROGRESS for too long, have the higher layer application remove the node from exclude file and thus abort the decommission process. This will transition the node to Dead, NORMAL.

The specific higher layer application in my case is Cloudera Manager and I think it's possible to add this logic. However I don't know how easy it is to change all similar management applications.

bq.  HDFS-6791 mentioned another way to address the original issue. When nodes become dead, mark them DECOMMISSIONED and fix the replication to handle this case. In other words, get rid of Dead, DECOMMISSION_INPROGRESS state.

Do you mean allowing a {{DECOMMISSIONED}} node to be the source of a replica transfer? It seems a little fragile to me; intuitively, it could surprise upper layer applications that a {{DECOMMISSIONED}} node is still actively transferring data. But I would like to hear the opinions from other people., Yeah, the idea was to use {{DECOMMISSIONED}} node as the source node only when there is no {{NORMAL}} node available. Agree it breaks the state definition. , Thanks [~mingma] again for the comment.

This patch implements option #2. It also moved an utility function to {{DFSTestUtil}} so it's accessible in the new unit test., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12680362/HDFS-7374-001.patch
  against trunk revision 9a4e0d3.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The test build failed in hadoop-hdfs-project/hadoop-hdfs 

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8697//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8697//console

This message is automatically generated., This issue is definitely tricky. I agree with everyone's discussion thus far, thanks especially to [~mingma] for weighing in with insights from HDFS-6791. I think Zhe's proposal #2 is good, and we should work on getting it in. As a follow-on, we could also consider trying to expose more information to operators to help them decide if they should "force decom" by messing with the exclude file. The core issue IIUC is knowing if a force decom will result in data loss, which could probably be pieced together from fsck, but is by no means cheap to do.

With that, some light patch comments:

* I think the logic is a bit wrong right now, since it can shortcut a node from (DEAD, DECOM_IN_PROGRESS) to (DEAD, DECOMMED) if refresh is called when the node is in the exclude file, where IIUC what we want is to only allow (DEAD, NORMAL) to (DEAD, DECOMMED).
* Because of the above, would be good to move this logic into startDecommission instead, we also want to be doing some log prints even in this situation
* We can use GenericTestUtils#waitFor to do the waitForDatanodeState, it prints a nice stack trace as a benefit.
* Could clean up the imports in TestDeadDatanode, TestDecommissioningStatus
* TestDecomm, could remove the println, line longer than 80chars, could also add a test timeout

Thanks for working on this Zhe!, Thanks [~andrew.wang] for the review! The previous patch missed the {DEAD, DECOMM_IN_PROGRESS} -> {DEAD, DECOMMED} case. And I agree with moving the logic to {{startDecommission}}. The revised patch has also addressed the comments on unit testing code., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12681495/HDFS-7374-002.patch
  against trunk revision d005404.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDecommission

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8738//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8738//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8738//console

This message is automatically generated., The patch looks good, findbugs looks unrelated, but the TestDecommission failure is worrying and also failed for me locally. Could you take a look?, Hey [~mingma], I was looking a bit more at decom, and I see that we have this if statement at the end of {{isReplicationInProgress}}:

{code}
    if (!status && !srcNode.isAlive) {
      LOG.warn("srcNode " + srcNode + " is dead " +
          "when decommission is in progress. Continue to mark " +
          "it as decommission in progress. In that way, when it rejoins the " +
          "cluster it can continue the decommission process.");
      status = true;
    }
{code}

Logically, a (DEAD, DECOM_IN_PROGRESS) should be able to go to (DEAD, DECOMMED) if all of its blocks are fully replicated, but this if statement prevents {{isReplicationInProgress}} from ever returning false for a dead node. It seems like we can loosen this requirement?, [~andrew.wang], after a node is dead, all its blocks will be removed from blockmap. So if the node no longer joins the cluster, it isn't unclear how you can tell if all its blocks are fully replicated unless we track those blocks.

Another way to cover all these scenarios could be to get rid of {{DEAD, DECOM_IN_PROGRESS}} state. After the node is dead during decommission, transition to {{DEAD, DECOMMED}}. When the node rejoins the cluster, transition it to {{LIVE, DECOM_IN_PROGRESS}}., Hmm, so one situation we've seen is that the cluster is 100% healthy (no under-rep blocks) and dead DNs still get stuck in the D_I_P state. We can safely transition even dead nodes to DECOMMED in this situation.

Going backwards from (DEAD, DECOMMED) back to (LIVE, D_I_P) feels a little weird. IMO DECOMMED should mean that a node can safely be removed from the cluster, even for dead nodes. That won't necessarily be true in this case., So maybe we can use "if all blocks in the whole cluster are fully replicated" instead of "if all blocks of that dead node are fully replicated" as the criteria to move that dead node to decommed state?, Yea, precisely :) I don't know how realistic this is in an active cluster with lots of failing disks, but it'd fix it for some users at least., Yeah, that seems reasonable; How likely you get "whole cluster fully replicated" might depend on how you count it. If it is based on full scan of blockmap, the chance of getting "all blocks fully replicated" condition might be low given it also includes those newly added blocks for which not all DNs have sent IBR; in addition, it has to take the FSNameSystem lock for longer period of time. If it is based on {{BlockManager}}'s {{pendingReplicationBlocksCount}} +  {{underReplicatedBlocksCount}}, then the chance might be higher; and it is faster.

On the "track the blocks of those DECOMM_IN_PROGRESS DNs" note, it might be useful to add the feature later. It also helps another scenario, something [~kihwal] and [~daryn] mentioned before. {{isReplicationInProgress}} currently rescan all blocks of a given node each time the method is called; it isn't efficient as more blocks become fully replicated.

We can have a separate list of DECOMM_IN_PROGRESS blocks which points to the DECOMM_IN_PROGRESS DNs.  {{DecommissionManager}} will scan this list regularly. Each scan will reduce the list as blocks become fully replicated and calculated the latest list of DECOMM_IN_PROGRESS DNs. In normal decomm operations, the # of DECOMM_IN_PROGRESS DNs should be much smaller than the # of total DNs in large cluster; so the extra memory overhead might be acceptable., The failure is in {{testDecommissionWithNamenodeRestart}} and points to an interesting case: when the NN restarts, its DN {{isAlive}} bits are not turned on immediately. This will trigger our logic of immediately decommissioning already dead DNs.

We can move the logic back to {{refreshDatanodes}}. This way it only applies when the decomm command is triggered by the user/admin, instead of from {{registerDatanode}} -> {{checkDecommissioning}} -> {{startDecommission}}. 

Or we can update the logic if {{isAlive}} bit -- maybe it should be turned on when a DN registers itself., I went ahead and made the small change required in DatanodeManager#registerDatanode to make this work. The registration steps were happening in a different order for the "known DN" vs "unknown DN" cases, so I just reordered things to make sure an unknown DN is marked as alive before we check decom state., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12682000/HDFS-7374.003.patch
  against trunk revision 351c556.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestLargeBlock

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8765//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8765//console

This message is automatically generated., [~andrew.wang] Thanks for helping out! The new change (reordering in {{registerDatanode}}) looks good to me. The 2 failed tests both pass locally for me. , +1, the latest patch looks good to me as well.

Good stuff, Zhe, Ming, and Andrew., Thanks for the final sign-off ATM, I've committed this to trunk and branch-2. Thanks Zhe for the patch and Ming for the helpful advice throughout., Found the mismatch in the commit message: HDFS-7373 should be HDFS-7374.
{code}
HDFS-7373. Allow decommissioning of dead DataNodes. Contributed by Zhe Zhang.
{code}, {color:red}WARNING{color}: as already pointed out by [~xyao], due to a typo in HDFS-7374 commit message, there are 2 commits with the message prefix "HDFS-7373":
{noformat}
c0d666c HDFS-7373. Clean up temporary files after fsimage transfer failures. Contributed by Kihwal Lee
5bd048e HDFS-7373. Allow decommissioning of dead DataNodes. Contributed by Zhe Zhang.
{noformat}]