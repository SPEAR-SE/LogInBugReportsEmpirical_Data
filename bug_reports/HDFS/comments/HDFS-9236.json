[First patch., The path does:
* Add replica length check in syncBlock() so DN reports error instead of sending Long.MAX_VALUE to NN.
* Add replica length check on NN so it won't blindly update the replica length to a value larger than configured block size.
* Add extra debug logs to help trace the block recovery process.
* Add unit tests to verify the new exceptions.

I tested the patch with:
* org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery
* org.apache.hadoop.hdfs.server.namenode.TestCommitBlockSynchronization
* org.apache.hadoop.hdfs.TestLeaseRecovery
* org.apache.hadoop.hdfs.TestLeaseRecovery2
* org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover: This, especially the test case testPipelineRecoveryStress is a good system test that stresses all parts in the lease/block recovery code path., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  20m 36s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 2 new or modified test files. |
| {color:green}+1{color} | javac |   9m 13s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  11m 44s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 25s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 35s | The applied patch generated  3 new checkstyle issues (total was 391, now 390). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 41s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 48s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | native |   3m 38s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests |  65m 19s | Tests failed in hadoop-hdfs. |
| | | 117m 40s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |
|   | hadoop.hdfs.web.TestWebHDFSForHA |
|   | hadoop.hdfs.web.TestFSMainOperationsWebHdfs |
|   | hadoop.hdfs.web.TestWebHDFS |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12766356/HDFS-9236.001.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / da16c9b |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-HDFS-Build/12957/artifact/patchprocess/trunkFindbugsWarningshadoop-hdfs.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-HDFS-Build/12957/artifact/patchprocess/diffcheckstylehadoop-hdfs.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/12957/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/12957/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/12957/console |


This message was automatically generated., All tests pass when manually run on OSX and Linux (CentOS 6.4) with latest trunk. It looks like the failures are not caused by this patch.

checkstyle seems to be complaining about file & functions being too long. They are also not caused by this patch., Thanks to [~yzhangal] for offline review and valuable comments! In summary:
* It is difficult come up with a block size limit to enforce on NN. Especially when considering HDFS allows different files to specify their own block size.
** I will remove the NN side change in the next patch. I would still like to investigate if we can enforce a per file block size check.
* The sanity check on DN is useful although the chance of hitting the error in a production cluster is small.


, Removed block size check on NN., Hi [~twu],

Thanks for reporting the finding out the cause of the issue and the patch.

Besides what we discussed, some additional comments, all cosmetic:

1. 
About
{code}
      LOG.info("syncBlock for block " + block + ", all data-nodes don't have "
          + "block or their replicas have 0 length. The block cam be deleted.");
{code}
Change "data-node" to datanode, add "the" and fix a typo "cam":
{code}
      LOG.info("syncBlock for block " + block + ", all datanodes don't have the"
          + " block or their replicas have 0 length. The block can be deleted.");
{code}
BTW, should this be debug message or info? It seems to be helpful to be info, but
I will leave it to you.

2. Change the "data-node" to "datanode" and add header "syncBlock replicaInfo: " to 
the new debug messages in syncBlock method. 

3. Add block info to the exception 
{{throw new IOException("No replica is in the best expected state " + ...}}

4. Change "DN triggering" to "Datanode triggering", to be consistent.

Thanks.
, Hi [~yzhangal],

Thanks a lot for looking at the patch. Regarding your comments:
1: This is already been addressed in patch 2.
2 - 4: I will address this in the next patch.

Regards,
Tony , \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  20m 34s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   8m 40s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  11m  3s | There were no new javadoc warning messages. |
| {color:red}-1{color} | release audit |   0m 19s | The applied patch generated 1 release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 32s | The applied patch generated  1 new checkstyle issues (total was 142, now 141). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 35s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 43s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | native |   3m 37s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests |  65m 37s | Tests failed in hadoop-hdfs. |
| | | 116m 20s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate |
|   | hadoop.hdfs.server.namenode.TestStartup |
|   | hadoop.hdfs.server.datanode.TestReadOnlySharedStorage |
|   | hadoop.hdfs.server.namenode.ha.TestDNFencing |
|   | hadoop.hdfs.server.namenode.TestSaveNamespace |
|   | hadoop.hdfs.server.datanode.TestDnRespectsBlockReportSplitThreshold |
|   | hadoop.hdfs.server.namenode.TestListCorruptFileBlocks |
|   | hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade |
|   | hadoop.hdfs.util.TestByteArrayManager |
|   | hadoop.hdfs.server.namenode.TestHDFSConcat |
|   | hadoop.hdfs.qjournal.client.TestQuorumJournalManager |
|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |
|   | hadoop.hdfs.server.namenode.TestFsckWithMultipleNameNodes |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12766810/HDFS-9236.002.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / dc45a7a |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-HDFS-Build/13005/artifact/patchprocess/trunkFindbugsWarningshadoop-hdfs.html |
| Release Audit | https://builds.apache.org/job/PreCommit-HDFS-Build/13005/artifact/patchprocess/patchReleaseAuditProblems.txt |
| checkstyle |  https://builds.apache.org/job/PreCommit-HDFS-Build/13005/artifact/patchprocess/diffcheckstylehadoop-hdfs.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/13005/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/13005/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/13005/console |


This message was automatically generated., Addressed [~yzhangal]'s review comments., Hi [~twu],

Thanks for the updated rev 3 which looks reasonable to me.

Hi [~kihwal], would you please help taking a look? really appreciate it.

Thanks.
, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  18m 43s | Pre-patch trunk has 1 extant Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   8m 12s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  10m 35s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 24s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   1m 28s | The applied patch generated  2 new checkstyle issues (total was 142, now 142). |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 31s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 34s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   2m 31s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | native |   3m 15s | Pre-build of native portion |
| {color:green}+1{color} | hdfs tests |  49m 37s | Tests passed in hadoop-hdfs. |
| | |  96m 54s | |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12766845/HDFS-9236.003.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 8d2d3eb |
| Pre-patch Findbugs warnings | https://builds.apache.org/job/PreCommit-HDFS-Build/13008/artifact/patchprocess/trunkFindbugsWarningshadoop-hdfs.html |
| checkstyle |  https://builds.apache.org/job/PreCommit-HDFS-Build/13008/artifact/patchprocess/diffcheckstylehadoop-hdfs.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/13008/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/13008/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf900.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/13008/console |


This message was automatically generated., checksyle and pre-patch error are not related to this patch., Hi [~yzhangal],

Could you take another look at the updated patch?

Thanks,
Tony, Sorry for the delay [~e90tony]. I did a review and I'm +1 on rev 003, will commit soon.
, [~yzhangal] Thanks a lot for looking at the patch., Sorry [~twu], the patch no longer applies because of other commits. would you please update the patch? thanks.
, Hi [~yzhangal], I believe HDFS-9255 has moved block recovery related code to a different location. I will rebase my patch and upload a new one shortly., In v4 patch:
* Rebased to latest trunk (moved the changes to new file {{BlockRecoveryWorker.java}})., Thanks [~twu], +1 on rev4 pending jenkins. 
, The latest patch looks good to me overall. One minor comment: is it possible to assert expected exception thrown (e.g. by error message) ?, Hi [~liuml07], 

Thanks a lot for your comment. I debated about having an assert as well and think it has a few disadvantages (please correct me if I'm wrong):

# Assert can be disabled at runtime.
# Assert message is only visible on DN where the exception can propagate back to NN (and also visible on DN).
# Assert would have stopped the DN process, which seems to be an overkill.

Given these reasons I think throwing an exception is the better choice.

Tony , Sorry for the confusion.

By "assert expected exception thrown (e.g. by error message)", I mean {{asserTrue(ioe.getMessage().contains("ooxx"));}} in test, not in the DN code. I'm with you. I believe throwing an exception is correct and assert is wrong in this case., Thanks for clarifying. I'll post a updated patch shortly. , In v5 patch:
* Addressed [~liuml07]'s comment by updating the test case., +1 (non-binding) pending on Jenkins., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 5s {color} | {color:blue} docker + precommit patch detected. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 2m 58s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 32s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 31s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 1m 48s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk cannot run convertXmlToText from findbugs {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 5s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 51s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 39s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 31s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 30s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 30s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 0s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 5s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 48s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 52m 10s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_60. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 49m 14s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_79. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 19s {color} | {color:red} Patch generated 58 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 120m 33s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.7.0_79 Failed junit tests | hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=1.7.1 Server=1.7.1 Image:test-patch-base-hadoop-date2015-10-29 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12769622/HDFS-9236.004.patch |
| JIRA Issue | HDFS-9236 |
| Optional Tests |  asflicense  javac  javadoc  mvninstall  unit  findbugs  checkstyle  compile  |
| uname | Linux 7bcaa73db0fe 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/patchprocess/apache-yetus-c3a2069/precommit/personality/hadoop.sh |
| git revision | trunk / c293c58 |
| Default Java | 1.7.0_79 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_60 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_79 |
| findbugs | v3.0.0 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/13284/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/13284/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_60.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/13284/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_79.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/13284/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_60.txt https://builds.apache.org/job/PreCommit-HDFS-Build/13284/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_79.txt |
| JDK v1.7.0_79  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/13284/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/13284/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Max memory used | 226MB |
| Powered by | Apache Yetus   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/13284/console |


This message was automatically generated.

, Please hold on the patch. I doubt if it can happen in real case.
If a DN has a RUR, it will return {{RecoveryInProgressException}}. Then the primary DN abort the recovery., bq. If a DN has a RUR, it will return RecoveryInProgressException.
[~walter.k.su] A quick comment that DN could run an older version of HDFS than NN. And unknown DN bugs could violate the above assumption as well. Similar to what we saw on HDFS-9289., If a buggy DN does return RUR without throwing {{RecoveryInProgressException}}, please put the checking in the {{recover()}} after {{callInitReplicaRecovery()}}., Thanks a lot for [~walter.k.su] and [~zhz]'s comments!

[~walter.k.su], DN throws {{RecoveryInProgressException}} only when the received recovery ID is smaller than the existing RUR recovery ID:
{code:java}
  static ReplicaRecoveryInfo initReplicaRecovery(String bpid, ReplicaMap map,
      Block block, long recoveryId, long xceiverStopTimeout) throws IOException {
    ...
    final ReplicaUnderRecovery rur;
    if (replica.getState() == ReplicaState.RUR) {
      rur = (ReplicaUnderRecovery)replica;
      if (rur.getRecoveryID() >= recoveryId) {
        throw new RecoveryInProgressException(
            "rur.getRecoveryID() >= recoveryId = " + recoveryId
            + ", block=" + block + ", rur=" + rur);
      }
      final long oldRecoveryID = rur.getRecoveryID();
      rur.setRecoveryID(recoveryId);
      LOG.info("initReplicaRecovery: update recovery id for " + block
          + " from " + oldRecoveryID + " to " + recoveryId);
    }
}
{code}

So if the DN has a block that is already in RUR, and a new block recovery starts (with larger recovery ID), the DN does not throw {{RecoveryInProgressException}}.

The patch is focused on what happens after this point, where a buggy DN (or a unknown corner case causes DN) might report RUR as the replica's original state.

I think your suggestion of moving to check out of {{syncBlock()}} and into {{initReplicaRecovery()}} make sense. I implemented a check to simply exclude the replicas whose original state is >= RUR (they won't be used for recovery anyways). But the issue with this is that we might end up with an empty {{syncList}} and incorrectly tell NN to drop this block. I think the current place for the check in the patch is probably the safest. Please let me know what you think.

Again thanks a lot for taking the time to look at my patch.

, I agree with [~zhz] that a buggy DN could cause this issue.

And I agree with you said. Thanks for digging into the code. My mistake. I revise what I said:
  -If a DN has a RUR, it will return RecoveryInProgressException.-
   If a replica is RUR, a DN returns {{RecoveryInProgressException}} or its original state.

The thing is, it never returns RUR. It's weird {{syncBlock()}} assume there is a RUR in syncList.
It's just I prefer to keep the role of {{syncBlock}} simple as the javadoc said "Block synchronization.".
{{recover()}} does only one thing, checking, so it's a better place. If you insist, let's check the arguments at the top of {{syncBlock()}}., {{syncBlock}} already has an assumption that there's no RUR in syncList. {{bestState}} starts with RWR. {{bestState}} can't be {{RUR}}.
{code}
      // Calculate the best available replica state.
      ReplicaState bestState = ReplicaState.RWR;
      for (BlockRecord r : syncList) {
        if (rState.getValue() < bestState.getValue()) {
          bestState = rState;
        }
{code}

It's weird we add one more assumption that there is a RUR in syncList., Hi [~walter.k.su],

Thanks for the comments. 

Agree that {{bestState}} won't be {{RUR}}, but seems to me that it doesn't mean there may not be {{RUR}} in the syncList. Especially RUR may exist as a bug situation as discussed, and the fix is to try to issue a good message when there is bug.  So we actually can not assume there is no RUR in the syncList. Right? Thanks.



, I mean RUR shouldn't be put in syncList., Thanks [~walter.k.su], that makes sense.
, Thanks [~walter.k.su] and [~yzhangal] for your comments. I'll post a new patch which will exclude RURs from syncList., In v6 patch:
* Address [~walter.k.su]'s comment by excluding RUR replicas from syncList. {{syncBlock()}} now will work on a clean syncList containing only replicas that will be used for recovery.
* Converted the check in previous patches for {{Long.MAX_VALUE}} to an assert.
* Reworked the test case.
* Add some comments., Moving out all non-critical / non-blocker issues that didn't make it out of 2.7.2 into 2.7.3., HI [~twu],

Thanks for the new rev. Some nits.

a. Suggest to change
{code}
          if (info != null &&
              info.getGenerationStamp() >= block.getGenerationStamp() &&
              info.getNumBytes() > 0) {
            // Count the number of valid replicas received.
            ++validReplicaCnt;
            if (info.getOriginalReplicaState().getValue() <=
                ReplicaState.RWR.getValue()) {
              syncList.add(new BlockRecord(id, proxyDN, info));
            }
          }
{code}
to:
{code}
          if (info == null) {
           continue;
          }
          // Count the number of candidate replicas received.
          ++candidateReplicaCnt;
          if (info.getGenerationStamp() >= block.getGenerationStamp() &&
              info.getNumBytes() > 0 &&
              info.getOriginalReplicaState().getValue() <=
                ReplicaState.RWR.getValue()) {
              syncList.add(new BlockRecord(id, proxyDN, info));
         } else {
              //debug message about this replica, to indicate reason of not being chosen
              LOG.debug(...);
         }
     }
{code}
That is:
1. change {{validReplicaCnt}} to {{candidateReplicaCnt}}
2. consolidate the condition checking 
3. add an "else" branch in the code, and log a debug message in the "else" branch.

b. Then modify the following change accordingly.
{code}
      if (validReplicaCnt > 0 && syncList.isEmpty()) {
        throw new IOException("No replica for block " + block +
            " is in " + ReplicaState.RWR.name() + " or better state");
      }
{code}
to
{code}
      if (syncList.isEmpty()) {
        throw new IOException("Found " + candidateReplicaCnt + " replicas for "
            + block + " from (" + Arrays.asList(locs) + "). No replica met the requirements: "
            + " 1. validate generation timestap; " +
            + " 2. non-zero length"
            + " 3. original state is RWR or better");
      }
{code}

Thanks.

, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 7s {color} | {color:blue} docker + precommit patch detected. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 3m 10s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 34s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 33s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 1m 59s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk cannot run convertXmlToText from findbugs {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 13s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 1s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 41s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 33s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 33s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 34s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 34s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 16s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 13s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 12s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 51s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 65m 46s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_66. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 75m 43s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_79. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 25s {color} | {color:red} Patch generated 56 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 162m 19s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_66 Failed junit tests | hadoop.hdfs.server.datanode.TestBlockScanner |
|   | hadoop.hdfs.server.namenode.ha.TestDNFencing |
|   | hadoop.hdfs.server.namenode.snapshot.TestRenameWithSnapshots |
|   | hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes |
| JDK v1.7.0_79 Failed junit tests | hadoop.hdfs.server.namenode.ha.TestDNFencing |
|   | hadoop.hdfs.TestRecoverStripedFile |
|   | hadoop.hdfs.TestReadStripedFileWithDecoding |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped |
|   | hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits |
|   | hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=1.7.0 Server=1.7.0 Image:test-patch-base-hadoop-date2015-11-04 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12770451/HDFS-9236.006.patch |
| JIRA Issue | HDFS-9236 |
| Optional Tests |  asflicense  javac  javadoc  mvninstall  unit  findbugs  checkstyle  compile  |
| uname | Linux 93973b5919f8 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/patchprocess/apache-yetus-d0f6847/precommit/personality/hadoop.sh |
| git revision | trunk / 194251c |
| Default Java | 1.7.0_79 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_66 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_79 |
| findbugs | v3.0.0 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/13370/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/13370/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_66.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/13370/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_79.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/13370/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_66.txt https://builds.apache.org/job/PreCommit-HDFS-Build/13370/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_79.txt |
| JDK v1.7.0_79  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/13370/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/13370/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Max memory used | 226MB |
| Powered by | Apache Yetus   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/13370/console |


This message was automatically generated.

, Thanks [~twu] for the offline discussion. Consolidating the condition checking seems not quite right. 

We can largely do what your last rev does, with some change (along the line of my last review):

1. instead of validReplciaCnt, use candidateReplicaCnt
2. add debug log about the replicas filtered out

{code}
          if (info != null) {
             continue;
          }
          if (info.getGenerationStamp() < block.getGenerationStamp() ||
              info.getNumBytes() <= 0) {
            if (LOG.isDebugEnabled()) {
              LOG.debug(...);
            }
            continue;
          }
          // Count the number of candidate replicas found.
          ++candidateStateCnt;
          if (info.getOriginalReplicaState().getValue() <=
                ReplicaState.RWR.getValue()) {
            syncList.add(new BlockRecord(id, proxyDN, info));
          } else {
            if (LOG.isDebugEnabled()) {
              LOG.debug(...);
            }
          }
{code}

and 

{code}
      // None of the replicas reported by DataNodes has the required original
      // state, report the error.
      if (candidateReplicaCnt > 0 && syncList.isEmpty()) {
        throw new IOException("Found " + candidateReplicaCnt +
            " replica(s) for block " + block + " but no one is in " +
            ReplicaState.RWR.name() + " or better state." +
            " datanodeids=" + Arrays.asList(locs));
      }
{code}
, Thanks a lot [~yzhangal] for your comments. I incorporated them into the new patch.
I added the debug logs but kept the positive logic for determining which replica info to add to syncList in existing code/patch. IMO the positive logic is easier to read/understand., In v7 patch:
* Addressed [~yzhangal]'s review comments.
* Update the test case.
* Add a {{toString()}} method to pretty print {{ReplicaRecoveryInfo}}., The logic looks good to me. Thanks [~twu] for updating and [~yzhangal] for review.
There are many {{isDebugEnabled()}} guard. We can consider move to slf4j style. Well, that's not related to this jira., Thanks [~twu] for the new rev and  [~walter.k.su] for the review. I'm +1 on 007 pending jenkins. 
, Seems jenkins was not triggered, I did one here
https://builds.apache.org/job/PreCommit-HDFS-Build/13400/
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 9s {color} | {color:blue} docker + precommit patch detected. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 3m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 37s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 32s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 2m 5s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk cannot run convertXmlToText from findbugs {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 9s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 57s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 42s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 37s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 37s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 40s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 40s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 17s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 25s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 55s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 57m 49s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_60. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 56m 30s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_79. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 25s {color} | {color:red} Patch generated 58 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 135m 50s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_60 Failed junit tests | hadoop.hdfs.TestDecommission |
|   | hadoop.hdfs.server.blockmanagement.TestNodeCount |
|   | hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes |
| JDK v1.7.0_79 Failed junit tests | hadoop.hdfs.server.blockmanagement.TestNodeCount |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure130 |
|   | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=1.7.1 Server=1.7.1 Image:test-patch-base-hadoop-date2015-11-05 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12770663/HDFS-9236.007.patch |
| JIRA Issue | HDFS-9236 |
| Optional Tests |  asflicense  javac  javadoc  mvninstall  unit  findbugs  checkstyle  compile  |
| uname | Linux 5aeed2b7f49c 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/patchprocess/apache-yetus-e8bd3ad/precommit/personality/hadoop.sh |
| git revision | trunk / ff47f35 |
| Default Java | 1.7.0_79 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_60 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_79 |
| findbugs | v3.0.0 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/13400/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/13400/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_60.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/13400/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_79.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/13400/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_60.txt https://builds.apache.org/job/PreCommit-HDFS-Build/13400/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_79.txt |
| JDK v1.7.0_79  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/13400/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/13400/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Max memory used | 226MB |
| Powered by | Apache Yetus   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/13400/console |


This message was automatically generated.

, Looked at the failed tests and none are related to block recovery. Also manually ran the failed tests against latest code (on Linux, JDK1.7), all passes without error., FAILURE: Integrated in Hadoop-trunk-Commit #8769 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8769/])
HDFS-9236. Missing sanity check for block size during block recovery. (yzhang: rev b64242c0d2cabd225a8fb7d25fed449d252e4fa1)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
, Committed to trunk and branch-2. Thanks [~twu] for the contribution and [~walter.k.su] for the review.
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #648 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/648/])
HDFS-9236. Missing sanity check for block size during block recovery. (yzhang: rev b64242c0d2cabd225a8fb7d25fed449d252e4fa1)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #1371 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/1371/])
HDFS-9236. Missing sanity check for block size during block recovery. (yzhang: rev b64242c0d2cabd225a8fb7d25fed449d252e4fa1)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #638 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/638/])
HDFS-9236. Missing sanity check for block size during block recovery. (yzhang: rev b64242c0d2cabd225a8fb7d25fed449d252e4fa1)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2578 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2578/])
HDFS-9236. Missing sanity check for block size during block recovery. (yzhang: rev b64242c0d2cabd225a8fb7d25fed449d252e4fa1)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2518 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2518/])
HDFS-9236. Missing sanity check for block size during block recovery. (yzhang: rev b64242c0d2cabd225a8fb7d25fed449d252e4fa1)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
, ABORTED: Integrated in Hadoop-Hdfs-trunk-Java8 #579 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/579/])
HDFS-9236. Missing sanity check for block size during block recovery. (yzhang: rev b64242c0d2cabd225a8fb7d25fed449d252e4fa1)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
]