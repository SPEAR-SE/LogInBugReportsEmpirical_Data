[Patch attached which addresses this issue by clearing the pending DN message queue of any messages which mention a DN which is being removed from the DN map., Hi ATM,

Thanks a lot for discovering this tricky problem and providing the fix. As a learning experience, I read through the patch and had a few comments actually mainly questions):

1. "In an HA setup, the standby NN may receive messages from DNs for blocks which the standby NN is not yet aware of." When Standby NN receive the messages, it's actually aware of what message it received, but as time goes by, the messages may become stale due to DN reformatting. That's what you meant, right?

2. Since the DN that the queued messages are associated with is reformatted, these queued messages become stale and useless, and can be safely removed, right? or my real question is, are there any messages that need to applied even after DN reformatting?

3. I found a util function in DatanodeUtil.java
{code}
   public static String getMetaName(String blockName, long generationStamp) {
    return blockName + "_" + generationStamp + Block.METADATA_EXTENSION; 
  }
{code}
that you might consider using in 
{code}
public static boolean changeGenStampOfBlock(int dnIndex, ExtendedBlock blk,
      long newGenStamp) throws IOException {
{code}

4. I think the testcase you wrote is very nice to demonstrate the problem. 

Thanks.

, Hi Yongjun,

bq. When Standby NN receive the messages, it's actually aware of what message it received, but as time goes by, the messages may become stale due to DN reformatting. That's what you meant, right?

Nope, that's not what I was referring to. I was referring to why we queue messages in the first place in the standby NN. This happens because the standby NN is always a bit behind by the active in its knowledge of the namespace and set of blocks which exist, so when the active NN allocates new blocks the DNs will report things about these blocks to both NNs but the standby won't yet know about the existence of the blocks those messages refer to. In some sense these messages are from the future from the DN's perspective, so we queue them up.

bq. Since the DN that the queued messages are associated with is reformatted, these queued messages become stale and useless, and can be safely removed, right? or my real question is, are there any messages that need to applied even after DN reformatting?

No, if the storage ID of the DN is changing, we should assume that the DN no longer has the blocks that these messages were previously referring to.

bq. I found a util function in DatanodeUtil.java...

OK, I'll make this change when I do the next rev of this patch.

bq. I think the testcase you wrote is very nice to demonstrate the problem.

Thanks!, Thanks for answering my questions ATM, that helps!
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12642020/HDFS-6289.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6742//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6742//console

This message is automatically generated., I feel confident that the TestBalancerWithNodeGroup failure is spurious. It passes fine on my box, isn't really related to this code, and has been flaky off and on for a long time., {code}
+        // TODO(atm): This should be s/storedBlock/block, since we should be
+        // postponing the info of the reported block, not the stored block,
+        // though that actually exacerbates the bug, doesn't fix it.
{code}

Out of context, this comment won't make much sense -- what's "the bug" it's refe
rring to? Maybe you should file a separate follow-up JIRA here for this second i
ssue, since you aren't fixing it here?

Otherwise lgtm., Thanks for the review, Todd.

bq. Maybe you should file a separate follow-up JIRA here for this second issue, since you aren't fixing it here?

I could also just fix it here. It seems pretty transparently obvious that we should make that change. Do you agree? If so, I'll just post a patch fixing that as well., Is there any test you could write to show that bug? I agree with your logic, but surprised that there isn't some bug that it causes. Given that the current test isn't a regression test for that bug, maybe should tackle it separately?, You're right, should probably take care of this separately. I only incidentally discovered it while looking into this issue, but it really is a separate bug. I'll file another JIRA once this one's committed.

Latest patch makes the TODO more clear when read out of context, and addresses Yongjun's feedback.

Todd, this look OK to you?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12642380/HDFS-6289.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6759//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6759//console

This message is automatically generated., Can you double check that this test isn't made more flaky by this patch? I've seen this test fail once or twice before in precommits, but given that it's very much related to the code touched by this patch, we should probably investigate it a bit before committing.

Otherwise +1, Thanks a lot for the review, Todd. The TestDNFencingWithReplication test failed with the following error:

{noformat}
java.lang.RuntimeException: Deferred
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.checkException(MultithreadedTestUtil.java:130)
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.stop(MultithreadedTestUtil.java:166)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication.testFencingStress(TestDNFencingWithReplication.java:135)
Caused by: java.io.IOException: Timed out waiting for 2 replicas on path /test-3
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication$ReplicationToggler.waitForReplicas(TestDNFencingWithReplication.java:96)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication$ReplicationToggler.doAnAction(TestDNFencingWithReplication.java:78)
	at org.apache.hadoop.test.MultithreadedTestUtil$RepeatingTestThread.doWork(MultithreadedTestUtil.java:222)
	at org.apache.hadoop.test.MultithreadedTestUtil$TestingThread.run(MultithreadedTestUtil.java:189)
{noformat}

I'm fairly confident this was just a one-off flake, especially considering the code change in this patch is only triggered by DN restarts which TestDNFencingWithReplication doesn't do, but just to be sure I looped TestDNFencingWithReplication 50 times on my box and never saw a failure. I've also just kicked Jenkins to build this JIRA again, so hopefully it'll pass then. If it passes that, I'll go ahead and commit this based on your previous +1., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12642380/HDFS-6289.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6771//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6771//console

This message is automatically generated., The latest test failure is just because of the following:

{noformat}
java.net.BindException: Port in use: localhost:50070
	at sun.nio.ch.Net.bind(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:126)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
	at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
	at org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:853)
	at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:794)
{noformat}

I ran TestBlockRecovery several times on my box and it passes without issue.

I'm going to go ahead and commit this momentarily., I've just committed this to trunk and branch-2.

Thanks a lot for the reviews, Todd and Yongjun., SUCCESS: Integrated in Hadoop-trunk-Commit #5589 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5589/])
HDFS-6289. HA failover can fail if there are pending DN messages for DNs which no longer exist. Contributed by Aaron T. Myers. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1591413)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestPendingCorruptDnMessages.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #557 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/557/])
HDFS-6289. HA failover can fail if there are pending DN messages for DNs which no longer exist. Contributed by Aaron T. Myers. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1591413)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestPendingCorruptDnMessages.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1748 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1748/])
HDFS-6289. HA failover can fail if there are pending DN messages for DNs which no longer exist. Contributed by Aaron T. Myers. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1591413)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestPendingCorruptDnMessages.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1774 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1774/])
HDFS-6289. HA failover can fail if there are pending DN messages for DNs which no longer exist. Contributed by Aaron T. Myers. (atm: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1591413)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingDataNodeMessages.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestPendingCorruptDnMessages.java
]