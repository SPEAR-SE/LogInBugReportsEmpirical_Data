[Please have a look at this , Iam trying to analyse further on this issue and provide a patch for the same., I have attached a patch where the block id value will be incremented by 10 million , if the RollingUpgradeStartupOption=ROLLBACK. 

This would avoid client write failure immediately after rollback,  because of assigning same block id as blocks written before rollback , which are still in Finalized state (Which will be deleted after the second block report.)

Please review the patch and give your feedback. 
, Thank you for reporting this issue [~andreina]. This looks like an unfortunate interaction of rollback with sequential block IDs. Your fix will work for most deployments but I don't like adding a hard-coded 10M increment. Let me think about this some more., One thing I did not understand - the finalized block does not belong to any file after rollback. Hence it should never be added to the BlockInfo list and should be marked for deletion on the DN immediately., Hi Arpit Agarwal thanks for looking at this issue. 

bq.One thing I did not understand - the finalized block does not belong to any file after rollback. Hence it should never be added to the BlockInfo list and should be marked for deletion on the DN immediately.

Block would be marked for deletion only on the second block report ( which would take 6 hrs, as default value for dfs.blockreport.intervalMsec=6hrs). So within this time after rollback any client write operation will fail since block with the same id already exist at DN . 

To avoid the duplicate block id being assigned after rollback , i gave an initial patch considering there could be 10 million blocks written in worst case after upgrade and before rollback, hence incremented the block id by 10 million after rollback.

Please correct me if I'am wrong., Hi [~andreina],

Sorry about the delay in responding. I see what you are saying. The blocks are not deleted right away since {{processFirstBlockReport}} does not schedule deletions.

Rather than add a hard-coded increment we can reserve the msb of the blockid for an epoch. The epoch is toggled on every rollback. The low order 63 bits are incremented sequentially as before on block allocation. Now we will not hit any collisions with block ids allocated before the rollback.

The only drawback I can see is if a second rollback is attempted before all excess blocks from the previous rollback were deleted. We could increase the number of epoch bits to avoid collisions in that pathological case., bq. Rather than add a hard-coded increment we can reserve the msb of the blockid for an epoch. The epoch is toggled on every rollback. The low order 63 bits are incremented sequentially as before on block allocation. Now we will not hit any collisions with block ids allocated before the rollback.
This Idea is good, but the problem I am seeing is same as you mentioned. Multiple Rollbacks.

During the testing of a upgrade, there could be possibility of multiple rollbacks. With each rollback if toggle the msb, then block id range will be reduced by half since the actual no bits involved is reduced. And in any case we cannot re-toggle the bit.

What if blocks are scheduled for deletion on the first report itself only in case of RollBack?, Increasing the epoch bits to 5 or 6 will permit successive rollbacks in a short time interval while avoiding collisions. This addresses all practical scenarios. 2^58 blocks is orders of magnitude more than you would hit in a real cluster so I am not concerned about ID space reduction.

bq. What if blocks are scheduled for deletion on the first report itself only in case of RollBack?
This behavior was added before my time but I think deferring deletions speeds up cluster startup e.g. after upgrades. DN startup time is already an issue in large clusters., bq. Increasing the epoch bits to 5 or 6 will permit successive rollbacks in a short time interval while avoiding collisions. 
I agree for one version upgrade and rollback. 
But my point is, whatever bits changed will remain forever for the entire lifetime of the cluster. We cannot change it back, otherwise blockIds will be having the lesser value than previous. For 6 bits this gives 2^6 rollbacks at max.
Another issue is, we cannot persist this, since at this time NN will be at that startup phase. So in this case, if Namenode restarts again without writing any single block, then problem can occur again.

Am I missing something?

bq. This behavior was added before my time but I think deferring deletions speeds up cluster startup e.g. after upgrades. DN startup time is already an issue in large clusters.
I agree for the upgrade. But for the rollback, entire cluster has to be restarted anyway.

Another option is to write new blocks after to some new directory inside volume itself after rolling upgrade starts, similar to trash, ex: upgrade . And on finalize of upgrade move to finalized directory. And on Rollback just delete them., Any updates on this issue ? .]