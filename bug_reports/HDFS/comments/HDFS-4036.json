[We should also remove/fix the empty javadoc above unprotectedAddFile(..)., Patch uploaded., unprotectedAddFile does not throw UnresolvedLinkException because addNode unconditionally swallows the IOE (and ULE is a subclass of IOE) and returns null... 

{code}
try {
  newNode = addNode(path, newNode, UNKNOWN_DISK_SPACE);
} catch (IOException e) {
  return null;
}
{code}

This doesn't break symlinks because by the time we happen to have gotten to this point all the symlinks in path have already been fully resolved (otherwise symlinks would be broken because we hide the ULE that the client needs to get to resolve the link). But the issue here doesn't seem to be that we're claiming to throw a ULE when we don't, it's why are we unconditionally swallowing IOEs here w/o so much as a warning? That doesn't seem right., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12548699/HDFS-4036-trunk.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3311//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3311//console

This message is automatically generated., > ... But the issue here doesn't seem to be that we're claiming to throw a ULE when we don't, it's why are we unconditionally swallowing IOEs here w/o so much as a warning? That doesn't seem right.

Eli, I think the reason is that this method is "unprotected"., +1 patch looks good.  New tests are needed since it only changes the declaration., I have committed this. Thanks, Jing!, Integrated in Hadoop-Hdfs-trunk-Commit #2922 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2922/])
    HDFS-4036. Remove "throw UnresolvedLinkException" from FSDirectory.unprotectedAddFile(..).  Contributed by Jing Zhao (Revision 1398293)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1398293
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
, Integrated in Hadoop-Common-trunk-Commit #2860 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2860/])
    HDFS-4036. Remove "throw UnresolvedLinkException" from FSDirectory.unprotectedAddFile(..).  Contributed by Jing Zhao (Revision 1398293)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1398293
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
, Integrated in Hadoop-Mapreduce-trunk-Commit #2883 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2883/])
    HDFS-4036. Remove "throw UnresolvedLinkException" from FSDirectory.unprotectedAddFile(..).  Contributed by Jing Zhao (Revision 1398293)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1398293
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
, bq. Eli, I think the reason is that this method is "unprotected".

Nicholas, but why is it unprotected? In What cases should we unconditionally swallow an IOE? Shouldn't we at least log a debug? , Integrated in Hadoop-Mapreduce-trunk #1228 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1228/])
    HDFS-4036. Remove "throw UnresolvedLinkException" from FSDirectory.unprotectedAddFile(..).  Contributed by Jing Zhao (Revision 1398293)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1398293
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
, > Nicholas, but why is it unprotected? In What cases should we unconditionally swallow an IOE? Shouldn't we at least log a debug?

The method was there for a long time.  By "unprotected", I think it means that error checking must be done before calling the method.  That's why it swallow exceptions.  It is a bad coding style., Right, hence the suggestion to either fix it here (or add a debug log at least) or file a jira for doing so. , Hey Eil, I just have filed HDFS-4073.]