[This looks like a duplicate of HDFS-2323., Tom, Name node and datanodes are starting fine after the patch HDFS-2323.But still it is giving "No such file or directory" error on the console.

{code:xml}
./start-dfs.sh: line 50: /home/dev/HadoopRelease/hadoop-common-0.24.0-SNAPSHOT/libexec/../bin/hdfs: No such file or directory
Starting namenodes on []
localhost: starting namenode, logging to /home/dev/HadoopRelease/hadoop-common-0.24.0-SNAPSHOT/libexec/../logs/hadoop-root-namenode-linux-fr5y.out
localhost: starting datanode, logging to /home/dev/HadoopRelease/hadoop-common-0.24.0-SNAPSHOT/libexec/../logs/hadoop-root-datanode-linux-fr5y.out
Secondary namenodes are not configured.  Cannot start secondary namenodes.
{code}

{code:xml}
./stop-dfs.sh: line 26: /home/dev/HadoopRelease/hadoop-common-0.24.0-SNAPSHOT/libexec/../bin/hdfs: No such file or directory
Stopping namenodes on []
localhost: stopping namenode
localhost: stopping datanode
Secondary namenodes are not configured.  Cannot stop secondary namenodes.
{code}
, Provided a trivial patch to fix this issue. Please review it., Are you overlaying the Common and HDFS directories? The current scripts assume there is a common HADOOP_PREFIX, which you can achieve by using the commands in https://issues.apache.org/jira/browse/HDFS-2323?focusedCommentId=13100782&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13100782.
, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12494458/HDFS-2324.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.hdfs.TestDfsOverAvroRpc
                  org.apache.hadoop.hdfs.server.blockmanagement.TestHost2NodesMap
                  org.apache.hadoop.hdfs.server.datanode.TestReplicasMap

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/1247//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/1247//artifact/trunk/hadoop-hdfs-project/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/1247//console

This message is automatically generated., Thanks Tom for the info.

I am not overlaying the Common and HDFS directories. I am not getting any problem other than "/bin/hdfs: No such file or directory" for secondary name nodes. 

I feel it is better to handle this error also. Whoever doesn't want to overlay the Common and HDFS directories they can use without facing any problems.   

Please provide your opinion on this., Supporting the non-overlaid tarballs case is a bigger change, since it would involve dealing with every occurrence of HADOOP_PREFIX in the scripts. A better short term goal might be to produce a combined tarball using a hadoop-project-dist assembly. What do you think?, I think it would be better option of generating combined tar ball instead of overlaying Common and HDFS directories after generating individual tar balls., It looks like Alejandro has started on that in HADOOP-7642., It will be taken care as part of HADOOP-7642., Dropping fix-version from 'non-fixed' (didn't have code-fixes) JIRAs.]