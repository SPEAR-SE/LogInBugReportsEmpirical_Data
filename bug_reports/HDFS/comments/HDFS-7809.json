[Stack trace:

{panel}
2015-02-13 01:07:45,628 \[org.apache.hadoop.hdfs.server.datanode.DataNode$2@278a83a0\] WARN datanode.DataNode: recoverBlocks FAILED:
RecoveringBlock\{BP-xxxxx:blk_12345_10000; getBlockSize()=4150; corrupt=false; offset=-1; locs=\[1.2.3.4:1004, 1.2.3.5:1004, 1.2.3.6:1004\]\}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.NSQuotaExceededException):
Failed to record modification for snapshot: The NameSpace quota (directories and files) is exceeded: quota=50000 file count=50001
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyNamespaceQuota(DirectoryWithQuotaFeature.java:138)
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:153)
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.addSpaceConsumed(DirectoryWithQuotaFeature.java:96)
        at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addSpaceConsumed(INodeDirectory.java:136)
        at org.apache.hadoop.hdfs.server.namenode.INode.addSpaceConsumed2Parent(INode.java:484)
        at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addSpaceConsumed(INodeDirectory.java:138)
        at org.apache.hadoop.hdfs.server.namenode.INode.addSpaceConsumed2Parent(INode.java:484)
        at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addSpaceConsumed(INodeDirectory.java:138)
        at org.apache.hadoop.hdfs.server.namenode.INode.addSpaceConsumed2Parent(INode.java:484)
        at org.apache.hadoop.hdfs.server.namenode.INode.addSpaceConsumed(INode.java:474)
        at org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList.addDiff(AbstractINodeDiffList.java:125)
        at org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList.checkAndAddLatestSnapshotDiff(AbstractINodeDiffList.java:284)
        at org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList.saveSelf2Snapshot(AbstractINodeDiffList.java:296)
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.recordModification(INodeFile.java:305)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.finalizeINodeFileUnderConstruction(FSNamesystem.java:4202)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4419)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4383)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:699)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)
        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28073)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2053)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2047)
{panel}, Looks like the bug was in {{INode#isInLatestSnapshot}}: in branch-2.5 we missed the {{latestSnapshotId == Snapshot.NO_SNAPSHOT_ID}} case there, and recently HDFS-7056 fixed this. Let me see if I can add a unit test for this., In the meanwhile, HDFS-6651 excludes the snapshot diff from the quota calculation. Thus some quota violations can be avoided now., [This comment|https://issues.apache.org/jira/browse/HDFS-7056?focusedCommentId=14197363&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14197363] (#7) in HDFS-7056 should be the same issue with this one., Thanks, [~jingzhao]. It would have been nicer if the bug was dealt with in a separate jira. I will dupe this to one of the jiras.]