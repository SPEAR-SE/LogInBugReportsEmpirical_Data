[Patch attached, Added a new member variable in NetworkTopology.java called as numOfLiveRacks which will get the number of live Racks (i.e atleast 1 node in a rack will be 'In Service')
In BlockPlacementPolicyDefault.getMaxNodesPerRack() method, I am using numOfLiveRacks to determine maxNodesPerRack instead of numOfRacks., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12638610/HDFS-4861.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithNodeGroup
                  org.apache.hadoop.hdfs.TestDecommission

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6582//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6582//console

This message is automatically generated., Attaching patch correcting the reason for Test Failure, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12638791/HDFS-4861-v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6592//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6592//console

This message is automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12638791/HDFS-4861-v2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10597/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12638791/HDFS-4861-v2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10629/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12638791/HDFS-4861-v2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / aea26bf |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/13132/console |


This message was automatically generated., Recently we this bug in one of our production cluster.
We decommissioned multiple racks (more than 10) at once.
All the blocks with high replication factor were failing.
Since the maxNodesPerRack is determined without considering the number of racks that are decommissioning, the BlockPlacementPolicyDefault couldn't find any good target to copy the block.

I have one approach which can fix this bug:
We can get rid of maxNodesPerRack altogether.
NetworkTopology (and NetworkTopologyWithNodeGroup) can maintain list of all racks (and NodeGroup) in the current topology.
In BlockPlacementPolicyDefault#chooseRandom we can get a list of the racks given the scope.
If we get multiple racks from the above function, we can go  each rack and select a target in a round robin fashion.
In this way we can try to place the replica evenly on all the racks for higher replication factor blocks.

This case breaks for BlockPlacementPolicyRackFaultTolerant policy.
This policy is heavily dependent on maxNodesPerRack (and overriding its implementation) unlike other placement policies.
Asking the author [~walter.k.su] to share the motivation behind this policy.
My approach will try to place the high replication factor blocks on most racks but it will put first 2 out of 3 copies on one rack.

I have a patch that is working for branch-2.7 since BlockPlacementPolicyRackFaultTolerant is not committed to branch-2.7., [~shahrs87], why don't you post your proposed patch? If we must maintain the semantics of {{BlockPlacementPolicyRackFaultTolerant}}, we could move {{maxNodesPerRack}} and other necessary bits there., I can add some of the color about this policy, we need it for erasure coding for rack fault tolerance. We want to spread out the blocks in a stripe so none of them share a rack., bq. In BlockPlacementPolicyDefault#chooseRandom we can get a list of the racks given the scope. \n If we get multiple racks from the above function, we can go each rack and select a target in a round robin fashion.
So each rack has equal probability? If we add a new rack, and the new rack only has a new node, will the rack be overloaded?

I think {{clusterMap.getNumOfLiveRacks()}} is a good change. How about we do it first. {{BlockPlacementPolicyRackFaultTolerant}} can make use of it too.]