[Thanks for the detail report, but generally, we just decommission DNs on the active node, right?
, Somehow we have to define the proper decommission operation process. We need to update SNN exclude files at some point so that if NN fails over, it can have consistent view of what machines have been decommissioned. For example, we can try something like,

1. Update excludes files on both active and standby.
2. refreshNodes on active NN. Wait for it to complete. Hope it doesn't fail over before completion.
3. Don't refreshNodes on SNN. Administrators understand the SNN webUI won't be consistent with active NN.
4. Some time later after fail over happens, Administrators will refreshNodes on the new active. The new active will try to decomm these nodes again and quickly find out blocks on these nodes have sufficient replicas somewhere.

However, failover can happen during decomm, ideally we want to have preserve the decomm state over to the new active, for example via Zookeeper.

BTW, why does standby NN need to manage under replication and over replication given DNs will ignore them anyway?, To handle the automatic failover scenario, maybe we can have old SNN invoke refreshNodes when it becomes active NN. That will allow the decomm to continue.

If we don't want to allow people to run "dfsadmin -refreshNodes" on SNN, perhaps we want to improve the tool and webUI to provide the proper status., I guess we can let people only run refreshNodes on ANN. In that case, we may have the following:
# If the decommission finishes before any NN failover, the scenarios in the description can happen, i.e., SBN may have made inconsistent decision and keep trying the decommission. However, its commands will be ignored by DNs. And when failover happens, since the original SBN will clear all the replication queue and cached DN commands, finally this NN will generate the replication queues based on the correct information.
# If NN failover happens during the the decommission (the replication for the decommission is still on-going), still, the original SBN will clear all the replication queues and re-initialize them based on incoming block reports. Then if we run the refreshNodes on this NN, the NN may achieve a correct decision.
# We may want to disable the replication monitor for the SBN so that it will not try to send replication/invalidate commands to DNs., Thanks, Jing, Fengdong. It sounds like we can go with the "only decomm ANN" approach; the correctness can be guaranteed. However, it will be useful to further simplify the operations and improve SBN webUI quality. To summarize operational steps.

Option 1 - No code change; people have to ignore SBN webUI as the data is misleading.
1. Update excludes files on both ANN and SBN.
2. Run "dfsadmin -refreshNodes" only on ANN. Wait for it to complete.
3. If decomm finishes before any failover, do nothing. SBN webUI doesn't have updated node status.
4. If there is a failover before decomm, someone or script external to HDFS has to run "dfsadmin -refreshNodes" on the new ANN so that decomm can continue.

Option 2 - Code change to simplify the process and SBN web UI.
1. When old SBN become new ANN, it calls refreshNodes in FSNamesystem.startActiveServices. With this, option 1's step 4 can be skipped.
2. SBN can throws some exception when someone tries to run "dfsadmin -refreshNodes". That will make it clear not to run the command on SBN.
3. Make SBN webUI correct. For example, it can choose not to display # of dead/live/decommissioning/decommissioned nodes. Such data could become stale overtime people update include and exclude files but only run "dfsadmin -refreshNodes" on ANN.

Separately I can open another jira to disable the replication monitor for SBN.

Any comments?, Actually we do not need to push the refreshNodes in SBN to after failover? We can run refreshNodes in ANN and SBN around the same time and just disable the replication monitor for SBN. In that case the only side effect is that SBN can have a different internal view about replication.

, Ah, good point. If we disable the replication monitor for SBN, refreshNodes on SBN will still put nodes in decommissioning state, eventually after data is replicated to other nodes, SBN webUI will show those nodes have been decommissioned., Is this issue same as HDFS-3744?, Uma, I think they are related. Although it seems the patch in HDFS-3744 tries to enable the ability to run "dfsadmin -refreshNode" once to hit both ANN and SBN. This jira tried to address the inconsistent state between ANN and SBN after you run "dfsadmin -refreshNode" on both ANN and SBN, regardless of whether you run "dfsadmin -refreshNode" once or sequentially on ANN and SBN., Here is the fix. Couple notes:

1. It seems the code has already prevent SBN from updating excess blocks for common case like addStoredBlock. The case that is missing is node recommission scenario. So the fix is at recommission layer instead of fixing it inside processOverReplicatedBlock.
2. The fix in ReplicationMonitor to not do replication work isn't necessary to address this specific issue.
3. The fix has been tested manually in a real cluster. The added test case reproduced the excess replica scenario., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12639019/HDFS-6178.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6604//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6604//console

This message is automatically generated., The patch looks pretty good to me. One question: do we also want to add the isPopulatingReplQueues() check in BlockManager#isReplicationInProgress, when it adds the block to neededReplications?, Thanks, Jing. Updated patch per suggestion., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12639289/HDFS-6178-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6623//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6623//console

This message is automatically generated., +1 for the latest patch. I will commit it shortly., I've committed this to trunk and branch-2. Thanks for the contribution, [~mingma]!, SUCCESS: Integrated in Hadoop-trunk-Commit #5547 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5547/])
HDFS-6178. Decommission on standby NN couldn't finish. Contributed by Ming Ma. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589002)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #548 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/548/])
HDFS-6178. Decommission on standby NN couldn't finish. Contributed by Ming Ma. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589002)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1740 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1740/])
HDFS-6178. Decommission on standby NN couldn't finish. Contributed by Ming Ma. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589002)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1765 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1765/])
HDFS-6178. Decommission on standby NN couldn't finish. Contributed by Ming Ma. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1589002)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
]