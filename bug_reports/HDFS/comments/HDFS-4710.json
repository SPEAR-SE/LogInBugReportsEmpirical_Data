[Hi Gopal,

Good find.

I suppose the fix here is to change {{BlockReaderLocal}} and {{BlockReaderLocalLegacy}} to honor {{dfs.client.read.shortcircuit.buffer.size}} when {{dfs.client.read.shortcircuit.skip.checksum}} is turned on., It is not really a one-size-fits-all fix, I guess.

For large reads, a direct read would be the right option to avoid having another copy.

But for small repeated reads, it does make sense to bounce buffer.size chunks through a buffer to avoid syscall overhead., If we honor {{dfs.client.read.shortcircuit.buffer.size}}, the client can decide whether to use a bounce buffer or not, and if so, how big., If we wrap the underlying fd in a BufferedInputStream, then we get the above behavior, right? A read of larger than the buffer size will skip the buffer, but small reads avoid small syscalls, I did consider that as a fix, but a regular BufferedInputStream isn't seekable (particularly backwards), which would need to be reimplemented on top of it.

This needs some of the same ByteBuffer juggling used in the verify checksum mode - to buffer data in chunks, but drop them when seeking., Check out o.a.h.fs.BufferedFSInputStream which should handle buffering with seeking. (though we also need to commit HADOOP-9307 before using it, since there's currently a bug), I do agree that {{BufferedFSInputStream}} could be used to work around the problem.  Unfortunately it's fairy bugged at the moment due to HADOOP-9307.

I think the fix here should be in {{BlockReaderLocal}}.  It's just inconsistent that we buffer when checksums are enabled, but not when they are disabled.  Especially given that we have an explicit parameter for setting the buffer size, which we are ignoring at the moment in no-checksum mode., This patch fixes it so that SCR honors {{dfs.client.read.shortcircuit.buffer.size}} even when checksums are off.

I noticed that there wasn't really that much code shared between the checksum / no-checksum paths, so I factored them out into separate classes.  This gets rid of the if (checksum)... that we had everywhere.

Some users might want the existing "no-copy" behavior where we always read directly into the provided buffer.  They can continue to get that behavior by setting {{dfs.client.read.shortcircuit.buffer.size}} to 0 and {{dfs.client.read.shortcircuit.skip.checksum}} to true., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12588884/HDFS-4710.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4550//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/4550//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4550//console

This message is automatically generated., fix findbugs warning, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12589940/HDFS-4710.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4572//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4572//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12589940/HDFS-4710.002.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5682//console

This message is automatically generated., This was resolved in HDFS-5634 by having BlockReaderLocal honor the {{dfs.client.cache.readahead}} setting.  If it is set to a non-zero value, we will buffer rather than reading directly into the user-supplied buffer-- even when checksums are off.]