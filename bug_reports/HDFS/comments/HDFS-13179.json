[Hi [~gabor.bota]

If we do that, then the replica is not propagated to disk and the test case purpose of checking whether it is moved to disk or not is checked, if we add such a code right? Let me know If I am missing something here?

testDnRestartWithSavedReplicas will not be tested. So, I am thinking can we iterate for more than 30 seconds and check the test case behavior?, [~bharatviswa] if you are speaking about increasing the timeout, it won't fix the issue. Once the {noformat} addStoredBlock: Redundant addStoredBlock request received... {noformat} happens the test will fail. 
Do you maybe know what can cause the redundant request, and how to prevent it?
 , FWIW, not completely related but TestLazyPersistReplicaRecovery cannot even start in Windows:
{code}
[ERROR] testDnRestartWithSavedReplicas(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery)  Time elapsed: 2.804 s  <<< ERROR!
1450: Insufficient system resources exist to complete the requested service.

        at org.apache.hadoop.io.nativeio.NativeIO$Windows.extendWorkingSetSize(Native Method)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1373)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:497)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2769)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2677)
        at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1643)
        at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:885)
        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:497)
        at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:456)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LazyPersistTestCase.startUpCluster(LazyPersistTestCase.java:315)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LazyPersistTestCase$ClusterWithRamDiskBuilder.build(LazyPersistTestCase.java:414)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery.testDnRestartWithSavedReplicas(TestLazyPersistReplicaRecovery.java:36)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}
Probably this should be a separate JIRA though., [~gabor.bota]

This will happen, when the trigger block report is called, the block report sent block storage is still RAM_DISK, it is not DISK, then we get the Redundant addStoredBlock request. I mean, when we process the IBR (the block reported storage is still RAM_DISK).

 

I am not completely sure why this block not moved to disk, might be the RamDiskReplicaTracker scheduled for move, but not yet completed, and it has restarted datanode. So, this might be happening.

 

If you have a complete log of that run by chance, that will help to find the actual cause. As I am not able to repeat this issue., I've uploaded the successful and the failed test logs with some additional logging included., [~gabor.bota] [~bharatviswa] [~elgoiri] [~xiaochen]. I think the failed reason is the correct replica recovered from lazypersist was replaced by incorrect replica read from cache when restart datanode. The comparative analysis  the successful and the failed test logs  as follows:
{quote}success:
 15:05:43,550 INFO BlockPoolSlice - Successfully read replica from cache file : /tmp/run_tha_testU50PcZ/target/test/data/dfs/data/data1/current/BP-1756767659-172.17.0.1-1517497537101/current/replicas
 15:05:43,568 INFO FsDatasetImpl - Recovered 1 replicas from /tmp/run_tha_testU50PcZ/target/test/data/dfs/data/data2/current/BP-1756767659-172.17.0.1-1517497537101/current/lazypersist
{quote}
{quote}failed:
 15:07:17,309 INFO FsDatasetImpl - Recovered 1 replicas from /tmp/run_tha_testvb5u64/target/test/data/dfs/data/data2/current/BP-381858266-172.17.0.1-1517497631100/current/lazypersist
 15:07:17,310 INFO FsDatasetImpl - Time to add replicas to map for block pool BP-381858266-172.17.0.1-1517497631100 on volume /tmp/run_tha_testvb5u64/target/test/data/dfs/data/data2: 2ms
 15:07:17,311 INFO BlockPoolSlice - Successfully read replica from cache file : /tmp/run_tha_testvb5u64/target/test/data/dfs/data/data1/current/BP-381858266-172.17.0.1-1517497631100/current/replicas
{quote}
From above test logs we can find that the read replica from cache file execute in the front of recover replaca from lazypersist when success, but the execute sequence is in opposite when failed.The reason lead to this issue is block pool getAllVolumesMap() async by dispatch thread per volume when datanode restart, and the thread process speed may different in every time, so cannot guarantee which vloume finish first.

At last I have a question about What is the effect of cache file? And I think the transient Storage not need to write replica to cache file when shutdown volume.
 Thinks!]