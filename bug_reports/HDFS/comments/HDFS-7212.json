[I met. Maybe duplicate of https://issues.apache.org/jira/browse/HDFS-7187 and https://issues.apache.org/jira/browse/HADOOP-10404, I'm resolving this as a duplicate of HADOOP-10404, fixed in Apache Hadoop 2.6.0., This may also be HADOOP-11333.  In either case, I would say try with a later release with both fixes and verify that that fixes it., Still a problem in 2.6.0.

I have 6 datanodes in two racks. Periodically one or two nodes is 'suspended' with anything from 500 to 3000 blocked threads like this:
DataXceiver for client  at /62.148.41.209:39602 [Receiving block BP-874555352-10.34.17.40-1403595404176:blk_1133797477_60059942]
The datanode is marked as dead and the namenode starts to replicate the bloks. After some time, the datanode suddenly comes back, and the namenode has to delete a lot of blocks again. , Sometimes the datanoe dosen't come back by itself, and I have to restart it. Then even more blocks has too many replicas..., Wondering if you were seeing the same thing as HADOOP-11333 which is fixed in 2.7.0.  Does the stacktrace in HADOOP-11333 match what you were seeing?, No. This is not the same. When I earlier experienced that the number of connections exceeded the maximum, I increased the maximum.
My issue is the same as in this bug-entry.

My datanodes runs fine with 70-80 threads, the suddenly one node with a lot of blocks just stops writing the recieved blocks , and the thread keeps hanging on the reciever. Then the threads just accumulate until I have at least 600 bloked threads. 
I get one more line like this for each thread :
2015-03-20 20:15:56,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-874555352-10.34.17.40-1403595404176:blk_1120355987_46618200 src: /62.148.41.204:38037 dest: /62.148.41.209:50010
But it never reports the block recieved as it normally does. 
Then I can go in via MissionControl, and look at the thread.graph, ans find it rising massivly. after 650 secs, the name-node states that the datanode is dead, but it actually is not. I can stop/start metrics (via mbeans), and sometimes the datanode just flushes (kills) all blocked threads, and reconnects to the namenode. many times, however, I have to restart the datanode. It uses a good half hour on the step where it adds the blocks to the pool, and when it reconnets to the namenode, they first of all cleans up the over-replicated blocks. The namenode, of course, stosp all other processing when the datanode 'arrives', so any process adding files to the cluster is put 'on hold' by the namenode.
Very often during the cleanup with one datanode, another starts the same process with just starting the recieve-thread, and piles up a few hundred of them i blocked state.

My stacktrace (on the blocked thred) is like this:
DataXceiver for client  at /62.148.41.204:38919 [Receiving block BP-874555352-10.34.17.40-1403595404176:blk_1128803518_55065733] [51396] (BLOCKED)
   org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createTemporary line: 1226 
   org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createTemporary line: 114 
   org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init> line: 179 
   org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock line: 615 
   org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock line: 137 
   org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp line: 74 
   org.apache.hadoop.hdfs.server.datanode.DataXceiver.run line: 235 
   java.lang.Thread.run line: 745 

And just now, my datanode has appx 700 of those threads. , Hi [~Frode Halvorsen],

The thread is trying to take the {{FsDatasetImpl}} lock here:
{code}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createTemporary line: 1226 
{code}

As you can see.  This is a synchronized method of {{FsDatasetImpl}}.

Can you find the thread which is currently holding the {{FsDatasetImpl}} lock in your stack trace?  This will tell you which thread is the problem.

Also +1 for the suggestion of trying 2.7-SNAPSHOT to see if the issue is fixed there., I'm using 2.6 and also noticed that sometime DN's heartbeat were delayed for very long time, say more than 100 seconds.  I get the jstack twice and looks like they are all blocked by dataset lock, and which is held by a thread that is calling createTemporary, which is blocked to wait earlier incarnation writer to exit.  

"DataXceiver for client  at XXXXX" daemon prio=10 tid=0x00007f14041e6480 nid=0x52bc in Object.wait() [0x00007f11d78f7000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Thread.join(Thread.java:1194)
        - locked <0x00000007a33b85d8> (a org.apache.hadoop.util.Daemon)
        at org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline.stopWriter(ReplicaInPipeline.java:183)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createTemporary(FsDatasetImpl.java:1231)
        - locked <0x00000007b01428c0> (a org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createTemporary(FsDatasetImpl.java:114)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:179)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:615)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:235)
        at java.lang.Thread.run(Thread.java:662)
, I'll create a JIRA for the issue I met and submit a patch., I'll create a JIRA for the issue I met and submit a patch., I'll create a JIRA for the issue I met and submit a patch., I'll create a JIRA for the issue I met and submit a patch., Hi ,

 We have this similar issue , where DN sockets getting stuck. ( Hadoop version 2.4.0)
===
# lsof /var/run/hadoop-hdfs/dn | wc -l 
6876 

jstack shows ==
Thread 5295: (state = BLOCKED) 
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise) 
- java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=186 (Compiled frame) 
- java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await() @bci=42, line=2043 (Compiled frame) 
- org.apache.hadoop.net.unix.DomainSocketWatcher.add(org.apache.hadoop.net.unix.DomainSocket, org.apache.hadoop.net.unix.DomainSocketWatcher$Handler) @bci=99, line=316 (Interpreted frame) 
- org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry.createNewMemorySegment(java.lang.String, org.apache.hadoop.net.unix.DomainSocket) @bci=169, line=283 (Interpreted frame) 
- org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitShm(java.lang.String) @bci=212, line=413 (Interpreted frame) 
- org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitShm(java.io.DataInputStream) @bci=13, line=172 (Interpreted frame) 
- org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(org.apache.hadoop.hdfs.protocol.datatransfer.Op) @bci=149, line=92 (Compiled frame) 
- org.apache.hadoop.hdfs.server.datanode.DataXceiver.run() @bci=510, line=232 (Compiled frame) 
- java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)

 All most 6k threads in this state, after a while DN gets hung. Do we have details regarding this issue. Any patch available for it. Looks like similar kind of issue., Hello [~rajeshhadoop].  Based on these symptoms, you'll probably be interested in multiple bug fixes in short-circuit read that went in after Apache Hadoop 2.4.0: HADOOP-11333, HADOOP-11604, HADOOP-11648, HADOOP-11802 and HDFS-8429.]