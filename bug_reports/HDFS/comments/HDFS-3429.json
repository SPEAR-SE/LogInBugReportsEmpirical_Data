[hi Todd, referred with this problem you raised. I have a question. Can we support additional interface for reading block content from DataNode for local process or application to avoid the verification?, We already have a flag on the stream that determines whether the checksum is to be verified. The issue is that we don't plumb it down to the DN, so the DN is reading useless data in these cases. This should not affect user APIs., For the records, if you switch-on local read shortcircuit, then the read of the checksum file is avoided., Here's a prelim patch for this. It seems to work in my testing - I tested manually by stracing the datanode process and ensuring it doesn't open the checksum file when I run "hadoop fs -cat -ignoreCrc". I also extended an existing parallel pread test to run this code path to check that we're getting the offset calculations right, etc.

Will probably want to clean this up a bit more before commit, but posting here to get QA results and in case anyone wants to try it., Thanks for having a go at this one Todd.

{code}
-    if ( bytesPerChecksum <= 0 ) {
+    if ( type != Type.NULL && bytesPerChecksum <= 0 ) {
{code}

Type.NULL + bytesPerChecksum <= 0 is the flag that means 'skip checksum'?  If so, a comment wouldn't be amiss here.

Why not just let it fall through to Type.NULL?  It'll return DataChecksum w/ ChecksumNull.

Its ok adding extra param here:

{code}
-      final long length) throws IOException;
+      final long length,
+      final boolean sendChecksum) throws IOException;
{code}

It won't break the protocol?  We can go against older versions of 2.0.x-alpha?  I suppose we're pb'ing -- I can see that later in patch -- so probably fine?

Is this change related?  (Reading more, it just looks like you just moved the check higher up in the method -- ok)

{code}
+      length = length < 0 ? replicaVisibleLength : length;
{code}

Spacing:

{code}
+        	if (metaIn == null) {
{code}

Javadoc param name does not match name you have in method sig: maxBytesToSend

Looks like this patch defaults reading the checksum and sending it to the client.  Is that new?  Sending client the checksum?  The verify flag is already in the proto just not hooked up?

I got lost trying to follow the sizings in BlockSender... if its wrong, failure should be pretty spectacular.

Patch looks good Todd.




, I ran the tests included in patch locally and all passed.  Let me see what happens when hbase wants to do the checksumming..., Hey Stack, thanks for taking a look. I'll address the review comments in the next iteration, and clean up comments etc in the code.

It is indeed wire-compatible due to checksumming. Also, the existing behavior is to send the checksums to the client and verify them there (not on the DataNode). So we're maintaining that by default, and just providing the new flag to have the DN not read and not send., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12546141/hdfs-3429.txt
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 4 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDFSUpgradeFromImage

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3228//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3228//console

This message is automatically generated., The failed test seems like it might be legit. I will look into it., Pardon my limited knowledge of hdfs.
{code}
+      boolean needToReadChecksum = verifyChecksum || sendChecksum;
{code}
The variable name might be a bit confusing where checksum reading depends on sendChecksum flag.
{code}
-      checksumSize = checksum.getChecksumSize();
-      length = length < 0 ? replicaVisibleLength : length;
{code}
Why was the length adjustment omitted in BlockSender ctor ?
{code}
+   * @param maxChunks maximum number of bytes to send. If checksums are
{code}
The above javadoc is inconsistent with the following code change:
{code}
-  private int sendPacket(ByteBuffer pkt, int maxChunks, OutputStream out,
+  private int sendPacket(ByteBuffer pkt, int maxBytesToSend, OutputStream out,
{code}
{code}
+    // Number of chunks be sent in the packet
+    int numChunks = shouldReadChecksum() ? numberOfChunks(dataLen) : 0;
{code}
If we don't need to read checksum, why would numChunks be 0 ?, hi Todd,
I am using the hadoop-0.20.2, so I want to fix the problem in hadoop-0.20.2, can you give me some advices about how to fix problem in hadoop-0.20.2?
, patch for hadoop-0.20.2, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552806/hdfs-3429-0.20.2.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3473//console

This message is automatically generated., bq. The variable name might be a bit confusing where checksum reading depends on sendChecksum flag.

Not sure what you mean. We need to read the checksum off disk in either of two cases:
- verifyChecksum: we plan to verify it on the server side
- sendChecksum: we plan to send it to the client

These flags are used independently for various purposes, eg:
- Block scanner: the BlockSender is "sending" to a null sink, with the verifyChecksum flag set. This causes it to throw an error if the checksum doesn't match. 
- Normal read: the DataNode doesn't verify the checksum - instead, it just sends it to the client who verifies it
- Checksum-less read: neither verifies nor sends -- in this case, we don't want to read it off disk.

bq. Why was the length adjustment omitted in BlockSender ctor ?

It wasn't ommitted, just moved to a different part of the function.

bq. The above javadoc is inconsistent with the following code change:
Fixed

bq. If we don't need to read checksum, why would numChunks be 0 ?

If we're not reading checksums, then we don't need to "chunk" the data at all - we can send exactly as many bytes as are requested or fit into the packet. The concept of chunks is itself only relevant in the context of checksummed data. I'll add more commentary here.

bq. I am using the hadoop-0.20.2, so I want to fix the problem in hadoop-0.20.2, can you give me some advices about how to fix problem in hadoop-0.20.2?

I don't know if it's going to be possible to fix this for 0.20.2 without breaking wire compatibility. The patch you uploaded is likely not sufficient - have you tested it? Let's get this into trunk and branch-2 before worrying about an old maintenance branch.

bq. The failed test seems like it might be legit. I will look into it.

Indeed the failed test turned out to be because the upgrade test used files with checksums of length 60, which didn't divide evenly into the configured packet size. The new patch rounds down the packet size to align to a chunk boundary, which fixed the test.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552936/hdfs-3429.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDFSShell
                  org.apache.hadoop.hdfs.TestFSInputChecker
                  org.apache.hadoop.hdfs.TestPread
                  org.apache.hadoop.hdfs.TestParallelRead
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes
                  org.apache.hadoop.hdfs.TestSmallBlock

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3480//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3480//console

This message is automatically generated., I say my understand for this problem, there are two purposes the DN need to read checksum form meta file.
1. Server need to verify checksum, example Block scanner.
2. DFSClient need to verify checksum, in te case, DN read checksum but don't verify checnk,  instead , DN send checksum to DFSClient, DFSClient verify checksum.

So we need to two parameters to indicate the two purposes.
1. Constructor of BlockSender class has contained one verifyChecksum parameter, that can represent Server whether verify checksum.
2. FileSystem.setVerifyChecksum(boolean verifyChecksum) method can represent DFSClient whether verify checksum, so we need to send the parameter value to DN, and add one isClientVerifyChecksum parameter in BlockSender constructor。

If verifyChecksum and isClientVerifyChecksum parameters all are false, DN don't need to read checksum, and only need to send data to client, in the case, we only need to create one DataChecksum.CHECKSUM_NULL instance, the instance can guarantee DN don't read checksum form meta file（because the checksumSize of the DataChecksum.CHECKSUM_NULL instance is 0）.


The patch I commit contain these modifies. 



 

 
, Aha, I understand now what your patch is doing - the advantage of my earlier version was that it didn't enforce any "chunking" on the data path, but it made the code more complicated.

Here's an updated version of my trunk patch which takes an approach more similar to Liu Lei's. The patch should be smaller, and seems to still work., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12553067/hdfs-3429.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3482//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3482//console

This message is automatically generated., hi, I want to fix the problem in hadoop-0.20.2,  for compatibility， the DataTransferProtocol.DATA_TRANSFER_VERSION value need to be modified.  DataTransferProtocol.DATA_TRANSFER_VERSION value  is 16 in my current hadoop version.  I want to modify the value to 17, can I do that?  If not,  what value is appropriate？, The new patch for 0.20.2, include below content
1. Fix the TestDataTransferProtocol unit test
2. Modify the DataTransferProtocol.DATA_TRANSFER_VERSION value to 18, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12554468/hdfs-3429-0.20.2.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3552//console

This message is automatically generated., Hi Liu Lei. FYI, since 0.20.x and 1.0.x are stable branches, we can't make protocol-incompatible changes like this. We either need to figure out a creative hack which would allow for wire compatibility, or else keep this feature only targeted at 2.x, where protobufs allow us to safely make these types of changes., [~tlipcon], i applied your latest patch with a nit change: DataChecksum.Type.NULL => DataChecksum.CHECKSUM_NULL
In one of our test scenario(100% random read[get], less than 1% cache hit), we got about 2% qps improvement., Hi Liang. That seems like a lower improvement than expected. What is the ratio of working set vs RAM size? Are you using a version of HBase which has the HFile-checksums built in?, [~tlipcon], i am kicking another run with lower io request,  the previous senario was FULL io-bound both w/ & w/a the patch, maybe confuse the comparison, we use hbase 0.94.2 with hbase.regionserver.checksum.verify enabled, still no obvious difference be found at another 100%read scenario withou IO-bound

i did "strace -p <DN pid> -f -tt -T -e trace=file -o bbb" during a several minutes run（without patch）,then:
grep "current/finalized" bbb|wc -l
16905
grep meta bbb|wc -l
9858
grep meta bbb|grep open|wc -l
3286
grep meta bbb|grep stat|wc -l
6572
grep meta bbb|grep "\".*\"" -o|sort -n |uniq -c|wc -l
303
And most of those meta files size are several hundred of kilobytes, further more, our OS has a default read_ahead_kb: 128
so the benefit was not obvious seems make sense as well. Any idea, [~tlipcon] ?

But i am +1 for this patch, due to it can reduce some unnecessary IO & system call, Hi Liang. I'm not sure if 0.94.2 has the code right to take advantage of this new feature quite yet -- given you see a bunch of the .meta files being read, it seems like it doesn't. So, that would explain why you don't see a performance difference., O, [~tlipcon], you missed my words: "without patch"

the strace showed the statistic without patch.  
After applied the patch, i could not see so much meta files be opened, and the hbase-secific issue is :  HBASE-5074 , fixed at 0.94.0, Hi Liang. In order to see a better improvement from this patch, you'd need a dataset per node which is on the order of 100x bigger than the available buffer cache -- ie so that the checksums themselves do not fit in cache. Talking with folks at Facebook, where they have a similar improvement in place, they saw a ~30-40% improvement in random read performance due to a similar reduction in IOPS. I believe they have TBs of data per node in this cluster., +1, the latest patch looks pretty good to me. My only advice would be to make TestPread#testPreadDFSNoChecksum also test the non-transferTo path for completeness, but I doubt seriously doing so will expose any bugs., [~tlipcon], en, sounds good for your point, i'll rerun under the suggest workload this week if get a chance:), i'm sorry, i didn't have a test env to retry this week..., I re-verified it with:  1)5T+ test-data 2)5 dn/rs 3)random get
each run the whole hdfs/hbase cluster was restarted and the os cached was cleared.
The applied latency value is nearly half of the original un-applied value,   thanks again, [~tlipcon], Great, thanks Liang for the help with testing! I think this needs to be rebased a little bit before it's committed, but I'll work on it., Turns out this didn't need rebasing. This is the same patch, but also included the single-line test improvement that Aaron suggested above., Committed to trunk and branch-2. (didn't wait for Jenkins since this patch is equivalent to the previous patch, just one test line added, and I re-ran that test manually)., Integrated in Hadoop-trunk-Commit #3226 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3226/])
    HDFS-3429. DataNode reads checksums even if client does not need them. Contributed by Todd Lipcon. (Revision 1433117)

     Result = SUCCESS
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1433117
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/datatransfer.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelRead.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPread.java
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12564769/hdfs-3429.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 2 release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3835//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/3835//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3835//console

This message is automatically generated., The release audit warnings above are a side effect of HADOOP-9097 (unrelated to this patch). Commented there., Integrated in Hadoop-Yarn-trunk #97 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/97/])
    HDFS-3429. DataNode reads checksums even if client does not need them. Contributed by Todd Lipcon. (Revision 1433117)

     Result = SUCCESS
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1433117
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/datatransfer.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelRead.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPread.java
, Integrated in Hadoop-Hdfs-trunk #1286 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1286/])
    HDFS-3429. DataNode reads checksums even if client does not need them. Contributed by Todd Lipcon. (Revision 1433117)

     Result = FAILURE
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1433117
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/datatransfer.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelRead.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPread.java
, Integrated in Hadoop-Mapreduce-trunk #1314 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1314/])
    HDFS-3429. DataNode reads checksums even if client does not need them. Contributed by Todd Lipcon. (Revision 1433117)

     Result = FAILURE
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1433117
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/DataTransferProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Receiver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/Sender.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/datatransfer.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelRead.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestParallelReadUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPread.java
]