[This looks like a duplicate of HDFS-2264 to me. Please see the prior discussion there. Ahad, if you agree, let's close this JIRA as a duplicate., bq. This looks like a duplicate of HDFS-2264 to me. Please see the prior discussion there. Ahad, if you agree, let's close this JIRA as a duplicate.
+1, Hi Aaron,

You are right.This is a dupe of HDFS-2264. Do you want to mark it as such
or would you like me to do it ?

Thanks,

Ahad


, I've just resolved this as a duplicate., Hi Aaron,

Upon further investigation, I think my bug and HDFS-2264 are reaching
different conclusions as to why clientPrototocl should be represented by a
different config key. I wonder if there are two different bugs surfacing
here ?

Ahad.


, Ahad,
  I can understand the problem in HDFS-2264, but the NameNode *should* use the same principal for both client and server. If there is a context where the _HOST isn't being expanded, that is a problem. Is that what you are hitting?, Hi Owen,

We discovered this problem because in our network the nodes set their
hostname to be a short name, and DNS returns the fully qualified host name
(i.e n01 and n01.region.prod.somecompany.com). The inconsistency arises
because in some cases the NN code uses the Java getHostName call to
retrieve the NN's hostname (returns n01) to form a principal name while
clients connecting to the NN use the Java getCanonicalName call (returns
n01.region....) to form principal names. We tried to address this issue by
explicitly setting the NN's principal via *dfs.namenode.kerberos.principal.
*

Unfortunately, the key *dfs.namenode.kerberos.principal *has different
meanings depending on the context in which it is used. In one case, the
Namenode uses it to establish the principal its server principal name. In
the other case, the same Namenode uses the same key to figure the principal
name to use with an incoming (client) connection. I believe the Hadoop
Security docs I have seen recommend that you create a unique (host
qualified) principal per machine (at least the CDH docs recommend this).
So, in this scenario you have different principal names for the NN and the
2NN (as an example). If someone uses the *dfs.namenode.kerberos.principal *key
to set an explicit principal name for the NN, authentication with the 2NN
breaks because the code in ServiceAuthorizationManager is unable to
construct a proper principal name for the 2NN from the explicit name set
for the NN.

Perhaps the short term fix is to better document how to use the *
dfs.namenode.kerberos.principal* config key. If you set its value to be an
explicit principal name, then you have to use the exact same principal name
across all nodes that try to authenticate with the secured NN protocol. If
you are using host qualified principal names for each node in the cluster,
then you must specify a pattern based principal name in
*dfs.namenode.kerberos.principal
*that can be used by the NN to both establish its own principal name and an
incoming client's principal name.

We worked around the issue by changing our NN / 2NN hostnames to match the
fully qualified names returned by DNS. Longer term, I would recommend that
we (a) fix the code in the NN to consistently use getCanonicalName whenever
it tries to use a hostname for the purposes of forming a principal name and
(b) perhaps split *dfs.namenode.kerberos.principal *into *
dfs.namenode.kerberos.principal* and *
dfs.namenode.kerberos.client.principal. *

I apologize for the lengthy answer :-)

Ahad.


, Hi Owen,

Upon further thought, perhaps it is best just to fix the canonical name
issue (a) and leave the DFS_NAMENODE_USER_NAME_KEY as it is (a single key).
It seems that the NN and everybody else  (clients) should be able to use
the same consistent principal naming scheme to login as the hdfs user.
Perhaps this was the intention all along. This does beg the question as to
why there is a need for a DFS_SECONDARY_NAMENODE_USER_NAME_KEY, as the 2NN
is basically a client of the NN ? Also, what is your recommended policy
with regards to hdfs principal names ? Should they be host qualified or not
? The host qualified scheme makes a lot of sense when you are distributing
keytabs to each host in the network, but it seems a bit inconvenient that
you cannot mix the two forms, especially in the case of an admin (for
example) that would like to use password auth to get an HDFS TGT for the
purposes of using a tool like DFSAdmin.

Thanks,

Ahad.



]