[Hi Juan, are you seeing any specific instance where the 64MB limit is a problem?, Got lots of messages like "Requested data length 72293417 is longer than maximum configured RPC length 67108864" after upgrading. 
fsck shows there are thousands of under-replicated blocks
After increased the RPC length, the remaining messages cleared out. 

Though the default block size is 64M, 128M seems a more common setting. wouldn't 128M make more sense?, Did you figure out which specific RPC call? Was it a block report? Also what version of Hadoop are you running?

We used to see this error message when the block count per DataNode would exceed roughly 6 Million. We fixed it in v2.4 by splitting block reports per storage. A large protocol message take seconds to process and can 'freeze' the callee if there is a lock held while processing it.

As a last resort this limit can be increased on a cluster-specific basis. I don't think it is a good idea to just change the default., [~arpitagarwal]
You're right, this is in block report, this is v2.0. the DN has large number of replicas.
Could you point out the JIRA that fixes this issue? thanks., HDFS-5153, Thx  a lot. 
One more question, The split is per storage directory / disk volume. 
Isn't there chance that a storage dir could still contain more than 10 million blocks in the future?, Possible but unlikely in the near future at least. Even if you assume a conservative 32MB average block size you would need 300TB disks.]