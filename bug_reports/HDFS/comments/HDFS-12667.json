[Thanks [~shahrs87] for creating the jira with good descriptions.

Let me first explain some background of HDFS-11210 for context - perhaps you already know most.

The problem HDFS-11210 tries to solve is the semantic guarantee a key rollover provides.

Before HDFS-11210, if someone:
# roll a EZ key (from v1 to v2)
# generate EDEK of that EZ key
It is possible to see the generated EDEK is encrypted with the v1 EZ key. If lucky enough, they could generate, see a v2 EZ key (returned by the synchronized call), then later generate another, see a v1 EZ key (from the async thread).

Since key rolling is for security purpose, it would be good to ensure EDEKs are generated with the new version EZ key. Also, re-encryption only makes sense if the versioning guarantee exists for key rolling. For this reason, we need to keep this semantic.

On the implementation, perhaps the locking could be more sophisticated, and do things smarter in the during the async fetching phase. Better approaches welcome., The requirement to ensure new edeks is fine.  The problem is the implementation.  It may severely penalize the common case performance for a rare event.  We are investigating alternate designs because the locking has to go.

First, the new locks protecting access to key's blocking queue negates concurrency.  When the queue is being refilled, all edek requests are blocked until the refill is done – even if there are still edeks available.  Furthermore the striped locking causes edek requests for other requests to unnecessarily block too.  This is an unacceptable penalty to the common case.

Second, the base requirement motivating the locks was to ensure after a reencrypt starts that no new creates will use old edeks.  However it appears to not be correct.  A create op may release the fsn lock, fetch an old edek, a reencrypt is issued which drains the queue and now expects new edeks, the in-progress create reacquires the lock and uses the old edek.  The race condition is tight, probably only an issue if the waiting creates should have been in the first batch, but it's wrong.

We are trying to integrate a internal patch for an async policy to prevent blocking a handler – doesn't matter that the fsn lock is released, blocking a handler is unacceptable.  The new locking model is incompatible.  Namely it forces the poll to be protected by the lock so the edek fetch cannot short out.
, Thanks [~daryn] for elaborating. I agree the implementation can cause the problem. Updated the link to HDFS-11210 as a broken by.

Sorry for breaking this, I'll investigate about the locking as well.

Just want to extend the discussion on the second point: It's theoretically true that a create may release the lock, spend significant amount of time during generate, long enough that it only comes back after the re-encryption is issued and has gone past this file...
In that case it sounds like we have to check the returned edeks with re-encryption (if any), after the create op reacquire that lock, right? That's a pretty head scratching scenario - generate again may solve it, but keyversion, being a String, can only be compared in an equalsTo fashion (rather than greaterThan / lessThan), so if someone roll the key on the KMS again during re-encryption (say v1->v2, and the re-encryption was submitted for the v0->v1 roll), every create after that point will have to generate twice - because re-encrypt isn't aware of the roll and still compares with v1, while the new creates are on v2, which for equalsTo comparison is indifferent than v0 v.s. v1.

, bq. Sorry for breaking this, I'll investigate about the locking as well.
hi [~xiaochen], thanks for commenting.
We (I and Daryn) have an implementation idea which will remove the stripped locking and will lock individual queue.
Give me couple of days to post 1st draft.
Just to clarify I am just working on 1st point and not the second., I was originally going to comment on another key roll occurring during the re-encrypt but deleted it because I already wrote too much. :).  The inability to numerically compare is indeed unfortunate because I too thought we could take advantage of a version check., bq. will lock individual queue.
Sounds good to me. Thanks for the heads up guys. :), attaching a preliminary patch.
By no means it is a committable patch., re-attaching the same patch for jenkins to pickup., Thanks for the patch [~shahrs87], this looks to be a good improvement on {{ValueQueue}}. Being able to cancel the future is pretty helpful...

Some preliminary comments:
{code}
  public void drain(String keyName) {
    try {
      keyQueues.get(keyName).drain();
    } catch (ExecutionException ex) {
      // TODO: dont know what is the right thing to do.
      // NOP
    }
  }
{code}
IMO we should throw as an IOE to tell the caller "failed to drain". KPCE is {{Private}} so theoretically okay. (Separate, this should be limited private - not hard to imagine people implementing their own KPCE extension...)

{code}
      // There can be a race in reading drainAgain value but there is no harm
      // in draining the queue once more to be double sure or I can make it
      // synchronized.
      if (drainAgain) {
        queue.clear();
        drainAgain = false;
      }
{code}
A little confused by this comment, {{drainAgain]} is protected by the {{PolicyBasedQueue]} object lock right? Could you elaborate?

{code}
    public void clear() {
      queue.clear();
    }
{code}
Is this supposed to be called outside of the class?, Thanks [~xiaochen] for the review.
bq. IMO we should throw as an IOE to tell the caller "failed to drain". 
Thanks for the suggestion. I will think on this line and will update in next patch.
bq. A little confused by this comment, {{drainAgain]} is protected by the {{PolicyBasedQueue]} object lock right? Could you elaborate?
You are correct. In the first draft, {{PolicyBasedQueue #getAtMost}} method was not synchronized. Thats why I added that comment.
But then later on, to avoid some race condition, I decided to add the synchronized method but forgot to remove that comment.
bq. Is this supposed to be called outside of the class?
Same explanation as above. In first draft, I was calling this method  when {{ValueQueue#drain}} was called. But then I added {{PolicyBasedQueue#drain}} method but forgot to remove {{PolicyBasedQueue#clear}} method., submitting again..hopefully jenkins will run on the last patch., bq. Some preliminary comments:
[~xiaochen]: do you have any further comments  before I address your above comments in next revision.
[~daryn]: It would help to get your review also., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 17m 12s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  3s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 33s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 12m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 12m 10s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 36s{color} | {color:orange} hadoop-common-project/hadoop-common: The patch generated 14 new + 21 unchanged - 3 fixed = 35 total (was 24) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 57s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m  1s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 30s{color} | {color:red} The patch generated 3 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 98m 41s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-12667 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12894215/HDFS-12667-001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 57c40806b509 3.13.0-123-generic #172-Ubuntu SMP Mon Jun 26 18:04:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 9711b78 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/21877/artifact/out/diff-checkstyle-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21877/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/21877/artifact/out/patch-asflicense-problems.txt |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21877/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, I haven't found time to look at what this means to hdfs startFiles yet, but please feel free to post the next patch., bq. IMO we should throw as an IOE to tell the caller "failed to drain".
It does no good throwing an IOE also. Since all the callers {{KMSClientProvider#drain}} and {{EagerKeyGeneratorKeyProviderCryptoExtension.CryptoExtension#drain}} can't throw anything back.
{{ValueQueue#drain}} will throw {{ExecutionException}} only if its not able to load values from {{CacheLoader}} so IMO its safe to ignore the exception.
On the other hand, I am thinking to remover CacheLoader in the first place.

bq. A little confused by this comment, {{drainAgain]} is protected by the {{PolicyBasedQueue]} object lock right? Could you elaborate?
Fixed in #2 patch.

bq. Is this supposed to be called outside of the class?
Removed in #2 patch., | (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 10s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 38s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 53s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 11m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 11m 39s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 36s{color} | {color:orange} hadoop-common-project/hadoop-common: The patch generated 1 new + 20 unchanged - 4 fixed = 21 total (was 24) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m  0s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 34s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 81m 10s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-12667 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12895068/HDFS-12667-002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 7bb82d218c77 3.13.0-119-generic #166-Ubuntu SMP Wed May 3 12:18:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ed24da3 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_131 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/21905/artifact/out/diff-checkstyle-hadoop-common-project_hadoop-common.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21905/testReport/ |
| modules | C: hadoop-common-project/hadoop-common U: hadoop-common-project/hadoop-common |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21905/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, [~xiaochen], [~daryn]: can you please review., Thanks for the new patch [~shahrs87], looks really good! Happy to see this fix also brought in simplification:
{{2 files changed, 170 insertions(+), 295 deletions(-)}}  :)

bq.  I am thinking to remover CacheLoader in the first place.
Not sure I understand the proposal correctly, could you elaborate? We should keep the expireAfterAccess logic for compatibility reasons.

Some comments on the patch, mostly nits:
- {{PolicyBasedQueue}} and some of its methods could be private. This includes the class definition, {{size()}} and {{drain()}}.
- Please keep the old thread naming convention, and initialize this with a {{ThreadFactory}}. (aka.{{.setNameFormat(REFILL_THREAD)}}).  {code}executor = Executors.newFixedThreadPool(numFillerThreads);{code}
- {{copiedElements}} can be declared inline when {{queue.drainTo}} happens
- ok with the drain logic. Suggest to add some comments in the catch {{ExecutionException}} block.
- about {{drainAgain}}:
{code}
// It is possible that future reloaded the queue even if we cancel
// the fututre.
{code}
What is the exact scenario that this could happen? And why drain again at the next {{getAtMost}} solves this?

Test comments:
- I was initially surprised to see we had to change a lot of existing tests. But upon further looking, it seems it's mostly a side-effect due to the removal of sync-then-async. Given we only {{queue.take()}} up to {{numToFill}}, I think it should be okay. Only tradeoff is now it has to go through the process of submitting a future and let the executor to actually run it, instead of the old synchronized way - but I think that's okay too.
- There are some {{println}} saying size = 9. Maybe we can just assert it?
- Existing issue, {{testgetAtMostPolicyATLEAST_ONE}} javadoc comment should say {{Policy = ATLEAST_ONE}} instead of ALL.
- {{testgetAtMostPolicyATLEAST_ONE}}, after we {{getAtMost("k1", 10}}, we should then also verify there is another async filler running.
- {{testgetAtMostPolicyALL}}'s comment (Synchronous call: 1. 2.) is obsolete., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HDFS-12667 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDFS-12667 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12895068/HDFS-12667-002.patch |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/23597/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

]