[Looks like the issue happened during the standby->active transitioning process of the namenode. The namenode had not finished tailing the editlog, which contained information about generating the corresponding token, thus failed to find the token in its local cache. 

A possible solution is to let the client side retry the same operation in this scenario. Thus if the namenode could not find the token during the transition, it can wrap the InvalidToken exception in the RetriableException and ask the client to retry., Initial patch just for review., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607285/HDFS-5322.000.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5135//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5135//console

This message is automatically generated., Update the patch., # You can make retrievePassword method shorter by {{return checkToken().getPassword();}}. Please add assert for ensuring the method is synchronized and document it in the method javadoc.
# Can you refactor HDFS DelegationTokenSecretManager#checkToken to smaller methods? Also can you check for isTransitionToActive() only if the token in the cache is null?
# " is expired" -> " has expired"
# startingActiveService - does it need to be volatile. We can just use it with in lock, right?
# When reviewing the code, I found this hack. Given this, do you see any issue in the way we are changing the method signature? I do not and cannot understand why this hack was added. If you think this hack is not necessary, lets remove it in another jira.
{code}
      // FIXME: this is a hack to get around changing method signatures by
      // tunneling a non-InvalidToken exception as the cause which the
      // RPC server will unwrap before returning to the client
      InvalidToken wrappedStandby = new InvalidToken("StandbyException");
      wrappedStandby.initCause(se);
      throw wrappedStandby;
{code}, Thanks for the review, Suresh! Update the patch to address your comments. 

bq. startingActiveService - does it need to be volatile. We can just use it with in lock, right?

We need to access startingActiveService in inTransitionToActive without acquiring the lock. So volatile may be necessary here.

bq. Given this, do you see any issue in the way we are changing the method signature?

I checked the code and have not found any specific dependency of the InvalidToken yet. [~daryn], could you verify that we do not need the hack here?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607394/HDFS-5322.000.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5139//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5139//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607411/HDFS-5322.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5142//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5142//console

This message is automatically generated., +1 for the patch. [~daryn], please comment on why we folded StandbyException into InvalidToken instead of just throwing StandbyException?, [~jingzhao], this breaks the Hive build. We may have to continue the current hack.

{quote}
    [javac] ...\org\apache\hadoop\hive\thrift\HadoopThriftAuthBridge20S.java:467: unreported exception org.apache.hadoop.ipc.RetriableException; must be caught or declared to be thrown
    [javac]         return encodePassword(secretManager.retrievePassword(tokenid));
{quote}, Haven't looked at the patch yet, but StandbyException had to be wrapped into InvalidToken due to checked exceptions.  Otherwise, a bunch of method signatures needed to be changed and the goal was a quick/small fix., I think this change will also break yarn which is why I was hesitant to change the signatures.

Do you know why checkOperation isn't throwing the expected StandbyException?, After some discussion with [~sureshms], we think we can just add a new retrievePassword API into SecrectManager, and do not change the old API. Upload a new patch to demonstrate this solution. I will add some unit tests for this shortly., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607638/HDFS-5322.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5152//console

This message is automatically generated., Add a unit test., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607651/HDFS-5322.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

      {color:red}-1 javac{color}.  The applied patch generated 1544 javac compiler warnings (more than the trunk's current 1524 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.security.token.delegation.TestDelegationToken
                  org.apache.hadoop.security.TestDoAsEffectiveUser
                  org.apache.hadoop.hdfs.security.TestClientProtocolWithDelegationToken
                  org.apache.hadoop.hdfs.security.token.block.TestBlockToken

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.ipc.TestSaslRPC

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5153//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5153//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html
Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5153//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5153//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607689/HDFS-5322.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1544 javac compiler warnings (more than the trunk's current 1524 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.security.token.delegation.TestDelegationToken
                  org.apache.hadoop.security.TestDoAsEffectiveUser
                  org.apache.hadoop.hdfs.security.token.block.TestBlockToken
                  org.apache.hadoop.hdfs.security.TestClientProtocolWithDelegationToken

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.ipc.TestSaslRPC

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5157//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5157//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html
Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5157//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5157//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607743/HDFS-5322.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5163//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5163//console

This message is automatically generated., I don't like the {{SaslRpcServer}}  becoming aware of {{StandbyException}} and {{RetriableException}} - it violates the abstract of the rpc server from the NN.  These exceptions aren't relevant to every server.

Downgrading the visibility of {{AbstractDelegationTokenSecretManager#getPassword}} to non-public also seems like a problematic API change for other secret managers.

Again, the basic question driving this change is why {{FSNamesystem#checkOperation(OperationCategory.WRITE)}} is not throwing during a transition to active?  The namespace is _not writable_ until active so that behavior is incorrect and should be fixed.  There should not be a discrepancy with how token and non-token connections (plain, kerberos) are handled.  Likewise other calls within the NN are being "lied" to about the state of the namespace., bq. Again, the basic question driving this change is why FSNamesystem#checkOperation(OperationCategory.WRITE) is not throwing during a transition to active?

During the transition (Standby -> Active), the current code first sets the state of the NN to Active, then starts the active service, during which the NN still needs to tail the remaining editlog. If a delegation token is contained in that last part of editlog, 1) FSNamesystem#checkOperation(OperationCategory.WRITE) will not throw anything since the NN's state has already been changed to Active, 2) the new ANN cannot find the token in its cache since it has not finished applying the editlog. We should allow clients to retry since after NN finishes reading the editlog the delegation token can be recognized.

In the meanwhile, if we let the NN first start active service, then change its state to standby, your original hack in HADOOP-9880 can work, since a standbyexception will be thrown. But this change will 1) extend the failover time, and 2) trigger unnecessary client failover. And I'm not sure if this will break other code., Fix the unit test and make the visibility of getPassword() still default., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607836/HDFS-5322.005.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5164//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5164//console

This message is automatically generated., +1 for the patch. Minor nit: "The same functionality with" -> "The same functionality as". It is okay even if you do not address it.

Also can you please add information related to tests you have done.
, Update the patch:

1) The StandbyException and RetriableException are still be wrapped by SaslException, thus currently we have to reuse the original hack. We can open a separate jira to fix the hack.

2) I've tested the patch in a secured cluster. By injecting some code we can reproduce the same issue described above and we can see the client's side retry with the patch., More details about the system test we run in the cluster:

1. Disable the EditlogTailer thread in the standby NN first;
2. Generate a new delegation token T1 in ANN;
3. Trigger the NN failover, but in the meanwhile, delay the startActiveService process in the new ANN so that it cannot apply the editlog corresponding to T1;
4. Start a client using T1 for listing files. Because T1 is still not in the cache of the ANN we can trigger the same issue as described in this jira. 

And in step 4 we can see the client's side retry., +1 for the change. The change seems straightforward. I will commit this once our internal QA tests are run and verify that this does not cause any regressions., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12607924/HDFS-5322.006.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5168//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5168//console

This message is automatically generated., Bunch of secure HA tests were run last night with this change and we did not see test failures because of this.

+1, +1 for the patch
, SUCCESS: Integrated in Hadoop-trunk-Commit #4588 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4588/])
HDFS-5322. HDFS delegation token not found in cache errors seen on secure HA clusters. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1531436)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java
, I've committed this to trunk, branch-2 and branch-2.2., SUCCESS: Integrated in Hadoop-Yarn-trunk #360 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/360/])
HDFS-5322. HDFS delegation token not found in cache errors seen on secure HA clusters. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1531436)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1550 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1550/])
HDFS-5322. HDFS delegation token not found in cache errors seen on secure HA clusters. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1531436)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1576 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1576/])
HDFS-5322. HDFS delegation token not found in cache errors seen on secure HA clusters. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1531436)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java
, bq. During the transition (Standby -> Active), the current code first sets the state of the NN to Active, then starts the active service, during which the NN still needs to tail the remaining editlog

This is what I was questioning.  Isn't it wrong for the NN to claim it's active/writable when it's not?  It seems like another state is needed to indicate a transition is in progress - and that state indicates the namespace isn't writable.

Otherwise kerberos and known token connections are going to block all the handler threads during the transition.  Which means ha admin commands may become blocked during the transition which may be a serious problem., bq. Isn't it wrong for the NN to claim it's active/writable when it's not? It seems like another state is needed to indicate a transition is in progress - and that state indicates the namespace isn't writable.

Agree. I think the current code wants to achieve this through the FSNamesystem R/W lock: the startActiveService method holds the write lock and blocks other methods. Then since we remove the FSNamesystem lock in retrievePassword, the original implementation does not work for delegation token part. We should file a separate jira to track this.]