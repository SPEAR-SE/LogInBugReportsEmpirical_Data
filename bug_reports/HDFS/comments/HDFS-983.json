[{code}
2010-02-17 00:03:23,763 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.IOException: Found lease for non-existent file /tmp/appeventtype=��^C��ҵ^Q�ѧq   �ﺀ����^W2m�^L�|�,�'�/K��4S��@��JzIԶO{�t����P-�I���Kҁ����/part-00012-967449531
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)
        at java.lang.Thread.run(Thread.java:619)
{code}
, I wonder if this is related to HADOOP-6522?, Hi todd, thanks for the pointer to HADOOP-6522. Can you hazard a guess why encoding issue could cause the failures to generate Namenode checkpoints?, My only guess is that there's a null character in that string, and maybe one UTF8 mechanism is used to write it, and a different one to read it? TestUTF8 shows that we have three different UTF8 mechanisms inside Hadoop or Java, Decode every byte of the above filename.

[239, 191, 189, 239, 191, 189, 94, 67, 239, 191, 189, 239, 191, 189, 210, 181, 94, 81, 239, 191, 189, 209, 167, 113, 32, 32, 32, 239, 191, 189, 239, 186, 128, 239, 191, 189, 239, 191, 189, 239, 191, 189, 239, 191, 189, 94, 87, 50, 109, 239, 191, 189, 94, 76, 239, 191, 189, 124, 239, 191, 189, 44, 239, 191, 189, 39, 239, 191, 189, 47, 75, 239, 191, 189, 239, 191, 189, 52, 83, 239, 191, 189, 239, 191, 189, 64, 239, 191, 189, 239, 191, 189, 74, 122, 73, 212, 182, 79, 123, 239, 191, 189, 116, 239, 191, 189, 239, 191, 189, 239, 191, 189, 239, 191, 189, 80, 45, 239, 191, 189, 73, 239, 191, 189, 239, 191, 189, 239, 191, 189, 75, 210, 129, 239, 191, 189, 239, 191, 189, 239, 191, 189, 239, 191, 189]

No null character found. So there may be some other reason causes this., Scott: That also assumes that the bytes we have for the files above are what was actually requested. That might not be the case. We'd really need to get the input data from before the first transformation to know that, I think. , Just discussed with Rodrigo and Dmytro. The filename may be decoded so it loses all the null characters. So this problem may still related to HADOOP-6522., @Andrew: You are right. The null character may have already been removed. Currently I think HADOOP-6522 is quite likely to be the cause for this one., Do you think we should reopen HADOOP-6522 to commit to branch-20, then? I don't know that it caused your particular bug, but certainly seems possible.

This also seems pretty reproducible. Anyone have time to try creating some files with null bytes on a test cluster?, I have not seen this problem after we imported the patch from HADOOP-6522. I am going to close this  issue.]