[Here is the full stack trace from the Accumulo master log.   

{noformat}
2015-05-14 17:14:54,301 [recovery.HadoopLogCloser] WARN : Error recovering lease on hdfs://10.1.5.6:10000/accumulo/wal/worker11+9997/3a731759-3594-4535-8086-245eed7cd4c2
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): failed to create file /accumulo/wal/worker11+9997/3a731759-3594-4535-8086-245eed7cd4c2 for DFSClient_NONMAPREDUCE_950713214_16 for client 10.1.5.158 because pendingCreates is non-null but no leases found.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3001)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLease(FSNamesystem.java:2955)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.recoverLease(NameNodeRpcServer.java:591)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.recoverLease(ClientNamenodeProtocolServerSideTranslatorPB.java:641)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

        at org.apache.hadoop.ipc.Client.call(Client.java:1468)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy15.recoverLease(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.recoverLease(ClientNamenodeProtocolTranslatorPB.java:584)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy16.recoverLease(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.recoverLease(DFSClient.java:1238)
        at org.apache.hadoop.hdfs.DistributedFileSystem$2.doCall(DistributedFileSystem.java:278)
        at org.apache.hadoop.hdfs.DistributedFileSystem$2.doCall(DistributedFileSystem.java:274)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.recoverLease(DistributedFileSystem.java:274)
        at org.apache.accumulo.server.master.recovery.HadoopLogCloser.close(HadoopLogCloser.java:55)
        at org.apache.accumulo.master.recovery.RecoveryManager$LogSortTask.run(RecoveryManager.java:96)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at org.apache.accumulo.fate.util.LoggingRunnable.run(LoggingRunnable.java:35)
        at java.lang.Thread.run(Thread.java:745)
{noformat}, I hit this on an HBase cluster a few weeks ago but was never able to track down what did it. At the time I presumed I had messed up the HDFS installation and just filed HBASE-13540 and HBASE-13602 to make it easier to work around.

I might be able to track down some old logs from HBase hitting it if it'll help., Is this related to HDFS-8344?

The message "File ... has not been closed. Lease recovery is in progress. ..." which came from {{FSNamesystem#internalReleaseLease}} seems to imply that the block has no valid replica yet.

{code}
      uc.initializeBlockRecovery(blockRecoveryId);
      leaseManager.renewLease(lease);
      // Cannot close file right now, since the last block requires recovery.
      // This may potentially cause infinite loop in lease recovery
      // if there are no valid replicas on data-nodes.
      NameNode.stateChangeLog.warn(
                "DIR* NameSystem.internalReleaseLease: " +
                "File " + src + " has not been closed." +
               " Lease recovery is in progress. " +
                "RecoveryId = " + blockRecoveryId + " for block " + lastBlock);
{code}
, I ran into this again while doing more Accumulo testing.   When I ran fsck, I noticed it complained about only 2 of 3 replicas.

{noformat}
$ hdfs fsck /accumulo/wal/worker9+9997/edd1e126-2a9d-4e4d-bfaf-b1d9297fbe25 -openforwrite -files -blocks -locations
FSCK started by ec2-user (auth:SIMPLE) from /10.1.5.85 for path /accumulo/wal/worker9+9997/edd1e126-2a9d-4e4d-bfaf-b1d9297fbe25 at Thu Jun 18 20:52:47 UTC 2015
/accumulo/wal/worker9+9997/edd1e126-2a9d-4e4d-bfaf-b1d9297fbe25 583619497 bytes, 1 block(s), OPENFORWRITE:  Under replicated BP-16428079-10.1.5.35-1434651935105:blk_1073745249_4675{blockUCState=COMMITTED, primaryNodeIndex=2, 
replicas=[ReplicaUnderConstruction[[DISK]DS-f29e50f3-055a-4970-aa19-848e8f3caba5:NORMAL:10.1.5.137:50010|RBW], ReplicaUnderConstruction[[DISK]DS-057e290c-012c-4a48-b64d-a9d540984f18:NORMAL:10.1.5.234:50010|RBW], ReplicaUnderConstruction[[DISK]DS-96cebc69-2159-4551-a2aa-8651d4d361d7:NORMAL:10.1.5.174:50010|RWR]]}. Target Replicas is 3 but found 2 replica(s).
{noformat}

In the message above it says {{Target Replicas is 3 but found 2 replica(s)}}, however in the {{replicas=...}} section of the message there are three replicas listed.   I went to the 3 datanodes listed and found the block existed on each node and had the same md5 checksum., I found in the test environment where I Am seeing this that the following is set in hdfs-site.xml. 

{code:xml}
  <property>
    <name>dfs.namenode.replication.min</name>
    <value>3</value>
  </property>
{code}, I'm pretty sure I've seen this in SolrCloud during recovery as well. Thanks for filing, Keith!, Before primary DN calls commitBlockSynchronization, it synchronized 2 RBW replicas, and make them finalized. Then primary DN calls commitBlockSynchronization, to complete the lastBlock and close the file. The question is, your {{dfs.namenode.replication.min}} is 3, the last block can't be completed. NameNode shouldn't issue blockRecovery in the first place because lastBlock can't be completed anyway.

If your {{dfs.namenode.replication.min}} is 3, you should make sure you write to 3 DNs when you setup the pipeline. You can increase the replication number, or setup the {{ReplaceDatanodeOnFailure}} policy.
The default policy is 
{noformat}
    /**  // ReplaceDatanodeOnFailure.java
     * DEFAULT condition:
     *   Let r be the replication number.
     *   Let n be the number of existing datanodes.
     *   Add a new datanode only if r >= 3 and either
     *   (1) floor(r/2) >= n; or
     *   (2) r > n and the block is hflushed/appended.
     */
{noformat}
It's likey you end up with 2 replicas using default policy. You can try {{always}} replace the failed DataNode to make sure you have 3 RBW replicas. If client accidently failed, blockRecovery can go on., Looks related to HDFS-9194 possibly., It may not be related to HDFS-9194. I am seeing this on a 2.7.1 cluster (which has HDFS-6651 already (HDFS-9194 is duplicating HDFS-6651)), for those that run into this, but need to get system working again:
1) move problem wal to temp directory
2) then cp the file back into original location

, I am pretty sure this is fixed by HDFS-11817. So I'll resolve this one.]