[{code}
2015-04-30 14:11:08,235 WARN  datanode.DataNode (DataNode.java:checkStorageLocations(2284)) - Invalid dfs.datanode.data.dir /archive1/dn : 
org.apache.hadoop.util.DiskChecker$DiskErrorException: Directory is not writable: /archive1/dn
{code}
Since datanode dir was considered as invalid, the datanode did not add the dir to its block map.  All the block under that dir won't be report to NN., Yes that's what I figured too, but I'm suggesting that just because a write lock cannot be obtained doesn't mean the blocks can't be read when they are clearly there.

Instead of causing user visible data unavailability it should still provide access to the data with any new writes going to other nodes/partitions. It would also need to be reported that the partition is in read-only state (due to some underlying ext4 filesystem issue) in the NameNode jsp / dfsadmin -report etc., To clarify, a read-only filesystem should not prevent the blocks from being included in the block report to the NameNode and reported as existing, it should merely prevent new block writes to that partition until resolved., Hi Hari, HDFS currently does not support read-only file systems.  I agree it is good support it.]