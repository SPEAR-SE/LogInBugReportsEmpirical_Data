[thanks for reporting.

Datanode will not be considered as decommissioned till the blocks are replicated  .

If you dn't have enough nodes to replicate, you can decrease the replication. 

Bytheway Jira for tracking the issues,please post your queries in [mailing list|https://hadoop.apache.org/mailing_lists.html]., Thanks for your reply.

I actually have enough nodes,so i think i don't need to decrease the replication.Is there any way for me to fix this issue?, Can you please elaborate some more?

how many nodes are present in that cluster..? and what's replication factor..? did you observer anything from the logs that why only one under-replicated block..?, There are 63 nodes in this cluster,and the replication factor is 2. 

INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block: blk_1259375690_313841209\{blockUCState=COMMITTED, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-3c2d408b-4b55-459e-bb91-2316fcdcc87f:NORMAL:10.0.1.123:50010|RBW]]}, Expected Replicas: 2, live replicas: 0, corrupt replicas: 0, decommissioned replicas: 1, excess replicas: 0, Is Open File: true, Datanodes having this block: 10.0.1.123:50010 , Current Datanode: 10.0.1.123:50010, Is current datanode decommissioning: true 

 , Looks file is open hence this block will not replicated till it closed. you need to close that file.("hdfs fsck -openforwrite / -files -blocks -locations", this is command to get openfile )

which hadoop version are you using..?

if it's hadoop-2.7 +, you can use "hdfs debug recoverLease -path <opefilepath>" , or use recoverlease api for below +2.7 versions.

Bytheway Namenode will trigger the recover lease after onehour.

Even you can delete that file,incase if you dn't require., i use the command " hdfs fsck -openforwrite / -files -blocks -locations  " ,but there are too much more open file ,how can i get the only on open file which i need:(, By the way ,according to the log , i use the blockid to find the file 

hadoop fsck / -files -blocks |grep blk_1259375690_313841209

but this is no result,what can i do next...., you can use like following.

"hdfs fsck -openforwrite / -files -blocks |grep blk_1259375690_313841209", /tmp/spark_app_logs/application_1507106888031_134440_2.snappy.inprogress 655409544 bytes, 5 block(s), OPENFORWRITE:  MISSING 1 blocks of total size 118538632 B

0. BP-133118156-10.0.0.111-1456816414840:blk_1253648091_180801059 len=134217728 Live_repl=2

1. BP-133118156-10.0.0.111-1456816414840:blk_1254800403_181706610 len=134217728 Live_repl=2

2. BP-133118156-10.0.0.111-1456816414840:blk_1256151435_182483128 len=134217728 Live_repl=2

3. BP-133118156-10.0.0.111-1456816414840:blk_1257769728_184101936 len=134217728 Live_repl=2

4. BP-133118156-10.0.0.111-1456816414840:blk_1259375690_313841209\{blockUCState=COMMITTED, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-3c2d408b-4b55-459e-bb91-2316fcdcc87f:NORMAL:10.0.1.123:50010|RBW]]} len=118538632 MISSING!

 

I find it . The block is missing, :) fix it under your suggestion.

Thanks]