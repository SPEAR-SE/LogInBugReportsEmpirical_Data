[A similar fix as HADOOP-12559, but in the {{Balancer}}. Adding the renewal logic before each {{Balancer}} iteration because the dispatch runs multiple operations with NN within the iteration., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red} 0m 0s {color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 38s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 47s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 42s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 57s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 13s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 32s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 6s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 53s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 59s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 59s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 47s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 47s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 19s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs: patch generated 1 new + 38 unchanged - 0 fixed = 39 total (was 38) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 0s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 11s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 26s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 24s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 59s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 75m 10s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_66. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 77m 7s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_91. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 22s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 182m 15s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_66 Failed junit tests | hadoop.hdfs.shortcircuit.TestShortCircuitCache |
|   | hadoop.hdfs.TestEncryptionZones |
|   | hadoop.hdfs.server.blockmanagement.TestBlockManager |
|   | hadoop.hdfs.TestLeaseRecovery2 |
|   | hadoop.hdfs.security.TestDelegationTokenForProxyUser |
|   | hadoop.hdfs.server.namenode.TestFSImageWithSnapshot |
|   | hadoop.hdfs.TestDFSClientRetries |
|   | hadoop.hdfs.server.datanode.TestBlockScanner |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
| JDK v1.8.0_66 Timed out junit tests | org.apache.hadoop.hdfs.TestLargeBlock |
|   | org.apache.hadoop.hdfs.TestSmallBlock |
|   | org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
| JDK v1.7.0_91 Failed junit tests | hadoop.hdfs.server.datanode.TestDirectoryScanner |
|   | hadoop.hdfs.TestMissingBlocksAlert |
|   | hadoop.hdfs.server.datanode.TestBlockScanner |
|   | hadoop.hdfs.TestDataTransferKeepalive |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ca8df7 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12784210/HDFS-9698.00.patch |
| JIRA Issue | HDFS-9698 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 0ecee1c1844e 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 6eacdea |
| Default Java | 1.7.0_91 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_66 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_91 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/14234/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14234/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_66.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14234/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_91.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/14234/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_66.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14234/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_91.txt |
| JDK v1.7.0_91  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/14234/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Max memory used | 77MB |
| Powered by | Apache Yetus 0.2.0-SNAPSHOT   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/14234/console |


This message was automatically generated.

, Hey Zhe, overall idea seems good, though I wonder if there's a place we can do this closer to the RPC? e.g. we do this in DFSClient or openConnection, pretty low down. 

Which call is it that hits the authentication error? I'm guessing it's one of the RPC proxies in NameNodeConnector, but confirming will help us figure out where exactly to add this call. If we have opportunities to use DFS rather than a raw RPC proxy, that'll also help address this issue., Thanks Andrew for taking a look. Below is one possible stack trace caused by long running balancer. There are a few other cases where we didn't capture the stack trace. I think it's also possible for the {{Dispatcher}} to hit expired TGT before {{LeaseRenewer}} -- it calls NN connector for multiple purposes.
{code}
GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] 
2015-06-02 09:18:48,316 WARN [LeaseRenewer:hdfs@nameservice1] ipc.Client (Client.java:run(670)) - Couldn't setup connection for xxx
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] 
at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212) 
at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413) 
at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:552) 
at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:367) 
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:717) 
at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:713) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Subject.java:415) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642) 
at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712) 
at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367) 
at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463) 
at org.apache.hadoop.ipc.Client.call(Client.java:1382) 
at org.apache.hadoop.ipc.Client.call(Client.java:1364) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206) 
at com.sun.proxy.$Proxy16.renewLease(Unknown Source) 
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:563) 
at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:606) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) 
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) 
at com.sun.proxy.$Proxy17.renewLease(Unknown Source) 
at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:845) 
at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417) 
at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442) 
at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71) 
at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298) 
at java.lang.Thread.run(Thread.java:745)
{code}

I think there's a tradeoff about where to put the renew logic. The code will be cleaner and more consolidated if we put it low down (single renew logic works for multiple cases), but it will also cause the renew method ({{checkTGTAndReloginFromKeytab}}) to be called too often. It's not a terribly heavy method but still some overhead., Thanks for working on this, Zhe. Looks like we do have the relogin-from-keytab logic in client RPC ({{client#handleSaslConnectionFailure}}). Do you know why it was not triggered in the failure?, +1 for Jing's comment about how this is already done in the RPC client.  I came here to say the same thing.  I wouldn't expect to need additional re-login logic.

[~zhz], is it possible that you saw this exception in an environment that was experiencing the HADOOP-10786 bug?  That bug caused the re-login to fail silently for certain specific combinations of the JDK version and Hadoop version., Move non-critical issue out of 2.6.4 to 2.6.5., Thanks Jing and Chris for the helpful discussion. Yes I believe the current logic already handles relogin. 

The production clusters encountering the bug actually have the HADOOP-10786 fix. Unfortunately we didn't keep complete error logs except for the one snippet above. Only reasons I can think of are 1) keytab issue; 2) retry disabled in config. I'll close this JIRA and open a new one when the issue next surfaces., FWIW, I think I'd briefly update here for future reference. Thanks all for the helpful comments above!

I've seen similar problems, that the Balancer fails with {{Failed to find any Kerberos tgt}} after several hours. The problem turns out to be Kerberos usage IMHO, and not a bug in hadoop.

According to [Kerberos docs|http://web.mit.edu/kerberos/krb5-1.13/doc/admin/conf_files/krb5_conf.html], there're {{ticket_lifetime}} and {{renew_lifetime}}. The former being the lifetime of the TGT, which it can be renewed to extend to a maximum value of the later.
In the failure scenario, a TGT is generated by the user and provided to the balancer (which means in the balancer context, {{UserGroupInformation.isLoginTicketBased() == true}}). {{client#handleSaslConnectionFailure}} is behaving correctly on extending the {{ticket_lifetime}}. But there's no way to extend beyond the {{renew_lifetime}}, and I think a new TGT has to be generated which should not be hadoop's responsibility in this case., Running balancer as an interactive process, the assumption is that kinit has already run, and there is a ticket sitting in cache.  After the renew_lifetime, there isn't anything else the balancer process can do to help.  It can't automatically kinit again or otherwise prompt the user to login again.

Maybe it would be nice to give the balancer the ability to login from a keytab?  That way, the RPC client would re-login from the keytab after expiration, which means the process could remain authenticated indefinitely.  With some people wanting to run balancer non-stop in "daemon mode", that might be a reasonable feature to add., Agree. In the end we may want to run balancer as a daemon service like SecondaryNN., Thanks Chris and Jing for the quick response! FYI - I created HDFS-9804., Thank you, [~xiaochen].  Sounds like a good plan.]