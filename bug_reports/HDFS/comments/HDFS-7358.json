[[~stack], let's add a wait timeout to see if the clients still get stuck.  If no, the bug is in ByteArrayManager since there are released arrays but the allocate calls remain blocked.  If yes, it means that there is no one releasing arrays so that clients have to wait; the bug may be outside ByteArrayManager.  Could you try the patch below?

h7358_20141104_wait_timeout.patch, With the patch in place, it takes longer to hit the stall and then when I do, I get the loggings about wake up:

{code}
2014-11-04 16:55:57,202 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 9999/10000, free=0], freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,202 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=60367, aboveThreshold, [131072: 9998/10000, free=1], recycled? true
2014-11-04 16:55:57,202 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 9999/10000, free=0], freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,202 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 9998/10000, free=1], freeQueue.offer, freeQueueSize=2
2014-11-04 16:55:57,202 DEBUG [sync.3] util.ByteArrayManager: allocate(65565): count=60368, aboveThreshold, [131072: 9997/10000, free=2], recycled? true
2014-11-04 16:55:57,202 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 9998/10000, free=1], freeQueue.offer, freeQueueSize=2
2014-11-04 16:55:57,202 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 9997/10000, free=2], freeQueue.offer, freeQueueSize=3
2014-11-04 16:55:57,203 DEBUG [sync.2] util.ByteArrayManager: allocate(65565): count=60369, aboveThreshold, [131072: 9996/10000, free=3], recycled? true
2014-11-04 16:55:57,203 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 9997/10000, free=2], freeQueue.offer, freeQueueSize=3
2014-11-04 16:55:57,203 DEBUG [sync.1] util.ByteArrayManager: allocate(65565): count=60370, aboveThreshold, [131072: 9996/10000, free=3], recycled? true
2014-11-04 16:55:57,203 DEBUG [sync.4] util.ByteArrayManager: allocate(65565): count=60371, aboveThreshold, [131072: 9997/10000, free=2], recycled? true
2014-11-04 16:55:57,203 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=60372, aboveThreshold, [131072: 9998/10000, free=1], recycled? true
2014-11-04 16:55:57,203 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 9999/10000, free=0], freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,203 DEBUG [sync.2] util.ByteArrayManager: allocate(65565): count=60373, aboveThreshold, [131072: 9998/10000, free=1], recycled? true
2014-11-04 16:55:57,203 DEBUG [sync.3] util.ByteArrayManager: allocate(65565): count=60374, aboveThreshold, [131072: 9999/10000, free=0], recycled? false
2014-11-04 16:55:57,204 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,204 DEBUG [sync.4] util.ByteArrayManager: allocate(65565): count=60375, aboveThreshold, [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,204 DEBUG [sync.1] util.ByteArrayManager: allocate(65565): count=60376, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,204 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,204 DEBUG [sync.1] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,204 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,204 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=60377, aboveThreshold, [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,204 DEBUG [sync.2] util.ByteArrayManager: allocate(65565): count=60378, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,204 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,205 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,205 DEBUG [sync.3] util.ByteArrayManager: allocate(65565): count=60379, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,205 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,205 DEBUG [sync.3] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,205 DEBUG [sync.1] util.ByteArrayManager: allocate(65565): count=60380, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,205 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,205 DEBUG [sync.1] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,205 DEBUG [sync.4] util.ByteArrayManager: allocate(65565): count=60381, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,206 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,206 DEBUG [sync.4] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,206 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=60382, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,206 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,206 DEBUG [sync.0] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,206 DEBUG [sync.2] util.ByteArrayManager: allocate(65565): count=60383, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,206 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,206 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,206 DEBUG [sync.3] util.ByteArrayManager: allocate(65565): count=60384, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,206 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,207 DEBUG [sync.3] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,207 DEBUG [sync.1] util.ByteArrayManager: allocate(65565): count=60385, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,207 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,207 DEBUG [sync.1] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,207 DEBUG [sync.4] util.ByteArrayManager: allocate(65565): count=60386, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,207 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,207 DEBUG [sync.4] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,207 DEBUG [sync.1] util.ByteArrayManager: allocate(65565): count=60387, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,208 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,208 DEBUG [sync.1] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,208 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=60388, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:57,209 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488681_1099513376802] util.ByteArrayManager: recycle: array.length=131072, [131072: 10000/10000, free=0], notifyAll, freeQueue.offer, freeQueueSize=1
2014-11-04 16:55:57,209 DEBUG [sync.0] util.ByteArrayManager: wake up: [131072: 9999/10000, free=1], recycled? true
2014-11-04 16:55:57,209 DEBUG [sync.2] util.ByteArrayManager: allocate(65565): count=60389, aboveThreshold, [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:58,209 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:55:59,209 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:00,210 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:01,210 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:02,210 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:03,210 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:04,210 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:05,210 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:06,210 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:07,211 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:08,211 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:09,211 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:10,211 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:11,211 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:12,212 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
2014-11-04 16:56:13,212 DEBUG [sync.2] util.ByteArrayManager: wake up: [131072: 10000/10000, free=0]: wait ...
....
{code}, Since the clients still get stuck with wait-timeout, it seems that there is no one releasing arrays so that clients have to wait.  The "wake up: [131072: 10000/10000, free=0]: wait ..." messages just say that the allocate call wakes up, checks allocations# (but still full) and then sleep again.

[~stack], do you know why the clients are not releasing arrays?, Tried to change TestByteArrayManager but not able to reproduce the bug.

However, I did find a bug in the test -- it should not call Future.get() when holding a lock.

h7358_20141104.patch: fixes TestByteArrayManager and changes notifyAll() to notify()., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679417/h7358_20141104.patch
  against trunk revision b2cd269.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8652//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8652//console

This message is automatically generated., See how we are 'Waiting for ack for: 42' twice in the below log snippet though we wrote out a seq no. 43.  At about same time the allocate/recycle numbering goes 'off' at this time because its waiting on an ack that doesn't ever arrive so there is an outstanding allocation with a corresponding recycle that will never come (Should + one.releaseBuffer(byteArrayManager); be inside a finally block?) If I run with one thread only, I don't see this issue. It is only with two or more. My little program has 5 threads writing and calling sync.

I turned this feature off and see that we are skipping ack numbers from time to time so this is problem is not brought on by this feature but you can't use this feature till its fixed.  Looking...

{code}
...
2014-11-05 11:16:47,293 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=43, aboveThreshold, [131072: 1/10, free=1], recycled? true
2014-11-05 11:16:47,293 DEBUG [sync.0] hdfs.DFSClient: DFSClient writeChunk allocating new packet seqno=41, src=/user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083, packetSize=65532, chunksPerPacket=127, bytesCurBlock=31232
2014-11-05 11:16:47,293 DEBUG [sync.0] hdfs.DFSClient: DFSClient flush() : bytesCurBlock 32088 lastFlushOffset 31579
2014-11-05 11:16:47,293 DEBUG [sync.0] hdfs.DFSClient: Queued packet 41
2014-11-05 11:16:47,293 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 41
2014-11-05 11:16:47,293 DEBUG [DataStreamer for file /user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083 block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DataStreamer block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940 sending packet packet seqno:41 offsetInBlock:31232 lastPacketInBlock:false lastByteOffsetInBlock: 32088
2014-11-05 11:16:47,294 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DFSClient seqno: 40 status: SUCCESS status: SUCCESS status: SUCCESS downstreamAckTimeNanos: 487791
2014-11-05 11:16:47,294 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] util.ByteArrayManager: recycle: array.length=131072, [131072: 2/10, free=0], freeQueue.offer, freeQueueSize=1
2014-11-05 11:16:47,294 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DFSClient seqno: 41 status: SUCCESS status: SUCCESS status: SUCCESS downstreamAckTimeNanos: 465086
2014-11-05 11:16:47,294 DEBUG [sync.1] util.ByteArrayManager: allocate(65565): count=44, aboveThreshold, [131072: 1/10, free=1], recycled? true
2014-11-05 11:16:47,295 DEBUG [sync.1] hdfs.DFSClient: DFSClient writeChunk allocating new packet seqno=42, src=/user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083, packetSize=65532, chunksPerPacket=127, bytesCurBlock=31744
2014-11-05 11:16:47,295 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] util.ByteArrayManager: recycle: array.length=131072, [131072: 2/10, free=0], freeQueue.offer, freeQueueSize=1
2014-11-05 11:16:47,295 DEBUG [sync.1] hdfs.DFSClient: DFSClient flush() : bytesCurBlock 32853 lastFlushOffset 32088
2014-11-05 11:16:47,295 DEBUG [sync.1] hdfs.DFSClient: Queued packet 42
2014-11-05 11:16:47,295 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 42
2014-11-05 11:16:47,295 DEBUG [DataStreamer for file /user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083 block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DataStreamer block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940 sending packet packet seqno:42 offsetInBlock:31744 lastPacketInBlock:false lastByteOffsetInBlock: 32853

2014-11-05 11:16:47,295 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=45, aboveThreshold, [131072: 1/10, free=1], recycled? true
2014-11-05 11:16:47,295 DEBUG [sync.0] hdfs.DFSClient: DFSClient writeChunk allocating new packet seqno=43, src=/user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083, packetSize=65532, chunksPerPacket=127, bytesCurBlock=32768
2014-11-05 11:16:47,295 DEBUG [sync.0] hdfs.DFSClient: DFSClient flush() : bytesCurBlock 32853 lastFlushOffset 32853
2014-11-05 11:16:47,295 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 42
2014-11-05 11:16:47,296 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DFSClient seqno: 42 status: SUCCESS status: SUCCESS status: SUCCESS downstreamAckTimeNanos: 504509
2014-11-05 11:16:47,296 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] util.ByteArrayManager: recycle: array.length=131072, [131072: 2/10, free=0], freeQueue.offer, freeQueueSize=1

2014-11-05 11:16:47,296 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=46, aboveThreshold, [131072: 1/10, free=1], recycled? true
2014-11-05 11:16:47,296 DEBUG [sync.0] hdfs.DFSClient: DFSClient writeChunk allocating new packet seqno=44, src=/user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083, packetSize=65532, chunksPerPacket=127, bytesCurBlock=32768
2014-11-05 11:16:47,296 DEBUG [sync.0] hdfs.DFSClient: DFSClient flush() : bytesCurBlock 34208 lastFlushOffset 32853
2014-11-05 11:16:47,296 DEBUG [sync.0] hdfs.DFSClient: Queued packet 44
2014-11-05 11:16:47,297 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 44
2014-11-05 11:16:47,297 DEBUG [DataStreamer for file /user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083 block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DataStreamer block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940 sending packet packet seqno:44 offsetInBlock:32768 lastPacketInBlock:false lastByteOffsetInBlock: 34208

2014-11-05 11:16:47,297 DEBUG [sync.1] util.ByteArrayManager: allocate(65565): count=47, aboveThreshold, [131072: 2/10, free=0], recycled? false
2014-11-05 11:16:47,297 DEBUG [sync.1] hdfs.DFSClient: DFSClient writeChunk allocating new packet seqno=45, src=/user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083, packetSize=65532, chunksPerPacket=127, bytesCurBlock=33792
2014-11-05 11:16:47,297 DEBUG [sync.1] hdfs.DFSClient: DFSClient flush() : bytesCurBlock 34383 lastFlushOffset 34208
2014-11-05 11:16:47,297 DEBUG [sync.1] hdfs.DFSClient: Queued packet 45
2014-11-05 11:16:47,297 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 45
2014-11-05 11:16:47,297 DEBUG [DataStreamer for file /user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083 block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DataStreamer block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940 sending packet packet seqno:45 offsetInBlock:33792 lastPacketInBlock:false lastByteOffsetInBlock: 34383
2014-11-05 11:16:47,298 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DFSClient seqno: 44 status: SUCCESS status: SUCCESS status: SUCCESS downstreamAckTimeNanos: 527654
2014-11-05 11:16:47,298 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] util.ByteArrayManager: recycle: array.length=131072, [131072: 3/10, free=0], freeQueue.offer, freeQueueSize=1

2014-11-05 11:16:47,298 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=48, aboveThreshold, [131072: 2/10, free=1], recycled? true
2014-11-05 11:16:47,298 DEBUG [sync.0] hdfs.DFSClient: DFSClient writeChunk allocating new packet seqno=46, src=/user/stack/test-data/2256ed2b-6cc1-4144-88a5-227baf11842c/HLogPerformanceEvaluation/wals/hlog.1415215004083, packetSize=65532, chunksPerPacket=127, bytesCurBlock=34304
2014-11-05 11:16:47,298 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] hdfs.DFSClient: DFSClient seqno: 45 status: SUCCESS status: SUCCESS status: SUCCESS downstreamAckTimeNanos: 486422
2014-11-05 11:16:47,298 DEBUG [sync.0] hdfs.DFSClient: DFSClient flush() : bytesCurBlock 35403 lastFlushOffset 34383
2014-11-05 11:16:47,298 DEBUG [ResponseProcessor for block BP-410607956-10.20.84.26-1391491814882:blk_1075488801_1099513376940] util.ByteArrayManager: recycle: array.length=131072, [131072: 3/10, free=0], freeQueue.offer, freeQueueSize=1
2014-11-05 11:16:47,298 DEBUG [sync.0] hdfs.DFSClient: Queued packet 46
2014-11-05 11:16:47,298 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 46
...
{code}, > ... (Should + one.releaseBuffer(byteArrayManager); be inside a finally block?) ...

You make a good point that the array may not be released when the pipeline eventually fails.  We cannot call releaseBuffer(..) in a finally block since, for the usual error cases, client will reconstruct the pipeline and retry sending the same packets.  I will think about how to fix it., Looking at packet sequence numbers, it seems like this just how it works -- that a later seqnumber acks outstanding ones (I don't know enough to call it otherwise -- maybe you know [~szetszwo]?) -- and if so, we will have outstanding allocations and our counts will be off.  Thanks., > ... that a later seqnumber acks outstanding ones ...

The pipeline expects an ack for every packets.  It won't have acks with skipped seq no., Makes sense.  There is a bug in dfsoutputstream then? I can get skipping of seqno without this feature enabled., Without ByteArrayManager enabled, using tip of 2.6 logging at DEBUG level grepping 'Waiting for ack' I see us skipping packet seqnos.  See below. See doubled '8', '22', and '26'.


{code}
2014-11-05 14:08:57,240 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 1
2014-11-05 14:08:57,243 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 2
2014-11-05 14:08:57,245 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 3
2014-11-05 14:08:57,246 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 4
2014-11-05 14:08:57,246 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 5
2014-11-05 14:08:57,249 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 6
2014-11-05 14:08:57,250 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 7
2014-11-05 14:08:57,252 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 8
2014-11-05 14:08:57,252 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 8
2014-11-05 14:08:57,253 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 10
2014-11-05 14:08:57,254 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 11
2014-11-05 14:08:57,255 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 12
2014-11-05 14:08:57,255 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 13
2014-11-05 14:08:57,257 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 14
2014-11-05 14:08:57,258 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 15
2014-11-05 14:08:57,258 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 16
2014-11-05 14:08:57,259 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 17
2014-11-05 14:08:57,261 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 18
2014-11-05 14:08:57,262 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 19
2014-11-05 14:08:57,263 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 20
2014-11-05 14:08:57,264 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 21
2014-11-05 14:08:57,265 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 22
2014-11-05 14:08:57,265 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 22
2014-11-05 14:08:57,267 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 24
2014-11-05 14:08:57,267 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 25
2014-11-05 14:08:57,268 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 26
2014-11-05 14:08:57,268 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 26
2014-11-05 14:08:57,270 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 28
2014-11-05 14:08:57,270 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 29
2014-11-05 14:08:57,271 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 30
...
{code}, > ... I can get skipping of seqno without this feature enabled.

Do you mean missing seqno=43?  It is because of "flush() : bytesCurBlock 32853 lastFlushOffset 32853".  The seq 43 packet is described without being sent.  This is a leakage.  We need to release the array first., > ... grepping 'Waiting for ack' I see us skipping packet seqnos. See below. See doubled '8', '22', and '26'.

For all these case, the next seq numbers (i.e. 9, 23, 27) are missing.  I guess it the same as the seqno=43 case mentioned previously -- Client call flush().  At that time, bytesCurBlock==lastFlushOffset so that the packet is discarded without being queued/sent.  (I beg you won't see "Queued packet" for seq 9, 23 and 27.)  Client will wait for the previous seq no.  That why there are doubled seq no shown in "Waiting for ack"., Again, a bug is: when a packet is discarded, it should release the array.

Another bug: when the pipeline fails, it should release the arrays in the data/ack queues., bq. I beg you won't see "Queued packet" for seq 9, 23 and 27.

Yes.... See below listing of corresponding 'Queued packet' with missing 9, etc.

{code}
2014-11-05 14:08:54,166 DEBUG [main] hdfs.DFSClient: Queued packet 0
2014-11-05 14:08:54,796 DEBUG [main] hdfs.DFSClient: Queued packet 0
2014-11-05 14:08:54,796 DEBUG [main] hdfs.DFSClient: Queued packet 1
2014-11-05 14:08:57,240 DEBUG [sync.0] hdfs.DFSClient: Queued packet 1
2014-11-05 14:08:57,243 DEBUG [sync.1] hdfs.DFSClient: Queued packet 2
2014-11-05 14:08:57,245 DEBUG [sync.2] hdfs.DFSClient: Queued packet 3
2014-11-05 14:08:57,246 DEBUG [sync.3] hdfs.DFSClient: Queued packet 4
2014-11-05 14:08:57,246 DEBUG [sync.4] hdfs.DFSClient: Queued packet 5
2014-11-05 14:08:57,249 DEBUG [sync.0] hdfs.DFSClient: Queued packet 6
2014-11-05 14:08:57,250 DEBUG [sync.1] hdfs.DFSClient: Queued packet 7
2014-11-05 14:08:57,252 DEBUG [sync.2] hdfs.DFSClient: Queued packet 8
2014-11-05 14:08:57,253 DEBUG [sync.4] hdfs.DFSClient: Queued packet 10
2014-11-05 14:08:57,254 DEBUG [sync.0] hdfs.DFSClient: Queued packet 11
2014-11-05 14:08:57,255 DEBUG [sync.1] hdfs.DFSClient: Queued packet 12
2014-11-05 14:08:57,255 DEBUG [sync.2] hdfs.DFSClient: Queued packet 13
2014-11-05 14:08:57,256 DEBUG [sync.3] hdfs.DFSClient: Queued packet 14
2014-11-05 14:08:57,258 DEBUG [sync.4] hdfs.DFSClient: Queued packet 15
2014-11-05 14:08:57,258 DEBUG [sync.0] hdfs.DFSClient: Queued packet 16
2014-11-05 14:08:57,259 DEBUG [sync.1] hdfs.DFSClient: Queued packet 17
2014-11-05 14:08:57,261 DEBUG [sync.2] hdfs.DFSClient: Queued packet 18
2014-11-05 14:08:57,262 DEBUG [sync.3] hdfs.DFSClient: Queued packet 19
2014-11-05 14:08:57,263 DEBUG [sync.4] hdfs.DFSClient: Queued packet 20
2014-11-05 14:08:57,264 DEBUG [sync.0] hdfs.DFSClient: Queued packet 21
2014-11-05 14:08:57,265 DEBUG [sync.1] hdfs.DFSClient: Queued packet 22
2014-11-05 14:08:57,267 DEBUG [sync.3] hdfs.DFSClient: Queued packet 24
2014-11-05 14:08:57,267 DEBUG [sync.4] hdfs.DFSClient: Queued packet 25
2014-11-05 14:08:57,268 DEBUG [sync.0] hdfs.DFSClient: Queued packet 26
2014-11-05 14:08:57,269 DEBUG [sync.2] hdfs.DFSClient: Queued packet 28
2014-11-05 14:08:57,270 DEBUG [sync.3] hdfs.DFSClient: Queued packet 29
2014-11-05 14:08:57,271 DEBUG [sync.4] hdfs.DFSClient: Queued packet 30
...
{code}

, [~stack], could you try the following patch?

h7358_20141105.patch: releases array when discarding a packet for flush()., Weird. Your patch is not showing up in attachments., Sorry, I was re-uploading the file.  You should see it now., There it is.  Let me try., Better but still an odd skip: 28, 68...

{code}
2014-11-05 15:15:45,573 DEBUG [main] hdfs.DFSClient: Waiting for ack for: 0
2014-11-05 15:15:46,263 DEBUG [main] hdfs.DFSClient: Waiting for ack for: 1
2014-11-05 15:15:48,680 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 1
2014-11-05 15:15:48,684 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 2
2014-11-05 15:15:48,685 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 3
2014-11-05 15:15:48,685 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 4
2014-11-05 15:15:48,687 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 5
2014-11-05 15:15:48,688 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 6
2014-11-05 15:15:48,690 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 7
2014-11-05 15:15:48,690 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 8
2014-11-05 15:15:48,691 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 9
2014-11-05 15:15:48,693 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 10
2014-11-05 15:15:48,693 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 11
2014-11-05 15:15:48,695 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 12
2014-11-05 15:15:48,695 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 13
2014-11-05 15:15:48,697 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 14
2014-11-05 15:15:48,697 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 14
2014-11-05 15:15:48,698 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 16
2014-11-05 15:15:48,698 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 17
2014-11-05 15:15:48,702 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 18
2014-11-05 15:15:48,703 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 19
2014-11-05 15:15:48,704 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 20
2014-11-05 15:15:48,704 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 21
2014-11-05 15:15:48,705 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 22
2014-11-05 15:15:48,706 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 23
2014-11-05 15:15:48,707 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 24
2014-11-05 15:15:48,708 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 25
2014-11-05 15:15:48,708 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 26
2014-11-05 15:15:48,709 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 27
2014-11-05 15:15:48,710 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 28
2014-11-05 15:15:48,710 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 28
2014-11-05 15:15:48,711 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 30
2014-11-05 15:15:48,712 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 31
2014-11-05 15:15:48,713 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 32
2014-11-05 15:15:48,713 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 33
2014-11-05 15:15:48,714 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 34
2014-11-05 15:15:48,715 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 35
2014-11-05 15:15:48,716 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 36
2014-11-05 15:15:48,717 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 37
2014-11-05 15:15:48,717 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 38
2014-11-05 15:15:48,718 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 39
2014-11-05 15:15:48,718 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 40
2014-11-05 15:15:48,720 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 41
2014-11-05 15:15:48,721 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 42
2014-11-05 15:15:48,723 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 43
2014-11-05 15:15:48,724 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 44
2014-11-05 15:15:48,724 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 44
2014-11-05 15:15:48,726 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 46
2014-11-05 15:15:48,727 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 47
2014-11-05 15:15:48,727 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 48
2014-11-05 15:15:48,728 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 49
2014-11-05 15:15:48,730 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 50
2014-11-05 15:15:48,731 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 51
2014-11-05 15:15:48,731 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 52
2014-11-05 15:15:48,733 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 53
2014-11-05 15:15:48,734 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 54
2014-11-05 15:15:48,734 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 55
2014-11-05 15:15:48,736 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 56
2014-11-05 15:15:48,737 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 57
2014-11-05 15:15:48,737 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 58
2014-11-05 15:15:48,738 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 59
2014-11-05 15:15:48,739 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 60
2014-11-05 15:15:48,739 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 61
2014-11-05 15:15:48,740 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 62
2014-11-05 15:15:48,741 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 63
2014-11-05 15:15:48,741 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 64
2014-11-05 15:15:48,742 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 65
2014-11-05 15:15:48,742 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 66
2014-11-05 15:15:48,743 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 67
2014-11-05 15:15:48,744 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 68
2014-11-05 15:15:48,744 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 68
2014-11-05 15:15:48,744 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 70
2014-11-05 15:15:48,745 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 71
2014-11-05 15:15:48,747 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 72
2014-11-05 15:15:48,748 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 73
2014-11-05 15:15:48,749 DEBUG [sync.3] hdfs.DFSClient: Waiting for ack for: 74
2014-11-05 15:15:48,749 DEBUG [sync.4] hdfs.DFSClient: Waiting for ack for: 75
2014-11-05 15:15:48,750 DEBUG [sync.0] hdfs.DFSClient: Waiting for ack for: 76
2014-11-05 15:15:48,751 DEBUG [sync.1] hdfs.DFSClient: Waiting for ack for: 77
2014-11-05 15:15:48,751 DEBUG [sync.2] hdfs.DFSClient: Waiting for ack for: 78
...
{code}, Hi Stack, the patch won't fix skipping seq no, which is not a bug.  The patch fixes releasing arrays when skipping happens.  So could you test it with ByteArrayManager enabled to see if the client still gets stuck?, Patch works for me. My little program with 5 writing/syncing threads runs to completion.

+1 on patch.

Do we still need to handle case where exception waiting on ack; we'll not clean up an allocation?
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679673/h7358_20141105.patch
  against trunk revision bc80251.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 13 warning messages.
        See https://builds.apache.org/job/PreCommit-HDFS-Build/8667//artifact/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager
                  org.apache.hadoop.hdfs.crypto.TestHdfsCryptoStreams
                  org.apache.hadoop.hdfs.server.namenode.TestCheckpoint

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8667//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8667//console

This message is automatically generated., > Patch works for me. My little program with 5 writing/syncing threads runs to completion.

Hi [~stack], do you want to open source your test program?

> Do we still need to handle case where exception waiting on ack; we'll not clean up an allocation?

Yes, I will check if there are other leakage for the error cases., bq. Hi stack, do you want to open source your test program?

Its a small tool in HBase code base used testing the writing of HBase WAL files. I ran it like this:

{code}for i in 10 20 100; do for j in 1 2 3; do perf stat ${HOME}/hbase/bin/hbase --config $HOME/conf_hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -threads $i -iterations 100000 -key
Size 50 -valueSize 100 &> "/tmp/$1.${i}.${j}.txt"; done; done{code}

It was writing small HDFS cluster., h7358_20141106.patch: releases arrays during close., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12680047/h7358_20141106.patch
  against trunk revision ae71a67.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestHFlush

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8685//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8685//console

This message is automatically generated., A few comments:

+    private volatile boolean isClosed = false;

'isClosed' is name of the method in javabeany-speak.  Rename 'closed'

Is 'State' the right name for this inner class that carries state-of-stream-close and stuff to run on close?  CloseState?  Or CloseAndCleanup. Do you need this class?  It can't just be a method to call on close?

Otherwise looks good. I can test the patch later...

, > Is 'State' the right name for this inner class that carries state-of-stream-close and stuff to run on close? ...  Do you need this class? It can't just be a method to call on close?
 
DFSOutputStream is big and lack of organization.  It has 30+ fields in DFSOutputSteam alone, not counting inner classes such as DataStreamer. I think it is better separate to group the fields describing the state of the stream together.  Since I am not going to move the other fields for the moment, let's keep "closed" as a field.  Here is a new patch.

h7358_20141107.patch
, [~stack], have you changed dfs.bytes-per-checksum in your test?  What is version of Hadoop?
{code}
2014-11-04 16:55:57,202 DEBUG [sync.0] util.ByteArrayManager: allocate(65565): count=60367, aboveThreshold, [131072: 9998/10000, free=1], recycled? true
{code}
I wonder why it allocates a 65565 (> 64kB) array.  See also HDFS-7308., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12680257/h7358_20141107.patch
  against trunk revision 06b7979.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancer

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestParallelShortCircuitReadUnCached

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8693//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8693//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12680257/h7358_20141107.patch
  against trunk revision 06b7979.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestHFlush

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8692//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8692//console

This message is automatically generated., h7358_20141108.patch: fixes TestHFlush failure., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12680428/h7358_20141108.patch
  against trunk revision 737d928.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8699//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8699//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8699//console

This message is automatically generated., Something wrong with test-patch?  This is no findbugs warning in the given newPatchFindbugsWarningshadoop-hdfs.html., In hbase we do 16k dfs.bytes-per-checksum Thanks for filing the 64k issue. I was going to ask. Instead let me dig and try and add findings to new issue.

I tried the patch on my little rig and it works; no more getting stuck.

We need all this new synchronization on Packet? Any chance of instead tracing to figure where a Packet might be referenced (and we'd go writeData to the released internal buf) after its been closed? (Is this how you fixed TestHFlush?)

nit: rename setClosed to close if you end up making a new patch.

, > We need all this new synchronization on Packet? ...

TestHFlush calls write(..), interrupts the thread and then calls close().  Write(..) simply puts data to the packet queue and DataStreamer, a separated thread, will get the packets from the queue and then write it to socket.  Now, we set buf to null during close.  DataStreamer may get NPE when accessing the data.  So the synchronization is required.

> nit: rename setClosed to close if you end up making a new patch.

We already has close(), which is the public user API for closing the stream., bq. So the synchronization is required.

What of Packet#writeTo did the buffer release?

bq. We already has close(), which is the public user API for closing the stream.

Ok.

Here's a few numbers:

With the feature turned off:

||threads||seconds||ops/second||
|10|133.662|7481.558|
|10|133.599|7485.086|
|10|134.046|7460.125|
|20|140.972|14187.215|
|20|141.949|14089.566|
|20|140.861|14198.395|
|100|153.941|64959.953|
|100|153.751|65040.223|
|100|153.372|65200.953|

With version of patch that does NOT synchronize Packet:

||threads||seconds||ops/second||
|10|126.219|7922.737|
|10|127.792|7825.216|
|10|124.829|8010.959|
|20|138.132|14478.904|
|20|137.051|14593.108|
|20|139.604|14326.236|
|100|149.311|66974.297|
|100|149.849|66733.844|
|100|149.537|66873.078|

Here is latest patch numbers:

||threads||seconds||ops/second||
|10|127.079|7869.121|
|10|128.357|7790.771|
|10|129.122|7744.614|
|20|135.525|14757.426|
|20|139.531|14333.731|
|20|135.595|14749.807|
|100|149.802|66754.781|
|100|149.262|66996.289|
|100|149.925|66700.016|

Threads in above are client threads. Actual number of writing and syncing threads stays constant at 1 and 5.  More threads just means more writing per second.

Comparing the last run of 100 threads with the feature off vs the last run of the latest patch I see more stalls, about same instructions per cycle but less cycles so it comes out a bit better.

Perf summary on unpatched run:
{code}
 Performance counter stats for '/home/stack/hbase/bin/hbase --config /home/stack/conf_hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -threads 100 -iterations 100000 -keySize 50 -valueSize 100':

     587172.254075 task-clock                #    3.666 CPUs utilized
        18,700,961 context-switches          #    0.032 M/sec
         4,596,456 CPU-migrations            #    0.008 M/sec
           650,547 page-faults               #    0.001 M/sec
   891,035,644,874 cycles                    #    1.518 GHz                     [83.31%]
   674,789,502,548 stalled-cycles-frontend   #   75.73% frontend cycles idle    [83.32%]
   400,621,650,589 stalled-cycles-backend    #   44.96% backend  cycles idle    [66.74%]
   422,912,592,386 instructions              #    0.47  insns per cycle
                                             #    1.60  stalled cycles per insn [83.41%]
    78,498,471,337 branches                  #  133.689 M/sec                   [83.37%]
     2,768,724,048 branch-misses             #    3.53% of all branches         [83.26%]

     160.168742742 seconds time elapsed
{code}

Here is patched version perf output.
{code}
 Performance counter stats for '/home/stack/hbase/bin/hbase --config /home/stack/conf_hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -threads 100 -iterations 100000 -keySize 50 -valueSize 100':

     556038.390042 task-clock                #    3.550 CPUs utilized
        18,699,748 context-switches          #    0.034 M/sec
         4,534,830 CPU-migrations            #    0.008 M/sec
           636,724 page-faults               #    0.001 M/sec
   843,860,285,154 cycles                    #    1.518 GHz                     [83.29%]
   642,851,753,015 stalled-cycles-frontend   #   76.18% frontend cycles idle    [83.34%]
   384,260,620,446 stalled-cycles-backend    #   45.54% backend  cycles idle    [66.66%]
   392,462,867,299 instructions              #    0.47  insns per cycle
                                             #    1.64  stalled cycles per insn [83.36%]
    71,358,339,182 branches                  #  128.333 M/sec                   [83.43%]
     2,712,426,902 branch-misses             #    3.80% of all branches         [83.29%]

     156.646653202 seconds time elapsed
{code}

, > What of Packet#writeTo did the buffer release?

Packet#writeTo writes data from the buffer to the socket.

Thanks for testing the patch.  Do you think the patch is good to go?, bq. Packet#writeTo writes data from the buffer to the socket.

Yes. Could it release the buffer when it is done writing? Then could undo the synchronizations on Packet?

No biggie though. The extra synchronization barely shows in my macro numbers posted above.

bq. Do you think the patch is good to go?

It fixes the reported issue. +1.



, > ... Could it release the buffer when it is done writing? ...

I did it in some early patches but it did not work.  The packet transfer might fail.  When it fails, we need to re-send the packet.  So that we cannot release buffer until we receive the ack., bq. I did it in some early patches but it did not work. The

Ok. Thanks. +1 then.  According to the macro numbers above, the difference is little if anything between no-sync and sync on Packet (latest patch and the just previous):

{code}
 Performance counter stats for '/home/stack/hbase/bin/hbase --config /home/stack/conf_hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -threads 100 -iterations 100000 -keySize 50 -valueSize 100':

     560533.518789 task-clock                #    3.583 CPUs utilized
        18,884,751 context-switches          #    0.034 M/sec
         4,651,373 CPU-migrations            #    0.008 M/sec
           649,422 page-faults               #    0.001 M/sec
   849,474,024,940 cycles                    #    1.515 GHz                     [83.31%]
   647,838,434,834 stalled-cycles-frontend   #   76.26% frontend cycles idle    [83.27%]
   390,806,881,663 stalled-cycles-backend    #   46.01% backend  cycles idle    [66.68%]
   395,809,837,047 instructions              #    0.47  insns per cycle
                                             #    1.64  stalled cycles per insn [83.34%]
    72,112,316,041 branches                  #  128.649 M/sec                   [83.35%]
     2,658,785,438 branch-misses             #    3.69% of all branches         [83.40%]

     156.430287629 seconds time elapsed
{code}, Thanks Stack for reporting, reviewing and testing this.

I have committed this., FAILURE: Integrated in Hadoop-trunk-Commit #6538 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6538/])
HDFS-7358. Clients may get stuck waiting when using ByteArrayManager. (szetszwo: rev 394ba94c5d2801fbc5d95c7872eeeede28eed1eb)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHFlush.java
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #5 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/5/])
HDFS-7358. Clients may get stuck waiting when using ByteArrayManager. (szetszwo: rev 394ba94c5d2801fbc5d95c7872eeeede28eed1eb)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHFlush.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestByteArrayManager.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #743 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/743/])
HDFS-7358. Clients may get stuck waiting when using ByteArrayManager. (szetszwo: rev 394ba94c5d2801fbc5d95c7872eeeede28eed1eb)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHFlush.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1933 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1933/])
HDFS-7358. Clients may get stuck waiting when using ByteArrayManager. (szetszwo: rev 394ba94c5d2801fbc5d95c7872eeeede28eed1eb)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHFlush.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk-Java8 #5 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/5/])
HDFS-7358. Clients may get stuck waiting when using ByteArrayManager. (szetszwo: rev 394ba94c5d2801fbc5d95c7872eeeede28eed1eb)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHFlush.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1957 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1957/])
HDFS-7358. Clients may get stuck waiting when using ByteArrayManager. (szetszwo: rev 394ba94c5d2801fbc5d95c7872eeeede28eed1eb)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHFlush.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk-Java8 #5 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/5/])
HDFS-7358. Clients may get stuck waiting when using ByteArrayManager. (szetszwo: rev 394ba94c5d2801fbc5d95c7872eeeede28eed1eb)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/ByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHFlush.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestByteArrayManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
]