[{{hasAcl}} has RPC specific checks to determine if ACLs are not supported.  The jersey initialization of the query param fails to create the enum.  There is no good way to detect that ACLS aren't enabled w/o a hack to catch illegal arguments and mince the message string. :(

The ACLs should have been returned in the file status, or at least a flag to indicate ACLs are present., Here is a recap of how we got here.

In the HDFS-4685 design document, we considered the possibility of changing {{FileStatus}}, but we rejected it for compatibility reasons.  {{FileStatus}} is a {{Writable}}, and we didn't see a way to change it without impacting forward and backward compatibility of serialization.

Instead, we chose to reserve a previously unused bit of {{FsPermission}} as an ACL indicator bit.  This was implemented in HADOOP-10220.

Later, there was additional debate about whether or not the ACL bit was a good idea, and eventually it was backed out in HDFS-5923.  The rationale was that the ACL bit created a risk of inconsistency bugs inside the NameNode metadata.  (A code bug could cause us to set the {{AclFeature}} on the inode, but not activate the ACL bit.)  Removing the ACL bit eliminated that entire class of possible bugs.

Related to this, HDFS-5932 added the alternate way for ls to identify if the file had an ACL by trying {{getAclStatus}}, with error handling to detect the missing method on the server side for compatibility or method simply not implemented in the {{FileSystem}} subclass.  Unfortunately, we didn't anticipate that different file system implementations would have other failure behaviors, such as what is reported here for WebHDFS.

It seems we need a compromise that allows for some kind of ACL indicator and avoids brittle error response parsing.  We're seeing the issue with WebHDFS right now, but other custom file systems could have unexpected trouble too.  How about if we go ahead and add the ACL bit back in, but we don't use it as a piece of persistent data inside the NameNode?  We'll just toggle the flag on as needed in outbound {{FileStatus}} responses.  That still ought to avoid the potential consistency problems.  [~wheat9] and [~daryn], what do you think?

I'm attaching a patch that implements that approach.  This was mostly just a matter of resurrecting code that I previously wrote on the HDFS-4685 branch and then discarded.  IOW, this is mostly code that already passed review once.  The logic in {{FSDirectory}} is different this time though., I think this should a fix at the NN side first. NN should return 404 / 500 instead of a valid JSON:

{noformat}
curl http://localhost:50070/webhdfs/v1/adslfj?op=FOO
{"RemoteException":{"exception":"IllegalArgumentException","javaClassName":"java.lang.IllegalArgumentException","message":"Invalid value for webhdfs parameter \"op\": No enum constant org.apache.hadoop.hdfs.web.resources.GetOpParam.Op.FOO"}}
{noformat}

That should allow hasACL to return the correct results. However, to be backward compatible with old server, it looks to me that catching the exception is the only viable approach., If we want to change the WebHDFS error responses, let's discuss that outside the scope of this issue.  Changing the error responses will open up a set of other compatibility concerns if existing clients are dependent on the current error responses.

bq. However, to be backward compatible with old server, it looks to me that catching the exception is the only viable approach.

Yes, specifically the problem with the patch I posted is that even though it would fix the combination of a 2.3.0 client and a 2.4.1 server, it wouldn't work for the combination of a 2.4.1 client and a 2.4.0 server.  The 2.4.0 server wouldn't set the ACL bit, so the 2.4.1 client would never display the '+' in the ls output.

In hindsight, I really wish we had kept the ACL bit in some form, but it's too late now.  I'll put together a different patch for catching the exception., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12643186/HDFS-6326.1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6808//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6808//console

This message is automatically generated., Here is patch v2, changing the exception handling in ls.  I took a very defensive approach and just caught {{Exception}}.  This fixes the immediate problem and also anticipates any future problems related to custom {{FileSystem}} implementations.  Of course, it's not generally a good idea to do a blanket catch of {{Exception}}.  In this case though, the worst thing that can happen is that we skip displaying the '+', which I think is preferable over causing the ls command to fail if there are other unanticipated failures related to {{getAclStatus}}.

In addition to running the ACL-related unit tests, I also did some manual testing.  I tested ls using URLs with the webhdfs scheme against a 2.3.0 cluster, and it worked.  I also tested against a trunk cluster and confirmed that I was still getting the '+' appended., Catching all exceptions bothers me a bit that if the call fails for other transient errors, that acls are assumed to not be supported.  Maybe it doesn't seem like a big deal today because it's "only" for one command, but it will be if I ever get around to adding readline support to FsShell...

It's probably more appropriate for WebHdfsFileSystem to detect if acls are supported upon the first call.  It keeps the ugliness out of FsShell.  It would be great to also push the hdfs-specific exception handling into DistributedFileSystem.

Given that we now have extensible protobufs and json, is there any reason why FileStatus, or FSPermission, etc don't return a boolean if there's an acl on the file?  Then an acl call is may only be made when necessary.  The performance impact of doubling the rpc/webhdfs calls for ls concerns me, especially for recursive ls on a large directory, when acls are likely to be the exception rather than the common case., I needed to trigger Jenkins manually:

https://builds.apache.org/job/PreCommit-HDFS-Build/6836/
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12643408/HDFS-6326.2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6836//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6836//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12643408/HDFS-6326.2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-common-project/hadoop-common:

org.apache.hadoop.http.TestHttpServer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6837//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6837//console

This message is automatically generated., Daryn, thank you for taking a look at this.

bq. It's probably more appropriate for WebHdfsFileSystem to detect if acls are supported upon the first call. It keeps the ugliness out of FsShell. It would be great to also push the hdfs-specific exception handling into DistributedFileSystem.

I agree this would look cleaner, but then I would worry about a long-lived client process recording that ACLs aren't supported, then a NameNode gets upgraded/reconfigured to support ACLs, but the client continues to think it's unsupported.  This isn't a problem for the shell, because the process is short-lived.  Maybe allow for a periodic retry to check for ACL support again?

bq. Given that we now have extensible protobufs and json, is there any reason why FileStatus, or FSPermission, etc don't return a boolean if there's an acl on the file? Then an acl call is may only be made when necessary.

I discussed this in my first comment on this issue.  Here are some more details.

Regarding {{FileStatus}}, the same question came up on MAPREDUCE-5809 today.  Here is a copy-paste of my response:

We considered adding the ACLs to {{FileStatus}}, but this would have been a backwards-incompatible change. {{FileStatus}} implements {{Writable}} serialization, which is more brittle to version compared to something like protobuf. {{FileStatus#write}} doesn't embed any kind of version number, so there is no reliable way to tell at runtime if we are deserializing a pre-ACLs {{FileStatus}} or a post-ACLs {{FileStatus}}. This would have had a high risk of breaking downstream code or mixed versions that had used the {{Writable}} serialization. An alternative would have been to skip serializing ACLs in {{FileStatus#write}}, but then there would have been a risk of NPE for clients expecting a fully serialized object. This is discussed further in the HDFS-4685 design doc on page 12:

https://issues.apache.org/jira/secure/attachment/12627729/HDFS-ACLs-Design-3.pdf

Regarding {{FsPermission}}, at one point the HDFS-4685 feature branch was using a previously unused bit in the {{FsPermssion}} short as an ACL indicator.  This got pushback though on HDFS-5923, and the ACL bit changes were backed out completely.  Here is a relevant comment:

https://issues.apache.org/jira/browse/HDFS-5923?focusedCommentId=13898370&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13898370

If you look at my attached HDFS-6326.1.patch, it reintroduces the ACL bit as a transient field, toggled on the outbound {{FileStatus}} responses from NameNode, but not persisted to fsimage.  Making it transient addresses some of the objections in HDFS-5923, but still leaves the specific objections mentioned in the linked comment.

[~daryn] and [~wheat9], the two of you have been most involved in the debate on this, so would you please review this comment, reconsider the trade-offs, and post your current opinion?  At this point, considering all of the problems, I'm in favor of reintroducing the ACL bit (patch v1).  The one problem with this approach is that if a 2.4.1 shell talks to a 2.4.0 NameNode, then it wouldn't display the '+', because the 2.4.0 NameNode wouldn't be toggling the ACL bit.  I think it's worth accepting this relatively minor bug for one release to get us past the larger issues., Just copy past from my previous comments in HDFS-5923:

{quote}
The v0 patch takes a more aggressive approach, which removes the ACL bit completely. The rationale is the following:
Some applications might assume that FsPermission stay within the range of 0~0777. Changing FsPermission might lead to unexpected issues.
There are not many users care about whether the file has ACL except for ls. Since ls is not in the critical path, ls can make a separate getAclStatus() call to calculate the ACL bit.
{quote}

The user shouldn't care about whether the file has ACL in most cases, since the server side enforces the ACL. The only two meaningful exception that I can think of right now are (1) to perform ls, and (2) to do {{distcp -p}}.

Note that in the second case but it requires calling {{getAclStatus()}} explicitly anyway. Given the issues that Chris has pointed out, I still think that it might be worthwhile to put down some hacks in ls to make it work (since {{ls}} is not on the critical path). Any thoughts?, I'm short on time, so I only skimmed the jira that removed the bit.  I think an acl bit can be reintroduced to FsPermission in a backwards compatible way.

The current {{FsPermission#fromShort(short)}} appears to discard upper bits other than the sticky bit.  This can work to our advantage for compatibility.  So what if {{FsPermission#readFields}} called a private {{fromShort(short, boolean parseAclBit)}}?  It wouldn't persist the bit in the mask, nor serialize it to ensure complete client compatibility.

The NN can use a private {{FsAclPermission}} to simply allow setting and serializing the bit.  Now user code cannot attempt to mess with the ACL bit.  The NN wouldn't need to persist the ACL bit in the permission but rather just inject it in the response.

bq.  Note that (distcp -p) requires calling getAclStatus() explicitly anyway.

This worries me even more than ls -R!!  Distcp-ing large directories with no ACLs just became 2X expensive., bq. Note that (distcp -p) requires calling getAclStatus() explicitly anyway.
AFAIK, now distcp doesn't require calling getAclStatus(). New distcp -pa option (now patch available in MAPREDUCE-5809) will require calling getAclStatus() for preserving ACLs. Distcp -pa option will check if the filesystem supports ACL by calling getAclStatus() once and fail to copy if the filesystem does not support ACL.
bq. Distcp-ing large directories with no ACLs just became 2X expensive.
I think it doesn't become expensive if -pa option is not used., bq. The current FsPermission#fromShort(short) appears to discard upper bits other than the sticky bit.

This is dangerous as in general many clients assumes that the permission is in 000~1777. Violating this assumption can be problematic. See my comments in HDFS-5923 for details.

bq. This worries me even more than ls -R!! Distcp-ing large directories with no ACLs just became 2X expensive.

distcp won't call {{getAclStatus()}} at all expect the user specifies -p, which requires a call to {{getAclStatus()}} anyway. I don't see why distcp is more expensive than before., bq. This is dangerous as in general many clients assumes that the permission is in 000~1777. 

I don't understand this comment.  I'm proposing that the client will never see anything but that range.  The bit is transparently stripped off during deserialization., Earlier comments are correct that there is no concern about distcp.  It only preserves ACLs if you specify a new option, so you don't pay a cost if you're not using ACLs.  (This is actually irrelevant to the discussion of 2.4.1 anyway.  The distcp -pa patch is uncommitted and currently targeting 2.5.0.)

bq. It wouldn't persist the bit in the mask, nor serialize it to ensure complete client compatibility.

[~daryn], if I understand your proposal completely:
# {{FsPermission#fromShort}} would recognize the ACL bit and set the private {{aclBit}} member to true.  This way, the client can see an ACL bit.
# {{FsPermission#toShort}} would ignore the private {{aclBit}} member.  This is to keep values in the range 0000-01777.
# {{FsPermission#write}} would ignore the private {{aclBit}} member.  This follows from #2, since {{write}} is implemented in terms of {{toShort}}, and once again it's done to keep visible values in the range 0000-01777.
# {{FsAclPermission}} would be a class in HDFS (not Common), and it would be the only thing capable of serializing the ACL bit.

Am I understanding your proposal correctly?  My concern with this is that it creates a confusing API on the client side, because there isn't symmetry between serialization and deserialization.  Round-tripping an {{FsPermission}} instance would lead to data loss.

BTW, WebHDFS responses also include the octal representation of {{FsPermission}}.  If we don't want the ACL bit visible there, then we'd need to address that separately.  This probably means a separate "aclBit" field in the JSON response and a client-side special path to look for "aclBit" in the JSON and then toggle it on before calling {{FsPermission#fromShort}}.

But is it really necessary to restrict the client-visible range to 0000-1777?  Is the problem theoretical or real?  Do we know any real clients that are directly referencing the {{toShort}} value?  The only valid way to interpret that data would be bitmask operations, and if any client is looking at the proposed ACL bit today, then they are seeing undefined behavior.  (IOW, the client code is buggy.)  I'd prefer not to incur a lot of code complexity restricting the range to 0000-1777, but if there is no other choice, then I'm willing to compromise.  Let's just consider if it's really necessary., I'm attaching patch version 3.  This takes the approach of hiding the actual ACL bit from {{FsPermission}} serialization/deserialization.  This definitely hides it from clients looking directly at the permission bits (though I expressed a doubt earlier that there would be any such clients in reality).

The implementation is slightly different from what Daryn suggested in his last comment.  {{FsPermission#getAclBit}} is implemented to return false always.  {{FsAclPermission}} overrides it to return true always.  {{FSDirectory}} uses {{FsAclPermission}} on outbound responses as needed.  {{PBHelper}} translates permission bits to either {{FsPermission}} or {{FsAclPermission}} as needed.  In this way, the protobuf translation layer is the only place that would see an ACL bit turned on in the actual bytes, and we minimize the impact on the {{FsPermission}} code.  I've changed the WebHDFS JSON representation of {{FileStatus}} too to use a separate {{aclBit}} field instead of toggling it on in {{permission}}.

In my last comment, I expressed a concern that we don't really need this much complexity, but I'm willing to go with this approach if others are convinced that it's necessary., bq. o we know any real clients that are directly referencing the toShort value?

The web UI:

{code}
'helper_to_permission': function (v) {
      var symbols = [ '---', '--x', '-w-', '-wx', 'r--', 'r-x', 'rw-', 'rwx' ];
      var sticky = v > 1000;

      var res = "";
      for (var i = 0; i < 3; ++i) {
        res = symbols[(v % 10)] + res;
        v = Math.floor(v / 10);
      }
...
{code}

Note that in the webhdfs response, the permission is in 0000-1777 in decimals. That's the reason why the above code is written that way. I expect this is true for other clients that have to parse the permission bits themselves.
, I still need to look at the latest code from [~cnauroth], but [~wheat9], the assumption in the web UI code is completely wrong and prevents any future use of any upper bits in the mask.  I think avoiding the performance penalty of double rpc/http calls just to print a "+" is more important than working around a web bug., bq. ...the assumption in the web UI code is completely wrong and prevents any future use of any upper bits in the mask.

+1

I'll provide a new version of the patch that includes a fix for the web UI bug.  BTW, my v3 patch has a small bug in the bitwise operations to check the ACL bit.  I'll fix that in the next patch too., I was going to comment on the lack of bitwise and after the shift.  :)

Other comments/questions:
* Bits 10 & 11 are for set-uid/gid so we should probably use bit 12 to be safe.
* Since the acl bit is intended to be invisible to the client, shouldn't it be ignored in {{FSPermission#equals}}?
* Same for {{FsPermission#toString}}, should probably be left to FsShell or others to add the "+".
* In {{AclCommands#processPath}}, is it guaranteed (probably yes?) that the owner/group/etc will be identical in the acl vs. file status?  If no, maybe the first line should be changed to call getAclStatus if the bit is set, and most of the subsequent code left alone?
* Should we avoid changing {{PBHelper. convert(FsPermission)}} and just let {{FsAclPermission#toShort}} serialize the bit?
* Does {{PBHelper.convert(FsPermissionProto)}} need to be changed?  Assuming it's only used to decode incoming permissions in a rpc call where we don't care about the bit?
, bq. the assumption in the web UI code is completely wrong and prevents any future use of any upper bits in the mask.

The point is to demonstrate how other clients of webhdfs can potentially depend on this behavior. Though the bug is within our reach and can be fixed by ourselves, the webhdfs protocol is intended to be implemented by 3rd-party clients which are totally out of our control. I have no problem to declare this is a wrong assumption but the sad fact is that 3rd-party clients might depend on it.

bq. I think avoiding the performance penalty of double rpc/http calls just to print a "+" is more important than working around a web bug.

Having the bit obviously makes implementing ls / web ui much easier, but it comes with a performance cost on the critical path. We've implemented this approach before and reverted it to the current state, as Chris mentioned in earlier comments.

{{ls}} is not in the critical path, but adding a new field into the {{FileStatus}} response is. Provided that the {{FileStatus}} field is relatively short (~200 bytes), adding a field for ACL increases the network traffic for listing files by 5%. The overhead is indeed significant as listing files is the first step of distcp which is done within a single thread. We've seen use cases that this step can take more than 7 hours.

Again having an ACL bit makes things easier to implement, but failing to justify the performance concerns leads Chris and I to decide to keep ACL out of the critical path, and to move the complexity to ls / web UI.
 
Maybe I don't understand what you're proposing, can you propose your changes more concretely, just like what Chris did in  https://issues.apache.org/jira/browse/HDFS-6326?focusedCommentId=13991788&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13991788, Daryn, thank you for looking.

bq. I was going to comment on the lack of bitwise and after the shift.  :-)

Darn, there is no hiding my shame.  :-)

bq. Bits 10 & 11 are for set-uid/gid so we should probably use bit 12 to be safe.

Yes, we can use bit 12.

bq. Since the acl bit is intended to be invisible to the client, shouldn't it be ignored in {{FSPermission#equals}}?

OK, let's do that.  I'll re-raise my concern that this whole business of hiding the bit causes confusion in the client-side API.  Now, we'll have a situation where 2 instances can appear equal even though one has the ACL bit and the other doesn't.  I suppose it's best that we're at least consistent in ignoring it, so let's remove it from {{equals}}.

bq. Same for {{FsPermission#toString}}, should probably be left to {{FsShell}} or others to add the "+".

Yes, same as above.

bq. In {{AclCommands#processPath}}, is it guaranteed (probably yes?) that the owner/group/etc will be identical in the acl vs. file status?

Yes, this is guaranteed, barring race conditions involving a concurrent process running chown interleaved between the {{getFileInfo}} and the {{getAclStatus}} RPCs.

bq. Should we avoid changing {{PBHelper. convert(FsPermission)}} and just let {{FsAclPermission#toShort}} serialize the bit?

Yes, we can do that.

bq. Does {{PBHelper.convert(FsPermissionProto)}} need to be changed? Assuming it's only used to decode incoming permissions in a rpc call where we don't care about the bit?

This change is required so that the shell (and others) can get a {{true}} return from {{FsPermission#getAclBit}} via the {{FsAclPermission}} subclass.  The patch is hiding the bit from the 16-bit representation seen by callers of {{FsPermission#toShort}}, but we need to preserve the information somehow.  The PB conversion layer seems to be the only remaining choice, but let me know if you had another idea in mind., Here is a recap of some offline discussion between me and Haohui.  Summary: we are reaching consensus around inclusion of an ACL bit, with the approach of hiding the bit from other callers as done in my v3 patch.  There will be some additional changes, which I'll fold into a new version of the patch to upload later.  This also will incorporate Daryn's feedback.

bq. I have no problem to declare this is a wrong assumption but the sad fact is that 3rd-party clients might depend on it.

We agreed on taking a conservative approach of using an ACL bit, but making it invisible to current callers of {{FsPermission#toShort}}.  The technique is already demonstrated in my v3 patch.

bq. Provided that the FileStatus field is relatively short (~200 bytes), adding a field for ACL increases the network traffic for listing files by 5%.

This is not a problem for the HDFS {{ClientProtocol}} RPC.  That protocol already encodes {{FsPermission}} as a protobuf uint32, and my patch isn't going to change that.  It's just going to toggle on a bit of the uint32 that was unused previously.

This is a problem for WebHDFS, which will start returning an additional {{"aclBit": true}} in JSON responses.  To mitigate this, the {{aclBit}} field will be excluded from the JSON response when there is no ACL.  IOW, you'll never see {{"aclBit": false}}.  This means that if ACLs are not in use, then there is no additional cost.  Patch v3 already implements it this way.
, bq. failing to justify the performance concerns leads Chris and I to decide to keep ACL out of the critical path, and to move the complexity to ls / web UI.

Performance concern: The additional rpc and/or http calls.  I misspoke when I had 2X the calls.  It's 1 listStatus per directory, N-many getAclStatus per item just to decide whether to print a "+" or a " ".  Let's take a real world example from a busy cluster: 200ms queue time, 2ms response time.  Listing a directory with 100 items will take an extra ~20s.  Listing 1000 items will take an extra ~3.4m.   We have users that ls larger datasets.

This is unacceptable., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644230/HDFS-6326.3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6875//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6875//console

This message is automatically generated., bq. Should we avoid changing PBHelper. convert(FsPermission) and just let FsAclPermission#toShort serialize the bit?

Actually, I take it back.  We can't do this part.  The point of {{FsAclPermission}} was to hide the ACL bit from being visible in the bit representation.  If the subclass just overrides {{toShort}} to do this, then it defeats the purpose.  I'll keep this logic in {{PBHelper}}., Here is patch v4 incorporating the feedback mentioned in the last several comments.

While fixing the web UI, I also noticed that the logic for printing 't' vs. 'T' for the sticky bit was incorrect.  Due to the way the for loop works, this was always checking the sticky bit itself instead of the other execute bit.  At the point where the check is performed, the sticky bit is always on anyway, so it always chose to print 't'.  I fixed this bug too., While v4 is under review, I'll repeat my manual compatibility testing using a mix of 2.3.0 and trunk components., I've repeated my compatibility testing on patch v4 and confirmed that it still works., Patch looks good but it feels a bit odd leaking the details of the acl bit logic into the protobuf layer.  Here's a partial untested patch based on yours that attempts to contain the logic a bit more to make it more easily usable by other fs impls.  I'll mangle it so pre-commit fails.

Feel free to borrow from it, or if you don't like it, +1 to the current patch., Daryn, that looks like a good suggestion to me.  Here is patch v5 incorporating those ideas.  Mostly, this is just folding your code right into the v4 patch with some additional comments.  Note however that I kept the {{FsAclPermission}} class as {{Private}} inside the HDFS package, not Common.  I'd prefer not to publish this into the public API footprint.

I was hesitant to add a boolean member to track true/false for ACL presence because of the risk of increasing memory footprint.  However, I confirmed with jmap -histo that it makes no difference after your suggested change:

{code}
375:             1             32  org.apache.hadoop.fs.permission.FsPermission
382:             1             32  org.apache.hadoop.hdfs.protocol.FsAclPermission
{code}

Memory utilization of {{FsPermission}} and {{FsAclPermission}} are very much dominated by Java object overhead/padding, so adding the extra boolean didn't make a difference.  Haohui and I have had some offline discussion about optimizations here, but that's not directly relevant to this issue.

[~daryn], can you please let me know if you're still +1 for v5?  Thanks!, I agree it's probably a good idea to keep FsAclPermission private.  I'm still +1, but arguably there's conflict of interest in me approving the code I helped contribute. :)  You may want to consider getting a second opinion but it sounds like Haohui has offline approved the change?, The patch looks good. I just kicked the precommit build., Thanks, Daryn.  Yes, Haohui discussed offline yesterday and he is in agreement on the approach.  He's now out of office for a while though, and since this is a blocker, I'd like to get a +1 quickly.  [~kihwal], since you took a look, could you give the final +1?, The precommit is still running. It ran common tests and {{org.apache.hadoop.http.TestHttpServer}} timed out. You can speed up the process by verifying it is not caused by this jira, while precommit is still running.

Precommit is still running hdfs tests after common tests. I expect it to be done around 18:35 UTC, which is less than an hour from now.  I will check back in an hour., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644668/HDFS-6326.5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.
        See https://builds.apache.org/job/PreCommit-HDFS-Build/6898//artifact/trunk/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.http.TestHttpServer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6898//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/6898//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6898//console

This message is automatically generated., Here is patch v6.  I fixed the JavaDoc warning.  I also added one more test in {{FSAclBaseTest}} asserting that {{setPermission}} cannot mess with the ACL bit.  There is no problem related to this in the main code, because it uses {{FsPermission#toShort}} to serialize the permissions, and we've made sure that the ACL bit isn't visible through that method.  I want this test in place though to catch regressions in case that implementation detail ever changes.

For convenience, here is the incremental diff since last time, so reviewers don't need to re-read the whole thing:

{code}
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsPermission.java b/hadoop-common-
index c9fa89d..ee84437 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsPermission.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsPermission.java
@@ -159,7 +159,7 @@ public short toShort() {
   }
 
   /**
-   * Encodes the object to a short.  Unlike {@link toShort()}, this method may
+   * Encodes the object to a short.  Unlike {@link #toShort()}, this method may
    * return values outside the fixed range 00000 - 01777 if extended features
    * are encoded into this permission, such as the ACL bit.
    *
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java b/hadoop-hdf
index 4aeeb85..f36483e 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hdfs.DFSTestUtil;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hdfs.protocol.AclException;
+import org.apache.hadoop.hdfs.protocol.FsAclPermission;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.security.AccessControlException;
 import org.apache.hadoop.security.UserGroupInformation;
@@ -815,6 +816,23 @@ public void testSetPermissionOnlyDefault() throws IOException {
   }
 
   @Test
+  public void testSetPermissionCannotSetAclBit() throws IOException {
+    FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short)0750));
+    fs.setPermission(path, FsPermission.createImmutable((short)0700));
+    assertPermission((short)0700);
+    fs.setPermission(path,
+      new FsAclPermission(FsPermission.createImmutable((short)0755)));
+    INode inode = cluster.getNamesystem().getFSDirectory().getNode(
+      path.toUri().getPath(), false);
+    assertNotNull(inode);
+    FsPermission perm = inode.getFsPermission();
+    assertNotNull(perm);
+    assertEquals(0755, perm.toShort());
+    assertEquals(0755, perm.toExtendedShort());
+    assertAclFeature(false);
+  }
+
+  @Test
   public void testDefaultAclNewFile() throws Exception {
     FileSystem.mkdirs(fs, path, FsPermission.createImmutable((short)0750));
     List<AclEntry> aclSpec = Lists.newArrayList(
{code}
, Submitted to Jenkins again:

https://builds.apache.org/job/PreCommit-HDFS-Build/6901/
, Did you check the findbugs warning? Two different FsACLPermission objects can have the same hash code and equal.  Probably it is okay for us functionally. If that is the case, we can address it in a separate jira., Probably we can have FsACLPermission#equals()  simply call super.equals() to make findbugs happy., I looked over the patch one more time.  
+1 pending precommit.  You may fix the findbugs warning in a separate non-blocker jira., Thanks, Kihwal.  I did miss that findbugs warning, so thank you for pointing it out.  It's intentional that {{equals}} does not consider {{aclBit}}, as per some prior comments.  I'll file a separate jira to override {{equals}} anyway and just call {{super.equals}} to satisfy findbugs., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644858/HDFS-6326.6.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6901//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/6901//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6901//console

This message is automatically generated., +1 TestCacheDirective is a known breakage. Please file a separate jira to fix the findbugs warning., Thanks, Kihwal.  I'm going to commit this.  I filed HDFS-6402 to fix the findbugs warning, and I'll post a patch on that soon., I have committed this to trunk and branch-2.  I ran into a merge conflict on branch-2.4, so I need to apply a slightly different patch, which I have attached.  [~kihwal], would you please also review and +1 the branch-2.4 patch?  The differences from trunk are:
# {{FSAclBaseTest#testSetPermissionCannotSetAclBit}} needed to make a slightly different call to get the {{INode}} to check, taking a hop through {{FSDirectory#getRoot}}.
# The web UI code is structured differently, so you'll see changes in src/main/webapps/hdfs/explorer.html and src/main/webapps/hdfs/explorer.js.
# The web UI code for {{helper_to_acl_bit}} is slightly different, because the branch-2.4 code is following a different pattern., +1 for the 2.4 patch.  I noticed the difference in the patch file sizes, but it turned out to be git vs. svn., I have committed to branch-2.4.  Big thanks to Daryn, Haohui and Kihwal for helping to work through this issue., SUCCESS: Integrated in Hadoop-trunk-Commit #5605 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5605/])
HDFS-6326. WebHdfs ACL compatibility is broken. Contributed by Chris Nauroth. (cnauroth: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594743)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsPermission.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/AclCommands.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Ls.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/FsAclPermission.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/explorer.html
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/dfs-dust.js
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/AclTestHelpers.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java
, Since we have ACL bit, ACL could be added FileStatus in a compatible manner.  Won't you agree?, [~szetszwo], I still see a compatibility problem with adding the ACL directly to {{FileStatus}}.  This is a quote from the ACLs design doc on HDFS-4685:

{quote}
An alternative implementation would have been to add Acl as a member field of the
existing FileStatus, FsPermission or PermissionStatus classes.  This is rejected for
compatibility reasons.  If a new version of the client (post­ACLs) deserializes an old version of
the FileStatus (pre­ACLs), then this could cause bugs.  If we assume a null Acl if we encounter
EOF during deserialization, then there is a risk of null pointer dereferences in downstream code.
{quote}

I can't think of a workaround to the problem of brittle {{Writable}} serialization.  I explored a few ideas, and they all introduce other bugs as a side effect:
* Keep reading all the way through to EOF during deserialization and see if you find a serialized ACL after the existing fields.
** This doesn't work, because {{readFields}} has no way to know if the underlying stream really contains multiple serialized objects.
* Attempt deserialization with ACL, but fall back to the old deserialization without ACL if it fails.
** This is likely to fail for similar reasons as above, because subsequent data in the stream could contain data misinterpreted as an ACL.  Deserialization wouldn't fail, but it would result in wrong data.  This also doesn't address the problem of an old pre-ACLs client attempting to deserialize a new {{FileStatus}} containing an ACL.
* Just skip the ACL in {{write}} and {{readFields}}.
** This has the problem that round-tripping a {{FileStatus}} would lose an ACL, and clients instead would expect to maintain the full state of the object.

Also, there is the consideration of bandwidth consumption for inclusion of the ACL data in RPC responses.  Each {{AclEntry}} is 3 enums + a string, and the number of entries ranges from 2 - 29.  The ACL could dominate over the size of all existing data in the {{FileStatus}}, even though most use cases don't consume the ACL.  This part could be addressed by something like a "fetchAcl=true" optional parameter on the RPC, passed only for use cases that need it like getfacl, but there are still the compatibility problems.

I just thought of one more possibility.  We could add whole new RPCs, i.e. {{getListingExtended}} and {{getFileInfoExtended}}, that return a whole new object, {{FileStatusExtended}}, and we could put the ACL in the new object.  Existing clients would stick to the old RPCs and old {{FileStatus}}, so there would be no compatibility concerns.  Uses cases that care about the ACL, like getfacl and distcp -pa, can change their code to call the new RPCs.  Obviously, the downside of this is cloning a bunch of code and expanding our API footprint on {{FileSystem}}.  It's the only way I can see doing it in a backwards-compatible way though.  If you think this is worth exploring, then we could file a new jira for it.  Let me know what you think., > I can't think of a workaround to the problem of brittle Writable serialization. ...

We could check if the ACL bit is set in the permission field of FileStatus. I.e.

{code}
//FileStatus
  @Override
  public void write(DataOutput out) throws IOException {
    Text.writeString(out, getPath().toString(), Text.DEFAULT_MAX_LEN);
    out.writeLong(getLen());
    out.writeBoolean(isDirectory());
    out.writeShort(getReplication());
    out.writeLong(getBlockSize());
    out.writeLong(getModificationTime());
    out.writeLong(getAccessTime());
    getPermission().write(out);
    Text.writeString(out, getOwner(), Text.DEFAULT_MAX_LEN);
    Text.writeString(out, getGroup(), Text.DEFAULT_MAX_LEN);
    out.writeBoolean(isSymlink());
    if (isSymlink()) {
      Text.writeString(out, getSymlink().toString(), Text.DEFAULT_MAX_LEN);
    }
    if (permission.getAclBit()) {
      //write ACL
      ...
    }
  }

  @Override
  public void readFields(DataInput in) throws IOException {
    String strPath = Text.readString(in, Text.DEFAULT_MAX_LEN);
    this.path = new Path(strPath);
    this.length = in.readLong();
    this.isdir = in.readBoolean();
    this.block_replication = in.readShort();
    blocksize = in.readLong();
    modification_time = in.readLong();
    access_time = in.readLong();
    permission.readFields(in);
    owner = Text.readString(in, Text.DEFAULT_MAX_LEN);
    group = Text.readString(in, Text.DEFAULT_MAX_LEN);
    if (in.readBoolean()) {
      this.symlink = new Path(Text.readString(in, Text.DEFAULT_MAX_LEN));
    } else {
      this.symlink = null;
    }
    if (permission.getAclBit()) {
      //read ACL
      ...
    }
  }
{code}
, bq. We could check if the ACL bit is set in the permission field of FileStatus.

This would work fine as an optimization, so that we don't need to serialize an empty list when there is no ACL.  However, it wouldn't fix the compatibility problem.  Suppose a 2.3.0 process attempts to deserialize a {{FileStatus}} that had been serialized by a 2.5.0 process.  The 2.5.0 process would write the ACL, but the 2.3.0 process wouldn't have the code to check the ACL bit and deserialize.  That would leave the input stream positioned at the incorrect location after completion of {{FileStatus#readFields}}.  The next read after that would have unpredictable results, possibly misinterpreting serialized ACL data as something else or possibly just throwing an exception., Serializing the acl isn't so much a problem as deserializing.  Older non-acl aware clients will get out of sync decoding the stream.

Other than the couple FsShell acl commands, and distcp -pa, is there a common use case for a client to know/care about acls?  If not, increasing the rpc response size for a few rare use cases probably isn't worth it?, I'm aware of no other use cases for visibility of the ACLs right now.  I agree that it's not worth the extra bandwidth cost in common usage., >  Suppose a 2.3.0 process attempts to deserialize a FileStatus that had been serialized by a 2.5.0 process. The 2.5.0 process would write the ACL, but the 2.3.0 process wouldn't have the code to check the ACL bit and deserialize.

This is a forward compatibility example.  I guess we could not support it in any class implementing Writable since it was not designed to support forward compatibility.  We may either make the change to 3.0 only or add another class, say FileStatusWithACL.

> Other than the couple FsShell acl commands, and distcp -pa, is there a common use case for a client to know/care about acls? If not, increasing the rpc response size for a few rare use cases probably isn't worth it?

For the RPC response size, we may add a parameter to indicate whether ACL should be returned (or we may add a new method, say getFileStatusWithACL.)

> I'm aware of no other use cases for visibility of the ACLs right now. ...

Perhaps, this is the most important question -- Is it useful to add ACL to FileStatus, or adding a new method and/or a FileStatus subclass?  It seems only help distcp -pa.  It may not be significant enough and so we may want to defer implementing it., FAILURE: Integrated in Hadoop-Yarn-trunk #562 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/562/])
HDFS-6326. WebHdfs ACL compatibility is broken. Contributed by Chris Nauroth. (cnauroth: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594743)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsPermission.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/AclCommands.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Ls.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/FsAclPermission.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/explorer.html
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/dfs-dust.js
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/AclTestHelpers.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1754 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1754/])
HDFS-6326. WebHdfs ACL compatibility is broken. Contributed by Chris Nauroth. (cnauroth: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594743)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsPermission.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/AclCommands.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Ls.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/FsAclPermission.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/explorer.html
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/dfs-dust.js
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/AclTestHelpers.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1780 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1780/])
HDFS-6326. WebHdfs ACL compatibility is broken. Contributed by Chris Nauroth. (cnauroth: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1594743)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/permission/FsPermission.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/AclCommands.java
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Ls.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/FsAclPermission.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/explorer.html
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/dfs-dust.js
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/AclTestHelpers.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java
]