[The staleness corrects itself if either another writer opens the file for "appending" to it or the hard limit of 1 hour (the lease recovery period) expires. But I agree that your proposal is better. +1

It introduces additional latency for the getFileSatus() call, but if we do this only for files that have a lease on it (i.e. a writer was writing to this file), then it should be ok.

Additionally, the current getFileStatus() call does not retrieve block location information from the namenode. This has to be enhanced to return the location of at least the last block of a file., Ping!

I'm tempted to close this as stale, but it would be good for someone more familiar with the issue to do that., sync() does not update the length in NN.  So getFileSatus() will return the correct length immediately as Dhruba mentioned.

Anyway, sync() is already removed from trunk (HDFS-3034).  hsync(..) with UPDATE_LENGTH flag could be used instead.  So this becomes not-a-problem anymore.  Resolving ..., > ... getFileSatus() will return the correct length ...

Oops, it should be "... getFileSatus() will NOT return the correct length ..."., HDFS file length reported by ls may be less than the number of bytes found when reading.  I created the mismatched file by kill -9 during a copy so that the client doesn't shutdown its connection to the namenode properly.  This misreported length persisted after restarting hdfs.

{quote}
$ hdfs dfs -copyFromLocal junk17 /tmp/.
2015-06-09 13:09:25,742 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
^Z
[1]+  Stopped                 hdfs dfs -copyFromLocal junk17 /tmp/.
$ kill -9 %1

[1]+  Stopped                 hdfs dfs -copyFromLocal junk17 /tmp/.
$ fg
-bash: fg: job has terminated
[1]+  Killed: 9               hdfs dfs -copyFromLocal junk17 /tmp/.
$ hdfs dfs -ls /tmp
2015-06-09 13:09:45,730 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
drwxrwx---   - jane supergroup          0 2015-05-28 14:26 /tmp/hadoop-yarn
drwx-wx-wx   - jane supergroup          0 2015-05-28 14:26 /tmp/hive
-rw-r--r--   1 jane supergroup 1073741824 2015-06-09 13:09 /tmp/junk17._COPYING_
$ hdfs dfs -ls /tmp
2015-06-09 13:09:55,345 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
drwxrwx---   - jane supergroup          0 2015-05-28 14:26 /tmp/hadoop-yarn
drwx-wx-wx   - jane supergroup          0 2015-05-28 14:26 /tmp/hive
-rw-r--r--   1 jane supergroup 1073741824 2015-06-09 13:09 /tmp/junk17._COPYING_
$ hdfs dfs -cat /tmp/junk17._COPYING_ | wc -c
 1207959752
$ hdfs dfs -ls /tmp
2015-06-09 13:11:21,389 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
drwxrwx---   - jane supergroup          0 2015-05-28 14:26 /tmp/hadoop-yarn
drwx-wx-wx   - jane supergroup          0 2015-05-28 14:26 /tmp/hive
-rw-r--r--   1 jane supergroup 1073741824 2015-06-09 13:09 /tmp/junk17._COPYING_
$ hdfs dfs -cp /tmp/junk17._COPYING_ /tmp/junk18
2015-06-09 13:13:38,963 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
$ hdfs dfs -ls /tmp
2015-06-09 13:13:45,575 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 4 items
drwxrwx---   - jane supergroup          0 2015-05-28 14:26 /tmp/hadoop-yarn
drwx-wx-wx   - jane supergroup          0 2015-05-28 14:26 /tmp/hive
-rw-r--r--   1 jane supergroup 1073741824 2015-06-09 13:09 /tmp/junk17._COPYING_
-rw-r--r--   1 jane supergroup 1207959552 2015-06-09 13:13 /tmp/junk18
{quote}

{quote}
$ hdfs version
Hadoop 2.6.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1
Compiled by jenkins on 2014-11-13T21:10Z
Compiled with protoc 2.5.0
From source with checksum 18e43357c8f927c0695f1e9522859d6a
{quote}, Turns out the file is still considered "open for write"...  This is an hour or two after the client aborted, and after restarting hdfs.  The standard behavior would be the file would be closed shortly after the client aborting.

{block}
kevin-beyer-mbp2:t kevin$ hdfs fsck /tmp/junk17._COPYING_ -blocks
2015-06-09 14:52:02,807 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Connecting to namenode via http://localhost:50070
FSCK started by kevin (auth:SIMPLE) from /127.0.0.1 for path /tmp/junk17._COPYING_ at Tue Jun 09 14:52:03 PDT 2015
Status: HEALTHY
 Total size:	0 B (Total open files size: 1073741824 B)
 Total dirs:	0
 Total files:	0
 Total symlinks:		0 (Files currently being written: 1)
 Total blocks (validated):	0 (Total open file blocks (not validated): 8)
 Minimally replicated blocks:	0
 Over-replicated blocks:	0
 Under-replicated blocks:	0
 Mis-replicated blocks:		0
 Default replication factor:	1
 Average block replication:	0.0
 Corrupt blocks:		0
 Missing replicas:		0
 Number of data-nodes:		1
 Number of racks:		1
FSCK ended at Tue Jun 09 14:52:03 PDT 2015 in 2 milliseconds


The filesystem under path '/tmp/junk17._COPYING_' is HEALTHY
kevin-beyer-mbp2:t kevin$ hdfs fsck -openforwrite
2015-06-09 14:52:25,236 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Connecting to namenode via http://localhost:50070
FSCK started by kevin (auth:SIMPLE) from /127.0.0.1 for path / at Tue Jun 09 14:52:26 PDT 2015
....................................................................................................
./tmp/junk17._COPYING_ 1073741824 bytes, 8 block(s), OPENFORWRITE: ..................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
..........................................................................Status: HEALTHY
 Total size:	3905069992 B
 Total dirs:	276
 Total files:	674
 Total symlinks:		0
 Total blocks (validated):	321 (avg. block size 12165327 B)
 Minimally replicated blocks:	321 (100.0 %)
 Over-replicated blocks:	0 (0.0 %)
 Under-replicated blocks:	0 (0.0 %)
 Mis-replicated blocks:		0 (0.0 %)
 Default replication factor:	1
 Average block replication:	1.0
 Corrupt blocks:		0
 Missing replicas:		0 (0.0 %)
 Number of data-nodes:		1
 Number of racks:		1
FSCK ended at Tue Jun 09 14:52:26 PDT 2015 in 167 milliseconds
{block}, I am not alone with this problem:

http://stackoverflow.com/questions/5347293/hdfs-says-file-is-still-open-but-process-writing-to-it-was-killed

http://stackoverflow.com/questions/19565791/hbase-distributed-log-splitting-keeps-failing-because-unable-to-get-a-lease

http://stackoverflow.com/questions/23833318/crashed-hdfs-client-how-to-close-remaining-open-files
, I've learned about soft and hard limits on the update lease.  After the hard limit expired, the file length corrected to same number of bytes found by reading.  So this is not a bug. 

However, I have a few ideas that might help:

1. The file stats could update when the soft limit expires.  This would reduce the window of inconsistency to 1 minute instead of 1 hour.

2. Allow the writing application to control the "safe" length and limit readers to the safe length.  Readers could set an option to read the unsafe bytes (or the default could read the full length, but that seems more dangerous although backwards compatible to current behavior).  If the lease is not recovered before the hard limit expires, the unsafe bytes are discarded (a writer option could control this as well).  This would allow applications to avoid partial record reads. 

3. A simple way for readers to detect that there is an active soft/hard lease on a file, probably in the FileStatus.

4. The hard limit duration should be an option when opening for write.  The default should be zero.

5. A simple way to terminate a hard lease.
]