[The problem does not only occur with bad datanodes but also with full datanodes.

During a long running job the distribution of blocks written to dfs is not uniform. With 35% dfs full, 5% of datanodes are already completely full, typically having double the number of blocks compared to average.

I would have assumed that when writing to dfs, datanodes on the same rack with more space are preferred over datanodes with less space. Am I wrong?, If you have almost full datanodes, you need HADOOP-3707. You are probably hitting HADOOP-3633 ant HADOOP-3685. These are all committed in 0.17.2. Log on some datanodes would show more info.

> I would have assumed that when writing to dfs, datanodes on the same rack with more space are preferred over datanodes with less space. Am I wrong?

Not really. That will result in imbalance in load. As long as a datanode has enough space it gets load.

, I agree with Raghu. As ling as the datanode has enough space, it can get load. However, the random selection of datanodes (within a rack) usually balances the disk-space usage on a cluster. Also, while deleting excess replicas, HDFS tries to delete replicas on machines that have least amount of free space., Although I do not completely disagree (I still can imagine that some smart algorithm could leverage knowledge about space to ensure better distribution without overloading any particular datanode), I wonder whether there is a bug in assigning blocks.

I checked the block distribution before and after a failed job.
Of 1623 nodes 5% of the nodes received at least 27% of new blocks.
Before the job started the 5% of nodes with highest number of blocks had 7% of all blocks,
at the end of the failed job the top 5% had 10% of all blocks.

Does this make sense? Is this something which got introduced going from 0.16 to 0.17.1 but then fixed in 0.17.2?

I will deploy 0.17.2 and rerun the same job, smarter is better than less smart :-) (though it is possible be too smart).

Though you may not be affected, another thing that influences load is how many blocks a node is currently writing. Slower nodes (either because of slower hardware or slower client writes) overall gets less blocks assigned than a faster nodes At any time NameNode tries to keep number blocks currently being written to less than 2*avg. 

The fact that some slower clients can influence load on datanode should be fixed I think.
 , In addition to run under 0.17.2 I started to run the balancer in the background for a while to ensure better block distribution. So far looks much better than before but I would like to keep this bug open (no longer as blocker) because I would like to monitor the cluster for possible degradation over time., The NameNode should be able to continue to provide administrative functions and file access even if file creations must be delayed or deferred. If the policy is to refuse creations, the client should receive a unambiguous message. 

So what should be the policy? It is probably infeasible to allocate the very last block. Is the cluster full when it takes too long to find a block? When too many DataNode report high utilization? If replication attempts fail too often?

, One can create a file even if there is no additional free disk space in the cluster. If an application then tries to allocate a block for this file, the block allocation will fail....but the zero-length file will continue to exist. If the application is happy with a zero-length file, then it can proceed ahead. Does this make sense?, > In addition to run under 0.17.2 [...]

Chirstian, you might need to increase "dfs.datanode.max.xcievers" to something like 1000+. The default is 256. If I remember correctly you have many simultaneous readers and writers to each datanode.
, Raghu, thank you for pointing this out. As a matter of fact, we already did that, after lessons learned on another cluster with 0.17.2., I hope the default is increased too. I don't see much of a reason for 256.]