[By seeing the HDFS-2290, this is an existing behaviour currently. Until unless, it replicate new block, it won't delete the corrupt block. 
Already asserted in some test cases in HDFS-2290.

see test from HDFS-2290:

{code}
/**
+   * The corrupt block has to be removed when the number of valid replicas
+   * matches replication factor for the file. In this test, the above 
+   * condition is achieved by increasing the number of good replicas by 
+   * replicating on a new Datanode. 
+   * The test strategy : 
+   *   Bring up Cluster with 3 DataNodes
+   *   Create a file  of replication factor 3
+   *   Corrupt one replica of a block of the file 
+   *   Verify that there are still 2 good replicas and 1 corrupt replica 
+   *     (corrupt replica should not be removed since number of good replicas
+   *      (2) is less  than replication factor (3)) 
+   *   Start a new data node 
+   *   Verify that the a new replica is created and corrupt replica is
+   *   removed.
+   * 
+   */
+  @Test
+  public void testByAddingAnExtraDataNode() throws IOException {
{code}


the below condition will not allow to invalidate block, even if we corrupt one more block.

{code}
    node.addBlock(storedBlock);

    // Add this replica to corruptReplicas Map
    corruptReplicas.addToCorruptReplicasMap(storedBlock, node, reason);
    if (countNodes(storedBlock).liveReplicas() >= bc.getReplication()) {
{code}

Replication factor 3, corrupt replica is 1, and live replicas are 2.
So, the above condition will not satisfy. It will just add into neededReplications. Since we don't have one more DN here, it will not be able to replicate also.

Here my concern is, if we corrupt one more block, 
Replication factor 3, corrupt replicas are 2, and live replica is 1.
Now also it will not be able to replicate and invalidate, we may end up running with one good replica, even though we have 3 DNs in cluster. In this smaller cluster, this is risk. In bigger clusters this problem will not come because, we will have some more nodes in cluster, so, the replication will happen suucessfully. This is only problem when we have cluster size is equal to replication factor at that moment., Yes, That's the behavior as per HDFS-2290. Adding additions nodes will replicate good copy and then remove the corrupt copies. 

In this special case, when there are more corrupt copies than  good copies and with no DN to replicate, it may be an improvement to remove one of the corrupt blocks. , I agree with Uma this works as designed. On a three-node cluster one can manually delete the corrupt replica, which should trigger replication., By seeing the HDFS-3586 comments, we can delete the corrupt replicas created during write pipeline failures ( changes in genstamp) which will allow replication to happen properly even in case of small cluster., Attaching a patch, which handles following

# Invalidates the replica with lesser genstamp, which would have been created during write pipeline failure.
# Invalidates excess corrupt replica

Please review, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12598367/HDFS-3493.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks
                  org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4841//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4841//console

This message is automatically generated., Hi Vinay, this patch looks good. any pending work there?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12598367/HDFS-3493.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6966//console

This message is automatically generated., Hi Vinay,

Would you mind I take it over and finish it?

Thanks,
Juan, If all replicas are corrupted, I think we don't want to delete them. So
modified case 2 to "case 2: corrupted replicas + live replicas > Replication factor" and verified all unit tests, those two broken one are passed now., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12648550/HDFS-3493.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage
                  org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7041//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7041//console

This message is automatically generated., Fix broken tests, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12648751/HDFS-3493.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7053//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7053//console

This message is automatically generated., Thanks for taking this on Juan, fix looks good. Just a few nitty comments:

* some whitespace only changes in BlockManager
* some lines longer than 80 chars
* Maybe fold {{minReplicationSatisfied &&}} into {{corruptedDuringWrite}} like how you assign {{hasMoreCorruptReplicas}} for parity.
* In the test, do we need that sleep(10000)? I'm always wary of sleeps, since they lead to test flakiness.
* I think the comment should also read something like "DNs will detect new dummy blocks on restart". Would also be good to drop a comment about what you're doing with creating dummy blocks.
* I like to put nice conservative timeouts on my tests, e.g. {{@Test(timeout=120000}}.

+1 pending these though. [~vinayrpet], maybe you'd like to take a look too?, New patch to address Andrew's review comments.
Thanks Andrew for the review., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12649844/HDFS-3493.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7086//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7086//console

This message is automatically generated., +1 LGTM, will commit shortly., SUCCESS: Integrated in Hadoop-trunk-Commit #5693 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5693/])
HDFS-3493. Invalidate corrupted blocks as long as minimum replication is satisfied. Contributed by Juan Yu and Vinayakumar B. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1602291)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplication.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java
, Committed to trunk and branch-2, thanks to Vinay and Juan for working on this issue!, SUCCESS: Integrated in Hadoop-trunk-Commit #5694 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5694/])
Fixup CHANGES.txt message for HDFS-3493 (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1602292)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #582 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/582/])
Fixup CHANGES.txt message for HDFS-3493 (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1602292)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
HDFS-3493. Invalidate corrupted blocks as long as minimum replication is satisfied. Contributed by Juan Yu and Vinayakumar B. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1602291)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplication.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1773 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1773/])
Fixup CHANGES.txt message for HDFS-3493 (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1602292)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
HDFS-3493. Invalidate corrupted blocks as long as minimum replication is satisfied. Contributed by Juan Yu and Vinayakumar B. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1602291)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplication.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1800 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1800/])
Fixup CHANGES.txt message for HDFS-3493 (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1602292)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
HDFS-3493. Invalidate corrupted blocks as long as minimum replication is satisfied. Contributed by Juan Yu and Vinayakumar B. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1602291)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplication.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java
]