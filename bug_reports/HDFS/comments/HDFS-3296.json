[This is almost working in current trunk, as long as we set {{DYLD_LIBRARY_PATH}}.  (Patch attached.)  However, this then runs into a problem with test_libhdfs_zerocopy hanging.  There appears to be some difference in domain socket handling on Mac (or BSDs in general) that our code isn't handling well.  I'm still investigating., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12690681/HDFS-3296.001.patch
  against trunk revision ef237bd.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancer

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9151//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9151//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12690681/HDFS-3296.001.patch
  against trunk revision af9d4fe.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/10248//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/10248//console

This message is automatically generated., Is there any reason to not just set DYLD_LIBRARY_PATH the same as LD_LIBRARY_PATH? i.e., do we need a separate property?, We define {{<LD_LIBRARY_PATH>}} by appending to the existing {{LD_LIBRARY_PATH}} of the environment.  Similarly, {{<DYLD_LIBRARY_PATH>}} would need to append to the existing {{DYLD_LIBRARY_PATH}}.  Because of that difference, I don't think we can consolidate to a single property.  However, we probably could define a shared property in the {{<properties>}} section to reduce some of the duplication.  I'll do that in the next rev.

This is still blocked on figuring out failures in things like {{test_libhdfs_zerocopy}} on Mac.  There was some investigation related to this in HADOOP-11957, but it's not done yet., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 54s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 38s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 44s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 19s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | native |   3m  5s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests | 158m 42s | Tests failed in hadoop-hdfs. |
| | | 196m 20s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.hdfs.TestDistributedFileSystem |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12690681/HDFS-3296.001.patch |
| Optional Tests | javadoc javac unit |
| git revision | trunk / 5137b38 |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/11775/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/11775/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf901.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11775/console |


This message was automatically generated., Hi [~cnauroth], can you please rebase the latest patch? Thanks., Patch v002 is a rebased patch for the required build changes.  I still consider this effectively blocked on HADOOP-11957, where we need to address domain socket test failures on Mac., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red} 0m 0s {color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 41s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 7m 45s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 38s {color} | {color:green} trunk passed with JDK v1.7.0_85 {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 26s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 22s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 19s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 20s {color} | {color:green} trunk passed with JDK v1.7.0_85 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 22s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 7m 58s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 7m 58s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 8m 43s {color} | {color:green} the patch passed with JDK v1.7.0_85 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 8m 43s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 27s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 22s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 1s {color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 18s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 20s {color} | {color:green} the patch passed with JDK v1.7.0_85 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 8s {color} | {color:green} hadoop-project in the patch passed with JDK v1.8.0_66. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 40s {color} | {color:green} hadoop-hdfs-native-client in the patch passed with JDK v1.8.0_66. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 9s {color} | {color:green} hadoop-project in the patch passed with JDK v1.7.0_85. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 0m 41s {color} | {color:green} hadoop-hdfs-native-client in the patch passed with JDK v1.7.0_85. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 24s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 48m 14s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ca8df7 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12773907/HDFS-3296.002.patch |
| JIRA Issue | HDFS-3296 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  xml  |
| uname | Linux 0a8ddaa259b4 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 5f269a0 |
| JDK v1.7.0_85  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/13622/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client hadoop-project U: . |
| Max memory used | 75MB |
| Powered by | Apache Yetus   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/13622/console |


This message was automatically generated.

, I'm in the process of setting up a nightly build for OS X on the ASF build infrastructure.  We definitely need this patch just for hadoop-common to build.  So let's get this rebased and committed.  [~cnauroth], if you want, you or I can open another JIRA to do that with if you want., Hi [~aw].  Thanks for setting up a nightly on OS X!  Even just a basic build to catch compilation errors is a huge help.

The patch here won't help with any compilation problems.  This patch was just a small step towards fixing the libhdfs tests on Mac by setting up {{DYLD_LIBRARY_PATH}} with the right shared library dependencies.  It isn't sufficient though, because we still have a compatibility problem around domain socket usage.  This manifests as test failures in {{TestDomainSocketWatcher}} and unfortunately some of the libhdfs tests just hang.

If the immediate goal is a basic build on OS X, then what I'm currently seeing on trunk is a compilation error in the container executor.  This was introduced by patch YARN-4594, and I commented on the situation here:

https://issues.apache.org/jira/browse/YARN-4594?focusedCommentId=15139679&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15139679

To summarize that discussion, if I can get the build environment to target the Mac OS X 10.10 SDK, then I suspect it would work.  I wasn't able to follow up on it though.  I'm curious if you have any thoughts on this., bq. The patch here won't help with any compilation problems. 

Correct.  I'm well past that point and now trying to get unit tests to work.  native unit tests are failing in hadoop-common because they can't find libhadoop.so.  I ended up writing pretty much the exact same change to hadoop-project/pom.xml (since replaced with what you have here) and am manually patching the pom prior to launching maven on the build host.  We're now left with a bunch of other problems, including this likely related one:

(from https://builds.apache.org/view/H-L/view/Hadoop/job/hadoop-trunk-osx-java8/14/console )

{code}
testInvalidOperations(org.apache.hadoop.net.unix.TestDomainSocket)  Time elapsed: 0.012 sec  <<< FAILURE!
java.lang.AssertionError: Expected to find 'connect(2) error: ' but got unexpected exception:java.net.SocketException: error computing UNIX domain socket path: path too long.  The longest UNIX domain socket path possible on this host is 103 bytes.
	at org.apache.hadoop.net.unix.DomainSocket.connect0(Native Method)
	at org.apache.hadoop.net.unix.DomainSocket.connect(DomainSocket.java:256)
	at org.apache.hadoop.net.unix.TestDomainSocket.testInvalidOperations(TestDomainSocket.java:266)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

	at org.apache.hadoop.net.unix.DomainSocket.connect0(Native Method)
	at org.apache.hadoop.net.unix.DomainSocket.connect(DomainSocket.java:256)
	at org.apache.hadoop.net.unix.TestDomainSocket.testInvalidOperations(TestDomainSocket.java:266)
{code}

I think I'm going to set up a precommit job for Mac so that folks can manually trigger it to test Mac-specific patches.

bq. target the Mac OS X 10.10 SDK

The Mac mini in the build infrastructure is a 10.9 box.


, [~aw], I have filed HADOOP-13177 with a one-line patch for the Surefire configuration change to set {{DYLD_LIBRARY_PATH}}.  We can commit that one to move ahead with Jenkins runs on OS X.

I'm also attaching a rebased v003 patch here for just the change in hadoop-hdfs-native-client.  We won't want to commit this one, because this will just make {{test_libhdfs_zerocopy_hdfs_static}} hang indefinitely.  We need to get to the bottom of the domain socket issues on OS X before we can commit this one., Awesome thanks.

yeah, totally understand about the libhdfs issue. :), I'd like to share a patch I have been using. Not totally polished or thoroughly tested yet.

Patch 004
* Work around domain socket path limitation by using "/tmp" instead of $java.io.tmpdir
* On Mac, {{shutdown0}} can not unblock {{accept0}} which still holds on to a refcount, thus {{DomainSocket#close}} stuck in the loop. Fix the issue by using the "self-pipe" method. See {{accept1}} implementation for details.
* Add syslog calls to DomainSocket.c for easier debugging
* Set DYLD_LIBRARY_PATH to hadoop-hdfs-native-client pom. All libhdfs tests pass including the zerocopy test.

TODO:
* {{TestDomainSocket#testAsyncCloseDuringWrite}} failed
* Pass all ShortCircuitRead tests, TestDomainSocket#testAsyncCloseDuringWrite failure:
{noformat}
java.util.concurrent.ExecutionException: java.lang.NoSuchMethodError: <init>

	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.net.unix.TestDomainSocket.testAsyncCloseDuringIO(TestDomainSocket.java:245)
	at org.apache.hadoop.net.unix.TestDomainSocket.testAsyncCloseDuringWrite(TestDomainSocket.java:250)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.lang.NoSuchMethodError: <init>
	at org.apache.hadoop.net.unix.DomainSocket.writeArray0(Native Method)
	at org.apache.hadoop.net.unix.DomainSocket.access$300(DomainSocket.java:45)
	at org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream.write(DomainSocket.java:598)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.hadoop.net.unix.TestDomainSocket$3.call(TestDomainSocket.java:195)
	at org.apache.hadoop.net.unix.TestDomainSocket$3.call(TestDomainSocket.java:180)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}
]