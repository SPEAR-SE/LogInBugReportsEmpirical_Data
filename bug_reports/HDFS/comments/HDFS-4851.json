[I realized that HDFS-3655 is actually addressing the same issue and tried to revive that approach, but it ended up being super complicated to check and recheck the preconditions on lock acquisition.

Attached here instead is a simpler strategy: abort recovery if we end up waiting too long on this lock. While not optimal, it should be safe since the client can retry recovery again.

Since this is very hard to unit test, I tested by adding a loop that grabs the lock repeatedly in {{receivePacket}}, verified that this caused the deadlock, and then applied the patch and verified that the error message printed.

HDFS-3655 can be where we properly fix this issue, or more broadly re-examine finer grained locking during recovery., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12589794/hdfs-4851-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4574//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4574//console

This message is automatically generated., No tests because it's hard to write one, but relatively easy to test manually with my previous instructions., Thanks for working on this Andrew.
This can happen wherever we do stopWriter under fsdataset lock and if old writer needs to get this lock at that moment right.
As this only on recovery calls, your patch looks to be simple to fail the current recover if older DX not able to proceed due the current thread held the lock already on fsdataset. Other option may be, how about moving this stop writer call to other method and where we just get rbw in lock and then we just interrupt without lock. After this step only we call recoverRBW. (now recoverRBW need not stop old writer  as we moved that logic to separate call), Hey Uma, thanks for taking a look!

I may not understand your proposal entirely, but I found it pretty complex to interrupt while not holding the lock (see the patch in HDFS-3655 for the general idea).

The core issue is that more recovery threads can keep coming in, so even if we interrupt the current old writer, by the time we re-get the FSD lock to rbw.setWriter to ourselves, some other recovery thread might have again come in and we need to interrupt them too. Repeating the stopWriter requires re-doing the precondition checks in the three places we call stopWriter, each of which have different preconditions.

Would love if a simpler or better solution is present though, so please let me know if I missed something., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12589794/hdfs-4851-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4588//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4588//console

This message is automatically generated., Dupe on commit of HDFS-5016, fixes the same deadlock]