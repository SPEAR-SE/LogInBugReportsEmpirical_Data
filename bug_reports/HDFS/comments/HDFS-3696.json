[The following stack trace is from doing {{copyFromLocal}} with 140MB file. The map heap is 1G (-Xmx1000m) in the client side.

{noformat}
$ hadoop fs -copyFromLocal /tmp/xxx140m webhdfs://my.server.blah:50070/user/kihwal/xxx
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2786)
        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)
        at sun.net.www.http.PosterOutputStream.write(PosterOutputStream.java:61)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:54)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:80)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:52)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)
        at org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:240)
        at org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:219)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:165)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:150)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:306)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:278)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:145)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:260)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:244)
        at org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:122)
        at org.apache.hadoop.fs.shell.CopyCommands$Put.processArguments(CopyCommands.java:204)
        at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:254)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:304)
{noformat}, bq. The map heap
The max heap

For reading of 1G piping into a sink that is consuming data at 10 KB/s, VSZ stayed at 547420 KB and RSZ 87540 KB.  It doesn't go OOM but the VM size seems rather big., I also noticed this problem when I tested WebHdfsFileSystem with 3GB files in HDFS-3671.  After added HttpURLConnection.setChunkedStreamingMode(..), the test ran well.  Since it is only an one-line change, I will add it with the retry patch (HDFS-3667)., h3696_20120724.patch: add setChunkedStreamingMode(32kB).

I tried several chunk sizes for writing 300MB files.  32kB was the best in my test.

|| Chunk size || 1st || 2nd ||
| 4kB   |  3.95MB/s |  3.95MB/s |
| 16kB  |  7.81MB/s |  7.70MB/s |
| 24kB  | 12.58MB/s | 12.29MB/s |
| 32kB  | 14.15MB/s | 14.28MB/s |
| 48kB  | 14.25MB/s | 13.29MB/s |
| 64kB  | 13.65MB/s | 13.57MB/s |
| 128kB | 13.94MB/s | 13.15MB/s |
| 1MB   | 13.11MB/s | 13.45MB/s |
, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12537802/h3696_20120724.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestReplication
                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner
                  org.apache.hadoop.hdfs.TestPersistBlocks

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2901//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2901//console

This message is automatically generated., The patch only changes WebHdfsFileSystem and adds a new test.  The failed tests are not related., Revised the summary since this was not specific to "fs -put"., +1 for the patch., Kihwal do you want to run some tests with this patch as well?, bq. I tried several chunk sizes for writing 300MB files. 32kB was the best in my test.

Any idea why it performed best at 32KB? 

bq. Kihwal do you want to run some tests with this patch as well?

I think it's okay if the memory consumption is under control. All I did was a simple put., > Any idea why it performed best at 32KB?

Since TCP has a max packet size of 64kB, I thought the best should be somehow close to 64kB.  I was surprise that 32kB was better than 48kB in my experiment.  Perhaps it was due to some implementation details in the Java library., Thanks Suresh for reviewing it.

I have committed this., 0.23 needs a new patch since the test cannot be compiled., Integrated in Hadoop-Hdfs-trunk-Commit #2587 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2587/])
    HDFS-3696. Set chunked streaming mode in WebHdfsFileSystem write operations to get around a Java library bug causing OutOfMemoryError. (Revision 1365839)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365839
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
, Integrated in Hadoop-Common-trunk-Commit #2523 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2523/])
    HDFS-3696. Set chunked streaming mode in WebHdfsFileSystem write operations to get around a Java library bug causing OutOfMemoryError. (Revision 1365839)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365839
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
, Integrated in Hadoop-Mapreduce-trunk-Commit #2543 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2543/])
    HDFS-3696. Set chunked streaming mode in WebHdfsFileSystem write operations to get around a Java library bug causing OutOfMemoryError. (Revision 1365839)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365839
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
, h3696_20120724_0.23.patch: for 0.23., h3696_20120724_b-1.patch: for branch-1., Integrated in Hadoop-Hdfs-0.23-Build #325 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/325/])
    svn merge -c -1365843 for reverting HDFS-3696 since the test cannot be compiled. (Revision 1365846)
svn merge -c 1365839 from trunk for HDFS-3696. Set chunked streaming mode in WebHdfsFileSystem write operations to get around a Java library bug causing OutOfMemoryError. (Revision 1365843)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365846
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java

szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365843
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
, Integrated in Hadoop-Hdfs-trunk #1116 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1116/])
    HDFS-3696. Set chunked streaming mode in WebHdfsFileSystem write operations to get around a Java library bug causing OutOfMemoryError. (Revision 1365839)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365839
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
, Integrated in Hadoop-Mapreduce-trunk #1148 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1148/])
    HDFS-3696. Set chunked streaming mode in WebHdfsFileSystem write operations to get around a Java library bug causing OutOfMemoryError. (Revision 1365839)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365839
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
, Comment Thanks for the patch for branch-0.23.  +1 (non-binding) for it. I reviewed the change and ran the tests., Hi Robert, thanks for taking a look., I have committed this to 0.23 and branch-1., Integrated in Hadoop-Hdfs-0.23-Build #326 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/326/])
    HDFS-3696. Set chunked streaming mode in WebHdfsFileSystem write operations to get around a Java library bug causing OutOfMemoryError. (Revision 1366293)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366293
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/WebHdfsTestUtil.java
, Merged to branch-1.1., Due to delays in 1.1.0, incorporated in 1.1.0 from 1.1.1.]