[One reason why this comes up is because of the NFS gateway.  Since the gateway keeps files open for about 10 minutes after the last packet arrives from the client (by default, at least), there are a lot of times when someone copies a file to NFS via the gateway and then moves it.

My approach was just to use inode IDs for all the operations done by a file open for write: {{complete}}, {{addBlock}}, {{fsync}},  {{abandonBlock}}, and {{getAdditionalDataNodes}}.  In the cases where an inode ID was not being passed over the wire, I added one to the protobuf.  This is backwards compatible because the new protobuf fields are optional.  If the inode ID is not present, we fall back on the old behavior of using the full path instead., There's a test for this in HADOOP-9361 which attempts to [rename a file being appended to|https://github.com/steveloughran/hadoop-trunk/blob/stevel/HADOOP-9361-filesystem-contract/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/contract/AbstractAppendContractTest.java#L113]. Presumably that test will pass once this patch has gone through?, bq. There's a test for this in HADOOP-9361 which attempts to rename a file being appended to. Presumably that test will pass once this patch has gone through?

Yeah, I believe that will pass after this patch.

There is also a test included as part of this patch which is HDFS-specific, called {{testLeaseAfterRenameAndRecreate}}, which tests a similar thing.  The HDFS test also looks at some HDFS-specific stuff like leases., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12642289/HDFS-6294.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.TestSymlinkHdfsFileContext
                  org.apache.hadoop.hdfs.server.namenode.TestNamenodeRetryCache
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup
                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotReplication
                  org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks
                  org.apache.hadoop.hdfs.server.namenode.TestMetaSave
                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion
                  org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
                  org.apache.hadoop.hdfs.TestFileCreation
                  org.apache.hadoop.hdfs.server.namenode.TestFSDirectory
                  org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot
                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestNestedSnapshots
                  org.apache.hadoop.hdfs.TestQuota
                  org.apache.hadoop.hdfs.TestFileAppend3
                  org.apache.hadoop.fs.TestHDFSFileContextMainOperations
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
                  org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode
                  org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA
                  org.apache.hadoop.fs.TestSymlinkHdfsFileSystem
                  org.apache.hadoop.hdfs.TestDFSShell
                  org.apache.hadoop.hdfs.web.TestFSMainOperationsWebHdfs
                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot
                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestRenameWithSnapshots
                  org.apache.hadoop.hdfs.server.namenode.TestINodeFile
                  org.apache.hadoop.hdfs.server.namenode.TestHDFSConcat
                  org.apache.hadoop.hdfs.server.namenode.TestFileLimit
                  org.apache.hadoop.cli.TestHDFSCLI
                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport
                  org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks
org.apache.hadoop.hdfs.TestSetrepDecreasing

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6758//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/6758//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6758//console

This message is automatically generated., Do you mean we can support continue writing to files even though are renamed while writing?
Is it a valid/intended behavior?, bq. Do you mean we can support continue writing to files even though are renamed while writing? Is it a valid/intended behavior?
There was a lengthy discussion on this when inode id was added and, IIRC, allowing writes to continue after rename (of itself or one of ancestor dir) was thought to be the way we were heading.  After all, true inode-based file systems work similarly. One difference would be delete, after which no block allocation is allowed.

[~cmccabe]: rename & delete update the paths stored in LeaseManager. It will be nice if we can make it purely inode/inode id based. Rename won't cause any updates in LeaseManager.  Deletion of a large tree might be less efficient, but there may be a way to speed it up.
, bq. There was a lengthy discussion on this when inode id was added and, IIRC, allowing writes to continue after rename (of itself or one of ancestor dir) was thought to be the way we were heading. After all, true inode-based file systems work similarly. One difference would be delete, after which no block allocation is allowed.

Yeah.  This will make us more compatible with other filesystems.  Nearly all filesystems support write after rename.  And certainly it helps with NFS support.

bq. It will be nice if we can make it purely inode/inode id based. Rename won't cause any updates in LeaseManager. Deletion of a large tree might be less efficient, but there may be a way to speed it up.

Recursive deletion needs to recurse into all inodes anyway.  So we could just put the lease object into {{FileUnderConstructionFeature}} and avoid any kind of external lookup.  I would prefer to do this in a follow-up change, since it will make the patches smaller., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12642882/HDFS-6294.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6820//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6820//console

This message is automatically generated., NameNodeRpcServer.java: it looks like you can get rid of the // TODO comment.

TestLease.java:

      Assert.assertEquals(contents1,  DFSTestUtil.readFile(fs3, path2));
      Assert.assertEquals(contents2,  DFSTestUtil.readFile(fs3, path1));
 
There are two spaces after the comma in contentsN,
In the same file, I assume the out.close that you're adding was a random missing close() that you happened upon?

TestSetrep(Inc/Dec)reasing.java: are the timeouts you are adding related to the rest of the changes or just opportunistic/necessary?
, Hey Colin, nice work here, a few comments:

* If a client supplies a fileId, it's unnecessary to also supply a path. We can look it up on the server side, and it's a place for inconsistency. I think we could even remove it from the various ClientProtocol definitions, but would still need to pass a null for the PB field and still have it on the server-side to do the fallback.
* Sort of related, could try pushing the fallback up into NNRpcServer. The fallback logic probably belongs in a FSN helper method anyway. Could precondition check the exclusivity of supplying both a path and an ID here too.
* It'd be cool to do a thorough cleanse of {{src}} and paths in FSN, but the tradeoff there is worse error messages. I'm mixed on that; the path has a lot of good information in it, but cleaning e.g. checkLease to {{checkLease(String holder, INode inode)}} would be much clearer.
* FSN#abandonBlock, good opportunity to fix typo "fle"
* NNRpcServer#fsync, unnecessary TODO?
* Some javadoc needs to be updated, ClientProtocl#abandonBlock, FSNamesystem#fsync, INodesInPath.java:

static INodesInPath fromInode(INode inode) {

I wonder if this method might better be named 'fromINode' to be consistent with INodesInPath and INode.

    int depth = 0, index;

While not incorrect, putting two vars in the same decl feels a little non-standard compared to what I've seen in the rest of the HDFS code. Perhaps break it into two separate decls? final is used inconsistently in the rest of this same method. I think that path and inodes could also take final to be consistent with iip.

FSNamesystem.java:

INodeFile file = checkLease(src, holder, inode, fileId);

final? I know it wasn't decl'd final before, but since you moved it, might as well?

Other than those nits, LGTM. FWIW, +1.
, Thanks for the reviews, guys.

Charles said:

bq. NameNodeRpcServer.java: it looks like you can get rid of the // TODO comment.

Done

bq. There are two spaces after the comma in contentsN,

Removed

bq. In the same file, I assume the out.close that you're adding was a random missing close() that you happened upon?

Yeah, technically I could leave it as-is, but we should close files that we open, and this existing test didn't do that.  I also wanted to verify that there was no exception.

bq. TestSetrep(Inc/Dec)reasing.java: are the timeouts you are adding related to the rest of the changes or just opportunistic/necessary?

They're just nice to have.  It makes Jenkins work better and gives us a stack trace of where the test is hanging, when it hangs.  Two minutes is also a very generous timeout considering this test is usually a few seconds at most.

Andrew said:

bq. If a client supplies a fileId, it's unnecessary to also supply a path. We can look it up on the server side, and it's a place for inconsistency. I think we could even remove it from the  various ClientProtocol definitions, but would still need to pass a null for the PB field and still have it on the server-side to do the fallback.

The issue is backwards compatibility.  We want new clients to be compatible with older servers that will not examine the optional inode ID field, but will only look at the path.  It's also   kind of nice to know what the client is trying to close, if inode lookup fails for some reason.  So while I wouldn't object to a follow-on change which eliminates this (perhaps after a namenode/client negotiation of some kind?), the best thing to do in the short term is probably just leave it as-is.

bq. Sort of related, could try pushing the fallback up into NNRpcServer. The fallback logic probably belongs in a FSN helper method anyway. Could precondition check the exclusivity of        supplying both a path and an ID here too.

{{NameNodeRpcServer}} is a pretty thin shell right now.  I'm not sure if moving more logic up here makes sense, especially given the locking issues.  We'd also have to move the "check for    safe mode, check for retries, etc." boilerplate, which would be quite a refactor.

bq. It'd be cool to do a thorough cleanse of src and paths in FSN, but the tradeoff there is worse error messages. I'm mixed on that; the path has a lot of good information in it, but cleaning e.g. checkLease to checkLease(String holder, INode inode) would be much clearer.

I originally started by doing exactly what you describe with {{checkLease}}.  But then I realized that I needed this function to handle the case when {{inode == null}} (can't find inode ID), or else I'd have to write a lot of boilerplate checking for {{null}} everywhere.  So I added the {{src}} and {{fileId}} parameters so I could throw an informative exception in this case.

bq. FSN#abandonBlock, good opportunity to fix typo "fle"

fixed
bq. NNRpcServer#fsync, unnecessary TODO?

removed, I think Chris saw this one too :)

bq. Some javadoc needs to be updated, ClientProtocl#abandonBlock, FSNamesystem#fsync

updated javadoc for ClientProtocol#abandonBlock, FSNamesystem#fsync

Chris said:
bq. I wonder if this method might better be named 'fromINode' to be consistent with INodesInPath and INode.

Sadly, I can find examples of both "Inode" and "INode" in our code base.  :(
But I think INode is somewhat more common, so I'll change it to {{fromINode}}.

bq. While not incorrect, putting two vars in the same decl feels a little non-standard compared to what I've seen in the rest of the HDFS code

I looked in the Sun style conventions here: http://web.archive.org/web/20090806061850/http://java.sun.com/docs/codeconv/html/CodeConventions.doc.html
Didn't see anything about this... I think it's OK.

bq. final is used inconsistently in the rest of this same method. I think that path and inodes could also take final to be consistent with iip. \[and other final comments\]

done, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12643838/HDFS-6294.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestRenameWhileOpen

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6850//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6850//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12643838/HDFS-6294.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6852//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6852//console

This message is automatically generated., The test failure was the result of a case where a rename occurred, was recorded in the edit log, and then a getAdditionalBlock occurred and was recorded in the edit log.  Then the edit log was not finalized for some reason.  When the NN was next restarted, it saw a getAdditionalBlock for a path which didn't exist (because of the preceding rename) and threw an exception.

To fix this, I now obtain the latest path from the computed inode in getAdditionalBlock and in the other RPCs.  This ensures that what is recorded in the edit log is the latest path, not the path which the file was created with.

I also changed the test to disable edit log finalization, so that we will now always test starting up with an edit log that has the rename and getadditionalblock entries in it., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12643887/HDFS-6294.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 13 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestFileCreation

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6858//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6858//console

This message is automatically generated., Looks like I need to check that the inode object is not null before setting the path based on it., Looks like Jenkins didn't run this last time.  I can't seem to reach the jenkins web ui (and couldn't reach it yesterday either).  So I'll re-upload this and hope Jenkins runs it this time., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12644122/HDFS-6294.005.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 13 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6870//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6870//console

This message is automatically generated., +1 thanks Colin, committed.  thanks, all, SUCCESS: Integrated in Hadoop-Yarn-trunk #560 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/560/])
HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1593634)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/LeaseRenewer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodesInPath.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/ClientNamenodeProtocol.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSClientAdapter.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLease.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRenewer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPersistBlocks.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetrepDecreasing.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetrepIncreasing.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1778 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1778/])
HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1593634)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/LeaseRenewer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodesInPath.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/ClientNamenodeProtocol.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSClientAdapter.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLease.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRenewer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPersistBlocks.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetrepDecreasing.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetrepIncreasing.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1752 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1752/])
HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1593634)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/LeaseRenewer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodesInPath.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/ClientNamenodeProtocol.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSClientAdapter.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLease.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRenewer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPersistBlocks.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetrepDecreasing.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetrepIncreasing.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java
, SUCCESS: Integrated in Hadoop-trunk-Commit #5603 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5603/])
HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1593634)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/LeaseRenewer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/ClientProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodesInPath.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/ClientNamenodeProtocol.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSClientAdapter.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLease.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRenewer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPersistBlocks.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetrepDecreasing.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetrepIncreasing.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java
]