[The mmap() function is used to map between the address space of a process and a file or shared memory object. JVM uses MAP_NORESERVE flag to minimize swap space needed. According to http://man7.org/linux/man-pages/man2/mmap.2.html, no swap space can be reserved for a mapping when MAP_NORESERVE flag is used. When there is not enough swap space available in the system as reported in the test, the write will fail and a SIGBUS or SIGSEGV signal will be sent to the writing process as expected. Increase the swap space will solve the problem unless JVM change its mmap flag., [~xiaoyuyao]: I think you nailed it.  It's the fact that the JVM is using {{MAP_NORESERVE}} that leads to the SIGBUS when physical memory space is exhausted.  Good call.

[~gopalv]: the easiest way to solve this problem is for you to run with ramfs rather than tmpfs . ramfs can't swap, so you won't experience this issue.

We could also take matters into our own hands and do our own mmap, but I'm not sure how we could get a {{MappedByteBuffer}} class out of that.  It might end up being a pretty big change since we'd have to wrap all uses of the mmap (assuming we couldn't use {{MappedByteBuffer}}.)  I'm not sure that this is worth doing.  If you're swapping out the shared memory segments, that is kind of the canary in the coal mine that you don't have enough memory.  Possibly some other process is seriously overloading /dev/shm?, [~cmccabe]: this is a machine without any swap. 

The commit seems to be one of yours, can you explain why this suggests /dev/shm?

https://github.com/apache/hadoop-common/blob/990db99601b28e2a292a0c1322ffe4f334e90c47/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java#L32, bq. Colin Patrick McCabe: this is a machine without any swap.

I took another look at the code, and it looks like we're creating a sparse file by using {{ftruncate}}.  That, in turn, leads to the SIGBUS later when we try to access the offset in the file, and no memory is available to de-sparsify it.  To remedy this, I added a call to {{posix_fallocate}}.  This will lead to the space in memory being allocated at the time we create the shared file descriptor, rather than later when we read from it.

Because you are out of memory, you'll still get a failure... but the failure will happen during allocation, not later, and it will be an exception which is handled cleanly, not a SIGBUS which shuts down the JVM.  See if this patch works for you.

bq. The commit seems to be one of yours, can you explain why this suggests /dev/shm?

The configuration default is in {{/dev/shm}} because that is present on every modern Linux installation.  We always want the shared memory segment FD to be in memory, rather than on disk.  We have to read from this thing prior to every short-circuit read, so it needs to be fast.  ramfs would have been better, but this would require special setup which most users don't want to do right now.  Maybe this will change if we start recommending ramfs for HDFS-5851.   Anyway, ramfs and tmpfs will behave similarly when swap is off, as in your case., Hey Colin. Did you verify that tmpfs supports fallocate going back to old versions? Looking at the kernel git history, it was only added in mid 2012 (e2d12e22c59ce714008aa5266d769f8568d74eac) corresponding to version 3.5. So, I'm not sure if it would be supported on el6 for example (maybe they backported it, maybe not).

Doing a normal posix write() call to write some explicit zeros to the fd might be more portable and shouldn't really have any performance downside., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664433/HDFS-6912.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7772//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7772//console

This message is automatically generated., bq. Hey Colin. Did you verify that tmpfs supports fallocate going back to old versions? Looking at the kernel git history, it was only added in mid 2012 (e2d12e22c59ce714008aa5266d769f8568d74eac) corresponding to version 3.5. So, I'm not sure if it would be supported on el6 for example (maybe they backported it, maybe not).

I believe the glibc {{posix_fallocate}} wrapper falls back to using {{write()}} calls when {{fallocate}} itself is not supported by the kernel.  There is some discussion here:  https://lists.gnu.org/archive/html/bug-coreutils/2009-05/msg00207.html which talks about:

bq. i.e. fall back to using write() as the glibc posix_fallocate() implementation does.

But, I think it's simpler to just use {{write}} here.  Any performance advantage to using {{ftruncate}} + {{fallocate}} is going to be extremely tiny (or nonexistent) since this file is only 8192 bytes.  And {{write}} is much more portable.  So here is a new version that does that., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664475/HDFS-6912.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.io.nativeio.TestSharedFileDescriptorFactory

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7776//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7776//console

This message is automatically generated., The unit test was relying on the file position being 0.  I don't think anything else relies on this (we use mmap to access this) but in v3 of the patch, I made it restore the file position to 0 just for simplicity., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664509/HDFS-6912.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:

                  org.apache.hadoop.ha.TestZKFailoverControllerStress

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7779//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7779//console

This message is automatically generated., +1 LGTM, thanks Colin, Thanks [~cmccabe] &[~andrew.wang], will pull into my branches., SUCCESS: Integrated in Hadoop-Yarn-trunk #682 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/682/])
HDFS-6912. SharedFileDescriptorFactory should not allocate sparse files (cmccabe) (cmccabe: rev 8008f0e8191b1c7adbed96ed4c380208e3a37692)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.c
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1873 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1873/])
HDFS-6912. SharedFileDescriptorFactory should not allocate sparse files (cmccabe) (cmccabe: rev 8008f0e8191b1c7adbed96ed4c380208e3a37692)
* hadoop-common-project/hadoop-common/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.c
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1898 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1898/])
HDFS-6912. SharedFileDescriptorFactory should not allocate sparse files (cmccabe) (cmccabe: rev 8008f0e8191b1c7adbed96ed4c380208e3a37692)
* hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.c
* hadoop-common-project/hadoop-common/CHANGES.txt
]