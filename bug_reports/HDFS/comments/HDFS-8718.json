[Hi [~jianbginglover], do you have ARCHIVE storage policy configured on any files/directories? Also are any of your DataNodes configured with ARCHIVE storage?, No, I have upgraded from 2.5. I have not done any modification on hdfs-site.xml. , Hi [~jianbginglover], 

I think this log must be preceded with some other log message which looks like 
{code} Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=[DISK, ARCHIVE] , storagePolicy={HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true/false) {code}

If possible please share NN logs with may give clue on root cause.

Also, please confirm both the machines {{172.22.49.3 and 172.22.49.5}} are in same rack or not, [~kanaka] 

You are right. The two nodes are not in the same rack, and actually 172.22.49.3 has been configured in one rack without any other nodes.
{code}
===========

bh@TS-BHTEST-01 hadoop $ hdfs dfsadmin -printTopology
Rack: /default/rack_02
   172.22.49.2:50010 (TS-BHTEST-02)
   172.22.49.4:50010 (TS-BHTEST-04)
   172.22.49.5:50010 (TS-BHTEST-05)
   172.22.49.6:50010 (TS-BHTEST-06)
   172.22.49.7:50010 (TS-BHTEST-07)

Rack: /default/rack_03
   172.22.49.3:50010 (TS-BHTEST-03)

{code}
, Meanwhile, there are some DEBUG logs:
{code}
2015-07-09 11:33:05,406 DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to choose from local rack (location = /default/rack_03), retry with the rack of the next replica (location = /default/rack_02)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: 
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:690)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:605)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:511)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:362)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:213)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:110)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3718)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3683)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1407)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1313)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3654)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3606)
        at java.lang.Thread.run(Thread.java:745)
{code}, i met the same problem after upgraded the cluster to 2.7.1, and never config ARCHIVE storage policy.

{code:xml}
2016-04-19 10:20:48,083 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 7 to reach 10 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
...
2016-04-19 10:21:17,184 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 7 to reach 10 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2016-04-19 10:21:17,184 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 7 but only 0 storage types can be selected (replication=10, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK, DISK, DISK, DISK, DISK, DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2016-04-19 10:21:17,184 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 7 to reach 10 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
{code}

above NN log is all about ReplicationMonitor process one {{ReplicationWork}}, it depicts that {{ReplicationWork}} can not choose any proper targets which StorageType should be DISK although traverse all DN of the Cluster. then {{DISK}} is added to {{unavailableStorages}}, the next loop {{ARCHIVE}} is added to {{unavailableStorages}} because there is no ARCHIVE storage. After that throw NotEnoughReplicasException.

The core Question is *WHY it can NOT choose any proper datanode as target in {{ReplicationWork}} successfully, even if there are thousand DNs in the cluster*., hi [~jianbginglover] and [~kanaka], ReplicationMonitor stuck for long time since *Global Lock*, and this caused block replicating could not work as expected. I create new issue [HDFS-10453|https://issues.apache.org/jira/browse/HDFS-10453] to describe this problem in detail and upload patch with solution., +1, Thanks [~hexiaoqiao] for identifying the duplicates and [~jiangbinglover] for reporting this issue.

I'll go ahead and close this Jira as a dup of HDFS-10453.]