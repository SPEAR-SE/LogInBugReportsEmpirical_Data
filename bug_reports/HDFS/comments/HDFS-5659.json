[I think this was fixed with HDFS-5626, Thanks for your comment.
I don't think the issue was fixed because I'm using the trunk after HDFS-5626 was fixed., Thanks for the report, patch attached. Two notes:

* I refactored the PBHelper methods for DatanodeInfo in what I think is a compatible fashion. Ideally there'd only be one convert method, but I didn't want to tease out the behavior there.
* The test blocksize (512) was less than the page size (4096), so it was getting automatically rounded up to the page size on the DN, leading to unexpected numbers. The same issue crops up on the namenode when it comes to quotas and stats; we won't hit our perceived capacity if we're caching a bunch of (n%PAGE_SIZE+1) files because of this fragmentation. I don't think this is a big deal (we're looking at worst case 4k waste per cached file), but it's worth keeping in mind., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619673/hdfs-5659-1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5775//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5775//console

This message is automatically generated., bq. I refactored the PBHelper methods for DatanodeInfo in what I think is a compatible fashion. Ideally there'd only be one convert method, but I didn't want to tease out the behavior there.

This reflects the unfortunate fact that the same protobuf type is used for two rather different things.  I think the convert functions need different names that describe what they're doing.

bq. The test blocksize (512) was less than the page size (4096), so it was getting automatically rounded up to the page size on the DN, leading to unexpected numbers. The same issue crops up on the namenode when it comes to quotas and stats; we won't hit our perceived capacity if we're caching a bunch of (n%PAGE_SIZE+1) files because of this fragmentation. I don't think this is a big deal (we're looking at worst case 4k waste per cached file), but it's worth keeping in mind.

The operating system can't allocate less than 4kb no matter how small the file is.  So our bookkeeping reflects reality... a full 4kb page of physical memory is used up by mlocking one 1 byte file.  I made the block size smaller than the page size deliberately in the test to test some of those odd scenarios.  Probably, we should add a comment to that effect to the test, but let's leave the page size the same so we get coverage., Thanks Colin. Rather than changing the page size back to 512, I added a new PageRounder test to TestFsDatasetCache, which seems like a better place for it. No other changes., {code}
+      LOG.info("XXX: node " + dn.toString() + " cap " + cacheCapacity
+          + " used " + cacheUsed + " rem " + cacheRemaining);
{code}
seems like this got left in.

Like I said earlier, the names of {{PBHelper#convertDatanodeInfo}} and {{PBHelper#convert}} don't suggest what the differences are between them, or what reason I would have for using one rather than the other.  Please, use descriptive names here., bq. Like I said earlier, the names of PBHelper#convertDatanodeInfo and PBHelper#convert don't suggest what the differences are between them, or what reason I would have for using one rather than the other. Please, use descriptive names here.

actually, let's move this refactoring issue to HDFS-5694.  We'll want to do that in branch-2 as well, for one thing., I also would like to defer that refactor, it's probably safe, but it requires a nice code dive to be sure.

New patch attached. I removed the XXX debug print, and also fixed a few CacheAdmin things noticed along the way (a messy exception stacktrace, and being able to specify "never" for add/modify directive)., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620999/hdfs-5659-3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5809//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5809//console

This message is automatically generated., {code}
  private static Expiration parseExpirationString(String ttlString)
      throws IOException {
    Expiration ex = null;
    if (ttlString != null) {
      if (ttlString.equalsIgnoreCase("never")) {
        ex = CacheDirectiveInfo.Expiration.NEVER;
      } else {
        long ttl = DFSUtil.parseRelativeTime(ttlString);
        ex = CacheDirectiveInfo.Expiration.newRelative(ttl);
      }
    }
    return ex;
  }
{code}

why not move the parsing of "never" into {{DFSUtil#parseRelativeTime}}?

+1 once that's addressed, Moving this into {{DFSUtil#parseRelativeTime}} feels a little weird, since it returns a {{long}}, not a caching-specific {{Expiration}} right now. Do we want to choose a more generic special {{long}} value to encode never? Seems simpler to leave this as it is, if you're okay with that., bq. Do we want to choose a more generic special long value to encode never?

We could use {{CacheDirectiveInfo#Expiration#NEVER#getMillis()}}.

bq. Seems simpler to leave this as it is, if you're okay with that.

I would be fine with leaving it as-is, or just using {{NEVER#getMillis}}.  Whichever seems clearer., Thanks, I'd like to leave it as is. Will commit shortly., Committed to trunk., SUCCESS: Integrated in Hadoop-trunk-Commit #4950 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4950/])
HDFS-5659. dfsadmin -report doesn't output cache information properly. Contributed by Andrew Wang. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1554893)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #441 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/441/])
HDFS-5659. dfsadmin -report doesn't output cache information properly. Contributed by Andrew Wang. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1554893)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1633 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1633/])
HDFS-5659. dfsadmin -report doesn't output cache information properly. Contributed by Andrew Wang. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1554893)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1658 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1658/])
HDFS-5659. dfsadmin -report doesn't output cache information properly. Contributed by Andrew Wang. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1554893)
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java
, Closing tickets that are already part of a release.]