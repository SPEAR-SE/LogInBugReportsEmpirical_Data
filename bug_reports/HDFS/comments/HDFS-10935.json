[[~drankye] can you please take a look?, Sure, Chuang. I will look at it when back to office. Thanks., It's not Mac only. I repeated it on my Linux desktop. [~Sammi], would you help with this? Thanks., Hi, Kai, sure, I will look into it. , TestFileChecksum tests all passed on my Linux server. My JDK is "1.8.0_65".   

Hi  Wei-Chiu Chuang and Kai, can I know your JAVA version? , [~drankye] that's interesting. Thanks for confirming with me.
I am on JDK 1.8.0_91 on Mac OS X Yosemite 10.10.5., I ran the tests in multiple JDK versions ranging from JDK 1.8.0_05 to JDK 1.8.0_102 on CentOS 6.5 (I tested both with/without native libisal library), and I got the following error for all tests in TestFileChecksum.:

Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.46 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestFileChecksum
testStripedFileChecksumWithMissedDataBlocksRangeQuery5(org.apache.hadoop.hdfs.TestFileChecksum)  Time elapsed: 7.269 sec  <<< ERROR!
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:714)
        at io.netty.util.concurrent.ThreadPerTaskExecutor.execute(ThreadPerTaskExecutor.java:33)
        at io.netty.util.concurrent.SingleThreadEventExecutor.doStartThread(SingleThreadEventExecutor.java:692)
        at io.netty.util.concurrent.SingleThreadEventExecutor.shutdownGracefully(SingleThreadEventExecutor.java:499)
        at io.netty.util.concurrent.MultithreadEventExecutorGroup.shutdownGracefully(MultithreadEventExecutorGroup.java:160)
        at io.netty.util.concurrent.AbstractEventExecutorGroup.shutdownGracefully(AbstractEventExecutorGroup.java:70)
        at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.close(DatanodeHttpServer.java:259)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:1932)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:1985)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1962)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1936)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1929)
        at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:870)
        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:491)
        at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:450)
        at org.apache.hadoop.hdfs.TestFileChecksum.setup(TestFileChecksum.java:78)

Maybe it's just my env issue. Can't tell., [~Sammi] did you see NPE like the following in your tests? I am seeing the NPE which is reproducible consistently.

{quote}
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.DataNode.bpRegistrationSucceeded(DataNode.java:1498)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.registrationSucceeded(BPOfferService.java:361)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:749)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:279)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:782)
{quote}, Hi Wei-Chiu Chuang, "java.lang.OutOfMemoryError" indicate that JVM run out of memory. I would suggest you increase JVM heap memory using "-Xmx<size>" option, and then run the test case again., Hi Wei-CHiu, I'd like to know how you launch the TestFileChecksum test case, beside the IDE(such as Eclipse) or directly from the Linux command console? 
And I don't see any NullPointerException in my tests. I had one of my co-worker run the TestFileChecksum. The test also passed in his environment.  , Hi Sammi,
I ran the tests on Mac in IntelliJ and in command console. And I ran them in command console on Linux.

The NPE appeared twice and then it disappeared in my env so maybe it's not related., I increase heap size of the surefire plugin to 10240mb. It ran longer but still failed with the same error, and the log is full of "Too many open files" error. I also set ulimit -u 10240 to increase my max user threads but still no go. This machine has 64GB memory. The code I'm running is compiled with ISA-L native lib., But I had no problem running other tests. :(, Don't worry! I am doing investigation using Kai's environment since the problem is reproducible in his environment. , I have two local Hadoop repo, one is built with native isa-l lib and the other is not. The one without the native lib failed while the other passed. It seems java based codec doesn't work as expected.

I also have other tests where if I specifically corrupt 1, 2 or 3 strips out of 9, it does not recover if hadoop is built without native lib., Bump priority to critical as it seems to generate corrupt data., Hi [~jojochuang], it's found out that this "Checksum mismatches" issue is because the second checksum(the one with block reconstruct) is wrong.  The checksum is wrong is not because the reconstructed block content is wrong. It's because the code reuses an existing {{DataChecksum}} object while forgets to reset this checksum object state to initial state before using it.  So the checksum is contaminated by its old state. I have upload the patch to address this issue. , Initial patch, Hi [~Sammi] that's great finding!

I am still curious why the test only fails without native ISA-L library. Would it be possible to add a test to force tests to use non-native codec? Is that the right way to add a regression test?
[~drankye] [~zhz] would you like to review this patch?

Thanks, Thanks Sammi for the patch. It sounds reasonable.

Sure WeiChiu. Could you help verify the fix works for you or not in your repos? Thanks in advance., Cool. I've verified the patch passed TestFileChecksum tests in my Mac. This is great!, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  9m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  3s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  7s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 47m  2s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 70m  8s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Timed out junit tests | org.apache.hadoop.hdfs.server.datanode.TestHSync |
|   | org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade |
|   | org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyWriter |
|   | org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation |
|   | org.apache.hadoop.hdfs.server.datanode.TestFsDatasetCache |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Issue | HDFS-10935 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12835079/HDFS-10935-v1.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux fb13960ae5a3 3.13.0-92-generic #139-Ubuntu SMP Tue Jun 28 20:42:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / dbd2057 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/17271/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/17271/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17271/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks [~jojochuang] for helping verify the fix.

The patch LGTM and +1. Will commit it shortly., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #10681 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/10681/])
HDFS-10935. TestFileChecksum fails in some cases. Contributed by Sammi (kai.zheng: rev 287effff9327450240d65e27e31bed2649a7a100)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumReconstructor.java
, Committed to trunk. Thanks [~jojochuang] for reporting/verifying this and [~Sammi] for digging into the fix and contribution. , Did we file a follow-on JIRA to test both the Java coder as well as ISA-L in the unit tests? This is important for coverage., Correct, Andrew. I forgot to address that concern. Filing a follow-up jira now.]