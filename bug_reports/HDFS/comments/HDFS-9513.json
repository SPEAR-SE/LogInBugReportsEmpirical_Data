[  usually,can get the storageInfo by block,but in the case,it's seems to not work.
  the DatanodeManager#getDatanodeStorageInfos called by 
    1. FSNameSystem#commitBlockSynchronization
    2. FSNameSystem#getAdditionalDataNode
    3. FSNameSystem#updatePipelingInternal
  the first method is called from datanodes,it's compatibility. 
  when add a new block,the block with storageInfo will store at inodefile/blockMap/editLog,but if add new datanode for the pipeline,the addtional datanode and storageInfo not store,because the client need to try transfer the RWB block.
   so when call FSNameSystem#updatePipelingInternal,the old block's storageInfo is not enough,so can't recovery the storageInfo anyway.
   and if client is older than 2.3.0,the storageInfo is not useful,although the NN choose  locatedBlock with storageInfo ,but client wirte to DN not pass the storageInfoId

  , This issue is already reported in HDFS-6481 and its throwing HadoopIllegalArgmentException instead of AIOBException.

I think it would be good to provide a way to workaround this problem for the older clients rather than rejecting their requests straightway considering the cluster size (8000+) mentioned here to upgrade.
Patch provided might work as workaround for older clients. Any thoughts [~szetszwo]/[~arpitagarwal]

[~Deng FEI],I have few comments about the patch.

1. DatanodeDescriptor#getPerferedStorageInfo(), might not be required. instead one DUMMY storage, May be first storage of DN, can be chosen.

Currently {{DatanodeManager#getDatanodeStorageInfos(..)}} is used for 3 calls, getAdditionalDatanode(), updatePipeline(), commitBlockSynchronization(). In all these operations, Might get the same problem in case of old clients. Since client dont understand storages, we need not worry to select storage.
   a. getAdditionalDatanode(), updatePipeline() will be calls from clients directly. But only updatePipeline results will be stored as is. Even then, once the Incremental Block reports is received, these storage will be updated to have proper details in namenode.
   b. commitBlockSynchronization() will be from datanodes, and even in this case, it might not lead to this workaround as it will have new targets.

Updated code can look like this
{code}
if (old) {
// CHOOSE First storage as dummy for writing support of Old client as a
// workaround for HDFS-9513.
// Later when the block report comes, actual storage Id will be
// restored in blocks map.
storages[i] = dd.getStorageInfos()[0];
} else {
storages[i] = dd.getStorageInfo(storageIDs[i]);
}
{code}
2. Also, may be we can have a config for this workaround and only use when required. ? 

3. Can you post a patch for trunk and latest branch-2.7, with HDFS-6481 included? Can throw exception, in case of config disabled, or request is not from old client., Thanks for reporting the problem and working on it.

For commitBlockSynchronization and getAdditionalDatanode, NN has storageIDs stored in BlockUnderConstructionFeature.  So we should be able to get storageIDs from there.

For updatePipeline, NN has storageIDs stored in BlockUnderConstructionFeature except the newly added storageID.  We may choose a random storageID or the first storageID for this case.
, yes, i will work on it,later upload new patch, yes, i will work on it,later upload new patch, yes, i will work on it,later upload new patch,  DataNode#commitBlockSynchronization has storageInfos in old version which lower than 2.3.0,it's compatible., upload for trunk., [~szetszwo]please do a review if you have time,it's run our cluster for some months.And normally, datanodes will follow namenode's version,it's compatible yet., [~Deng FEI] [~szetszwo] [~brahmareddy]  i got similar problem,our hdfs client has been using 2.2.0 but hdfs cluster version is 2.6.0,is there any progress for this issue., [~duyanghao] You can refer to the latest patch,it works  , [~Deng FEI]  What do you mean by latest patch?Could you directly give the url?]