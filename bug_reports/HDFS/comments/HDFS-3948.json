[Here's the full backtrace:

{noformat}
testNamenodeRestart(org.apache.hadoop.hdfs.web.TestWebHDFS)  Time elapsed: 70.879 sec  <<< FAILURE!
java.lang.AssertionError: There are 1 exception(s):
  Exception 0: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RemoteException): bad state: CLOSED
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logEdit(FSEditLog.java:357)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logGenerationStamp(FSEditLog.java:777)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStamp(FSNamesystem.java:4719)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.allocateBlock(FSNamesystem.java:2434)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2191)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:471)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:297)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44080)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:898)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1693)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1689)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1687)

        at org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:166)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:303)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$500(WebHdfsFileSystem.java:110)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$1.close(WebHdfsFileSystem.java:704)
        at org.apache.hadoop.hdfs.TestDFSClientRetries$4.run(TestDFSClientRetries.java:844)
        at java.lang.Thread.run(Thread.java:662)

        at org.junit.Assert.fail(Assert.java:91)
        at org.apache.hadoop.hdfs.TestDFSClientRetries.assertEmpty(TestDFSClientRetries.java:986)
        at org.apache.hadoop.hdfs.TestDFSClientRetries.namenodeRestartTest(TestDFSClientRetries.java:910)

        at org.apache.hadoop.hdfs.web.TestWebHDFS.testNamenodeRestart(TestWebHDFS.java:209)
{noformat}, Also got this exception in HDFS-3616 and HDFS-4067. After checking the code, I guess this exception may be caused because of this process:

1. A FSDataOutputStream instance (out4) is created through WebHdfsFileSystem#create, in order to create and write a new file.

2. The request is redirected to a DN, where DFSClient#create is called to create the file in NN through RPC.

3. At this time, the test has called MiniDfsCluster#shutdownNameNode, and in NameNode#stop(), the FSNamesystem has been shutdown (where the FSEditLog will be close) but the RPCServer has not been closed yet.

4. The RPC request from DN is sent to NN and FSEditLog#logEdit is called for the creation. But at this time the FSEditLog has already been closed and FSEditLog#editLogStream has been set to null.

Therefore, if the assertion is enabled, a "bad state: CLOSED" will be returned to client finally (the case in HDFS-3948); if the assertion is not enabled, because FSEditLog#editLogStream has been set to null, a NPE will be returned as reported in HDFS-3822.

The attached patch can regenerate the exception., Correction: HDFS-3822's NPE should be caused by BlockManager race, as Eli commented., The prior patch actually generates the exception while NN executing FSNameSystem#startFile. The new patch can generate the same exception while NN executing FSNamesystem#allocateBlock (the same with the one reported in HDFS-3948)., Initial patch to fix the testcase. Because webhdfs does not support hflush, it is difficult to avoid the race between webhdfs@DN's writing and NN's shutdown. Thus in this patch, I close the corresponding FSDataOutputStream (instead of calling its hflush) when the testcase is run for webhdfs., +1 patch looks good., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12550724/HDFS-3948.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3397//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3397//console

This message is automatically generated., I have committed this.  Thanks, Jing!, Integrated in Hadoop-trunk-Commit #2929 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/2929/])
    HDFS-3948. Do not use hflush in TestWebHDFS.testNamenodeRestart() since the out stream returned by WebHdfsFileSystem does not support it. Contributed by Jing Zhao (Revision 1402270)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1402270
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java
, Integrated in Hadoop-Yarn-trunk #17 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/17/])
    HDFS-3948. Do not use hflush in TestWebHDFS.testNamenodeRestart() since the out stream returned by WebHdfsFileSystem does not support it. Contributed by Jing Zhao (Revision 1402270)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1402270
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java
, Integrated in Hadoop-Hdfs-trunk #1207 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1207/])
    HDFS-3948. Do not use hflush in TestWebHDFS.testNamenodeRestart() since the out stream returned by WebHdfsFileSystem does not support it. Contributed by Jing Zhao (Revision 1402270)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1402270
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java
, Integrated in Hadoop-Mapreduce-trunk #1237 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1237/])
    HDFS-3948. Do not use hflush in TestWebHDFS.testNamenodeRestart() since the out stream returned by WebHdfsFileSystem does not support it. Contributed by Jing Zhao (Revision 1402270)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1402270
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java
]