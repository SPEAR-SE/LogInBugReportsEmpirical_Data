[Historically, that's intentional.

Instead, the expected movement is to pull data to the secure cluster from the insecure cluster., Thanks a lot [~aw]! Would you please share some more information about the recommended way to pull data from insecure cluster to the secure cluster without using distcp? thanks.

, You should still be able to use distcp, you just have to run it on the secure cluster with the source being the insecure cluster. (and if they are incompatible versions, use WebHDFS!) If it doesn't work, that's a regression (HDFS-3905 and related)., Thanks Allen, sorry I did not describe it clear earlier, what you described is exactly what I was trying to report here., Also seeing this in 2.3.0 so it looks like it's been broken for a while!

With hdfs://
{noformat}
$ hadoop distcp -i /tmp/motd hdfs://insecure-namenode:9000/tmp/motd
14/07/30 23:45:20 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=true, maxMaps=20, sslConfigurationFile='null', copyStrategy='uniformsize', sourceFileListing=null, sourcePaths=[/tmp/motd], targetPath=hdfs://insecure-namenode:9000/tmp/motd}
14/07/30 23:45:21 INFO client.RMProxy: Connecting to ResourceManager at secure-resourcemanager/xxx.xxx.xxx.xxx:8032
14/07/30 23:45:22 ERROR tools.DistCp: Exception encountered 
java.io.IOException: Failed on local exception: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.; Host Details : local host is: "secure-gateway/xxx.xxx.xxx.xxx"; destination host is: "insecure-namenode":9000; 
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        at org.apache.hadoop.ipc.Client.call(Client.java:1410)
        at org.apache.hadoop.ipc.Client.call(Client.java:1359)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:671)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1746)
        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1112)
        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1108)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1108)
        at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1425)
        at org.apache.hadoop.tools.SimpleCopyListing.validatePaths(SimpleCopyListing.java:69)
        at org.apache.hadoop.tools.CopyListing.buildListing(CopyListing.java:79)
        at org.apache.hadoop.tools.GlobbedCopyListing.doBuildListing(GlobbedCopyListing.java:90)
        at org.apache.hadoop.tools.CopyListing.buildListing(CopyListing.java:80)
        at org.apache.hadoop.tools.DistCp.createInputFileListing(DistCp.java:327)
        at org.apache.hadoop.tools.DistCp.execute(DistCp.java:151)
        at org.apache.hadoop.tools.DistCp.run(DistCp.java:118)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.tools.DistCp.main(DistCp.java:375)
Caused by: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:734)
        at org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)
        at org.apache.hadoop.ipc.Client.call(Client.java:1377)
        ... 26 more
{noformat}

With webhdfs://
{noformat}
$ hadoop distcp -i /tmp/motd webhdfs://insecure-namenode:50070/tmp/motd
14/07/31 00:27:42 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=true, maxMaps=20, sslConfigurationFile='null', copyStrategy='uniformsize', sourceFileListing=null, sourcePaths=[/tmp/motd], targetPath=webhdfs://insecure-namenode:50070/tmp/motd}
14/07/31 00:27:42 INFO client.RMProxy: Connecting to ResourceManager at secure-resourcemanager/xxx.xxx.xxx.xxx:8032
14/07/31 00:27:44 ERROR tools.DistCp: Exception encountered 
java.io.IOException: Failed to get the token for tthompso, user=tthompso
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:341)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$600(WebHdfsFileSystem.java:105)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.shouldRetry(WebHdfsFileSystem.java:573)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:539)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run(WebHdfsFileSystem.java:424)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:953)
	at org.apache.hadoop.hdfs.web.TokenAspect.ensureTokenInitialized(TokenAspect.java:143)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:227)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getAuthParameters(WebHdfsFileSystem.java:381)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toUrl(WebHdfsFileSystem.java:402)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathRunner.getUrl(WebHdfsFileSystem.java:652)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.init(WebHdfsFileSystem.java:485)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:531)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run(WebHdfsFileSystem.java:424)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getHdfsFileStatus(WebHdfsFileSystem.java:678)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getFileStatus(WebHdfsFileSystem.java:689)
	at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1425)
	at org.apache.hadoop.tools.SimpleCopyListing.validatePaths(SimpleCopyListing.java:69)
	at org.apache.hadoop.tools.CopyListing.buildListing(CopyListing.java:79)
	at org.apache.hadoop.tools.GlobbedCopyListing.doBuildListing(GlobbedCopyListing.java:90)
	at org.apache.hadoop.tools.CopyListing.buildListing(CopyListing.java:80)
	at org.apache.hadoop.tools.DistCp.createInputFileListing(DistCp.java:327)
	at org.apache.hadoop.tools.DistCp.execute(DistCp.java:151)
	at org.apache.hadoop.tools.DistCp.run(DistCp.java:118)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.tools.DistCp.main(DistCp.java:375)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Failed to get the token for tthompso, user=tthompso
	at org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:157)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:318)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$700(WebHdfsFileSystem.java:105)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.getResponse(WebHdfsFileSystem.java:628)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:535)
	... 22 more
{noformat}, Hi [~tthompso]
Thanks for your input. The scenarios you described seem to copy from secure to insecure cluster, which is reported as HADOOP-10016 etc, that are still open issues. What I'm trying to capture is when the copy src is insecure cluster and the copy destination is secure cluster. Thanks.
, Yes but HADOOP-10016 is H1 (secure) -> H2 (insecure) (which on that note, I've also tried and it doesn't work).  This shows that secure talking to insecure is flat out broken, source or destination., Yes, indeed HADOOP-10016 is still open and both directions are broken. Thanks for trying.



 


, Submit patch 001 for webhdfs in trunk. Will add hftp fix for branch-2 later. 

Tested with real clusters to see that it fails before the fix and works after the fix. Thanks for reviewing.


 , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12659481/HDFS-6776.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7540//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/7540//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7540//console

This message is automatically generated., Version 002 to address test failure.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12659511/HDFS-6776.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7542//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/7542//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7542//console

This message is automatically generated., Version 003 to address findbugs issue.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12659555/HDFS-6776.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7548//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7548//console

This message is automatically generated., The posted patch also supports distcp from secure cluster to insecure cluster (as long as both clusters are hadoop 2, and issue the command from secure cluster side). 
, Sorry, but this patch is completely wrong.
# If security is enabled and an {{IOException}} happens for any reason - transient or legit - while acquiring a token, the client will continue to "work" because of spnego but if a job is submitted the tasks will all fail due to no token.
# Webhdfs should be using the same insecure fallback policy as RPC.
# Insecure RPC services return null if a token is requested.  Like DFSClient, the webhdfs client should be able to handle that condition instead of throwing the exception you see.
# Issuing a malformed OPEN call is not ok...
# Although irrelevant in like of the above, connection.connect() isn't doing what you think.  It proved the client could open a connection and send the request.  It doesn't prove the server allowed/authenticated the request.  The you read the response, the server should have been angry you issued an invalid open., HI [~daryn], thank you so much for the very helpful comments. I will look into addressing them in next revision.
, Hi [~daryn],

Thanks a lot for your earlier review and sorry for the delaying in addressing them, got some other critical issue to handle.

I just uploaded a new revision. This version tries to catch the null token case in a different way. I wish there is a NullTokenException type, right now I check the message content to see if the token is null.  If necessary, I can possibly introduce a new exception type, however, that means incompatibility.

Would you please help taking another look to see if this fix makes sense? Thanks.






, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12661931/HDFS-6776.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.security.token.delegation.web.TestWebDelegationToken
                  org.apache.hadoop.ipc.TestDecayRpcScheduler
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestRollingUpgradeRollback
org.apache.hadoop.hdfs.tools.TestDFSHAAdminMiniCluster
org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics
org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions
org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings
org.apache.hadoop.hdfs.TestHDFSServerPorts

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7639//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7639//console

This message is automatically generated., I'd like to elaborate a bit how my latest patch addressed Daryn's comments:
{quote}
    If security is enabled and an IOException happens for any reason - transient or legit - while acquiring a token, the client will continue to "work" because of spnego but if a job is submitted the tasks will all fail due to no token.
{quote}
With the latest patch (004), only the IOException thrown by DelegationTokenSecretManager.createCredentials}} method WHEN the token is null is interpreted as "the cluster is insecure". Because the null token is only returned by FSNamesystem when no secret manager is running (insecure cluster).  With this change, hopefully all other comments are addressed.

The patch currently parses the IOException message to detect null token. One other option is to let the server return null delegation token instead of throwing exception. Another option is to introduce a NullTokenException. 

Thanks.
, I ran the failed tests locally several times and don't see them fail. Uploading the same patch and try again.
Thanks.

  , {{NullTokenMsgHeader}} constant name should be all capitals.

I'm not sure I like looking for a string occurrence in the IOException message to detect the issue. I thought WebHdfs was recreating exceptions on the client side but it doesn't seem the case for these DT calls.
, Thanks a lot [~tucu00]! I will address your comments in next rev.

I'm currently using message parsing to detect null token returned from server due to lack of right exception. There is an advantage of this fix: we only need to patch secure cluster side, and it will work. To introduce a new exception means compatibility issue, if we decide to do so, we can file a follow-up jira for release 3.0? Thanks.

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662086/HDFS-6776.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestHDFSServerPorts
org.apache.hadoop.hdfs.server.datanode.TestFsDatasetCache
org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics
org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions
org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA
org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7647//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7647//console

This message is automatically generated., Uploaded 005 to address Alejandro's first comment.  In addition I replaced "contains" with "startsWith".
, Upload version 006 that introduces NullToken exception, which hopefully is better than earlier versions that uses msg parsing.

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662349/HDFS-6776.005.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestHDFSServerPorts
org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics
org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions
org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA
org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7652//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7652//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662355/HDFS-6776.006.NullToken.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl
                  org.apache.hadoop.hdfs.server.namenode.ha.TestDFSZKFailoverController

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings
org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions
org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics
org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA
org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithNodeGroup
org.apache.hadoop.hdfs.TestHDFSServerPorts

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7654//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7654//console

This message is automatically generated., I investigated the test failures and identified that they are caused by the recent commit of HDFS-6783, for which a new fix will be committed soon. 

Patch 006 introduces NullToken Exception, and is a more robust solution. Hi [~tucu00] and [~daryn], thanks for your earlier review, I'd really appreciate if you can take a look at 006. thanks a lot.
, Now the HDFS-6783 addendum fix is in trunk, submitting HDFS-6776.006.NullToken.patch again for a new test run.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662514/HDFS-6776.006.NullToken.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.ha.TestActiveStandbyElector
                  org.apache.hadoop.ha.TestZKFailoverController
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7665//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7665//console

This message is automatically generated., [~yzhangal], how about  the following:

* introduce a new exception on the server side 
* on the client side, catch this new exception (this will work against a cluster with the fix) and catch IOException looking for the string occurrence in the message (this will work against a cluster without the fix).

BTW, testcases seem unrelated., Thanks a lot Alejandro! What you suggested is a combined approach of 005 and 006 revisions I uploaded. I will have a revised patch later today. 
Hi [~daryn], it would be great if you could help taking a look, and your advice here is very much appreciated. Thanks.


, Hi [~tucu00] and [~daryn],

I uploaded version 007 to address Alejandro's last comment. Basically it's a combined solution of 005 and 006. Thanks for reviewing.
, latest patch  looks good to me just a minor thing:
Can you please add a comment in the catch of the IOException indicating why we are doing that (to support older clusters that do not throw the NullToken?

+1 from my side after this. 

[~daryn], are you OK with the current approach?, I'm unsure that fixing on the filesystem itself is the right approach to take. Based on the above comments (and please correct me if I'm wrong), I'm assuming that distcp is pulling data from a secure cluster to an insecure cluster.

# What happens if the {{WebHdfsFileSystem}} intends to connect to a secure cluster but the attacker has somehow disabled the security of the cluster or successfully launch a MITM attack which keep returning {{NullToken}}? Instead of ignoring the failures, I think that {{WebHdfsFileSystem}} should fail explicitly because that the system is compromised. This is important as copying from a secure cluster to an insecure cluster can unintentionally breach the confidentiality.

On the implementation:

# It looks like that {{WebHdfsFileSystem}} will issue a {{GET_DELEGATION_TOKEN}} request for each request since in catching {{NullToken}} nullifies {{delegationToken}}, which significantly affects the performance. 

I have encountered this issue in production. What I have done is to put the fix on the server side instead of the client side. I asked the NN of the insecure cluster to issue a dummy token, which works across all filesystems. That way at the very least the user has to be informed instead of allowing the data silently flowing from secure to insecure clusters., Thanks Alejandro. Uploaded version 008 to address the last comment of yours.
 , HI [~wheat9],

Thanks for your review and comments. The main goal of this jira is to support pull data from insecure cluster to secure cluster, which is not the same as you assumed. You can only issue the distcp command at the secure cluster side. However, it also works when you try to copy data from secure cluster to insure cluster, as long as you issue the command from the secure cluster side. So I think your comment 1 is not relevant here. Please correct me if I'm wrong.

About the comment 2, canRefreshDelegationToken is set to false for insecure cluster, thus WebHdfsFileSystem will not issue a GET_DELEGAtTION_TOKEN request for each request. See method {{protected synchronized Token<?> getDelegationToken() }}. 

Thanks.

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662862/HDFS-6776.008.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.crypto.key.TestValueQueue
                  org.apache.hadoop.ipc.TestIPC
                  org.apache.hadoop.ipc.TestCallQueueManager
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestIsMethodSupported

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7685//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7685//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662862/HDFS-6776.008.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.crypto.key.TestValueQueue
                  org.apache.hadoop.ipc.TestDecayRpcScheduler
                  org.apache.hadoop.ipc.TestCallQueueManager
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7687//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7687//console

This message is automatically generated., bq. So I think your comment 1 is not relevant here.

As pointed out that distcp has to be initiated from the secure cluster, but with this approach distcp / webhdfsfilesystem silently succeeds even if the other side is an insecure cluster. It is undesirable since it can easily lead to security incident.

bq. canRefreshDelegationToken is set to false for insecure cluster, thus WebHdfsFileSystem will not issue a GET_DELEGAtTION_TOKEN request for each request. See method {{protected synchronized Token<?> getDelegationToken() }}.

As distcp is always started at the secure cluster, which means that the {{WebHdfsFileSystem}} object is created inside the secure cluster, thus {{canRefreshDelegationToken}} should be true.
, HI [~wheat9],

I will address your second comment later. 

For your first one (distcp / webhdfsfilesystem silently succeeds), that's a good point. Right now, after security is examined, then there is the permission control, whoever issue the distcp command need to have right permission. Do you think we should disable copying stuff from secure cluster to insecure cluster, or to have another level of control? Thanks.
, [~wheat9]. for your second comment,  please look at WebHdfsSystem.getDelegationToken(), 
{code}
        if (token != null) {
          LOG.debug("Fetched new token: " + token);
        } else { // security is disabled
          canRefreshDelegationToken = false;
        }
{code}

Once a null token is taken, then canRefreshDelegationToken is set to false, no longer need to issue GET_DELEGATION_TOKEN request.

Thanks.
, BTW, the test failures of the last runs appear to be flaky.
, HI [~wheat9], may I know if my replies addressed your comments? thanks.
, bq. Right now, after security is examined, then there is the permission control, whoever issue the distcp command need to have right permission. Do you think we should disable copying stuff from secure cluster to insecure cluster, or to have another level of control? Thanks.

I'm not sure what you refer to. Copying between secure and insecure clusters is a valid use case. What I have been saying that {{WebHdfsFileSystem}} is not the right place to change since the use case and having a security breach becomes indistinguishable.
For this use case you might need to look at changing distcp or to ask the insecure cluster to issue dummy token.
, HI [~wheat9],  thanks for the elaboration. I assume you are talking about 
{quote}
What happens if the WebHdfsFileSystem intends to connect to a secure cluster but the attacker has somehow disabled the security of the cluster or successfully launch a MITM attack which keep returning NullToken? Instead of ignoring the failures, I think that WebHdfsFileSystem should fail explicitly because that the system is compromised. This is important as copying from a secure cluster to an insecure cluster can unintentionally breach the confidentiality.
{quote}
If secure HDFS can be attacked to return NullToken all the time, I wonder whether it can also be attacked to return a dummy token? 

Hi [~daryn] and [~tucu00], would you please comment? many thanks.
, HI [~wheat9], I would like share some more thoughts in addition to my last reply.

If the attacker has somehow disabled the security of the cluster, that means the cluster is compromised, damage could happen. I certainly agree that great effort should be taken to avoid the damage here.  But can WebHdfsFileSysem really tell whether it's compromised or not? that's my doubt. My previous reply was, if attacker can make the cluster keep return NullToken, it can make the cluster the return dummy token too. 

If there is a good way to detect whether a cluster is compromised easily, we should have a demon running somewhere all the time to do the checking constantly and report the situation to administrator immediately if it happens, instead of waiting for someone to issue a distcp command and catch it. 

My point is, we certainly should try our best to prevent the cluster from being attacked. Returning NullToken (notice here we are throwing an NullToken exception rather than returning null Token) or dummyToken does not seem to make much difference here. 

This is my limited understanding. More discussion is welcome.

Thanks.
, bq. Returning NullToken (notice here we are throwing an NullToken exception rather than returning null Token) or dummyToken does not seem to make much difference here.

The key difference is whether the user is aware of that there are potential security issues.

Note that the users know some information about the remote cluster, for example, whether the remote cluster is secure or not. If the user is connecting to a insecure cluster, he / she has to have the consent because there could be security concerns in this use case. Any solutions have to meet this requirement.

Just for references, if you looks at how distcp works when copying between secure / insecure clusters using rpc (i.e. {{hdfs://}}), there is configuration to tweak whether connecting to insecure clusters is allowed. Users can reject these connections for maximum security. If distcp over webhdfs / swebhdfs between secure / insecure works, then the user should be able to specify the same security requirement.

There are many ways to approach this requirement. For example, maybe you can add a parameter in distcp, or add a new configuration so that the remote cluster can return a dummy token? I think this requirement has to be addressed before this patch can go in., HI [~wheat9],

Thanks for the more info. What about I add a config similar like the RPC one you mentioned
{code}
{
this.fallbackAllowed = conf.getBoolean(CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,
        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);
{code}
and take NullToken exception as insecure only when the fallback is allowed?

We can make the new property WEB_CLIENT_FALLACK_TO_SIMPLE_AUTH_ALLOWED_KEY.

Now with this config introduced, and since we dedicate NullToken exception to be thrown when insecure cluster is asked for delegation token,  there is not much functionality difference between NullToken or dummy token. 

Agree?

Thanks.

, bq. Now with this config introduced, and since we dedicate NullToken exception to be thrown when insecure cluster is asked for delegation token, there is not much functionality difference between NullToken or dummy token.

WebHdfsFileSystem is the wrong place to have this configuration. If distcp fails to work with other filesystems that have the same issue, are you modifying all of them that get delegation tokens from insecure clusters and throwing NullToken for every possible filesystem implementation?

Just to clarify, I'm not necessarily pushing the dummy token proposal. It is just something that I've tried and worked. Can you please look at whether you can fix distcp? To me this is the right place to fix, though it might be nontrivial because yarn / mr require DTs as well., Hello [~wheat9],

WebHdfsFileSystem, HftpFileSystem etc each has its own method to get delegation (derived from the FileSystem.getDelegationToken method). I do see that the different file systems need to fixed to handle null token.

I think it'd preferred for each file system to have its own property similar like IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY, thus we have a granularity control on each file system.

Distcp is just a mapreduce job that access multiple filesystems via their APIs, other mapreduce jobs might have the same need and they would have similar problem. Basically a files ystem need to tell the client whether it's secure or not. The control is up to the the file system, not the client (distcp in this case). 

Just my thoughts.

Thanks.

, Addition to my previous thoughts - [~wheat9]:

This jira is about not being able to pull data from insecure HDFS cluster into secure HDFS cluster. You pointed out how "distcp hdfs:// hdfs://" current works with fallback controlled by the FALLBACK property, which is quite helpful here. Given that how "hdfs://" works, we are applying the same property to "webhdfs://". Only a couple of more (http, hftp@branch-2) that I expect we need similar fix (may I know what others are in your mind?). 

As you mentioned, another alternative is to fix Distcp but it'd be nontrivial (For this alternative, we still need to decide where the fallback property need to be defined. I think the property should be defined in the file system side, you can comment on my previous comment). 

For general problems, we tend to find simple solutions rather than complicated solutions.  If we don't have many places to fix with a simple solution, why would we choose a more complicated solution? I think you meant if we fix on the distcp side, then adding a new filesystem won't need a fix for this issue. Am I understanding you correctly? If not, would you please provide more info? Otherwise, I can also argue that with my proposed patch, adding new application like distcp won't need a fix for this issue, plus the proposed patch is simpler.

Thanks.
, bq. we are applying the same property to "webhdfs://". Only a couple of more (http, hftp@branch-2) that I expect we need similar fix (may I know what others are in your mind?).

The fallback happens in the negotiation phase. I think other filesystems need to have negotiation between clients and server if this is the route to take. It can happen in a later jira.

bq. For general problems, we tend to find simple solutions rather than complicated solutions. If we don't have many places to fix with a simple solution, why would we choose a more complicated solution?

bq. Otherwise, I can also argue that with my proposed patch, adding new application like distcp won't need a fix for this issue, plus the proposed patch is simpler.

What do you exactly mean by simplicity? Simplicity can mean few changes, or a clean design, or favorably both.

To me throwing a NullToken is more like a hack instead of an informed design choice. Let me just echo the differences between the NullToken and dummy token solution:

# Throwing a NullToken requires fixing every filesystems. Will NullToken later to leak into the FileSystem API? In contrast,  returning dummy tokens does not have this problem by design.
# Tokens are designed to be capability objects, thus returning dummy tokens matches the design rationales.
# It also only requires few changes as well.

bq.  I think you meant if we fix on the distcp side, then adding a new filesystem won't need a fix for this issue. Am I understanding you correctly?

This rationale is that distcp is the application, which is the only one able to tell for a particular cluster, whether it should go and get a delegation token. The benefit of not fixing any filesystems, is the effect rather than the cause., This is a simple http proxy that issues a dummy token for an insecure cluster., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664181/dummy-token-proxy.js
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7748//console

This message is automatically generated., HI [~wheat9], 

Thanks for the more info. Would you please indicate what are the other file systems in your mind that we need to fix if we go with NullToken approach? I can see very few here, thus the change is quite little.  Thanks.


, I'm revisiting some comments you made earlier [~wheat9]

{quote}
What do you exactly mean by simplicity? Simplicity can mean few changes, or a clean design, or favorably both.
{quote}

{quote}
Just to clarify, I'm not necessarily pushing the dummy token proposal. It is just something that I've tried and worked. Can you please look at whether you can fix distcp? To me this is the right place to fix, though it might be nontrivial because yarn / mr require DTs as well.
{quote}
The above statement  indicate that the solution to fix distcp does not satisfy the simplicity criteria you listed.

{quote}
This rationale is that distcp is the application, which is the only one able to tell for a particular cluster, whether it should go and get a delegation token. The benefit of not fixing any filesystems, is the effect rather than the cause.
{quote}
One thing you didn't state above, is that distcp as an application, it doesn't know whether the target cluster is secure or not, so it does NOT know whether it need to get a delegation. I wish there were an API to tell whether a cluster is secure or not, so the client can know whether it needs to  ask the cluster for delegation token.  Without this API,  the client has to do trial and fail, the cluster can choose to return either NullToken or dummy token to indicate it's not secure, but both would work. I thought that's why you stated "I'm not necessarily pushing on the dummy token proposal". 
 
I think NullToken is a reasonable approach that achieves both function correctness and simplicity. Except we need to have a property to alert user of the potential effect of accessing insecure cluster, as you suggested earlier.

Thanks.
, Today, with HTTP APIs, we don't have an 'allow fallback to simple auth' switch. It is something we should add.

Still, I don't think that will help address the usecase this JIRA tries to solve, distcp-ing from an insecure cluster to a secure cluster. For this usecase we will need to leave the switch in TRUE, which is today's current behavior (without the switch).

I would suggest, we fix the current usecase, and we follow it up with a JIRA adding the switch to the configuration and another to distcp. , Thanks [~tucu00].

I actually already worked out a version with a config property for webhdfs (defaulted to false). I was just trying to address [~wheat9]'s comments before posting it. With this property, user need to change the config from "false" to "true" before being able to copy from insecure cluster to secure cluster.

, I've tried to quickly scan the discussion.  I don't see how use of "NullToken" really helps.

Why isn't this patch achieving parity with hdfs vs. webhdfs?  RPC returns null for token requests when security is disabled.  Webhdfs should too instead of throwing an exception., HI [~daryn], thanks for your comments. 

When client doesn't know whether a target cluster is secure or not, and issued GET_DELEGATION_TOKEN request, there are three ways for the insecure cluster to respond:

- throw NullToken exception
- return dummy token
- return "null" token to be parsed in the respsonse.

In terms of functionality, they are pretty equivalent. But to achieving parity with hdfs,  I can try to work out a revision to return "null" token instead of throwing NullToken exception.

Thanks.

Hi [~wheat9], while I'm working on a new revision, would you please comment on this change? and my questions above? thanks.
, Uploaded patch 009. This version passes real "null" delegation token for webhdfs, when an insecure cluster is asked for deleagation token. Hope this addresses, In addition, I included a config property which has to be turn on to support fallback.

HI [~daryn] and [~wheat9], thanks a lot for your earlier comments, and hopefully this addressed them. Thanks.


, BTW, I'd like to restrict the solution of the jira for webhdfs only, and I modified the title of this jira to reflect that. At least with the fix, we can enable distcping between secure and insecure cluster. As we know, right now it's broken. For other interface, like hftp in branch-2. I will file follow-up jira to resolve them. Thanks.


, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664395/HDFS-6776.009.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.security.TestRefreshUserMappings
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7768//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7768//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664394/HDFS-6776.009.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7767//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7767//console

This message is automatically generated., I'd like to emphasize that with the latest patch of using "null" token instead of NullToken exception, user has to apply the same patch to both source and target cluster. With the prior revision that Alejandro commented, that combines NullToken and message parsing,, user just need to patch the secure cluster.

Thanks.

, IMO, enabling to work with an unpatched cluster (via message parsing) is a desirable capability as it does not require users to upgrade older clusters if they are just reading data from them., If all agree, I can still make the client side catch the IOException and parse the message, on top of the "null" token solution version 009.
The advantage of doing so is that we don't have to upgrade the insecure cluster that distcp is reading from.
Thanks.

, bq. I've tried to quickly scan the discussion. I don't see how use of "NullToken" really helps.
Why isn't this patch achieving parity with hdfs vs. webhdfs? RPC returns null for token requests when security is disabled. Webhdfs should too instead of throwing an exception.

Agree. This sounds quite reasonable to me.

Compared to other proposal, I think the NullToken solution is way too hacky and I'm opposed to it. The earlier comments elaborated the rationales., Hi [~wheat9], rev 009 was already done to address this comment. Would you please take a look?

I'm aware of one place that I can drop the check for IsSecurityEnabled() below in 009.
{code}
+      if (UserGroupInformation.isSecurityEnabled() && !fallbackAllowed) {
+        throw new IOException(FALLBACK_MSG);
+      }
{code}

The remaining question is, on top of 009, do we need to add the handling of catching IOException("Failed to get the token...") from the client side, so user doesn't need to upgrade the insecure cluster in order to pull data from it with distcp.

Thanks.

, I think the latest patch looks pretty good to me, and does indeed seem to address Daryn's earlier suggestion of just returning {{null}} as is currently done for RPC requests to get a delegation token.

I also agree with Tucu that it'd be nice to implement both the exception parsing scheme, as well as returning null, so that we have to upgrade fewer clusters to get distcp to work. Yes, it's a bit hacky, but having to upgrade a whole cluster in order to get distcp to work is pretty darn heavy weight. Haohui/Daryn - do you object to doing both, which would both get us going in the right direction as far as design _and_ make life easier for current users?

My only other suggestions are the following minor ones:

# I think you can indeed get rid of the {{isSecurityEnabled}} check as you suggested in your last comment, Yongjun.
# I think we shouldn't introduce a new config setting to allow fallback for webhdfs, but rather should just use the existing config setting to also allow fallback for WebHDFS. I can't think of any reason that users would be willing to allow insecure fallback for one interface but not another.

I'd personally be +1 once these are addressed., Hi [~atm], thanks a lot for the review and comments. I just uploaded rev 010 to address them.

I usually test against real clusters before I upload new rev, but my clusters are down now. I will test out tomorrow.

Thanks., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664836/HDFS-6776.010.patch
  against trunk revision ab638e7.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.security.TestRefreshUserMappings

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7846//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7846//console

This message is automatically generated., Hi [~atm],

I filed HDFS-6972 for the test failure and uploaded a quick patch. Since the fix for HDFS-6972 is pretty trivial, would you please help reviewing that too?

And would you please take a look at rev 010 here to see it all your comments are addressed?

Thanks a lot.



, This latest patch looks good to me. +1 on my end.

Tucu/Haohui/Daryn - does this look OK to you?, LGTM, {code}
+      if (token != null) {
+        token.setService(tokenServiceName);
+      } else if (!fallbackAllowed) {
+        throw new IOException(FALLBACK_MSG);
       }
-    }.run();
-    token.setService(tokenServiceName);
-    return token;
+      return token;
+    } catch (IOException ioe) {
+      if (ioe.getMessage().startsWith(SecurityUtil.NULL_TOKEN_MSG_HEADER)) {
+        // cluster that is insecure and doesn't have the fix of HDFS-6776
+        // throws IOException with msg that starts with
+        // SecurityUtil.NULL_TOKEN_MSG_HEADER when requested for delegation
+        // token. Catch it here and return null delegation token if 
+        // fallback is allowed
+        if (!fallbackAllowed) {
+          throw new IOException(FALLBACK_MSG, ioe);
+        }
+        return null;
+      }
+      throw ioe;
+    }
   }
{code}

This is too fragile. I'm unconvinced that this should happen in WebHdfsFileSystem.

However, it's okay to me to return a null here so the behavior is similar to DistributedFileSystem. The actual fallback logic can happen at the distcp side when building the file list, but maybe we can defer it to another jira.

Please add new unit tests to cover this behavior., Thank you all for the review, thanks [~atm] and [~tucu00] for the +1!

I tested in between two real clusters when the secure cluster has the fix, and the insecure one has AND doesn't have, all of them appear to work.

HI [~wheat9], to create a testcase with two clusters, one has kerberos enabled is going to be a challenge here. Given that I have tested with real clusters, with and without the fix, I wonder if it'd be ok with you that we don't have a test for this. Thanks.


, Hi [~wheat9], I checked and my understanding is that it is not possible to start a minicluster with kerberos today, so any this kind of testing has to be using two real clusters, and this is what I have done. Please correct me if I'm wrong.  Thanks.


, [~wheat9], if I'm understanding you correctly it seems like though this isn't your ideal solution, you're alright with it.

I agree with Yongjun that writing a unit test would be pretty difficult for this, and I'm personally satisfied with the level of manual testing that he's done.

Unless there's an objection in the next hour or two I'm going to go ahead and commit the latest patch, so please speak up if you're not OK with that., bq.  if I'm understanding you correctly it seems like though this isn't your ideal solution, you're alright with it.

Just to reiterate my last comments:

1. The fallback logic of the latest patch is too fragile. I'm still uncomfortable with it. I suggest for this jira just return {{null}} directly. That way it should allow the distcp to work. We can defer the discussion of fallback logic in another jira.
2. I agree that it would be difficult to write an end-to-end test, but at the very least there should be a unit test cover the new behavior of WebHdfsFileSystem, that is, testing whether WebHdfsFileSystem can get a null from an insecure cluster.

Please address the comments before committing the patch.

I'm sorry that I have slightly longer delay than I usually do, but please wait for my +1 before committing the patch., [~wheat9] - I'm assuming you're saying it's fragile because the latest patch is parsing some exception text. I agree that that sort of thing is in general fragile, and should be avoided. However, in this case, the alternative is to require that the remote cluster have its software upgraded to the newer version that contains the changed behavior. In my opinion (and, it seems, Tucu's and Yongjun's, as well) having to do that is so onerous for the operator as to warrant this (admittedly somewhat hacky) message parsing solution.

Do you have some alternative that will allow for this fixed distcp to work with a cluster which is not running this change? Because if not, I think we should go ahead and go with this solution, fragile as it may be, so as to potentially save users a whole heck of a lot of pain., Hi Guys, Thanks a lot for the review and feedback. Uploaded rev 001 with two testcases added (with fallback enabled and disabled) to address Haohui's comments.

Hello [~wheat9],

 Thanks for your review to drive to a more complete solution (now we have the unit tests).

About your comment #1, I think both Alejandro and ATM stated the benefit of having the a bit hacking message parsing to save the lots of pain of users otherwise, and I certainly agree with that. So I still included it. 

As of today, users have the pain not being able to distcp from insecure cluster to secure cluster. The symptom is the message as reported in this jira. With the patch, now we can return and detect null token and do distcp. The additional hacky meachnism to save user's pain is to parse and understand the message reported in the issue' symptom, it at least is not a regression (user used to just see exception thrown). When old versions which issue that messages are obsoleted in the future, we can certainly drop this hacky message parsing.

Thanks again.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12666403/HDFS-6776.011.patch
  against trunk revision 8f1a668.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7892//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7892//console

This message is automatically generated., bq. I'm assuming you're saying it's fragile because the latest patch is parsing some exception text. I agree that that sort of thing is in general fragile, and should be avoided.

I'm glad that we have a consensus here.

I do understand the concerns of upgrading the cluster. I handled the exact type of support case before, and I sincerely want the issue to be fixed. I even have no problem of putting in hacks.


What I've been opposing in all my comments is to land this hack in {{WebHdfsFileSystem}}. That seems a very bad idea to me.


Please note that {{WebHdfsFileSystem}} has much wide audiences than distcp alone -- do you agree that the impact of the hack should be minimized, so that it won't complicate the contract of {{WebHdfsFileSystem}} too much and make it harder to maintain down the road?

bq. Do you have some alternative that will allow for this fixed distcp to work with a cluster which is not running this change?

Disclaimer: I haven't tried out the solutions below, but I think it should work. :-)

Distcp collects the DTs when doing the listings. Later operations use these DTs to access the files. What can be done is to put the hack there, and to inject a corresponding token into token cache so that the filesystem no longer need to get the DT from the server. Conceptually you can think it as distcp grants itself a capability to access the insecure cluster.

That way (1) the fix can be restricted in distcp only, and (2) it is applicable for both webhdfs and hftp. 

Please take a look at {{DistCp#createInputFileListing}}, {{SimpleCopyListing#validatePaths}}, and {{TokenCache.obtainTokensForNamenodes()}} for more details., HI [~wheat9],

Thanks for the comments. A recap of what we discussed so far:

1. I think you agreed that returning null token is the right/reasonable approach. And sounds to me that you would be willing to give +1 if two comments are addressed: a), remove the message parsing hack,  b) add simplified tests about 
{quote}
testing whether WebHdfsFileSystem can get a null from an insecure cluster.
{quote}

2. We argued that although having the message parsing is hacky, we still want to do it to save big user pain. I think you are convinced about the need for this hack, except that you think the better place to add this hack is in distcp.  

Given that it's a hack for insecure cluster that's not upgraded with this fix, and we will remove this hack in the future when we don't need it, I prefer the simplicity of having this hack in webhdfs. 

You stated if we fix distcp as you suggested (which you also said to be non-trivial earlier), then we don't have to change hftp.  In my opinion, changing hftp is trivial, and hftp seems to be the only other place we need to add a similar fix as webhdfs - which I planned to file another jira for that as mentioned before.  Actually hftp was already removed from trunk, and it only exists in branch-2.

3. The latest patch rev (011) added the tests - thanks to your suggestion - and I think it's a more complete solution now. What about we file follow up jira for more discussion, given that a)  you  agreed on the null token solution if we remove the hack,  b) you agreed we do need a hack somewhere, and c) I think the hack will be removed when it's the time? 

Thanks.

, Some additional info [~wheat9]. Sorry for long post, but hopefully this gives more clarity. And I'd really appreciate that you could comment on my last and this comment at your earliest convenience.

4.
{quote}
Please note that WebHdfsFileSystem has much wide audiences than distcp alone – do you agree that the impact of the hack should be minimized, so that it won't complicate the contract of WebHdfsFileSystem too much and make it harder to maintain down the road?
{quote}
About the webhdfs contract you were referring to in the above comment:  we are supposed to be able to access insecure cluster from secure cluster side even if it's not distcp, is this not true?

Assuming it's true, the problem is that, we currently see Exception with "Failed to get the token ..." message . In my opinion. this is not what the webhdfs contract should be, it's actually the bug we are dealing with. Fixing the bug at webhdfs (instead of throwing the exception, trying to understand that the exception meant null token thus insecure cluster) actually seems necessary to me. So I don't understand why this complicates the contract of webhdfs. We had consensus that the hack is needed to save user's big pain. Again, this msg-parsing hack is only executed when accessing cluster not upgraded with this fix, and the hack will be removed in the future, so the impact seems minimum to me. 

If you disagree, would you please describe a specific example scenario that this would make it harder to maintain?

5. 
{quote}
Distcp collects the DTs when doing the listings. Later operations use these DTs to access the files. What can be done is to put the hack there, and to inject a corresponding token into token cache so that the filesystem no longer need to get the DT from the server. Conceptually you can think it as distcp grants itself a capability to access the insecure cluster.
{quote}
I think you were suggesting the dummy token approach rather than null token. Or maybe you meant to insert null token to token cache. You made the earlier comment saying we can have another jira for this discussion as quoted below, so I think we can do what you said, unless you changed mind:

{quote}
However, it's okay to me to return a null here so the behavior is similar to DistributedFileSystem. The actual fallback logic can happen at the distcp side when building the file list, but maybe we can defer it to another jira.
{quote}

Thanks.
, I've made it very clear that I'm opposed to put fragile hacks into {{WebHdfsFileSystem}}, though I'm okay if it's done at the application level (e.g. distcp). Unless this is addressed, I cannot give my +1.

If you are not familiar with the distcp code, I'll take a look and see whether I can post a patch for it., Hi [~wheat9], 

I know that you are opposed to put the msg-parsing hack to webhdfs. However, you also have said:
{quote}
However, it's okay to me to return a null here so the behavior is similar to DistributedFileSystem. The actual fallback logic can happen at the distcp side when building the file list, but maybe we can defer it to another jira.
{quote}

I had quite some questions for you in my last two comments, I'd appreciate that you could comment on them. That way, we can understand more about your concern why it's so fragile as you said. 

Do you agree that a correct webhdfs contract is not to fail with the exception when accessing insecure cluster, rather, it should be able to access insecure cluster? This is a very important question that I hope you could answer.

We agree that the msg-parsing is a bit hacky, but why hack in webhdfs is so much worse than in distcp, given webhdfs doesn't work without a fix?

BTW, FYI, not to say that it's good thing to do so, there was already code doing msg parsing in webhdfs:
{code}
      // extract UGI-related exceptions and unwrap InvalidToken
      // the NN mangles these exceptions but the DN does not and may need
      // to re-fetch a token if either report the token is expired
      if (re.getMessage().startsWith("Failed to obtain user group information:")) {
        String[] parts = re.getMessage().split(":\\s+", 3);
        re = new RemoteException(parts[1], parts[2]);
        re = ((RemoteException)re).unwrapRemoteException(InvalidToken.class);
      }
{code}
Do you consider this fragile?

Disclaimer, In the patch I did here, it's not because there was existing code like quoted above. Rather it's because the solution has its simplicity which we discussed earlier.

Thanks.
, Hello [~wheat9],

Accessing insecure cluster via webhdfs from secure cluster is broken today, I claimed this should not be the right webhdfs contract (instead a bug) in my earlier comments. This problem is not just about distcp, but other applications too. Fixing webhdfs itself has the advantage that we don't have to fix all applications. I argued about this along the way, and would like to emphasize. 

I'd like to give an example here: if you issue "hadoop fs -lsr webhdfs://<insecurecluster>" from secure cluster side, you would see it fail the same way.  Fixing distcp as you proposed would not solve this, but my proposed solution does.

Thanks.
, [~wheat9],

Hope the example I gave is convincing that webhdfs is the right place to fix. I think we wouldn't want to tell user that "sorry, webhdfs contract doesn't allow accessing insecure cluster from secure cluster, if you need to, please hack your application like how distcp does". Would you please comment at your earliest convenience? Thanks.
, bq. Hope the example I gave is convincing that webhdfs is the right place to fix.

Just to reiterate, given the fact that {{WebHdfsFileSystem}} has much wider audiences other than {{distcp}}, the decision should be made by the application not by {{WebHdfsFileSystem}}. This is a design decision and it should not be violated by a hack, if you want to avoid long term pain.

See https://issues.apache.org/jira/browse/HDFS-6776?focusedCommentId=14121719&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14121719

Note that I've mentioned this issue explicitly for multiple times -- and again, I cannot +1 until this is addressed.

bq.  "sorry, webhdfs contract doesn't allow accessing insecure cluster from secure cluster, if you need to, please hack your application like how distcp does"

See https://issues.apache.org/jira/browse/HDFS-6776?focusedCommentId=14121719&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14121719
, I suggest the following route to proceed on this jira:

# Implement [~daryn]'s suggestion, that is, making {{WebHdfsFileSystem}} the same of {{DistributedFileSystem}} w.r.t. returning null in insecure clusters. This should be the ideal fix and at least allow distcp to work in clusters with this patch. I can +1 this fix.
# Create another jira to discuss how to implement the fallback mechanism., Hello [~wheat9],

Just want to clarify, I have already implemented [~daryn]'s suggestion in rev 011. Rev 011 also included the message parsing. What you are suggesting is, to take out the message parsing part. Would you please quickly confirm?

I can certainly do that.  But just to reiterate, with what you suggested, in order to see the fix work, user need to upgrade both source and destination cluster with this fix, which could be quite a pain on user side.

Thanks.
, Since [~yzhangal]'s latest patch already contains the correct fix in #1, and this is very useful fix, maybe we can separate the fallback mechanism out and commit the fix first? In this way we can make faster progress I guess.

For the fallback mechanism, to keep it in WebHdfsFileSystem may be an overkill since currently in HDFS the main use case for accessing data across clusters in distcp. But we can discuss it in a separate jira.

What do you think [~yzhangal]?, HI [~jingzhao],

Thanks for your review and comment. Some more info here to share.

In rev 011, I did implement #1 (returning null token from insecure cluster). In addition, rev 011 contains fallback logic in webhdfs based on #1 (the null token returned from insecure cluster). 

On top of that, a message parsing was added as part of the fallback logic, such that user doesn't have to upgrade insecure cluster. I agree that the the message parsing is a bit hacky and removing it is a cleaner solution. However, the trade-off is what we have been debating: to have the hack or to force user to upgrade insecure cluster. 

I certainly can drop the message parsing part if that's what we have to do here, although I think it doesn't hurt much to have this hack for the lots of user pain we could have saved (please also refer to HDFS-7026 which I created to make an existing msg parsing better). 

Thanks a lot.
, Hi [~wheat9],

I just uploaded rev 012 which removed the msg parsing. Would you please take a look at it?

I will file follow-up jira for fallback logic and other stuff, e.g., even issuing "hadoop fs -lsr webhdfs://<insecureCluster>" from secure cluster fails the same way.

Thanks.

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667216/HDFS-6776.012.nomsgparsing.patch
  against trunk revision df8c84c.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.ha.TestActiveStandbyElectorRealZK
                  org.apache.hadoop.ipc.TestCallQueueManager
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7952//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7952//console

This message is automatically generated., Since the remaining work is trivial, I just posted a patch so that webhdfs returns null for {{getDelegationToken()}} when security is disabled., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667262/HDFS-6776.013.patch
  against trunk revision d989ac0.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestQuota
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7957//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7957//console

This message is automatically generated., HI [~wheat9],

I noticed that you took out the config property that enables the fix. So distcp will silently accepts copy between secure and insecure cluster. Would anyone please comment if  this is acceptable? This property was actually initially brought up by you here and I like the idea, it reminds user what s/he is doing.

I also noticed that you made some simplification to some code, I will take it as as a review suggestion with next rev.

Thanks., It's not acceptable to allow secure clients to fall back from secure to insecure communication without a configuration option to explicitly enable that. When we've discovered behavior such as this in the past, we've treated it as a security vulnerability. In fact, the config option that Yongjun referenced earlier was introduced in response to such a vulnerability. You can find more info about that here:

http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2013-2192

I'm fine going with the "HDFS-6776.012.nomsgparsing.patch" patch that Yongjun posted earlier. That contains a simple fix for the acute issue, doesn't include message text parsing that Haohui finds unacceptable, and does support configurably disabling the fallback.

Jing/Haohui - is that alright with you?, The v14 patch is a minimized version of the v12 patch from Yongjun., Thanks, Haohui.

Yongjun - does this latest patch look OK to you?, Hi [~atm], thanks for commenting on the fallback config thing. I'm glad that we have an agreement here.

Hi [~wheat9], 

Thanks for the patches you posted. Great to see that we are getting so close to commit now. I saw one issue with the test code in rev 014, I posted rev 015 to address it. The issue is, if other IOException than the one we expect is thrown (for example, when building the cluster), the test would still succeed.

Thanks again for your earlier comments which helped to drive to a more complete solution here.

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667308/HDFS-6776.014.patch
  against trunk revision 7498dd7.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7962//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7962//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667335/HDFS-6776.015.patch
  against trunk revision 7498dd7.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7964//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7964//console

This message is automatically generated., Hi [~wheat9] and [~jingzhao],

I just uploaded rev 016 for another minor change. I just found that [~atm] is on vacation today. Would you please take a look to see if it looks good to you and help commit if so? thanks a lot.
, Instead of an {{IOException}}, it should probably throw {{AccessControlException}} to avoid unnecessary retries that are doomed to fail., Hi [~daryn], thanks for the good suggestion. Uploaded rev 017 to address it!

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667422/HDFS-6776.016.patch
  against trunk revision 90c8ece.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7966//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7966//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667435/HDFS-6776.017.patch
  against trunk revision 90c8ece.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7967//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7967//console

This message is automatically generated., +1. Patch v17 LGTM. 

IMO it will be a pita for users with multiple clusters the fact that we are not parsing the exception message. Please lets have a follow up JIRA for it., [~wheat9], do you have any concern with the patch v17? Else I'd like to commit it by EOD PST today., Thanks [~tucu00]!

Hi  [~wheat9], [~jingzhao], thanks for your review and comments. I will certainly file follow-up jira for further discussion of related issues. Thanks.
, {code}
+  @VisibleForTesting
+  public static final String CANT_FALLBACK_TO_INSECURE_MSG =
+      "The client is configured to only allow connecting to secure cluster";
+
+ catch (AccessControlException ace) {
+      Assert.assertTrue(ace.getMessage().startsWith(
+          WebHdfsFileSystem.CANT_FALLBACK_TO_INSECURE_MSG));
{code}

Please inline the string. It is sufficient to do it through {{@Test(expected=AccessControlException.class)}}.
, Hi [~wheat9],

In the following three statements, if any of the first two failed with IOException, the test would succeed instead of fail. If the third one failed for a different reason and throw exception, the test would still succeed. That's wrong behavior. My way of doing it made it succeed only when when we get the expected exception, don't you agree it's more accurate? Please explain why you think it's not more accurate otherwise.
{code}
      1. cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      2. final FileSystem webHdfs = WebHdfsTestUtil.getWebHdfsFileSystem(conf,
          WebHdfsFileSystem.SCHEME);
      3. webHdfs.getDelegationToken(null);
{code}

Thanks.
, BTW Haohui, I recalled that we debated on a very similar thing in a different jira a while back, and you seemed to be convinced there:

https://issues.apache.org/jira/browse/HDFS-5939
Haohui Mai added a comment - 21/Feb/14 13:50

Thanks.


, One correction Haohui, you were talking about AccessControlException. When you read my prior comments, please replace IOException with AccessControlException,  which could be thrown with different message too. The point is, the test only succeeds with the expected exception, and fails otherwise. Thus it's more accurate. Please refer to HDFS-5939 for a similar discussion there.
, I'll commit this momentarily, if there are further discussions on how to make the testcase more accurate we can follow up in a new JIRA., {code}
The point is, the test only succeeds with the expected exception, and fails otherwise.
{code}

In general it is a bad idea to make your test case depend on the exact message that the server is throwing out, since the message itself is not part of the contract but the class of the exception usually does.

It should be fine if you don't want to change it, but just for the record, the test is fragile., Thanks Haohui. I think it's very common to see same type of exceptions thrown with different message (if this happens, the test would succeed, which is wrong), on the other hand,  it would be less common to have another exception of same type to have the exact same message. See I had used string constant to make it a contract. between the production code and test code. So I think having the message parsing is less fragile than without. Thanks.


, Thanks YongJun and thanks Haohui for reviewing it.

Committed to trunk and branch-2., Thanks a lot [~tucu00]!

Many thanks to [~aw], [~tthompso], [~daryn], [~atm], [~wheat9], and [~jingzhao] for the review and comments.

I just created HDFS-7036 and HDFS-7037 as follow-up.
, FAILURE: Integrated in Hadoop-Yarn-trunk #676 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/676/])
HDFS-6776. Using distcp to copy data between insecure and secure cluster via webdhfs doesn't work. (yzhangal via tucu) (tucu: rev bbff44cb03d0150f990acc3b77170893241cc282)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1867 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1867/])
HDFS-6776. Using distcp to copy data between insecure and secure cluster via webdhfs doesn't work. (yzhangal via tucu) (tucu: rev bbff44cb03d0150f990acc3b77170893241cc282)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1892 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1892/])
HDFS-6776. Using distcp to copy data between insecure and secure cluster via webdhfs doesn't work. (yzhangal via tucu) (tucu: rev bbff44cb03d0150f990acc3b77170893241cc282)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java
, HI Folks on the watch list, 

As [~tucu00] and [~atm] pointed out earlier that it's going to be quite a pain on user without a solution for HDFS-7036, and I agree. So I'm trying to see if we can converge to a solution there. Your comment in HDFS-7036 is very much appreciated. 

Thanks.

]