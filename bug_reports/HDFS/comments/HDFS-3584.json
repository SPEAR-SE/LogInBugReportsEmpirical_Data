[*please check following log for blk_-7909104799008701972* 

{noformat}
2012-06-25 13:48:59,603 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: blk_-7909104799008701972 added as corrupt on ****DN1:50010 by /****DN1 because block is COMPLETE and reported genstamp 96470 does not match genstamp in block map 96309
2012-06-25 13:48:59,604 DEBUG org.apache.hadoop.hdfs.StateChange: UnderReplicationBlocks.update blk_-7909104799008701972_96309 curReplicas 2 curExpectedReplicas 3 oldReplicas 3 oldExpectedReplicas  3 curPri  2 oldPri  3
2012-06-25 13:48:59,604 DEBUG org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.UnderReplicationBlock.update:blk_-7909104799008701972_96309 has only 2 replicas and needs 3 replicas so is added to neededReplications at priority level 2
2012-06-25 13:48:59,604 DEBUG org.apache.hadoop.hdfs.StateChange: BLOCK* block RECEIVED_BLOCK: blk_-7909104799008701972_96470 is received from DatanodeRegistration(****DN1, storageID=DS-1986831640-****DN1-50010-1340363042399, infoPort=50075, ipcPort=50020, storageInfo=lv=-40;cid=CID-fdfc6cef-05b1-4900-b5f9-cc275dfd343c;nsid=415242063;c=0)
2012-06-25 13:48:59,607 DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Reported block blk_-7909104799008701972_96470 on ****DN2:50010 size 524288 replicaState = FINALIZED
2012-06-25 13:48:59,608 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: blk_-7909104799008701972 added as corrupt on ****DN2:50010 by /****DN2 because block is COMPLETE and reported genstamp 96470 does not match genstamp in block map 96309
2012-06-25 13:48:59,608 DEBUG org.apache.hadoop.hdfs.StateChange: UnderReplicationBlocks.update blk_-7909104799008701972_96309 curReplicas 1 curExpectedReplicas 3 oldReplicas 2 oldExpectedReplicas  3 curPri  0 oldPri  2
2012-06-25 13:48:59,609 DEBUG org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block blk_-7909104799008701972_96309 from priority queue 2
2012-06-25 13:48:59,609 DEBUG org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.UnderReplicationBlock.update:blk_-7909104799008701972_96309 has only 1 replicas and needs 3 replicas so is added to neededReplications at priority level 0
2012-06-25 13:48:59,610 DEBUG org.apache.hadoop.hdfs.StateChange: BLOCK* block RECEIVED_BLOCK: blk_-7909104799008701972_96470 is received from DatanodeRegistration(****DN2, storageID=DS-485536663-****DN2-50010-1340362102909, infoPort=50075, ipcPort=50020, storageInfo=lv=-40;cid=CID-fdfc6cef-05b1-4900-b5f9-cc275dfd343c;nsid=415242063;c=0)
2012-06-25 13:48:59,678 DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Reported block blk_-7909104799008701972_96470 on ****DN3:50010 size 524288 replicaState = FINALIZED
2012-06-25 13:48:59,679 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: blk_-7909104799008701972 added as corrupt on ****DN3:50010 by /****DN3 because block is COMPLETE and reported genstamp 96470 does not match genstamp in block map 96309
2012-06-25 13:48:59,681 DEBUG org.apache.hadoop.hdfs.StateChange: UnderReplicationBlocks.update blk_-7909104799008701972_96309 curReplicas 0 curExpectedReplicas 3 oldReplicas 1 oldExpectedReplicas  3 curPri  4 oldPri  0
2012-06-25 13:48:59,681 DEBUG org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.UnderReplicationBlock.remove: Removing block blk_-7909104799008701972_96309 from priority queue 0
2012-06-25 13:48:59,682 DEBUG org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.UnderReplicationBlock.update:blk_-7909104799008701972_96309 has only 0 replicas and needs 3 replicas so is added to neededReplications at priority level 4
2012-06-25 13:48:59,682 DEBUG org.apache.hadoop.hdfs.StateChange: BLOCK* block RECEIVED_BLOCK: blk_-7909104799008701972_96470 is received from DatanodeRegistration(****DN3, storageID=DS-598160968-****DN3-50010-1340382093938, infoPort=50075, ipcPort=50020, storageInfo=lv=-40;cid=CID-fdfc6cef-05b1-4900-b5f9-cc275dfd343c;nsid=415242063;c=0)
2012-06-25 13:48:59,683 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=BP-1988075715-****DN1-1340361925673:blk_-7909104799008701972_96309, newgenerationstamp=96470, newlength=524288, newtargets=[****DN1:50010, ****DN2:50010, ****DN3:50010], closeFile=true, deleteBlock=false)
2012-06-25 13:48:59,683 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:root (auth:SIMPLE) cause:java.io.IOException: Unexpected block (=BP-1988075715-****DN1-1340361925673:blk_-7909104799008701972_96309) since the file (=append_1654216706419640) is not under construction
2012-06-25 13:48:59,685 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.commitBlockSynchronization from ****DN1:15704: error: java.io.IOException: Unexpected block (=BP-1988075715-****DN1-1340361925673:blk_-7909104799008701972_96309) since the file (=append_1654216706419640) is not under construction
java.io.IOException: Unexpected block (=BP-1988075715-****DN1-1340361925673:blk_-7909104799008701972_96309) since the file (=append_1654216706419640) is not under construction
2012-06-25 13:49:00,413 DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Block blk_-7909104799008701972_96309 cannot be repl from any node
{noformat}, Here I can see client1 are not able to get the lease renewed with in soft limit period and client2 try to trigger lease recovery
I can see this from the exception @ client

Client log
{noformat}
2012-06-28 19:40:56,452 INFO  hdfs.TestHDFSAPI (TestHDFSAPI.java:writeFile(168)) - Writing File with client DFS[DFSClient[clientName=DFSClient_clientIDD36655341694508_139393306_1, ugi=B00902108 (auth:SIMPLE)]] File = /home/test/writefile_36673810701636
2012-06-28 19:41:04,619 INFO  hdfs.TestHDFSAPI (TestHDFSAPI.java:writeFile(179)) - Creating File with client DFS[DFSClient[clientName=DFSClient_clientIDD36655341694508_139393306_1, ugi=B00902108 (auth:SIMPLE)]] File = /home/test/writefile_36673810701636
2012-06-28 19:42:28,004 INFO  hdfs.TestHDFSAPI (TestHDFSAPI.java:writeFile(186)) - Closing File with client DFS[DFSClient[clientName=DFSClient_clientIDD36655341694508_139393306_1, ugi=B00902108 (auth:SIMPLE)]] File = /home/test/writefile_36673810701636 file size= 524288
2012-06-28 19:42:32,680 INFO  hdfs.TestHDFSAPI (TestHDFSAPI.java:appendFile(76)) - Going Append File with client DFS[DFSClient[clientName=DFSClient_clientIDD36661058974201_139393306_1, ugi=B00902108 (auth:SIMPLE)]]File = /home/test/writefile_36673810701636file size= 524288
org.apache.hadoop.hdfs.protocol.RecoveryInProgressException: Failed to close file /home/test/writefile_36673810701636. Lease recovery is in progress. Try again later.
{noformat}, Thanks Brahma and Amith for digging into it.

Seems like a bug.
We are triggering the recovery from append on leaseExpired check, that means that we are trusting that, older client might have gone down. So, there is no renewal from clients and soft limit expired. And append call is triggering the recovery himself and throwing the exception to user, saying file not yet closed try again later. Here we are renewing the lease now from append call itself.

{code}
 if (lease.expiredSoftLimit()) {
          LOG.info("startFile: recover lease " + lease + ", src=" + src +
              " from client " + pendingFile.getClientName());
          boolean isClosed = internalReleaseLease(lease, src, null, lease.expiredSoftLimit());
          if(!isClosed)
            throw new RecoveryInProgressException(
                "Failed to close file " + src +
                ". Lease recovery is in progress. Try again later.");
        }
{code}

and in internalReleaseLease:

{code}
    case UNDER_RECOVERY:
      final BlockInfoUnderConstruction uc = (BlockInfoUnderConstruction)lastBlock;
      // setup the last block locations from the blockManager if not known
      if (uc.getNumExpectedLocations() == 0) {
        uc.setExpectedLocations(blockManager.getNodes(lastBlock));
      }
      // start recovery of the last block for this file
      long blockRecoveryId = nextGenerationStamp();
      lease = reassignLease(lease, src, recoveryLeaseHolder, pendingFile);
      uc.initializeBlockRecovery(blockRecoveryId);
      leaseManager.renewLease(lease);
{code}

Here block recovery will happen in background in primary DN and will be returned.

But unfortunately now close call came from the old client and file got closed. Seems like this happend under high load.
But block ids already bumped in DNs and will rejected as file closed with older genstamps at NN side.
commitBlockSynchronization also failing due to this reason.

I think we need to block the older clients to close the file at this stage?

what if append call takes the new lease ownership and removes the older client lease?

close call anyway checking the lease expiration.

{code}
 try {
      pendingFile = checkLease(src, holder);
    } catch (LeaseExpiredException lee) {
      INodeFile file = dir.getFileINode(src);
      if (file != null && !file.isUnderConstruction()) {
        // This could be a retry RPC - i.e the client tried to close
        // the file, but missed the RPC response. Thus, it is trying
        // again to close the file. If the file still exists and
        // the client's view of the last block matches the actual
        // last block, then we'll treat it as a successful close.
        // See HDFS-3031.
        Block realLastBlock = file.getLastBlock();
        if (Block.matchingIdAndGenStamp(last, realLastBlock)) {
          NameNode.stateChangeLog.info("DIR* NameSystem.completeFile: " +
              "received request from " + holder + " to complete file " + src +
              " which is already closed. But, it appears to be an RPC " +
              "retry. Returning success.");
          return true;
        }
      }
      throw lee;
    }
{code}
I am not sure , I am missing some thing here.
would greatly appreciate your suggestions on this.
  , Hi All,

Do you have any comments on this issue?
, Todd, could you please comment on this, for further discussion if you have time?, HDFS-10240 appears to report a similar issue., This is definitely the same as HDFS-10240, and it is reproducible even with one client thread., bq. I think we need to block the older clients to close the file at this stage?
My take is that once recoverLease() or whatever calls that starts lease recovery reaches NameNode, NN should reject follow-up close()/recoverLease() calls until it is able to recover the lease. If the lease recovery doesn't complete, you just can't safely assume the file can be closed (e.g. file doesn't have sufficient number of replicas)

bq. what if append call takes the new lease ownership and removes the older client lease?
Meaning append takes over lease without a lease recovery, so it doesn't bump up GS? That doesn't sounds right to me., Can we close this one once HDFS-10240 is committed?, Close this one. Thanks for reminder [~elgoiri]]