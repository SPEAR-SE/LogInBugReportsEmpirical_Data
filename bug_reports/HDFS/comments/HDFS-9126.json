[[~pingley]
Can you attach the logs or the error messages  for more clarity  ?, hi,I attach some logs,can you help me fix the probrom? thank you., Any resolution to this? , We have already encounter simliar issue. As logs shown, an unexpected 45 sec timeout occured when active namenode's corresponding healthMonitor was checking namenode's health(health check request emited at 2017-07-06 17:22:12). The timeout makes healthMonitor believes active namenode had failed, and thus closed zk client to quit leader election. However, we did not find any exception in namenode's log and gc log. A more wired thing is that namenode's ends logging at 2017-07-06 17:20:37.

After analysing the source code, the scenario is shown below:
1. when activce namenode' corresponding healthMonitor is checking NN's health, a 45 secends timeout makes state switched to SERVICE_NOT_RESPONDING.
2. Active NN quits leader election. zkClinet close connection and, thus, temporary lock znode on zk is deleted.
3. standby NN gain the leadership from zk in order to switch itself to new active NN.
4. To prevent 'Split-Brain', before switching to active state, standby NN sends rpc request to switch current active NN' state to standby.
5. If this rpc request fails, standby NN tries to kill target NN jvm via ssh. This is the exactly same to our case.
6. After killing target NN successfully, standby NN switchs itself to activce.

Though we have understood the scenario, what happend in previous active NN is still unknown. Sadly, Jstack trace can be found no longer since jvm is killed by standby nn. Does anyone understand or have similar prolem? Thanks for sharing in advance.

namenode log
2017-07-06 17:20:25,944 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.74.160:50010 is added to blk_8181385931_7145837309{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-8b015249-3dfc-46d6-b575-a5217dd3e40e:NORMAL|RBW], ReplicaUnderConstruction[[DISK]DS-a88850be-de4f-4cf8-b6ec-10c8116c4226:NORMAL|RBW], ReplicaUnderConstruction[[DISK]DS-09006ca0-3bd0-41a2-b4b9-90a28682031b:NORMAL|RBW]]} size 0
2017-07-06 17:20:25,945 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.8.230:50010 is added to blk_8181385931_7145837309{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-8b015249-3dfc-46d6-b575-a5217dd3e40e:NORMAL|RBW], ReplicaUnderConstruction[[DISK]DS-a88850be-de4f-4cf8-b6ec-10c8116c4226:NORMAL|RBW], ReplicaUnderConstruction[[DISK]DS-09006ca0-3bd0-41a2-b4b9-90a28682031b:NORMAL|RBW]]} size 0
2017-07-06 17:20:25,945 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.74.79:50010 is added to blk_8181385931_7145837309{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-8b015249-3dfc-46d6-b575-a5217dd3e40e:NORMAL|RBW], ReplicaUnderConstruction[[DISK]DS-a88850be-de4f-4cf8-b6ec-10c8116c4226:NORMAL|RBW], ReplicaUnderConstruction[[DISK]DS-09006ca0-3bd0-41a2-b4b9-90a28682031b:NORMAL|RBW]]} size 0
2017-07-06 17:20:25,945 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hive-mobdss/hive_2017-07-06_17-19-52_775_5225473556650420863-1/_task_tmp.-ext-10003/_tmp.000000_0 is closed by DFSClient_attempt_1482378778761_39818303_m_000000_0_1289941958_1
2017-07-06 17:20:25,946 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/ns_spider/p_site_product/2a79796e1be609fc26ecf1ab58f5aac9/.tmp/733c7b7008594135ae6fcb540f0ca4d5 is closed by DFSClient_hb_rs_slave557-prd3.hadoop.com,60020,1498730232536_-2089669913_35
2017-07-06 17:20:26,667 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/mobdss/.staging/job_1482378778761_39818325/job_1482378778761_39818325_1.jhist for DFSClient_NONMAPREDUCE_-907911548_1
2017-07-06 17:20:30,826 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/yyadmin/.staging/job_1482378778761_39818328/job_1482378778761_39818328_1.jhist for DFSClient_NONMAPREDUCE_-1343712942_1
2017-07-06 17:20:32,051 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/yyadmin/.staging/job_1482378778761_39818327/job_1482378778761_39818327_1.jhist for DFSClient_NONMAPREDUCE_-904958265_1
2017-07-06 17:20:33,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/yyadmin/.staging/job_1482378778761_39818341/job_1482378778761_39818341_1.jhist for DFSClient_NONMAPREDUCE_1250585342_1
2017-07-06 17:20:37,402 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds


zkfc log
2017-07-06 17:22:12,264 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of Na
meNode at hostname1/hostname1:8020: Call From hostname1/hostname1 to namenode1-pr
d3.hadoop.com:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 45000 millis timeout while wait
ing for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/hostname1:25190 remote=name
node1-prd3.hadoop.com/hostname1:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
2017-07-06 17:22:12,265 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING
2017-07-06 17:22:12,265 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at namenode1-prd3.hadoop
.com/hostname1:8020 entered state: SERVICE_NOT_RESPONDING
2017-07-06 17:22:12,265 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at namenode
1-prd3.hadoop.com/hostname1:8020 and marking that fencing is necessary
2017-07-06 17:22:12,265 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2017-07-06 17:22:12,269 INFO org.apache.zookeeper.ZooKeeper: Session: 0x350703139e63a4d closed
2017-07-06 17:22:12,269 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessio
nId 0x350703139e63a4d
2017-07-06 17:22:12,270 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2017-07-06 17:22:19,020 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of Na
meNode at hostname1/hostname1:8020: Failed on local exception: java.io.EOFException; Host Details : l
ocal host is: "hostname1/hostname1"; destination host is: "hostname1":8020; 
2017-07-06 17:22:21,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hostname1/hostname1:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MIL
LISECONDS)
2017-07-06 17:22:21,023 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of Na
meNode at hostname1/hostname1:8020: Call From hostname1/hostname1 to namenode1-pr
d3.hadoop.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:
  http://wiki.apache.org/hadoop/ConnectionRefused
  

journal log
2017-07-06 17:22:18,906 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage FileJournalManager(root=/home/bigdata/hadoop/journal/hadoop1)
2017-07-06 17:22:19,022 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is EditLogFile(file=/home/bigdata/hadoop/journal/hadoop1/current/edits_inprogress_0000000054649908467,first=0000000054649908467,last=0000000054650023722,inProgress=true,hasCorruptHeader=false)
2017-07-06 17:22:19,194 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo(54649908467): EditLogFile(file=/home/bigdata/hadoop/journal/hadoop1/current/edits_inprogress_0000000054649908467,first=0000000054649908467,last=0000000054650023722,inProgress=true,hasCorruptHeader=false) -> startTxId: 54649908467 endTxId: 54650023722 isInProgress: true
2017-07-06 17:22:19,195 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Prepared recovery for segment 54649908467: segmentState { startTxId: 54649908467 endTxId: 54650023722 isInProgress: true } lastWriterEpoch: 23 lastCommittedTxId: 54650023721
2017-07-06 17:22:19,316 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo(54649908467): EditLogFile(file=/home/bigdata/hadoop/journal/hadoop1/current/edits_inprogress_0000000054649908467,first=0000000054649908467,last=0000000054650023722,inProgress=true,hasCorruptHeader=false) -> startTxId: 54649908467 endTxId: 54650023722 isInProgress: true
2017-07-06 17:22:19,316 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Skipping download of log startTxId: 54649908467 endTxId: 54650023722 isInProgress: true: already have up-to-date logs
2017-07-06 17:22:19,317 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment 54649908467: segmentState { startTxId: 54649908467 endTxId: 54650023722 isInProgress: true } acceptedInEpoch: 24
2017-07-06 17:22:19,340 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Validating log segment /home/bigdata/hadoop/journal/hadoop1/current/edits_inprogress_0000000054649908467 about to be finalized
2017-07-06 17:22:19,421 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /home/bigdata/hadoop/journal/hadoop1/current/edits_inprogress_0000000054649908467 -> /home/bigdata/hadoop/journal/hadoop1/current/edits_0000000054649908467-0000000054650023722
2017-07-06 17:22:59,927 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastWriterEpoch from 23 to 24 for client /hostname2


gc log
2017-07-06T17:18:05.519+0800: 36901803.972: [GC2017-07-06T17:18:05.521+0800: 36901803.975: [ParNew: 7064064K->67185K(8738176K), 0.2323100 secs] 51648481K->44652702K(82138496K), 0.2362590 secs] [Times: user=2.64 sys=0.03, real=0.23 secs] 
2017-07-06T17:18:18.967+0800: 36901817.421: [GC2017-07-06T17:18:18.970+0800: 36901817.423: [ParNew: 7057777K->79674K(8738176K), 0.1799340 secs] 51643294K->44666348K(82138496K), 0.1840240 secs] [Times: user=2.84 sys=0.00, real=0.19 secs] 
2017-07-06T17:18:38.142+0800: 36901836.596: [GC2017-07-06T17:18:38.145+0800: 36901836.598: [ParNew: 7070266K->55166K(8738176K), 0.1605790 secs] 51656940K->44643432K(82138496K), 0.1637720 secs] [Times: user=2.64 sys=0.00, real=0.17 secs] 
2017-07-06T17:18:52.320+0800: 36901850.773: [GC2017-07-06T17:18:52.322+0800: 36901850.776: [ParNew: 7045758K->62376K(8738176K), 0.1141060 secs] 51634024K->44652240K(82138496K), 0.1179480 secs] [Times: user=1.56 sys=0.01, real=0.12 secs] 
2017-07-06T17:19:04.957+0800: 36901863.411: [GC2017-07-06T17:19:04.960+0800: 36901863.414: [ParNew: 7052968K->63039K(8738176K), 0.1575490 secs] 51642832K->44653620K(82138496K), 0.1620030 secs] [Times: user=2.04 sys=0.01, real=0.17 secs] 
2017-07-06T17:19:19.620+0800: 36901878.073: [GC2017-07-06T17:19:19.622+0800: 36901878.075: [ParNew: 7053631K->61205K(8738176K), 0.1163170 secs] 51644212K->44652939K(82138496K), 0.1204990 secs] [Times: user=1.82 sys=0.02, real=0.13 secs] 
2017-07-06T17:19:37.739+0800: 36901896.192: [GC2017-07-06T17:19:37.741+0800: 36901896.195: [ParNew: 7051797K->60432K(8738176K), 0.1181070 secs] 51643531K->44653501K(82138496K), 0.1213480 secs] [Times: user=1.89 sys=0.00, real=0.12 secs] 
2017-07-06T17:19:52.404+0800: 36901910.857: [GC2017-07-06T17:19:52.406+0800: 36901910.860: [ParNew: 7051024K->65565K(8738176K), 0.1908720 secs] 51644093K->44660271K(82138496K), 0.1942620 secs] [Times: user=2.71 sys=0.00, real=0.20 secs] 
2017-07-06T17:20:07.543+0800: 36901925.997: [GC2017-07-06T17:20:07.545+0800: 36901925.999: [ParNew: 7056157K->67362K(8738176K), 0.1630430 secs] 51650863K->44662989K(82138496K), 0.1662440 secs] [Times: user=2.35 sys=0.02, real=0.17 secs] 
2017-07-06T17:20:22.728+0800: 36901941.181: [GC2017-07-06T17:20:22.730+0800: 36901941.184: [ParNew: 7057662K->84616K(8738176K), 0.2662860 secs] 51653290K->44681927K(82138496K), 0.2696280 secs] [Times: user=4.49 sys=0.00, real=0.27 secs], Hard to say what was causing this issue. The user/dev list is a better place to bring this up.]