[{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12670573/HDFS-7131.000.patch
  against trunk revision 7b8df93.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.fs.TestUrlStreamHandler

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8157//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8157//console

This message is automatically generated., Hi Jing.  This is a nice find.  I have just a few minor suggestions.
# Instead of {{IOUtils#closeStream}}, I recommend using {{IOUtils#cleanup}} and passing in the {{LOG}} instance.  If close fails, then logging the details might help with troubleshooting.
# Let's close {{prevCommittedTxnId}} in a finally block.  There are a few I/O operations between opening the file and closing it.  If one of those operations gets an I/O error, we wouldn't want to leak the file descriptor.
# I don't think rollback needs to reinitialize {{committedTxnId}}.  On the next access, the existing file would get reopened by {{BestEffortLongFile#lazyOpen}}.  Since we just rolled back, I'd expect this to be the old file containing the correct transaction ID from before the upgrade.  I tried commenting out this part of the patch, and {{TestDFSUpgradeWithHA}} still passed.  Let me know if you think I missed something here.
, Thanks for the review, Chris! Update the patch to address your comments., +1 for the latest patch, pending Jenkins run.  Thank you for fixing this, Jing., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12671299/HDFS-7131.001.patch
  against trunk revision 9f9a222.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.
        {color:red}-1 core tests{color}.  Failed to build the native portion of hadoop-common prior to running the unit tests in   hadoop-hdfs-project/hadoop-hdfs

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8209//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8209//artifact/PreCommit-HADOOP-Build-patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8209//console

This message is automatically generated., The findbug is "Class org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Policy defines non-transient non-serializable instance field condition", which I believe is unrelated. Just trigger the Jenkins again., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12671299/HDFS-7131.001.patch
  against trunk revision 9f9a222.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1264 javac compiler warnings (more than the trunk's current 1263 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.crypto.random.TestOsSecureRandom

                                      The test build failed in hadoop-hdfs-project/hadoop-hdfs 

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8211//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8211//artifact/PreCommit-HADOOP-Build-patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8211//artifact/PreCommit-HADOOP-Build-patchprocess/newPatchFindbugsWarningshadoop-common.html
Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8211//artifact/PreCommit-HADOOP-Build-patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8211//console

This message is automatically generated., The findbugs warning on {{AbstractDelegationTokenSecretManager}} is unrelated and documented elsewhere.  I can't reproduce the warning in {{ReplaceDatanodeOnFailure}} locally.  I'm not sure what's going on with that one., Both the findbugs warning and the javac warning should be unrelated. I will commit the patch shortly., I've committed this to trunk and branch-2., FAILURE: Integrated in Hadoop-trunk-Commit #6114 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6114/])
HDFS-7131. During HA upgrade, JournalNode should create a new committedTxnId file in the current directory. Contributed by Jing Zhao. (jing: rev e9c37de485f8d4dcb04afb0d4cb887cc09d317c9)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #692 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/692/])
HDFS-7131. During HA upgrade, JournalNode should create a new committedTxnId file in the current directory. Contributed by Jing Zhao. (jing: rev e9c37de485f8d4dcb04afb0d4cb887cc09d317c9)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1883 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1883/])
HDFS-7131. During HA upgrade, JournalNode should create a new committedTxnId file in the current directory. Contributed by Jing Zhao. (jing: rev e9c37de485f8d4dcb04afb0d4cb887cc09d317c9)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1908 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1908/])
HDFS-7131. During HA upgrade, JournalNode should create a new committedTxnId file in the current directory. Contributed by Jing Zhao. (jing: rev e9c37de485f8d4dcb04afb0d4cb887cc09d317c9)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
]