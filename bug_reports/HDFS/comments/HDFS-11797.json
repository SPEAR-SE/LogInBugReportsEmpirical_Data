[We are also seeing a similar issue once in a while

{code}
 org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for blk_123456789_123456 blockMap has 0 but corrupt replicas map has 1
 org.apache.hadoop.ipc.Server: IPC Server handler 34 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from xxx.xxx.xxx.xxx:xxxxx Call#91 Retry#0 java.lang.ArrayIndexOutOfBoundsException
{code}

The issue shows up in 'getListing' operation from the client
{code}
org.apache.hadoop.ipc.RemoteException(java.lang.ArrayIndexOutOfBoundsException): java.lang.ArrayIndexOutOfBoundsException

	at org.apache.hadoop.ipc.Client.call(Client.java:1426)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy14.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:587)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy15.getListing(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1801)
	at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.hasNextNoFilter(DistributedFileSystem.java:1047)
	at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.hasNext(DistributedFileSystem.java:1022)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)
	at org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:59)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:301)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:318)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
        ...
{code}
 , Thank you [~jnp], the inconsistent data structures are a rare occurrence and have shown this behavior (whether in the form of NullPointerException or ArrayIndexOutofBoundsException) since at least 2.6. A little torn on whether this should be marked as critical or not. Thoughts?, We see this error occurring a few times in recent months. Do you have a patch? It's hard to reproduce and I have not yet figured out how exactly it happened. , This does cause client operations to fail, and causes some applications to fail as well. Once a file gets in this state, recovery path back to a consistent state is not clear.
Do you have a test case or steps to reproduce this? Unfortunately, we don't have a corresponding exception trace at the Namenode., It causes fsck to fail as well, until the corrupt file is removed. The AIOOBE error was logged without stacktrace in NameNode for all occurrence for us though., bq. The AIOOBE error was logged without stacktrace in NameNode for all occurrence for us though.
You can get the stack trace, if you find the first occurrence of it. Apparently, jvm somehow omits the trace after first time., For HDFS-9958 case we did manage to find a stack trace after some searching which pointed to corrupt replica being on a failed volume. After that fix, the latest case is different since no failed volumes are involved and unlike last time I could not find a stack trace to support this exception. Clearly there are other scenarios where this inconsistency can happen. However, triggering a full block report from the nodes having the replica got rid of this bad state. [~jojochuang], since the cause of this is not known and the the one caused by volume failures has been addressed, my approach for this JIRA was to allocate the array to numNodes size and let the {{j < numMachines}} logic take care of truncating the array if needed in any rare case. 

The patch does not have a test yet. Appreciate any thoughts on this approach to the fix., I have not checked the details, but is it related to HDFS-11445 (more specifically, this [comment|https://issues.apache.org/jira/browse/HDFS-11445?focusedCommentId=15898236&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15898236]) ?, The symptoms match exactly. Thanks [~jingzhao].  Will wait for [~jnp], [~jojochuang]'s comments before closing this as a duplicate., A few months ago I knew at least one way the datastructures would lose sync, but I stack overflowed and can't remember exactly what it was other than maybe UC-related...

I have a slightly refined patch that more gracefully handles the out-of-sync conditions, and optimizes the common case (no corruption) to not redundantly analyze the locations.  Will try to post this week., Thanks Kuhu and Jing.

Quickly reviewed the patch, it does look like it fixed the same issue. If I understand the fix correctly, there are certain cases where block manager fails to remove stale replicas when block replica information is updated, causing inconsistency of corrupt replica count between block manager and other data structures. However, I am not sure if there are other (unfixed) cases where this happens as well.

[~brahmareddy] would you mind to take a look?, Thanks you all for looking into this issue. 

Hi [~kshukla], thanks for reporting and working the issue, I assume the release you are running doesn't have HDFS-11445 fix.

My understanding of HDFS-11445 is, when we tried to remove a corrupt replica, we only removed it from blockMap, and we "forgot" to remove it from the corruptReplicaMap, thus caused the inconsistency.

Hi [~daryn], if my understanding is correct here, the fix you mentioned at 

https://issues.apache.org/jira/browse/HDFS-11797?focusedCommentId=16042960&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16042960

could be a follow-up jira. Do you agree?

Thanks.

, Yes,it's almost hitting HDFS-11445 and  now {{BlockInfoContiguousUnderConstruction#setGenerationStampAndVerifyReplicas()}} and {{BlockInfoContiguousUnderConstruction#commitBlock()}} return the list of stale replicas and then removing these stored blockInfo in BlockManager.., Reviewed again. I believe this is resolved via HDFS-11445.
Also verified that for every place where a block is removed from BlocksMap, it is also removed from CorruptReplicasMaps. So I think we can close as a dup of HDFS-11445., I'm going to close it as a dup of HDFS-11445. Feel free to reopen if this is not the case. Thanks [~kshukla]!, For future reference, I can confirm an occurrence of this bug happened to a customer of us, and I was able to find the sequence of incidences leading to this bug, which is exactly what HDFS-11445 fixes.

# A datanode was shutdown, making the replica stale.
# NameNode detected the staleness, adding it to corruptReplicaMap. Because the replica was on a DataNode that was out of date, the replica was not invalidated. So the corruptReplicaMap had the replica, and blockMap had the replica as well.
# The block was updated, causing the stale replica removed from blockMap. *but it was not removed from corruptReplicaMap*
# a client calling getBlockLocations caused AIOOE because of the mismatch.
{noformat}
2017-10-10 14:48:10,664 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Inconsistent number of corrupt replicas for blk_1041920008_1133174794 blockMap has 0 but corrupt replicas map has 1
2017-10-10 14:48:10,665 WARN org.apache.hadoop.ipc.Server: IPC Server handler 5 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 10.103.4.11:56487 Call#5239908 Retry#0
java.lang.ArrayIndexOutOfBoundsException: 2
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createLocatedBlock(BlockManager.java:982)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createLocatedBlock(BlockManager.java:929)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.createLocatedBlocks(BlockManager.java:1031)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2059)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2008)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1921)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:572)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:89)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)
{noformat}]