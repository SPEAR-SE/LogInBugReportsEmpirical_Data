[bq. how this impacts other applications 

All signs point to that it's a big enough problem that it's being treated as a kernel bug.  See, e.g., https://bugs.launchpad.net/ubuntu/+source/commons-daemon/+bug/1699772 

We probably shouldn't get too worried about this for Hadoop until we see what everyone else in doing. Plus Hadoop 3.x already provides a way to do a temporary work around: just replace the hadoop_start_secure_daemon function in hadoop-user-functions.sh., We can also use {{-Xss1280k}} as workaround as suggested [here | https://issues.apache.org/jira/browse/DAEMON-363?focusedCommentId=16060779&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16060779], this value will be a safer option as it's closer to the default value 1024k.
This has to be set for jsvc process.
After setting this value, tested the stack size allocated for Threads in java process launched by jsvc

{quote}
7f962acb2000-7f962adf0000 rw-p 00000000 00:00 0   stack:19767    Size: 1272 kB
--
7f962adf3000-7f962af31000 rw-p 00000000 00:00 0    stack:19766    Size: 1272 kB
--
7f962af34000-7f962b072000 rw-p 00000000 00:00 0   stack:19765    Size: 1272 kB
--
7f962b075000-7f962b1b3000 rw-p 00000000 00:00 0   stack:19764   Size: 1272 kB
--
7f962b1b6000-7f962b2f4000 rw-p 00000000 00:00 0    stack:19763   Size: 1272 kB
{quote}, I've written up a blog post on how to get around this in Hadoop 3.x: https://effectivemachines.com/2017/06/24/workaround-secure-datanode-crashes/ ., [~aw] Thanks for the comments and the links to the blog and ubunutu. As you suggested let us hold off on any fix in Hadoop, until we get a better picture. 
From the ubuntu link we can see that this is a major regression causing failures across java stack.  , As per this RedHat Errata ([https://access.redhat.com/errata/RHBA-2017:1674]), upgrade to newer Kernel 3.10.0-514.26.2.el7.x86_64 fixes the issue. FYI.]