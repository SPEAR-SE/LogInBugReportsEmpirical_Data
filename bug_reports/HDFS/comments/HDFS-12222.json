[Ping [~rkanter] since we discussed this offline.

Also, I see that FIF uses a lot of BlockLocation methods. I wonder what "getOffset" for instance really should return for parity blocks., Hi [~andrew.wang], For usages like FIF BlockLocation does it make more sense to expose parity blocks and data blocks via different functions in LocatedFileStatus? BlockLocation seems to be lower abstraction for FileInputFormat. 

{code} if (file instanceof LocatedFileStatus) {
          blkLocations = ((LocatedFileStatus) file).getDataBlockLocations();
        } else {
          blkLocations = fs.getFileDataBlockLocations(file, 0, length);
        }
{code}, I understand that we need to avoid the confusion between erasure coded and replicated files when calculating splits for the format. If we want to expose more info like data blocks and parity blocks (or their locations), one question is what we can do with them? Note all the blocks or locations need to be used together to read the data., I like Ajay's idea, since it makes it very easy for current users to migrate their current code. One downside compared to an iterator is that it allocates another array of BlockLocation references, but since this array is small I think it's not a big deal.

[~drankye] could you elaborate on your question? This new API would only be used by schedulers or other locality-aware apps., bq. This new API would only be used by schedulers or other locality-aware apps.
This addressed my question, so the exposed info is useful. Thanks [~andrew.wang]!, Hi guys, I'd like to take this one., Impala also relies on the BlockLocation array returned from LocatedFileStatus.getBlockLocations() for scheduling. See for example HdfsScanNode#computeScanRangeLocations():
https://github.com/apache/incubator-impala/blob/master/fe/src/main/java/org/apache/impala/planner/HdfsScanNode.java

I'm certainly not an HDFS expert, but as a user I'd be happy with something like a BlockLocation#isParity() method or returning a special invalid value for BlockLocation#getOffset() for distinguishing between data and parity blocks. I'm also fine with [~ajayydv]'s proposal to separate the data and parity block APIs, but I wonder if we can leave the existing getBlockLocations() call and have it only return data blocks (without parity blocks) for maintaining API compatibility. In most cases, I don't think users should/will care about the parity blocks., I've checked the related code and found it is not easy to provide other functions to get parity or data blocks.
The problem is, LocatedFileStatus is a subclass of FileStatus, both located in the hadoop-common module, which does not have file related erasure coding policy information. Without that specific policy information, LocatedFileStatus has no idea which BlockLocation is actually a parity block. 

After discussed with Kai offline, one approach is to add an ECSchema into LocatedFileStatus so that we can determine which blocks are parity blocks if erasure coding is enabled. 
Any suggestions here? Thanks., I thought a little bit more about this, Huafeng. Could you help check if it works for you? Thanks!

Even we use ECSchema info in hadoop common side codes, it's still tricky to use that info to parse for a erasure coded block locations in hadoop common side since we may need couple with HDFS internals.

Could we have a hadoop common class like {{ErasureCodedBlockLocation}} which contain methods to get data/parity block locations plus cell size info and it can be passed into a new {{LocatedFileStatus}} constructor. The object of ErasureCodedBlockLocation can be constructed with parsed info in HDFS side., Hi [~drankye],  you're right. I think it's a better way and I'll give it a try., Hi guys, I just uploaded an initial patch which only sketches the basic idea. 
In the current implementation, the LocatedFileStatus that FIF fetched is transformed from HdfsLocatedFileStatus if the underlying file system is HDFS. And the BlockLocation is actually a block group in the erasure coding case. 
In my first patch, I added an ErasureCodedBlockLocation into LocatedFileStatus and this property will be set if HdfsLocatedFileStatus is erasure coded., Thanks for picking this up Huafeng!

I liked Alex's proposal above to have the current public APIs for BlockLocation return just data blocks. Then rework the HDFS internals that need both data and parity blocks to call a new API. We might already have this separation in place, since the DFSClient uses the private-only LocatedBlock and HdfsFileStatus classes.

This sketch looks a bit different, but I think can be tweaked to fit. A couple review comments:

* If we agree that FileSystem users likely don't care about the details of the EC schema or even the parity blocks, then we don't need the ErasureCodedBlockLocation class. Just change the makeQualifiedLocated and related to just return data blocks.
* We need to be careful about strictly adding new methods when adding a new parameter, for compatibility., Using the existing API to return data block locations ignoring parity ones and adding a new API (or adapting existing EC related API) to return something like ErasureCodedBlockLocation should work fine to me. Besides the data block locations info, {{cellSize}} is also important because without it, you won't be able to know how much to read for a cell ( I assume somebody may hack like, not using DFSClient API to read)., I'd hope that there aren't any clients circumventing DFSClient to read data :) AFAIK, Hadoop apps universally read through the Hadoop input streams.

Also, if someone is going below DFSClient to read, they can also get the cellSize from the LocatedBlock/HdfsFileStatus information. It's still being sent on the wire protocol, it just wouldn't be exposed via the public Java APIs., bq. Also, if someone is going below DFSClient to read, they can also get the cellSize from the LocatedBlock/HdfsFileStatus information.
Yes, agree. It has to be that, since much more info would be needed to support that kind of hack and only HDFS specific API can provide.

So looks like what's exactly needed would be just having the existing API return all the data block locations in EC case instead of the replication locations. The location info can be used to support data processing scheduling, pretty enough and clean. Cool!, I just tweaked the patch according to your suggestions. Is it on the right way? 
And about the new API that returns both data and parity blocks, I tend to place this API in DFSClient and DistributedFileSystem, something like 
{code}
public ErasureCodedBlockLocation getECBlockLocation(Path p);
{code}

Is it a proper way to do that?, Hi [~andrew.wang], any comment on my latest update?, Hi Huafeng, thanks for the rev, looks good. Please proceed with a full patch., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 29s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 49s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 32s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 57s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 14m 18s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m 13s{color} | {color:orange} root: The patch generated 12 new + 225 unchanged - 1 fixed = 237 total (was 226) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 59s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 30s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 28s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  2m 54s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 30s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 87m 40s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:71bbb86 |
| JIRA Issue | HDFS-12222 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12885301/HDFS-12222.003.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 640e4ed9ab53 3.13.0-123-generic #172-Ubuntu SMP Mon Jun 26 18:04:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / ed162b7 |
| Default Java | 1.8.0_144 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/20994/artifact/patchprocess/diff-checkstyle-root.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/20994/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core U: . |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/20994/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Hi Huafeng, thanks for working on this! A few review comments:

* Please fix checkstyles
* Sorry I missed this from your last comment: I thought we didn't need the new getECBlockLocation method since normal users don't need to know about parity blocks. Can we remove DistributedFileSystem#getECBlockLocation and DFSClient#getECBlockLocation and the ECBlockLocation class?
* Looks like some places where a user can get a BlockLocation will still include parity blocks, like getFileBlockLocation. Could you also check the rest of the FileSystem API? We should add test coverage for these APIs too: getFileBlockLocation, listFiles, listLocatedStatus, etc.
* Also need to do the same changes for Hdfs.java for the FileContext API
* Need javadoc / comments in HdfsLocatedFileStatus and wherever else we modify to explain the intent. It'd also be great to have an example of the expected format of BlockLocation[] for a replicated vs. EC file.
* Could you verify that fsck -files -blocks -locations still returns parity blocks? We should add a unit test for this if there isn't one.
* Also, could you add tests with a files of different sizes, e.g. bigger than one block group? It NPEs right now for me when I call listLocatedStatus in TestWriteReadStripe#testFileMoreThanABlockGroup1., Hi [~andrew.wang], thanks for your review! I just uploaded a new patch. In this patch I mainly:
* Removed the getECBlockLocation function and ECBlockLocation class.
* Fixed {{getFileBlockLocation}} of DFSClient.
* Add comments about {{getFileBlockLocation}}, {{listFiles}} and {{listLocatedStatus}} in {{FileSystem}}, {{DistributedFileSystem}} and {{FileContext}}
* Add comments about {{makeQualifiedLocated}} in {{HdfsLocatedFileStatus}}
* Add tests for {{DistributedFileSystem.getFileBlockLocation}}, {{DistributedFileSystem.listFiles}}, {{FileContext.getFileBlockLocation}} and {{FileContext.listFiles}} in case of ec with various file size.

And about 
{quote}
Could you verify that fsck -files -blocks -locations still returns parity blocks?
{quote}

I checked the output of {{fsck -files -blocks -locations}}, it does not have very detailed block location info of an erasure coded file. An output example of a 6+3 eraure coded file will be like 
{code}
0. BP-417570284-10.239.160.132-1504687036886:blk_-9223372036854775792_1001 len=6291456 Live_repl=9  [blk_-9223372036854775792:DatanodeInfoWithStorage[127.0.0.1:54859,DS-09a24593-5cbc-444c-ad43-ab1b39c65887,DISK](LIVE), blk_-9223372036854775791:DatanodeInfoWithStorage[127.0.0.1:54863,DS-80d7a2bb-5acc-437c-936a-bd28314e2a8c,DISK](LIVE), blk_-9223372036854775790:DatanodeInfoWithStorage[127.0.0.1:54883,DS-05a880c7-0fa2-4683-a382-06ec7d975fd3,DISK](LIVE), blk_-9223372036854775789:DatanodeInfoWithStorage[127.0.0.1:54854,DS-8a5cf2da-1c7e-4942-b57c-8755ddb3cfcb,DISK](LIVE), blk_-9223372036854775788:DatanodeInfoWithStorage[127.0.0.1:54871,DS-95c64656-3131-413c-b400-0f14612b387d,DISK](LIVE), blk_-9223372036854775787:DatanodeInfoWithStorage[127.0.0.1:54867,DS-fbf6ea90-8829-44ce-8681-b5f53be726c1,DISK](STALE_BLOCK_CONTENT), blk_-9223372036854775786:DatanodeInfoWithStorage[127.0.0.1:54875,DS-d40bfede-c5c9-4cb0-8b5e-92ead1bbb4da,DISK](LIVE), blk_-9223372036854775785:DatanodeInfoWithStorage[127.0.0.1:54879,DS-c999124f-3d0e-4f6c-bd31-5f0fdff86fca,DISK](STALE_BLOCK_CONTENT), blk_-9223372036854775784:DatanodeInfoWithStorage[127.0.0.1:54850,DS-7ff8f0ed-b62a-40a9-8966-b16f71532712,DISK](LIVE)]
{code} 

So you mean we should also remove the parity blocks info?, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 20s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 38s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m  4s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 34s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 10m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 10m 53s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  2m  9s{color} | {color:orange} root: The patch generated 6 new + 486 unchanged - 1 fixed = 492 total (was 487) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 33s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  7m 36s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 22s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}101m 32s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  3m 32s{color} | {color:green} hadoop-mapreduce-client-core in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 36s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}190m 28s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.TestReconstructStripedBlocks |
|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure160 |
|   | hadoop.hdfs.TestClientProtocolForPipelineRecovery |
|   | hadoop.hdfs.server.blockmanagement.TestReconstructStripedBlocksWithRackAwareness |
|   | hadoop.hdfs.server.namenode.TestReencryptionWithKMS |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure170 |
|   | hadoop.hdfs.server.blockmanagement.TestBlockManager |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure020 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure090 |
|   | hadoop.hdfs.TestLeaseRecoveryStriped |
|   | hadoop.hdfs.TestReadStripedFileWithDNFailure |
|   | hadoop.hdfs.TestErasureCodingPoliciesWithRandomECPolicy |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure150 |
| Timed out junit tests | org.apache.hadoop.hdfs.TestWriteReadStripedFile |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:71bbb86 |
| JIRA Issue | HDFS-12222 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12885764/HDFS-12222.004.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 513cbdc5b1f7 3.13.0-123-generic #172-Ubuntu SMP Mon Jun 26 18:04:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / b6e7d13 |
| Default Java | 1.8.0_144 |
| findbugs | v3.1.0-RC1 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/21035/artifact/patchprocess/diff-checkstyle-root.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/21035/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21035/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core U: . |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21035/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Hi Huafeng, thanks for revving,

I was most of the way through reviewing this patch, but decided to pull out a debugger to understand the current format. I might have mis-interpreted the format when I filed this bug, we might not need to change anything at all.

The BlockLocations are modeled right now as one location per block group. For an RS(3,2) file of length 4*BLOCK_SIZE+123, it'd look like:

{noformat}
BlockLocation(
    offset: 0,
    length: 3*BLOCK_SIZE, 
    hosts: {"host1:9866", "host2:9866", "host3:9866", "host4:9866", "host5:9866"})
BlockLocation(
    offset: 3*BLOCK_SIZE,
    length: 123,
    hosts: {"host1:9866", "host4:9866", "host5:9866"})
{noformat}

I think this is parsed fine by FileInputFormat and Impala and so on. It looks like a single really big block. We don't even need to remove the parity blocks. Impala at least has the idea of {{maxScanRangeLength}} so it can split really big blocks.

Huafeng, what do you think? If you agree, we can still use this JIRA to add better Javadocs to explain what BlockLocation[] looks like for EC files, and also the test cases., Hi Andrew, I agree with you. I'll update the patch soon., Hi Huafeng, thanks for working on this, I took a look at the 005 patch, a few review comments:

* I recommend moving the BlockLocation[] format example to FileSystem#getFileBlockLocations and FileContext#getFileBlockLocations, which are public APIs. Then, we can link to it from the other code locations.
* Related, the javadoc links to TestDistributedFileSystemWithECFile don't work since prod code doesn't depend on test, recommend instead doing the above change and linking to the public method
* Recommend adding a link to FileSystem#getFileBlockLocations to LocatedFileStatus#getBlockLocations.
* Recommend adding a format explanation for a single BlockLocation (replicated and EC) to BlockLocation as well, along with the link to FileSystem and FileContext.

Regarding this blurb:

{code}
   * In HDFS implementation, the BlockLocation of returned LocatedFileStatus
   * will have different formats for replicated and erasure coded file. Please
   * refer to HDFS for more details.
{code}

I'd prefer we drop this in most places, since this format should be compatible for downstreams, and it'll be well documented on the public API classes noted above., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 26s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 16m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 16s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 15s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 18s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 13m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 13m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  2m 10s{color} | {color:green} root: The patch generated 0 new + 354 unchanged - 2 fixed = 354 total (was 356) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 58s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 17s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  8m 14s{color} | {color:red} hadoop-common in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 24s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red}105m 52s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 29s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}193m 26s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.net.TestDNS |
|   | hadoop.hdfs.TestReplaceDatanodeOnFailure |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure000 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure100 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure060 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure210 |
|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure010 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure130 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure200 |
|   | hadoop.hdfs.TestReconstructStripedFile |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure170 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure040 |
|   | hadoop.hdfs.TestClientProtocolForPipelineRecovery |
|   | hadoop.hdfs.TestLeaseRecoveryStriped |
|   | hadoop.hdfs.TestEncryptedTransfer |
| Timed out junit tests | org.apache.hadoop.hdfs.TestWriteReadStripedFile |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:71bbb86 |
| JIRA Issue | HDFS-12222 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12886588/HDFS-12222.006.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 682171c88b03 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / e74d1be |
| Default Java | 1.8.0_144 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/21087/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/21087/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/21087/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs U: . |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/21087/console |
| Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, +1 LGTM thanks for working on this Huafeng, will commit shortly., Committed to trunk and branch-3.0, thanks for the contribution Huafeng!, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #12856 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/12856/])
HDFS-12222. Document and test BlockLocation for erasure-coded files. (wang: rev f4b6267465d139bfdaf75e25761672eaf61d8a11)
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileContext.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/BlockLocation.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/AbstractFileSystem.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsLocatedFileStatus.java
* (add) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystemWithECFile.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocatedFileStatus.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/fs/Hdfs.java
, Thanks [~andrew.wang] for your advice and help!]