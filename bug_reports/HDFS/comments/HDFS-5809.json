[In BlockPoolSliceScanner, the code below

if (((now - getEarliestScanTime()) >= scanPeriod) || ((!blockInfoSet.isEmpty()) && !(this.isFirstBlockProcessed()))) { verifyFirstBlock(); } else { ....}

may cause this problem in my opinion.
After three weeks from last block pool scanning, the condition : (now - getEarliestScanTime()) >= scanPeriod) will be true, and at this time, race between scanning and hdfs append may make the datanode drop into the infinite loop. Because code which update the EarliestScanTime place after the code which throw the FNFE.

When i changed scanPeriod to 1 hour, the datanode drop into infinite loop in several minutes. 
Then i changed scanPeriod to default : 504 hours, the datanode did not drop into infinite loop in a long time., Please execute my poor English. : )
I found that int BlockPoolSliceScanner, blockInfoSet can contain two block which has the same block id, because BlockScanInfo compare by lastScanTime. 
Then int method updateScanStatus, the BlockScanInfo can not be updated, so ((now - getEarliestScanTime()) >= scanPeriod) will be always true. 
This cause datanode drop into infinite loop. , My ugly path like this:
{code}
    if ( info != null ) {
      delBlockInfo(info);
    } else if(blockInfoSet.contains(block)){
        delBlockInfo(info);
        info = new BlockScanInfo(block);
    } else {
      // It might already be removed. Thats ok, it will be caught next time.
      info = new BlockScanInfo(block);
    }
{code}, After change code like above, datanode didn't drop into infinite loop last three days with scanPeriod set 1 hour., Hi ikweesung,

Thanks for finding this.  Do you mind if I take this one?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12647684/HDFS-5809.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7018//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7018//console

This message is automatically generated., test failure is HDFS-3930, not related., Hi Colin Patrick McCabe.
Thank you for you take this one.  
, +1, the patch looks good to me. I agree that writing a unit test for this would be fairly difficult, and the fix is really quite clear, so I'm OK committing it without a test.

Thanks a lot for taking care of this, Colin, and tanks much to ikeweesung for reporting this issue., SUCCESS: Integrated in Hadoop-trunk-Commit #5883 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5883/])
HDFS-5809. BlockPoolSliceScanner and high speed hdfs appending make datanode to drop into infinite loop (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1610790)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/DataNodeTestUtils.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #614 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/614/])
HDFS-5809. BlockPoolSliceScanner and high speed hdfs appending make datanode to drop into infinite loop (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1610790)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/DataNodeTestUtils.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1833 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1833/])
HDFS-5809. BlockPoolSliceScanner and high speed hdfs appending make datanode to drop into infinite loop (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1610790)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/DataNodeTestUtils.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1806 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1806/])
HDFS-5809. BlockPoolSliceScanner and high speed hdfs appending make datanode to drop into infinite loop (cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1610790)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/DataNodeTestUtils.java
]