[Hello [~chackra].  You mentioned that you saw this in Hadoop 2.6.0.  You might be interested in several enhancements in block reporting in later versions.  I think the most relevant ones for the symptoms you describe are HDFS-7097 and HDFS-7435.  Some other good ones are HDFS-7923 and HDFS-9710., Cluster details :

Version - Hadoop-2.6.0
No of datanodes - 1200
NN hardware - 74G heap allocated to NN process, 40 core machine
Total blocks - 80M+
Total Files/Directories - 60M+
Total FSObjects - 150M+
ipc.ping.interval=60s (default)
dfs.blockreport.initialDelay=120
dfs.namenode.service.handler.count=600

NN takes more than 1 minute to process FBR because of write lock getting released while processing report for each storage. Since block report initial delay set to 120s, NN is flooded with FBR from all DN's. After processing report for each storage, lock contention is more and by the time it completes processing for all storages (10 data dirs configured), DN gets timeout. 

What if we acquire lock at the start and release it only after processing reports for all storages? Since FBR call frequency is very less (only during startup of NN or DN, once every 6 hours, or when a disk failure happens in DN) will this change impact the normal heartbeat/IBR flow? Or acquiring lock at each storage report processing is done intentionally? I could not find any comment related to this in HDFS-4987 Please correct me If I am wrong.

I could see possible config options to overcome this to increase block report initial delay or increasing ipc.ping.interval. Also may be 600 is not correct value for service handler count. Is there any guideline to set service handler count depending upon cluster size?

Thanks.
, Thanks [~cnauroth] for the response. These fixes seems relevant to resolve the issue which we are facing currently. We will see if we can backport these fixes.

As a quick fix to handle in 2.6.0, do you think this can be solved by tuning any config? And is there any guideline to set service handler count depending upon cluster size?, Hi [~chackra], Chris mentioned a good list of Jiras to help with this problem.

Meanwhile you can try increasing {{dfs.blockreport.initialDelay}}. A rule of thumb I often use is 2*numDataNodes seconds. So for a 500 node cluster, I'd set it to 1000. It is a very conservative value and it increases the total NN startup time but I find it effective to improve the startup stability of the NameNode on older releases like 2.6.x that don't have some of the performance fixes listed by Chris.
, Thanks [~arpitagarwal] for the suggestion. Sure, will increase the {{dfs.blockreport.initialDelay}} and try it. Do you suggest to decrease {{dfs.namenode.service.handler.count}} from 600 (1200 node cluster)? Because other than heartbeat, the most frequent service RPC call will be IBR as there could be multiple IBR's between 2 successive heart beat (interval set to 10s). IBR also needs write lock and hence not sure whether 600 handler count really helps here or not., 600 is rather high. 200 handlers should be plenty for a 1200 node cluster., A good rule of thumb for handler count is _20 * log2(num nodes)_. This one is from the excellent _Hadoop Operations_ book., I checked one of our mid-size clusters with 2,400 nodes and 120M blocks. During the last start-up, the start-up safe mode was about 14 minutes long. But it runs 2.7.2 +  patches, so it might behave a bit differently from yours. There are few things that affect the safe mode time greatly:
- A bigger initial delay so that reports are more widely spaced
- Sufficiently big CMS "young gen" size to absorb influx of large requests. So far CMS seems to work better than G1 for big namenodes.
- Have datanodes break up full block reports by storage. This  makes each FBR RPC smaller, so the impact of timeout-retransmit can be lower.

I think the number of handlers is too high. It makes the call queue bigger, so more things will queue up and timeout., Thanks [~arpitagarwal], Hi [~kihwal], Thanks for the pointers. In our cluster, NN is configured with G1. 
In our case also, NN comes out of safe mode in 15 or 20 mins. But still it is flooded with FBR from DN's as all FirstFBR gets timed out and NN gets error only while sending output (but updates its state and comes out of safe mode).
{quote}
Have datanodes break up full block reports by storage. This makes each FBR RPC smaller, so the impact of timeout-retransmit can be lower.
{quote}
Are you suggesting to tune {{dfs.blockreport.split.threashold}} to make DN to send FBR per storage? currently average total blocks per DN is 200k around. So if I reduce {{dfs.blockreport.split.threashold}} from 1Million (default) to 100k or 150k, then this would make FBR RPC smaller. Is this what you meant?, bq. Are you suggesting to tune dfs.blockreport.split.threashold ...
Yes. Each FBR rpc will be smaller, so the impact of timeout-retransmit will be lower. Also NN will process individual report quicker.

NN creates lots of temporary objects and their life time can be longer than expected, especially when requests need to sit in the call queue for a long time. A larger CMS young gen has been shown to absorb them well and the young gen collection time does not become excessive as most objects get freed rather than copied. Of course it is a bit different during start-up when the data structures are being built up.  We thought a high rate of allocation and freeing will go well with what G1 is designed for, but so far we haven't found a magic recipe that works better than CMS for large namenodes.   I hear it does a better job for HBase region servers though., [~kihwal] These are valuable inputs for us. Thanks.
{noformat}
Yes. Each FBR rpc will be smaller, so the impact of timeout-retransmit will be lower. Also NN will process individual report quicker.
{noformat}
By doing so, are we not delaying the next heartbeat sent from DN too long as each RPC call might consume upto 60s. Or this is affordable to do since FBR will happen only once in 6 hours? , Closing this jira as the issue was resolved after increasing {{dfs.blockreport.initialDelay}} value. Thanks for all the inputs.]