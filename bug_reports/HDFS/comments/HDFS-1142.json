[Here's a patch that makes lease recovery reassign to the NN lease holder.

One thing I am somewhat worried about (not new with this patch, but related) is that lease reassignment is not logged to the edit log. Thus, if the NN restarts, the lease will revert to the old holder. Thoughts?, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444164/hdfs-1142.txt
  against trunk revision 942863.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/174/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/174/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/174/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/174/console

This message is automatically generated., Failed test is the one that's been failing all the recent builds., New patch on top of trunk (since HDFS-1141 got committed), can we re-run hadoopQA tests on the new patch?, Yea, I toggled patch available when I uploaded, should be on the queue., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444247/hdfs-1142.txt
  against trunk revision 943306.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/356/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/356/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/356/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/356/console

This message is automatically generated., Toggling cuz Hudson build screwed up (silly NoClassDefFound thing, not this patch), -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12444247/hdfs-1142.txt
  against trunk revision 943306.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/357/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/357/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/357/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/357/console

This message is automatically generated., +1 code looks good to me. can somebody else review this patch too?, Hey Todd, one question:  why does FSNameSystem.internalReleaseLease() now take the recovery holder as a parameter?  It's only called in 2 places (startFileInternal and checkLeases) and both cases use the same constant.  The patches you had for hdfs-142 had the constant within the function when you called reassignLease().  Is there a reason to move it up?  just wondering if you anticipate the recovery lease holder being something other than a constant that needs to flow from further up?  (otherwise it's just a parameter that's always a constant), oh, I should add, looks good though :)
(just curious on that one)
, sam: actually, that change was done in a prior HDFS patch by Konstantin. I asked him to take a look at this one as well before we commit it. I agree that we can probably drop the argument now, but let's wait to hear from Konst., oh right, I see that didn't change.

+1 on this diff then.  lgtm, I guess it would be good to save the lease in the edits log when the owner of the lease changes to "NN-leaseholder". This will ensure that when the NN restarts, the lease does not revert back to the original client., I opened HDFS-1149 to persist lease reassignment to the edit log., Sorry, took me a while.
The idea with lease recovery after soft limit expiration was that it is done under the same lease holder. Here is why.
Expiration of the soft limit means that somebody else can claim the lease, and if he succeeds, then he is the new owner, if not, then not.
So here several clients may compete for the same lease. They will call {{create()}} and get {{RecoveryInProgressException}} in response, which indicates that they should retry. The old client if still there can also compete for the lease. It has an advantage over other clients, because it does not need to go through the recovery process, but that seems fair.
If you reassign the lease to {{HDFS_NameNode}}, then its timeouts will reset, see {{reassignLease()}}. And this will change the behavior. The clients trying to claim the file will be getting {{AlreadyBeingCreatedException}}, which means they cannot compete for the file anymore, and should fail.
Suppose there is only one new client, and the old owner had died already. The client tries {{create()}}. This triggers lease recovery on NN, which starts the recovery under {{HDFS_NameNode}}, and throws {{RecoveryInProgressException}} back to the client. The client retries as expected, and the next time gets {{AlreadyBeingCreatedException}}. Thinking that somebody else got lucky before him the client bails out, which is not right as there is nobody esle competing for the file. 
Does that makes sense? I don't see a problem here. Do you have failing tests because of that?
That by the way explains the parameter {{internalReleaseLease()}}

- Introduction of {{NN_LEASE_RECOVERY_HOLDER}} constant definitely makes sense.
- Persisting leases is not an issue if we do not reassign.
- For future reference it is very undesirable to declare public methods in {{FSNamesystem}} to provide access to them from tests. The tests should either be in the right package or alternatively the {{FSNamesystem}} methods should be access via {{NameNodeAdapter}}, that's why it was introduced in the first place, see HDFS-563.
, Hi Konstantin. Thanks for the detailed response.

bq. Suppose there is only one new client, and the old owner had died already. The client tries create(). This triggers lease recovery on NN, which starts the recovery under HDFS_NameNode, and throws RecoveryInProgressException back to the client. The client retries as expected, and the next time gets AlreadyBeingCreatedException. Thinking that somebody else got lucky before him the client bails out, which is not right as there is nobody esle competing for the file. 

What if we specifically compare the holder to the HDFS_Namenode special value, and in this case throw RecoveryInProgressException instead of AlreadyBeingCreatedException?

bq. Does that makes sense? I don't see a problem here. Do you have failing tests because of that?

Yes - please see the new test case included in the patch above. The issue is that the client can continue to do things like completeFile or allocate new blocks while recovery is underway.

bq. For future reference it is very undesirable to declare public methods in FSNamesystem to provide access to them from tests

I agree. However, in order to do mockito spying on commitBlockSynchronization, using a trampoline class like NameNodeAdapter would not work. If you agree with my above points, I can see if I can move the spy call into NameNodeAdapter itself.
BTW, isn't this the point of the "Private" InterfaceAudience annotation?


Let me know if you agree with the above idea (throwing RecoveryInProgressException when the lease is held by HDFS_NameNode)., Comparing the holder with HDFS_Namenode will work, but current logic should work too, so why change it.
I suspect this is more like a problem of completeFile() and the new block allocation, than the lease recovery's. But let me look deeper into the test cases.

"Private" InterfaceAudience annotation will not prevent people from using the methods., One of the tests {{testRecoveryOnBlockBoundary()}} fails because you explicitly call
{code}
stm.hflush();
fail("Sync was allowed after recovery started");
{code}
The test +assumes+ that {{hflush()}} should not succeed after the recovery started, but this assumption is incorrect.
Btw, the test falls into infinite loop after that, because you infinitely trying to append to a file when the file system is already closed.
The other two tests run fine with current code., bq. The test assumes that hflush() should not succeed after the recovery started, but this assumption is incorrect.

HBase is using contracts like this to do IO fencing during regionserver recovery operations (eg see discussion on HBASE-2231 and HBASE-2238). The ability to use lease recovery to lock out a writer whose soft lease has expired provides lock-like functionality tied to the storage. Without HDFS itself providing a file locking primitive, it becomes essentially impossible to do correct recovery in systems like HBase where an old writer needs to be forcibly disallowed from continued progress. Using an external system like ZK since it is asynchronous in nature.

Let me think if we have an alternate route to the same kind of behavior using the semantics you're describing., Lease recovery works as designed, imo.
The lock service is provided by ZooKeeper.
I don't think we should overload the semantics and complicate things that are already complex enough.

May be setting harld_limit = soft_limit will help with fencing. It seems this is the behavior you are looking for.

I'll remove the blocker flag for now., Hey Konstantin,

I agree that this shouldn't be marked blocker while discussion is going on.

Let me better explain the context with regards to HBase. HBase uses ZK already to determine regionserver liveness. If a region server dies, it loses its ZK session, and thus an ephemeral znode disappears. The master notices this, initiates commitlog recovery for that server, and eventually reassigns the regions elsewhere. To provide proper database-like semantics, we need to ensure that once log recovery commences, the regionserver cannot write any more to that log (otherwise writes might be lost forever).

Of course this all works fine if the regionserver has truly died. A big issue we face, though, is one of long garbage collection pauses (sound familiar?). In some cases, the pauses can last longer than the zk session timeout. Thus, the hbase master decides that the server has died and does log splitting, region reassignment, etc. Unfortunately, in this scenario, the region server then comes back to life and flushes a few more writes to the log file, which summarily get lost forever even though the client thinks they're committed. The regionserver eventually "notices" that it lost its ZK session and shuts itself down, but in practice it often has time to get off some last edits before doing so.

Clearly, using locks in ZK is subject to the same issue above - the issue is that our ZK coordination is not synchronous with our storage access.

There are two solutions I can think of here: (a) the "STONITH" technique ( http://en.wikipedia.org/wiki/STONITH ) - we could run the regionservers in a container service which allows us to kill -9 the regionserver when we think it should be dead. But this is obviously more complicated with regard to deployment, additional RPCs, etc. (b) file access revocation - this is what we're trying to do with lease recovery and what you're suggesting should not be possible.

Here's a question - as you described it, the original lease holder and the recovering lease holder race to recover the lease. If the original holder wins the recovery, are we guaranteed that no interceding appends have occurred? eg what happens if the recovering process wins, opens the file for append, and immediately closes it. Are we guaranteed then that another flush() call from the client at that point would definitely fail, or can it transparently regain the lease from the now-closed file?, bq. May be setting harld_limit = soft_limit will help with fencing. It seems this is the behavior you are looking for.

Since this is a cluster-wide setting I don't think it's a good idea. We only want this kind of lease-stealing behavior for hlog recovery - for other normal HDFS access, we definitely want the original lease holder to be able to renew its lease after soft limit has elapsed (assuming that no one else has tried to steal it), Todd,

Thanks for elaborating on the HBase / ZK interaction. Sounds like you are trying to build HA there.

> Since this is a cluster-wide setting I don't think it's a good idea. 

In my previous comment I explained what the soft limit means - the original client is able to reclaim its lease by simply renewing it. Expiration of the hard limit means that the original client does not own the file anymore, and can claim the ownership only via reopening the file, same as everybody else. 
After soft limit expires hdfs can either reassign it to a new client or keep under the old ownership. Reassigning it to NN during lease recovery contradicts the definition of the soft limit. The revoking ownership behavior you are describing corresponds to expiration of the hard limit.

I do not understand how (b) file access revocation can be solved with lease recovery, even with your modified semantics. The region server may wake up before soft limit expiration but after the region reassignment.
What really needed is to first revoke file access, then start reassigning regions. Revoking file access is achieved by just opening it with another client. AFAU, regions reassignment should wait until the access is revoked.

> Since this is a cluster-wide setting I don't think it's a good idea. We only want this kind of lease-stealing behavior for hlog recovery

But you are talking about clusterS-wide change of semantics only for those hlog recoveries.

I will be glad to continue discussing HBase problems. But "Lease recovery doesn't reassign lease when triggered by append()" is not a bug and is not a problem from HDFS point of view. 
I propose to close this issue as "Not a Problem" (The described issue is not actually a problem - it is as designed.), Let me understand this one. Suppose a client A has a write-lease on a file. It is doing some write/sync to this file. Then another client B decides to invoke append() to the same file. If we have exceeded the soft-timeout limit, then the B's append call will trigger lease recovery. However, at this point (when the lease recovery is starting), the lease is technically still owned by A. A can continue to sync/write data to the file. If client B now successfully acquires the lease, then from that moment onwards, any new sync/writes from A will start to fail. isn't that behaviour enough to support HBase region recovery?
, The question I asked above still stands:

{quote}
Here's a question - as you described it, the original lease holder and the recovering lease holder race to recover the lease. If the original holder wins the recovery, are we guaranteed that no interceding appends have occurred? eg what happens if the recovering process wins, opens the file for append, and immediately closes it. Are we guaranteed then that another flush() call from the client at that point would definitely fail, or can it transparently regain the lease from the now-closed file?
{quote}

If the above situation is prevented, I think we can rejigger the "recoverFile" operation that HBase does, and make this work. If the above is not prevented, we have to think harder. Maybe we can still do it by renaming the files after recovering the lease (since iirc in 21 the lease holder can rename a file that it holds open for write), {quote}
what happens if the recovering process wins, opens the file for append, and immediately closes it. Are we guaranteed then that another flush() call from the client at that point would definitely fail, or can it transparently regain the lease from the now-closed file?

{quote}

the original writer can transparently regain the lease.

If you want the original writer to not regain the lease, then the recovering process should keep the file opened for write and not close it., If the file is closed nobody can transparently regain the lease, because closed files don't have leases. I understand transparently as by calling write or flush, but without reopening it. So the answer to the question

> if the recovering process wins, opens the file for append, and immediately closes it. Are we guaranteed then that another flush() call from the client at that point would definitely fail

is Yes. 

I think Dhruba meant that if the recovering process closes the file, then the old client can access it by reopening it. But this is not transparent if my understanding of transparency is the same as yours.

, And if you keep the file open as Dhruba suggests then the old client will not be able to access it even by reopening., I think you've convinced me that this is indeed reasonable semantics and we can work with them by rejiggering things on the HBase side for 0.21. Thanks for the enlightening discussion., There's actually still one problem condition that can occur w/o the Namenode taking ownership of lease when it starts lease recovery:

1. Client A loses lease due to soft/hard limit at a block boundary
2. NN begins lease recovery and finalizes blocks
3. Client A gets new block, starts writing
4. NN closes the file and frees the lease
5. Client A can continue to write to blocks acquired w/o the lease
6. Client A has an error and the block is left as a blockBeingWritten in the datanode

results:
-clients cannot open the file in append since the last block has no locations
-last block shows up as a missing block
-lease recovery will never cleanup the last block since no lease exists

assigning the lease to the NN for recovery prevents step 3 above and can stop this

, note: I think this case also violates() sync semantics if during 5 the client called sync. , Sam. Sounds like you are looking at 0.20 version, because in 0.22 sync has been replaced with hflush(). Please clarify.
Could you also please indicate whether you can actually reproduce it or have a test case. If you do please open a new jira.
Looking at the code (0.22) I think there might be a problem there. 
In getAdditionalBlock() we do not reset recoveryId of the last block before allocating a new one.
The reset should be done in {{BlockInfoUnderConstruction.commitBlock()}}., Hi Konstantin,

Sorry, I am basing this mostly on the 0.20 code base.  I do not have a test case--this is purely analytical at this point and meant to be an argument for keeping this fix in 0.20 w/append + sync (hadoop 0.20 + hdfs-142 + hdfs-200).  From reading the design doc on hdfs-165, hflush would also be violated by this since the client would have received an ack for bytes in the real last block that will never be available for read.

Can you comment on this scenario with respect to 0.20?  if I get some time, I can try to create a test case for this--it seems doable.

, Thanks. Yes with 0.20 that would be a completely different issue. I did not closely follow the latest developments with sync(). Dhruba would know this better. 
There should be no violation for hflush(): if the old client renews the lease it will reset the blockRecoveryId, and the lease recovery that started before that will fail - no big deal.
I'll create a jira to investigate my findings in 0.22 (21)., hmm, the client never reacquires the lease in my example--it loses it, still is able to acquire blocks and write to the DNs, but those blocks do not complete (client dies) and report to the NN.  They are "lost" in that the lease recovery that completes by the NN will only know the block was allocated, but not any locations.  The lease will be removed and the blocks won't be recovered ever.

during this time, the client may have called sync/hflush and hence received acks for that data. This is the violation I think that might occur. I've looked at trunk and I'm not seeing how this is avoided there even...?

Thanks for your patience in discussing this--maybe there isn't a problem and I don't yet see how even trunk handles this?, konstantin : nvm last comment, i misread your comment (and forgot 22 == trunk)
, Interesting discussion...
As long as I understand, once lease recovery starts, the first thing that each replica datanode does is kill the block receiver. So the old client will fail since the pipleline is completely torn down.The idea is that no writer is allowed, i.e., no modification to the file is allowed from any client, while the file's lease recovery is in progress. So in this sense, it would be nice to reassign the lease to NN but still maintain the soft lease semantics that Konstantin described., a small sidenote:

re: killing writers, it does so *after* getting metadata, so there is still a window under which the client could start another lease recovery, it would complete, and it could start writing and call sync.  the 1st lease recovery kills threads, then truncate the block (based on the first set of lengths).  This violates sync/hflush semantics.  I don't know if there's a jira for this, but I had planned to make the change so the writers *are* killed first thing before getting meta-data.

, Sam: I believe the way it works in trunk is that writers are killed when recovery is started, and then the replicas enter the RUR state, so no more writes are allowed until a recovery happens.

In 0.20 that's not the case, see HDFS-1186, todd: ah yea, i had trunk open and just checked--it's exactly that way.  nice.  we do need 1186, though]