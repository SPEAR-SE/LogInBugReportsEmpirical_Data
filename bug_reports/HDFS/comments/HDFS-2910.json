[Repro steps
1) Start 2 NN's in active standby mode
2) Remove write permissions from shared edits dir
3) Upon log roll triggered by standby, the active gets error when finalizing the edit logs
4) FSEditLog.rollEditLogs() call endCurrentLogSegment() that hits exception. This exception is silently ignored assuming logSync will fix it later on
5) FSEditLog.rollEditLogs() now calls startLogSegment() that again hits exception. This exception is not silently ignored but is caught way up the stack.
6) After that shared edits dir comes back online by giving write permissions
7) Standby triggers log roll again. But active NN keeps complaining about BETWEEN_LOG_SEGMENTS with the following exception reported on the standby
12/02/07 00:46:55 INFO ha.EditLogTailer: Triggering log roll on remote NameNode localhost/127.0.0.1:28000
12/02/07 00:47:53 WARN ha.EditLogTailer: Unable to trigger a roll of the active NN
java.lang.IllegalStateException: java.lang.IllegalStateException: Bad state: BETWEEN_LOG_SEGMENTS
	at com.google.common.base.Preconditions.checkState(Preconditions.java:172)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.endCurrentLogSegment(FSEditLog.java:887)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.rollEditLog(FSEditLog.java:831)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.rollEditLog(FSImage.java:975)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4026)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:741)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:116)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:8068)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:439)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:878)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1608)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1604)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1602)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:203)
	at $Proxy11.rollEditLog(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:162)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:256)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.access$3(EditLogTailer.java:253)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:287), I think FSEditLog should not be starting a new segment when ending the last one failed. Specifically in this case, the failure should abortAllJournals and shutdown the HA NN.
Even if we fix the NN shutdown case, this bug still needs to be fixed or else the edit logs will be left behind in an inconsistent state.

, We should just do a hard exit here -- upon restart or failover, the new active NN will recover the logs., That is for the current policy of shutting down the NN on such errors. But if the NN continues to be active for short transient shared dir hiccups then this needs to be fixed. So I will let this JIRA remain active., In order to make the NN ride over a hiccup, it seems the solution is to add a more resilient JournalSet implementation -- ie either one that operates over a quorum of shared dirs, or one which has a more stubborn retry policy. Given that NFS itself already has built in retries and can be configured to arbitrary timeouts, it doesn't seem like we should worry about short hiccups -- any outage that makes it past the configured NFS retry/timeouts is likely to be worth causing a failover IMO., Sure. Perhaps that work would resolve this JIRA too., Hey Bikas. Is this still an issue after your other change to make it abort more aggressively on edits failure? In the absense of the more resilient JournalSet, I think the hard abort is the right answer here (same as a failed sync), Not an issue as long as the NN dies upon such errors (like it currently does). In case, that changes, this will need to be fixed because it does not work right now. So I did not close it. I tried to find a JIRA to make NN not die upon such errors but did not find one., Not an issue as long as the NN dies upon such errors (like it currently does). In case that changes, this will need to be fixed because it does not work right now. So I did not close it. I tried to find a JIRA to make NN not die upon such errors but did not find one. If that is never going to happen then we can close this., Maybe we can convert this to not be an HA subtask/component then? Trying to triage the remaining HA JIRAs to see what we need to get done before merging t trunk., Sure. How do I do that?, Bikas, you can find the 'Convert To Issue' option from 'More Actions' drop down list in the above.  , Converted this to a top-level issue per Todd's comment.]