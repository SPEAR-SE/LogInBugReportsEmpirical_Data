[[~brandonli] I wonder why TestOutOfOrderWrite directly calls Nfs3Utils.writeChannel() instead of going through OpenFileCtx. Is it *Not* supposed to test the reordering capability of the NFS gateway?, [~zhz], thanks for the question! Let's use HDFS-6855 to track the change of TestOutOfOrderWrite. Please feel free to contribute!, Hi Brandon, I'm a little confused by the description of this Jira. As I understand it, many test classes, including TestReaddir and TestWrites, are already unit tests not requiring manual startup of nfs3 and portmap. Is the purpose of this Jira to convert all remaining manual tests (such as TestUdpServer) to unit tests?, Most of the current unit NFS tests are not end-to-end tests, which means they directly invoke the internal methods like what's in TestReaddir. In this way, some funcation/feature can't be covered. For example, we can't validate the response format. This has to be validated by mounting the export and doing manual test on Linux. We found quite a few response format problems in the past, in a painful way, because we don't have enough end to end tests.

TestOutOfOrderWrite is an end-to-end test but it doesn't provide a framework or class to be used by other tests.

Specifcally, we need a few things. We may want to create more JIRAs to split the work:
1. utilities to package every NFS/mountd requests
2. utilityes to parse every NFS/mountd response
3. a test UDP client and TCP client, which can deliver request to NFS and get response. 

Once we have these utilities, we can create tests easily. For example, one could easily write some tests like:
1. send create request 
2. assert response status is OK 
3. send same create request 
4. assert response status is not OK
... ...

, By "internal methods" do you mean nfsd.read,write, readdir etc.? If those are to be avoided, I think we need to call 'mnt' and create a mount point (e.g., /mnt/test_hdfs/), and create regular FileInputStream/FileOutSstream out of it. Then we can test read, write, readdir, and client access privilege. Let me know if you think that's the right direction. Thanks., You also might be interested in the apache bigtop HCFS fuse mount tests (BIGTOP-1221), which attempt to test e2e operations for the hadoops fuse mount...
 
We could generalize this to supprt nfs mount as well...

Another alternative is to contribute the end to end tests as an extension to the bigtop tests.  Since bigtop builds and packages and is designed to e2e test Hadoop on a running system, these tests are quite common.

In fact we do have a end to end fuse mount test, maybe we can add an nfs one as well.

https://github.com/apache/bigtop/blob/master/bigtop-tests/test-artifacts/hadoop/src/main/groovy/org/apache/bigtop/itest/hadoop/hcfs/TestFuseHCFS.groovy


, [~jayunit100], thanks for pointing us to the bigtop test which looks very nice. 

Here we see two kinds of e2e tests with each covering different test cases:
1. send/receive request from TCP/UDP client with manually crafted RPC requests and can have fine control of the sequence of requests. The RPC request could have any right or wrong format combination to test server's reaction. The requests sequence can also be made to test regression of some specific bugs.

2. do operations from shell. It's the real e2e test and very convenient to invoke the request from real NFS clients. Moreover, running the same test on different platforms (or even different linux releases), the NFS server might get different sequence of requests(e.g., due to the linux kernel changes). 

I was previously talking about the first e2e tests. But after read Jay's comments, I think we need both kinds of tests in NFS project because hadoop developers may not always run bigtop tests to validate their code change. 

We could move this JIRA out from HDFS-4750 and promote it to be an umbrella JIRA to better manage the effort. comments? , [~brandonli] I agree with having these 2 flavors of testing. 

Regarding the 2nd option (shell operations), I think we should unify with the FUSE tests that [~jayunit100] has pointed out. NFS and FUSE differ in setup and cleanup, but share all I/O operations in between. We can further define a standard trace format and extend the framework to support trace-driven tests.

And I agree with moving this JIRA out from HDFS-4750 to be an umbrella, especially since we are connecting to FUSE tests as well., Hi brandon.  Thats a great idea.  If we can peel out the generic components of this, it will be a big win for all.   

Two possibilities:

- One possibility:  We could create a central JIRA in bigtop, and make this JIRA A sub task of it.    One advantage of that is that, since bigtop is where we are contributing our generic e2e smoke and filesystem tests, HDFS won't have to carry that load.
So your welcome to create a JIRA on our end i guess.

- Another: Maybe make it a hadoop common / yarn jira, as hadoop-common curates the code for hadoop file system interface tests., Great! I've promoted this JIRA as the umbrella JIRA for add the two e2e test frameworks. 

I understand the benefit of sharing the common testing code between FUSE and NFS in bigtop tests.
However, we can't ask hadoop developers to always run bigtop tests each time after they update the code. 

One way to avoid duplicate the same code is to invoke tests in NFS project from bigtop tests. Not sure if it's possible.



  , [~brandonli] I'm working on creating RPC-level testing classes (option 1 as defined in your comment above) and I have a question: how did you manually setup the DFSCluster, nfs3, and pormap, in order to pass the test? It seems the system won't allow the AUTH flavor of CredentialsNone and VerifierNone in create() and write() requests. , [~zhz], sorry for the late reply.
For option1, we don't have to start portmap and we can use SimpleUdpClient or SimpleTcpClient to send NFS request directly to its RPC port.

{quote}It seems the system won't allow the AUTH flavor of CredentialsNone and VerifierNone in create() and write() requests. {quote}
Yes. Currently AUTH_SYS is the only supported AUTH flavor. , Sorry I forgot to mention that the question was about the original TestOutOfOrderWrite specifically. In that class class CredentialsNone and VerifierNone are used in hand crafting create and write requests. , TestOutOfOrderWrite was created when AUTH_NONE was supported, but this test is not updated along with other code changes. The test won't pass with the current NFS implementation since we don't support AUTH_NONE anymore. , I see, thanks for the clarification. 

I just came across another issue in the code: if we send a CREATE3Request to the NFS gateway, the received RpcReply can't be correctly parsed by RpcReply.read(). Somehow xid becomes the second int instead of the first. [~brandonli] Do you think this is a bug in the RpcReply.read() method or the code that generates and sends the RPC reply?, The first 4 bytes of the RPC package you got might be the fragment header(RECORD MARKING). On the server side, we use RpcFrameDecoder to process the fragment header (referring bootstrap.setPipelineFactory() in SimpleTcpServer). Let me know if this is not the case., I created a new class under {{hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3}} named {{TestRPCMessagesInNFS}}. The current working copy is attached. It is based on the original {{TestOutOfOrderWrite}} class. The problem I'm getting is that the {{messageReceived()}} method still receives RPC message with the fragment header you mentioned above. Shouldn't it be taken off by {{RpcFrameDecoder}} before reaching {{messageReceived()}}?, [~zhz], the fragment decoder will always send the whole buffer to the next message handler, but the buffer read position will be moved by 4 bytes.
In your case, the XDR is built on a ByteBuffer instead of ChannelBuffer, e.g:
{noformat}
    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) {
      
      ChannelBuffer buf = (ChannelBuffer) e.getMessage();
      ByteBuffer b = buf.toByteBuffer().asReadOnlyBuffer();
      XDR rsp = new XDR(b, XDR.State.READING);
      // Get handle from create response
      ...
        RpcReply reply = RpcReply.read(rsp);
      ...
{noformat}

Also, in order to support parallel testing, the test should use a ephemeral port:
    config.setInt("nfs3.server.port", 0);

If currently there is not way to know which port is bound by NFS, we should add a method to either Nfs3/RpcProgramNfs3 to get it. 
Please feel free to create sub-tasks for your patches.
]