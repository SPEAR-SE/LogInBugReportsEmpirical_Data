[Yes, that is a good catch. Should be fixed for 0.16 as well., You are right. The example above shows wrong use of closeStream(). 

> it clear it can only be used after the write operation has failed and is being cleaned up.
But this is not true. Many times this used to close sockets where socket needs to be closed because of unrelated error (say i/o failed on a different stream).
, i.e. closeStream() should be used if and only if user wants to ignore IOException and its ok for ref to be null. , It would be good if folks can use standard Java i/o idioms with Hadoop.

http://www.ibm.com/developerworks/java/library/j-jtp03216.html

{noformat}
OutputStream out = fs.open(...);
try {
  out.write(...);
} finally {
  out.close();
}
{noformat}

When multiple files are involved the best thing is to nest the try blocks.

Shouldn't we try to make this idiom work well with HDFS?
, > Shouldn't we try to make this idiom work well with HDFS?

I am not sure why this would not work now..

, - The catch block in IOUtils.closeStream is currently empty.  We should at least log a message.

- BTW, IOUtils.closeSocket has a similar problem., > I am not sure why this would not work now..

Then shouldn't we use it, instead of using IOUtils.closeStream() at all?, > Then shouldn't we use it, instead of using IOUtils.closeStream() at all?

May be. I am not sure if we should enforce it every where. IMHO the readability becomes really bad when lot of streams are involved as in DataNode (2 sockets and their input out streams, and two file channels).

, > Shouldn't we try to make this idiom work well with HDFS?

This idiom is not obvious for multiple IOs.  For example, the following codes cannot handle exceptions correctly:
{code}
OutputStream out1 = fs.open(...);
OutputStream out2 = fs.open(...);
try {
  out1.write(...);
  out2.write(...);
} finally {
  out1.close();
  out2.close();  //not called if the previous line throws an exception
}
{code}

We still need something like IOUtils.closeStream to do try-catch if there are more then one IOs.
I suggest we define the following method in IOUtils
{code}
public static void closeIO(Closeable... io) throws IOException {
  List<IOException> ioexceptions = new ArrayList<IOException>();
  for(Closeable c : io) {
    try {io.close();}
    catch(IOException e) {ioexceptions.add(e);}
  }
  if (!ioexceptions.isEmpty()) {
    throw new IOException(...); //construct an IOException with the list
  }
}
{code}

Then, multiple IOs can be closed together
{code}
OutputStream out1 = fs.open(...);
OutputStream out2 = fs.open(...);
try {
  out1.write(...);
  out2.write(...);
} finally {
 IOUtils.closeIO(out1, out2);
}
{code}

, The standard idiom for multiple streams, as mentioned above, is nested try blocks, e.g.:

{noformat}
OutputStream out1 = fs.open(...);
try {
  OutputStream out2 = fs.open(...);
  try {
    out1.write(...);
    out2.write(...);
  } finally {
    out2.close();
} finally {
  out1.close();
}
{noformat}
, > The standard idiom for multiple streams, as mentioned above, is nested try blocks
+1 Nested try blocks probably work in most situations.

However, it does not work when the number of streams is unknown in compile time, e.g.
{code}
OutputStream[] out = new OutputStream[n];
...
{code}
Also, it makes the code ugly when the number is large, say > 3., Triggering Hadoop QA tests., 
Why is this a blocker? What is the specific bug that this patch fixes?, -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12377211/closeStream.patch
against trunk revision 619744.

    @author +1.  The patch does not contain any @author tags.

    tests included -1.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs -1.  The patch appears to introduce 2 new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1899/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1899/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1899/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/1899/console

This message is automatically generated., This bug has the potential that errors-writing-to-block file go undetected. This can (theoretically) result in a higher probability of data corruption., > This bug has the potential that errors-writing-to-block file go undetected. This can (theoretically) result in a higher probability of data corruption.

But every close() added in the patch is for a socket or socket stream. Strictly socket streams don't even need to be closed. Am I missing something?, Specifically, I am trying to see which part of the patch fixes the potential corruption., Now that you point it out that streams need not be closed, I agree with you. I cannot see a case when this can cause data corruption. If this is the case, this patch should not go into 0.16., 
I don't think this is a fix for trunk either (this does not change even if streams need to be closed). , Looking at the code more closely, it appears that there isn't a bug that this patch addresses. The DatanOde correctly ignores exceptions. This issue is left open to address coding style-related issues. One suggestion is to make it log an error message when the close throws an exception. Another suggestion is to change the name of this method to closeIgnoreExceptions()., This would need some rework if we want it for 0.17., Closing this as not a problem.]