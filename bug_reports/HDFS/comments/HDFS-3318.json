[Wrap the url's input stream in a stream bounded with a {{long}} based on content length.

Add impl to pass through the other 2 {{read}} methods to the bounded stream.  This yields a ~1.4X improvement in transfer times., Patch does not include tests due to the infeasibility of testing the transfer time/success of a 2GB+ file.  It's been manually tested with large customer files that are currently failing to copy., Before introduction of the client-side timeout in hftp, server-side would timeout in 200 seconds, which is the jetty keepalive timeout. Currently when the client-side times out, which is smaller than 200 seconds, hftp client thinks transfer has failed since it does not detect the end of transfer based on the content length header. This doesn't seem to happen when the file size is < 2GB.  HttpURLConnection.getContentLength() returns an int (max: 2^32-1) and it might be internally keeping track of progress as long as content-length is < 2GB.

As a side effect of the fix, it will shed 200 seconds off transfer times for files bigger than 2GB (for pre hftp client timeout), since it will no longer wait for the server side to close the connection., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12524067/HDFS-3318.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.hdfs.web.TestOffsetUrlInputStream
                  org.apache.hadoop.fs.TestUrlStreamHandler
                  org.apache.hadoop.hdfs.TestByteRangeInputStream

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2324//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2324//console

This message is automatically generated., Hi Daryn,

Can streamLength > 2^31 ? Would in then be bounded by that higher number and still cause issues? 

Why are you Overriding 
{noformat}
  @Override
  public int read(byte[] b) throws IOException {
    return read(b, 0, b.length);
  }
{noformat}

Wouldn't the original implementation be picked up from InputStream which has exactly the same code? I tested with this short program and it prints exactly what Michael Jackson used to say he is.

{noformat}
class A {
  public void printA() {
    System.out.println("A");
    printC();
  }
  public void printC() {
    System.out.println("C");
  }
}

class B extends A {
  @Override
  public void printC() {
    System.out.println("D");
  }
  
  public void printB() {
    System.out.println("B");
    printA();
  }
}

public class TestJAVA {
  public static void main(String arg[]) {
    B b = new B();
    b.printB();
  }
}
{noformat}

, I doubt the per-read buffer is going to be >2GB for at least 5-10 years.  By that time, I think java will have fixed the issue. :), I looked at the patch and it looks good to me.  I agree with Ravi that we do not need to override read(byte[] b).  I am a +1 (non-binding) on this., Removed {{read(byte[])}} and fixed related tests.

Another jira broke {{TestUrlStreamHandler}}.  Somehow there are no more {{fs.$scheme.impl}} keys in the conf.  There are only {{fs.AbstractFileSystem.$scheme.impl}} keys., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12524289/HDFS-3318-1.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests:
                  org.apache.hadoop.fs.TestUrlStreamHandler

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2328//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2328//console

This message is automatically generated., Talked offline with Daryn. He has a way of removing any of my doubts :) Thanks Daryn! +1, I think the patch largely looks good. I'm confused, however, by the change of how "filelength" is determined. 

It changed from this:
{code}
final String cl = connection.getHeaderField(StreamFile.CONTENT_LENGTH);
filelength = (cl == null) ? -1 : Long.parseLong(cl);
{code}
To this:
{code}
final String cl = connection.getHeaderField(StreamFile.CONTENT_LENGTH);
...
final long streamlength = Long.parseLong(cl);
filelength = startPos + streamlength;
{code}

Why does the filelength now begin at startPos? That change seems unrelated to this issue. Or am I missing something?

+1 once this question is addressed., I think the previous filelength works only if startPos is zero.  It is a bug., bq. Why does the filelength now begin at startPos?

It's another bug related to successfully reading the stream that I didn't fully fix, but fixed "enough".  When EOF is encountered, it checks {noformat}if (currentPos < filelength) { EOFException } {noformat} to decide if there was a premature EOF.  {{currentPos}} and {{filelength}} are *not relative* to {{startPos}}, thus it's not valid to compare the current pos to the stream length.

Ex. I have 128 bytes.  I seek 100 bytes into it.  The remaining content-length is 28.  My file length is not 28 bytes!  I read more 10 bytes and the connection unexpectedly closes.  The broken premature EOF condition fails to detect the fault because (110 < 28) is false.  The correct check is (110 < 100+28).

{noformat}
      filelength
------------------------
       ^----------------
startPos  content-length
{noformat}

I can file a separate jira for this 1-line fix if you'd like., I have committed this.  Thanks, Daryn!, Works for me. Just wanted to make sure it wasn't inadvertent., Integrated in Hadoop-Common-trunk-Commit #2131 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2131/])
    HDFS-3318. Use BoundedInputStream in ByteRangeInputStream, otherwise, it hangs on transfers >2 GB.  Contributed by Daryn Sharp (Revision 1330500)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1330500
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestByteRangeInputStream.java
, Integrated in Hadoop-Mapreduce-trunk-Commit #2147 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2147/])
    HDFS-3318. Use BoundedInputStream in ByteRangeInputStream, otherwise, it hangs on transfers >2 GB.  Contributed by Daryn Sharp (Revision 1330500)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1330500
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestByteRangeInputStream.java
, Integrated in Hadoop-Hdfs-trunk-Commit #2205 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2205/])
    HDFS-3318. Use BoundedInputStream in ByteRangeInputStream, otherwise, it hangs on transfers >2 GB.  Contributed by Daryn Sharp (Revision 1330500)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1330500
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestByteRangeInputStream.java
, Integrated in Hadoop-Hdfs-0.23-Build #239 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/239/])
    svn merge -c 1330500 from trunk for HDFS-3318. Use BoundedInputStream in ByteRangeInputStream, otherwise, it hangs on transfers >2 GB. (Revision 1330504)

     Result = UNSTABLE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1330504
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestByteRangeInputStream.java
, Integrated in Hadoop-Hdfs-trunk #1026 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1026/])
    HDFS-3318. Use BoundedInputStream in ByteRangeInputStream, otherwise, it hangs on transfers >2 GB.  Contributed by Daryn Sharp (Revision 1330500)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1330500
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestByteRangeInputStream.java
, Integrated in Hadoop-Mapreduce-trunk #1061 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1061/])
    HDFS-3318. Use BoundedInputStream in ByteRangeInputStream, otherwise, it hangs on transfers >2 GB.  Contributed by Daryn Sharp (Revision 1330500)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1330500
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestByteRangeInputStream.java
, Is this also a problem in branch-1?, Yes this probably also impacts branch-1., Yes, on branch-1, >2GB transfers incur a 200s penalty after transfer is complete., Marking this as an incompatible change as the following code broke distcp from 0.20.2 and 0.21 releases. The code below requires the content length header be present however that field wasn't introduced in trunk until 22 (by HDFS-1085), it came into branch-1 via the YDH merge. Filed HDFS-3671 to fix this.

{code}
-      filelength = (cl == null) ? -1 : Long.parseLong(cl);
-      in = connection.getInputStream();
+      if (cl == null) {
+        throw new IOException(StreamFile.CONTENT_LENGTH+" header is missing");
+      }
{code}, The Content-Length check will be removed by HDFS-3577.]