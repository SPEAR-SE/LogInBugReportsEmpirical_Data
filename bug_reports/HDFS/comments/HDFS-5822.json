[For what it is worth, I am attaching a patch to log the InterruptedException against trunk. I noticed that two locations in DFSOutputStream.java, where InterruptedException wasn't logged in 2.2.0, are now logged already in trunk. The patch logs the InterruptedException in other places. Any feedback is appreciated!, I think that the exception is ignored by purpose. The idea of sleeping is to stop accepting incoming requests and in the hope of the datanode can finish handling some ongoing requests and free up the memory., Thanks Haohui for the explanation! But is it at least log-worthy? Otherwise any possible and unanticipated event that interrupts the thread will be lost and could potentially complicates debugging. It seems in hdfs majority of the InterruptedException are at least logged.

Also, how about other cases in my patch?

, You can't log in an OOM situation since logging requires buffers. I fail to see the needs of loggings all {{InterruptedException}} of the services in the DataNode side -- I think they are only interrupted during shutdown, it would be quite confusing for the users to see these log entries which are benign., Swallowing {{InterruptedException}} is a well-known anti-pattern in Java, well-documented in multiple sources, and we're doing it all over the Hadoop codebase.  Without restoring the interrupted status, there is a rsik that additional logic running later on that same thread, which is dependent on seeing interrupted status for timely shutdown, won't see it.  This is something I'd like to see us clean up.  I don't think logging is necessary, but I do think restoring the interrupted status with a call to {{Thread.currentThread().interrupt()}} is necessary.

HDFS-4328 is an example of a real bug with noticeable symptoms that were caused by swallowing an {{InterruptedException}}.

https://issues.apache.org/jira/browse/HDFS-4328?focusedCommentId=13550470&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13550470]