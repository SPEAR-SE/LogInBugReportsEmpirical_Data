[2013-12-28 13:22:30,467 WARN org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol: Failed to updateBlock (newblock=BP-720706819-xxxxx-1389113739092:blk_5575900364052391670_517444, datanode=tmm-e8:11242)
java.io.IOException: File length mismatched.  The length of /usr/local/hadoop/hadoop_data/dfs/data2/datanode/hadoop/dfs/data/current/BP-720706819-xxxxx-1389113739092/current/rbw/blk_5575900364052391670 is 62484480 but r=ReplicaUnderRecovery, blk_5575900364052391670_320295, RUR
  getNumBytes()     = 62455808
  getBytesOnDisk()  = 62455808
  getVisibleLength()= -1
  getVolume()       = /usr/local/hadoop/hadoop_data/dfs/data2/datanode/hadoop/dfs/data/current
  getBlockFile()    = /usr/local/hadoop/hadoop_data/dfs/data2/datanode/hadoop/dfs/data/current/BP-720706819-xxxxx-1389113739092/current/rbw/blk_5575900364052391670
  recoveryId=517444
  original=ReplicaWaitingToBeRecovered, blk_5575900364052391670_320295, RWR
  getNumBytes()     = 62455808
  getBytesOnDisk()  = 62455808
  getVisibleLength()= -1
  getVolume()       = /usr/local/hadoop/hadoop_data/dfs/data2/datanode/hadoop/dfs/data/current
  getBlockFile()    = /usr/local/hadoop/hadoop_data/dfs/data2/datanode/hadoop/dfs/data/current/BP-720706819-xxxxx-1389113739092/current/rbw/blk_5575900364052391670
  unlinked=false
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkReplicaFiles(FsDatasetImpl.java:1063)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.updateReplicaUnderRecovery(FsDatasetImpl.java:1541)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.updateReplicaUnderRecovery(DataNode.java:1907)
        at org.apache.hadoop.hdfs.server.datanode.DataNode$BlockRecord.updateReplicaUnderRecovery(DataNode.java:1938)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2090)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1988)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:225)
        at org.apache.hadoop.hdfs.server.datanode.DataNode$2.run(DataNode.java:1869), Attached the patch, please review, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12621917/HDFS-5728.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5843//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5843//console

This message is automatically generated., Is this case happened only if we restart DN where crc has less data? as we convert all RBW replica states to RWR and here length will be calculated based on crc chunks. If that is the case, how about just setting the file length also to same after creating RWR state?, bq. Is this case happened only if we restart DN where crc has less data?
Yes
bq. as we convert all RBW replica states to RWR and here length will be calculated based on crc chunks. If that is the case, how about just setting the file length also to same after creating RWR state?
I too thought of same thing. That will be a implicit truncation without recovery being called. But I felt better we come through recovery flow itself and do truncation only on demand, {quote}
That will be a implicit truncation without recovery being called.
{quote}
Logically we already truncated in memory by having the integrity check. There is no use of considering data more than crc bytes covered.  And this truncation will not make recovery of block.  This is just making crc and blockFile having same length (as data integrity expects). Recovery will make actual block file truncation upto where new length proposed for block recovery., bq. Logically we already truncated in memory by having the integrity check. There is no use of considering data more than crc bytes covered. And this truncation will not make recovery of block. This is just making crc and blockFile having same length (as data integrity expects). Recovery will make actual block file truncation upto where new length proposed for block recovery.
I agree Uma. I will try post new patch based on your input, Attaching the updated patch as per Uma's comment
Please review, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12623909/HDFS-5728.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5918//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5918//console

This message is automatically generated., I have seen this recently after a partial power outage. The disks weren't full in this case. I manually truncated the block files to get it going., The approach seems okay. It is actually what I did manually to recover. The new test case seems to be adequate.
There are unnecessary lines of code added, though.

{code}
+          // truncate blockFile
+          blockRAF.setLength(validFileLength);
+
+          // read last chunk
+          blockRAF.seek(lastChunkStartPos);
+          blockRAF.readFully(b, 0, lastChunkSize);
{code}

In the above, the last chunk of the block doesn't have to be read. In {{truncateBlock()}}, which is called during {{recoverRbw()}}, this is needed in order to recompute the checksum and write out to the meta file. It is done this way since simply truncating meta file will cause checksum mismatch, if the new block size doesn't align with the chunk size.  In this jira, this is not necessary since meta files are not truncated.

It made me think about the case where a block file is smaller than expected. With the current code, 0 will be returned as the size. Instead, we could truncate the meta file if the block file length is non-zero.  But this should be rare since a block file is written before  the corresponding meta file., Thanks Kihwal for taking a look.
Attaching a patch by removing unnecessary lines as you suggested.
Please review., The build server has been down for more than a day, so precommit won't run any time soon.
+1 I won't wait for the build server to return. The previous version of patch was fine except the lines removed in the latest version.
, I've committed this to trunk and branch-2. Thank you for reporting and working on the patch, Vinay. Thanks for the reveiw, Uma., SUCCESS: Integrated in Hadoop-trunk-Commit #5036 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5036/])
HDFS-5728. Block recovery will fail if the metafile does not have crc for all chunks of the block. Contributed by Vinay. (kihwal: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561223)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #461 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/461/])
HDFS-5728. Block recovery will fail if the metafile does not have crc for all chunks of the block. Contributed by Vinay. (kihwal: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561223)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1678 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1678/])
HDFS-5728. Block recovery will fail if the metafile does not have crc for all chunks of the block. Contributed by Vinay. (kihwal: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561223)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1653 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1653/])
HDFS-5728. Block recovery will fail if the metafile does not have crc for all chunks of the block. Contributed by Vinay. (kihwal: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561223)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery.java
, Attaching a patch for branch-0.23.  At the method/class level, the change is identical. The difference comes from the layout changes in 2.x.]