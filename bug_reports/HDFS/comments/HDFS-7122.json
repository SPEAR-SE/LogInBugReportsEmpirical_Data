[Hey Jeff, thanks for reporting this. I agree that the skew you're seeing here is definitely abnormal even for random load balancing. I'm surprised the thread local Randoms are colliding like this since they're supposed to be initialized with unique seeds, but I'll look into this further and get back to you., I poked around with a unit test and was unable to reproduce skew that was quite this extreme. I'll admit that the workaround of only using one handler definitely does point at the thread-local Random being the issue, but since I'm unable to repro, it's going to be hard to test a fix.

Jeff, a couple questions for you in the meantime:

- Were you using the default block placement policy, or WithNodeGroup? I noticed you said you were using VMs.
- What JDK version were you using? I looked in JDK7u40's source, and it looks like the bare Random() constructor generates a unique seed in a thread-safe manner.
- What was your handler count before you changed it to 1?
- Is it possible you can test WIP work on your setup if I'm still unable to repro?, Another q, how was your rack topology set up? I'd like to mirror as much as possible in my unit test to try for that repro., I mainly use WithNodeGroup, but I did one test without it for the same reason you're thinking of, by setting the topology to host=rack (places the 2nd and 3rd replicas in different VMs on one host).  This didn't make any difference.  What does make a difference is the number of slave nodes.  It might be hard to see the skew if you have only a small number of DNs. Virtualization makes scale-out tests much easier! (Containers could also be used.)
7u55
handler count was 20.
If you can build me a tarball with the MR1 stuff in it, I'll test it.  I should have mentioned I'm using MR1, could that make a difference?, One physical rack, but I'm using HVE to create multiple "virtual racks", that is arbitrarily assigning groups of hosts to different racks.  I've tried splitting the 32 hosts into 2 racks and 16 racks.  I'm using the latter now since it gave me a more uniform distribution, but for testing we should use the minimum number of racks to enhance the non-uniformity., This is definitely a blocker of 2.6.  How about reverting HDFS-6268, given its bad design?, I agree this is something we should fix for 2.6. I'm working actively with Jeff on this one, so I think we can get it done in time., Nicholas, I think we don't need to revert HDFS-6268 immediately :) Agree with Andrew that we have time to prove and fix this.

Most of code in HDFS-6268 is to improve the sorting by distance, and the only related modification about this issue is changing _static random_ to _ThreadLocal random_ which improves performance a bit. But I don't see it will affect the randomness, and it's self-seed as Andrew seed. But of course the {{java.util.Random}} is not good enough for randomness (poor entropy), we can use better random generation approach (i.e. SecureRandom) if we can prove it's the root cause.

[~jcb], could you help to reproduce it couple of times? Want to know whether the blocks distributions are always so bad. If so, then we can retry using a non _ThreadLocal random_ and {{SecureRandom}} and see the result. If the blocks distributions are not always so bad, I suspect the issue existed even before HDFS-6268, so we should use better random generation approach or find a new way to solve the problem.,  Yi, yes I ran it many times tweaking various things to try to get a good distribution.  Although there was some variation, it was always much worse than before., Hi [~hitliuyi], one problem is that we are not yet sure how to fix the bug. Why not reverting HDFS-6268 and redo it in a right way?  Reverting a patch is not a big deal.  It is a quick and effective way to fix a newly introduced bug., Agree with your point for reverting it, thanks [~szetszwo]. , Actually before [~jcb] confirms the blocks distributions are always bad, I have concern if the issue existed and was random even before HDFS-6268 since {{java.util.Random}} is not good enough. 
Now agree to revert it since it seems an issue after HDFS-6268 and we fix it later., Can we hold off on the revert for now? There are a couple other JIRAs layered on top of HDFS-6268 right now, and as I said above, I'm working on this pretty actively and I think we can get it done by 2.6., [~andrew.wang], my preference is to revert the patch and commit the patch back with all the required fixes. But I will leave it up to you (reluctantly), if you think this can be fixed in a couple of days., It does not seem to have much activity on fixing this bug.  How about we set a deadline, say by the end of Friday 9/26, so that  we will revert  HDFS-6268 if this remains unfixed?  Even if HDFS-6268 is reverted, we could keep fixing the bug.  It still has a chance to get in again., Not all of the progress has been reported to this JIRA. I've brainstormed some with ATM and Todd, and I have a few ideas for patches that Jeff can try out. I'm going to coordinate with him one-on-one since I think he probably wants a patched CDH jar for easier testing, but I'll post any meaningful updates here too.

Tomorrow is a rather quick deadline. I think Arun said he was pushing back cutting the branch until next week, could we wait until then? I'm okay with reverting this stuff since the original fix isn't that a big deal, but it's easier for the code to move forwards, and the important date really seems to be resolving this before we release 2.6., [~andrew.wang], the HDFS-6268 patch committed was not carefully designed.  It changed something seemingly minor but had a big adverse impact.  That's a reason to revert such bad patch.

Also, we does not seem to have a clear way to fix the bug up to now.  Why not reverting it for fixing Apache trunk?

I understand that you are probably much care about fixing CDH than Apache trunk.  You probably work with your colleagues in Cloudera more closely than Apache open source.  However, Apache trunk is very important for many people.  There are other contributors following the JIRA.  We should respect them., Nicholas, if this patch is so obviously not carefully designed, could you please point out the issue? It'd save me debugging time and we could get this resolved very quickly then. Much appreciated.

Also, Jeff found this while testing with CDH, he reported it upstream, and the fix is going to be posted to this JIRA as soon as we figure it out. What exactly is the issue here? I see no lack of respect on my part. If anyone else in the community is interested in helping fix this bug, they are more than welcome., I reproduced the issue on a 31-slave physical cluster, without HVE/Nodegroups.  300 GB data set, 256 MB block size, which yields 1364 primary blocks and 2728 replica copies.  So there should be 44 primary blocks and 88 copies per slave.  See the attached file copies_per_slave.jpg for the actual distribution of copies.  Effect of dfs.namenode.handler.count=20 is not as severe as with 638 slaves, but still very significant., So the community is kept in the loop, I just gave Jeff a patched version of CDH5.1.2 with two new options to play with. A patch for CDH5.1.2 is attached. It's not intended for commit.

Option 1 tries using a ThreadLocalRandom rather than a ThreadLocal<Random>.

Option 2 uses a single Random., > Nicholas, if this patch is so obviously not carefully designed, could you please point out the issue? It'd save me debugging time and we could get this resolved very quickly then. Much appreciated.

[~andrew.wang], I did not say that the bug introduced by HDFS-6268 was easy to fix.  I might be wrong -- It did seem that the author of the patch might not have the domain knowledge on the problem he tried to fix so that the bug got committed unconsciously.

Indeed, the bug seems hard to fix.  We still do not have a clear direction on fixing it.  It is also going to take some time on testing the potential fixes. These are the reasons we should first revert the HDFS-6268 patch and then redo it.

> ... What exactly is the issue here? I see no lack of respect on my part. If anyone else in the community is interested in helping fix this bug, they are more than welcome.

This is a blocker bug.  There are other people watching on the bug.  You claimed that you would fix it but you seemed not care about posting a patch or discussing the solution here previously.  How could the others to help fixing the bug if they want to?, Thanks to Jeff's testing with the prelim patch, we were able to identify the ThreadLocal<Random> as the source of the issue. Using JDK7's ThreadLocalRandom (as in the prelim patch) still had the same issue.

This patch for trunk changes us back to a single static Random instance. This is how it was pre-HDFS-6268, so there shouldn't be a Random perf regression compared to that.

Reviews appreciated., Looks like a straight-forward revert of the ThreadLocal random portion of HDFS-6268. +1 pending jenkins., Created HADOOP-11152 to track the investigation of a better RNG., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12671824/hdfs-7122.001.patch
  against trunk revision 9e985e6.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 12 warning messages.
        See https://builds.apache.org/job/PreCommit-HDFS-Build/8253//artifact/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.crypto.random.TestOsSecureRandom
                  org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestCheckpoint

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8253//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8253//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8253//console

This message is automatically generated., The javadoc warnings look unrelated. The patchprocess file is empty. Findbugs also look unrelated. The tests are also long-standing flakies due to the open FD issue on the build slaves. No new tests, as the existing tests cover this functionality.

Thanks for reviewing Luke, I'll commit this shortly based on your +1., I've committed the fix to trunk and branch-2. Thanks again Luke for reviewing, I think the discussion on HADOOP-11152 is also going in interesting directions., FAILURE: Integrated in Hadoop-trunk-Commit #6147 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6147/])
HDFS-7122. Use of ThreadLocal<Random> results in poor block placement. (wang) (wang: rev c6c3247dc0dcb8c72ea00f3fb14a0879fcf49c56)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #696 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/696/])
HDFS-7122. Use of ThreadLocal<Random> results in poor block placement. (wang) (wang: rev c6c3247dc0dcb8c72ea00f3fb14a0879fcf49c56)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1887 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1887/])
HDFS-7122. Use of ThreadLocal<Random> results in poor block placement. (wang) (wang: rev c6c3247dc0dcb8c72ea00f3fb14a0879fcf49c56)
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1912 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1912/])
HDFS-7122. Use of ThreadLocal<Random> results in poor block placement. (wang) (wang: rev c6c3247dc0dcb8c72ea00f3fb14a0879fcf49c56)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java
, bq. I might be wrong – It did seem that the author of the patch might not have the domain knowledge on the problem he tried to fix so that the bug got committed unconsciously.

The original patch was also reviewed for several weeks and given a +1 by Aaron, who has been working full time on HDFS for several years. I also discussed the design with Andrew offline (though I apologize for not commenting on the JIRA). So, we had around 10 man-years of experience looking at the patch or its design and we still missed the bug. I wouldn't say that "domain expertise" was the issue there. Rather, it was a simple mistake which was quickly addressed after being discovered.

bq. I understand that you are probably much care about fixing CDH than Apache trunk. You probably work with your colleagues in Cloudera more closely than Apache open source. However, Apache trunk is very important for many people. There are other contributors following the JIRA. We should respect them.

Nicholas -- speaking of respect, could you please refrain from ad hominem attacks like this in the JIRA?

The data seems to show that Andrew works quite a bit with open source. In fact, he is the number one most prolific committer to Hadoop in the last 6 months, and the number two most prolific in the past year (#1 if you only count HDFS). I don't think dragging unsubstantiated attacks on the open source commitment level of different vendors is productive in the context of a bug fix., {quote}
The original patch was also reviewed for several weeks and given a +1 by Aaron, who has been working full time on HDFS for several years. I also discussed the design with Andrew offline (though I apologize for not commenting on the JIRA). So, we had around 10 man-years of experience looking at the patch or its design and we still missed the bug. I wouldn't say that "domain expertise" was the issue there. Rather, it was a simple mistake which was quickly addressed after being discovered.
{quote}
I am surprise that you are measuring the HDFS man-years and adding them up!  Do you think if it makes sense for 3 four-grade students to claim that they all together is equivalent to a twelve-grade student?

I agree that it is a simple mistake (i.e. not a complicated one).  However, I do not understand why we did not revert the original patch but committed another patch for reverting the code.

Please do post on JIRA if you have comments.  Otherwise, it is hard for other people to work with you.  Thanks.

{quote}
Nicholas – speaking of respect, could you please refrain from ad hominem attacks like this in the JIRA?
{quote}
I was responding to Andrew's earlier comments about he working on CDH with people in Cloudera but not working on Apache trunk with Apache open source community.  I am not sure why you have such interpretation.

{quote}
The data seems to show that Andrew works quite a bit with open source. In fact, he is the number one most prolific committer to Hadoop in the last 6 months, and the number two most prolific in the past year (#1 if you only count HDFS). I don't think dragging unsubstantiated attacks on the open source commitment level of different vendors is productive in the context of a bug fix.
{quote}

The man-years of experience counted earlier includes Andrew, Aaron and Todd.  The so called "most prolific in last 6 months" measurement includes only Andrew.  Is it suggesting that, for Andrew alone, the number of man-years is very small and, for Andrew, Aaron and Todd, the "most prolific in last 6 months" measurement is not quite good?, I think the fact that the problem has been identified and we have solved it is all that matters. Only suggestion I would make is to make the discussion public on the jira so that others who are not privy to it have the background.

I think [~andrew.wang], [~szetszwo], and [~tlipcon], if there are anything that need to be sorted out, please take it up in an email. We have all collaborated on HDFS for a long time. Continuing this conversation in the jira does not add much to the issue we are discussing. If you guys want to meet for a beer, it is on me :)
]