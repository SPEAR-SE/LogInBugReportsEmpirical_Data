[I wrote a samp
{noformat}
    Configuration conf = new Configuration();
    FileSystem fileSystem = FileSystem.get(conf);
    DistributedFileSystem dfsFileSystem = (DistributedFileSystem)fileSystem;
    DFSClient client = dfsFileSystem.getClient();
    String fileName = "/user/rushabhs/benchmarks/";
    HdfsFileStatus status = client.getFileInfo(fileName);
    Path mkdirPath = new Path("/.reserved/.inodes/"+ status.getFileId() + "/TestDFSIO/../test2");
    dfsFileSystem.mkdirs(mkdirPath, new FsPermission("777"));
{noformat}
It did create {{test2}} directory underneath {{/user/rushabhs/benchmarks/}}.
I ran the same test via fsshell also. It worked as expected.
[~raemarks]: I am running some version of 2.8.2 release which shouldn't be that different from 2.8.0
Can you confirm that you are seeing this in {{2.8.0}} and if possible can you share your program/script., [~shahrs87] By it worked as expected, do you mean it compressed the path to {{/user/rushabhs/benchmarks/test2}}, or threw an exception? I can confirm I'm seeing this in {{2.8.0}}:

{code}
# hadoop version
Hadoop 2.8.0
{code}

I am using a snakebite-like hadoop client. It does not perform any client-side path manipulation/validation - it relies on the NameNode to properly handle/report unsupported path formats. Here is my script:

{code}
 >> client = pydoofus.namenode.v9.Client('172.18.0.2', 8020, auth={'effective_user': 'hdfs'})
>> print client.mkdirs('/x/y/z', 0777, create_parent=True)
True
>> file_status = client.get_file_info('/x/y')
>> print file_status
FileStatus {
    file_type: DIRECTORY
    path: 
    length: 0
    permission: 
    Permission {
        mode: 0777
    }
    owner: hdfs
    group: supergroup
    modification_time: 1512066397076
    access_time: 0
    symlink: 
    replication: 0
    block_size: 0
    locations: None
    file_id: 16580
    num_children: 1
}
>> bad_path = '/.reserved/.inodes/' + str(file_status.file_id) + '/z/../foo'
>> print client.mkdirs(bad_path, 0777, create_parent=True)
True
>> print client.get_listing('/x/y/z')
DirectoryListing {
    entries: [FileStatus(file_type='DIRECTORY', path=u'..', length=0L, permission=Permission(mode=0777), owner=u'hdfs', group=u'supergroup', modification_time=1512066397083, access_time=0, symlink='', replication=0, block_size=0L, locations=None, file_id=16582L, num_children=1)]
    remaining: 0
}
>> print client.get_listing('/x/y/z/..')
DirectoryListing {
    entries: [FileStatus(file_type='DIRECTORY', path=u'foo', length=0L, permission=Permission(mode=0777), owner=u'hdfs', group=u'supergroup', modification_time=1512066397083, access_time=0, symlink='', replication=0, block_size=0L, locations=None, file_id=16583L, num_children=0)]
    remaining: 0
}
{code}

, Interestingly, this is not reproducible with WebHDFS if WebHDFS is used to make that bad path:

{code}
rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' -X PUT 'http://172.18.0.2:50070/webhdfs/v1/x/y/z?op=MKDIRS&user.name=hdfs'
{"boolean":true}200⏎                                                                                                                                                                                                                                                                     
 
rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' 'http://172.18.0.2:50070/webhdfs/v1/x/y?op=GETFILESTATUS'
{"FileStatus":{"accessTime":0,"blockSize":0,"childrenNum":1,"fileId":16587,"group":"supergroup","length":0,"modificationTime":1512068214198,"owner":"hdfs","pathSuffix":"","permission":"755","replication":0,"storagePolicy":0,"type":"DIRECTORY"}}200⏎                                  

rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' -X PUT 'http://172.18.0.2:50070/webhdfs/v1/.reserved/.inodes/16587/z/../foo?op=MKDIRS&user.name=hdfs'
{"boolean":true}200⏎
                                                                                                                                                                                                                                                                      
rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' 'http://172.18.0.2:50070/webhdfs/v1/x/y/z?op=LISTSTATUS'
{"FileStatuses":{"FileStatus":[]}}200⏎                                                                                                                                                                                                                                                                                      

rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' 'http://172.18.0.2:50070/webhdfs/v1/x/y?op=LISTSTATUS'
{"FileStatuses":{"FileStatus":[
{"accessTime":0,"blockSize":0,"childrenNum":0,"fileId":16589,"group":"supergroup","length":0,"modificationTime":1512068246054,"owner":"hdfs","pathSuffix":"foo","permission":"755","replication":0,"storagePolicy":0,"type":"DIRECTORY"},
{"accessTime":0,"blockSize":0,"childrenNum":0,"fileId":16588,"group":"supergroup","length":0,"modificationTime":1512068214198,"owner":"hdfs","pathSuffix":"z","permission":"755","replication":0,"storagePolicy":0,"type":"DIRECTORY"}
]}}200⏎    
{code}

However, if I run the problematic {{Mkdirs}} with our snakebite-like client then try to list with WebHDFS, the problem is once again exposed:
{code}
rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' 'http://172.18.0.2:50070/webhdfs/v1/x/y?op=LISTSTATUS'
{"FileStatuses":{"FileStatus":[
{"accessTime":0,"blockSize":0,"childrenNum":1,"fileId":16592,"group":"supergroup","length":0,"modificationTime":1512073040206,"owner":"hdfs","pathSuffix":"z","permission":"777","replication":0,"storagePolicy":0,"type":"DIRECTORY"}
]}}200⏎ 

rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' 'http://172.18.0.2:50070/webhdfs/v1/x/y/z?op=LISTSTATUS'
{"FileStatuses":{"FileStatus":[
{"accessTime":0,"blockSize":0,"childrenNum":1,"fileId":16593,"group":"supergroup","length":0,"modificationTime":1512073040207,"owner":"hdfs","pathSuffix":"..","permission":"777","replication":0,"storagePolicy":0,"type":"DIRECTORY"}
]}}200⏎    

rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' 'http://172.18.0.2:50070/webhdfs/v1/x/y/z/..?op=LISTSTATUS'
{"FileStatuses":{"FileStatus":[
{"accessTime":0,"blockSize":0,"childrenNum":1,"fileId":16592,"group":"supergroup","length":0,"modificationTime":1512073040206,"owner":"hdfs","pathSuffix":"z","permission":"777","replication":0,"storagePolicy":0,"type":"DIRECTORY"}
]}}200⏎       

rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' 'http://172.18.0.2:50070/webhdfs/v1/x/y/z/../foo?op=GETFILESTATUS'
{"RemoteException":{"exception":"FileNotFoundException","javaClassName":"java.io.FileNotFoundException","message":"File does not exist: /x/y/foo"}}404⏎   

rmarks@rmarks-wkstn ~> curl -sS -L -w '%{http_code}' 'http://172.18.0.2:50070/webhdfs/v1/.reserved/.inodes/16593?op=LISTSTATUS'
{"FileStatuses":{"FileStatus":[
{"accessTime":0,"blockSize":0,"childrenNum":0,"fileId":16594,"group":"supergroup","length":0,"modificationTime":1512073040207,"owner":"hdfs","pathSuffix":"foo","permission":"777","replication":0,"storagePolicy":0,"type":"DIRECTORY"}
]}}200⏎   
{code}

This shows there is no {{foo}} under {{y}}, {{..}} is visible under {{z}}, and {{foo}} is inaccessible except by using an inode path to access {{/x/y/z/..}}.
, bq. By it worked as expected, do you mean it compressed the path to /user/rushabhs/benchmarks/test2, or threw an exception?
By worked as expected, I meant it created {{/user/rushabhs/benchmarks/test2}} directory.
But the normalization of {{..}} was done on the dfs client side.
Since you are not doing any client validation, namenode is handling the raw path.
HDFS-5104 added the support to handle {{..}} in the path but exactly at 4th index {{/.reserved/.inodes/<inodeid>/../}} (split by /)
Pre HDFS-5104, if the path contains {{..}} when it reaches namenode, it threw {{InvalidPathException}}
But this patch introduced a bug where {{..}} can slip through.

{code:title=FsDirectory.java|borderStyle=solid}

byte[][] org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolveDotInodesPath(byte[][] pathComponents, FSDirectory fsd) throws FileNotFoundException
...
...
    // Handle single ".." for NFS lookup support.
    if ((pathComponents.length > 4)
        && Arrays.equals(pathComponents[4], DOT_DOT)) {
      INode parent = inode.getParent();
      if (parent == null || parent.getId() == INodeId.ROOT_INODE_ID) {
        // inode is root, or its parent is root.
        return new byte[][]{INodeDirectory.ROOT_NAME};
      }
      return parent.getPathComponents();
    }
{code}

According to above code snippet from HDFS-5104, it looks like that jira just expected to have {{..}}  at the 4th index of the path.
So the fix should be in {{DFSUtilClient#isValidName}} to throw {{InvalidPathException}} if {{..}} is present and not at the 4th index.
, [~brandonli], [~sureshms]: is the patch from {{HDFS-5104}} just expected to handle {{..}} _only_ at 4th index  if we split the path name by {{/}} ?, I've been poking this code a lot lately and it seems to only support the {{..}} if it's directly after the inode number, so yes only at the 4th index. If there is {{..}} after the 4th index and the 4th index is not {{..}}, it fails. Also, the NameNode appears to silently throw away any path components following {{/.reserved/.inodes/<inode number>/..}} instead of failing, which could be misleading. , bq.  If there is .. after the 4th index and the 4th index is not .., it fails. 
I don't understand what do you mean by it fails ?, Oops, not very descriptive of me. I meant if you try to get the file status of a file using a path containing {{..}} after the 4th index, but I was recalling something different and was incorrect. Please disregard that comment. 

What I mentioned about ignoring anything after the {{..}} in the 4th index is true though. For example, getFileStatus on {{'/.reserved/.inodes/<y's inode number>/../I/don't/exist}} would return y's parent's info ({{x}}) - it just throws away {{I/don't/exist}}.]