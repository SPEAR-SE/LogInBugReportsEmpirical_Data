[I checked all the operations about FSNamesystem.fsLock.readLock, and find that there is a line between readLock()/writeLock and try-finnaly block.
{code:java}
      if (isReadOp) { // first attempt is with readlock
        checkOperation(OperationCategory.READ);
        readLock();
      }  else { // second attempt is with  write lock
        checkOperation(OperationCategory.WRITE);
        writeLock(); // writelock is needed to set accesstime
      }
      src = FSDirectory.resolvePath(src, pathComponents, dir);
      try {
{code}

The function FSDirectory.resolvePath may throw a FileNotFoundException, cause the previously acquired lock not released.

And I do found the logs of FileNotFoundException, about 2 minutes before no useful logs appending to log files., stacks during hanging, attach a simple test case, which may cause the test program hang and may produce similar jstack dumps., After digging into the function FSDirectory.resolvePath, I discoved that this bug only affect the invalid paths with prefix /.reserved/.inodes. Yes, I am using hdfs-nfs as my clients, and the FIleNotFoundException may be caused by another question in hdfs-nfs.

I wrote a small program that can reproduce the problem. And I tested it in my 2.2 cluster.

I did not try higher version hadoop, but related source code is not changed in newest version 2.5.1, Hi Carrey, this had been fixed in HDFS-7045, target version is 2.6., Thanks Carrey for reporting the issue, mark it as invalid since it's already fixed in HDFS-7045., thanks [~hitliuyi]]