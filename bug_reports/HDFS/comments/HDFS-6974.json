[Core stack
{code}
4-192.168.1.138-1409307851902:blk_1073741825_1001]] ERROR datanode.DataNode (DataXceiver.java:run(243)) - 127.0.0.1:52841:DataXceiver
 error processing WRITE_BLOCK operation  src: /127.0.0.1:52858 dst: /127.0.0.1:52841                                                 
java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(II[BI[BIILjava/lang/String;JZ)V 
        at org.apache.hadoop.util.NativeCrc32.nativeComputeChunkedSumsByteArray(Native Method)                                       
        at org.apache.hadoop.util.NativeCrc32.verifyChunkedSumsByteArray(NativeCrc32.java:67)                                        
        at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:344)                                              
        at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:292)                                              
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.verifyChunks(BlockReceiver.java:416)                                 
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:551)                                
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:771)                                 
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:718)                                       
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:126)                                     
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:72)                                         
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:225)                                              
        at java.lang.Thread.run(Thread.java:745)                                                                                     
2014-08-29 11:24:15,800 [ResponseProcessor for block BP-126326924-192.168.1.138-1409307851902:blk_1073741825_1001] WARN  hdfs.DFSClie
nt (DFSOutputStream.java:run(880)) - DFSOutputStream ResponseProcessor exception  for block BP-126326924-192.168.1.138-1409307851902:
blk_1073741825_1001                                                                                                                  
java.io.EOFException: Premature EOF: no length prefix available                                                                      
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2081)                                               
        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)                                 
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:79
{code}

I can work around this in my tests by simply disabling JNI load, but it could be a problem that re-occurs elsewhere. The native lib is loading (so the flag is set), but the operations downstream are failing in more obscure ways.

What to do?
# nothing. This JIRA and stack trace can be used to identify the problem and fix (get rid of that lib/change the {{"java.library.path"}})
# do some post-load tests ... are all the methods there? But if not: what to do?
# include some version numbering on the library, so the first version of {{hadoop.dll}}, or {{hadoop.lib}} isn't blindly picked up. 

I'd argue for action 3, numbering the native lib with the Hadoop version built.
, There's another extension too: have a {{getVersion()}} call that returns version info (build info etc), which can be used to help in diags. I'd add that, but still look for hadoop-2.6.lib so that you could  have >1 lib on the path, Is this really any different than needing to set {{HADOOP_CLASSPATH}} correctly?  We don't handle mixing old jars into the classpath, so why should we handle mixing old {{hadoop.dll}} files into the path?  It seems inconsistent.  But maybe I'm missing something that makes this case different.

bq. There's another extension too: have a getVersion() call that returns version info (build info etc), which can be used to help in diags. I'd add that, but still look for hadoop-2.6.lib so that you could have >1 lib on the path

We don't make any guarantees that the libhadoop supplied with 2.6 will work with Hadoop 2.6.1.  libhadoop doesn't have a fixed or standardized API; it's just "the C half" of random bits of Hadoop code.

Think if you were making changes to the JNI code and redeploying.  You need to redeploy with the correct, new JNI code, not the old stuff.  This again, the same as with jar files... you wouldn't mix jar files from Hadoop 2.6 and Hadoop 2.6.1 in the same directory.  So I would argue for your solution #1.

We could perhaps give a better error message here.  We might be able to inject the git hash into the library, and error out if it didn't match the git hash in the jar files.  But then that means that partial rebuilds of the source tree no longer work, so maybe not., The existence of this ticket helped to track down the problem, but if it is feasible, a more meaningfull error message would help here.]