[The description is a little misleading here.  Basically, the problem is that this operation:

{code}
open("/mnt/fuse-dfs/t", O_CREAT | O_TRUNC | O_WRONLY, 0644);
{code}

gets translated into this sequence of fuse-dfs calls:

{code}
TRACE open /t
TRACE truncate /t
TRACE unlink /t
TRACE getattr /t
TRACE flush /t
TRACE release /t
{code}

(I'm assuming that another open would have followed if our unlink hadn't returned an error.)

There are a few different quality-of-implementation issues here:
* hdfs doesn't react too well to unlink of a file while it's open, which we're doing here
* truncate tries to do ts own create + close cycle in the middle, which basically means that we're trying to open a file for write while it's already open-- not good.

{{FUSE_CAP_ATOMIC_O_TRUNC}} could help stop fuse from translating open into SO MANY complicated fuse operations.  However, it's not supported for all kernel versions (I think definitely not on CentOS 5, for example.)  There are a bunch of hacks we could do to "fix" this on older kernels, but they won't be easy., dfs_open: Fix memory leak resulting from never freeing result of hdfsGetPathInfo.
    
dfs_open: when opening a non-empty file, use {{O_APPEND}} by default unless {{O_TRUNC}} is specified.
    
dfs_init: set {{FUSE_CAP_ATOMIC_O_TRUNC}}, {{FUSE_CAP_ASYNC_READ}}, {{FUSE_CAP_BIG_WRITES}} if they are available.  Set {{FUSE_CAP_DONT_MASK}} if {{options.no_permissions}} is set.
    
add unit test for {{open(O_TRUNC)}}, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552403/HDFS-4140.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3459//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3459//console

This message is automatically generated., {code}
+      // The file exists, and has non-zero size.  We don't want a regular open
+      // for write to truncate the whole file to zero-length-- that would be
+      // bad.  But if we just give an error here, a lot of programs will not
+      // run-- even those that never intended to do anything that we don't
+      // support.  So we open for append instead.  If the program tries to
+      // write to offset 0, it will get an error at that point (we don't
+      // support seek + write.)
+      flags |= O_APPEND;
{code}
I seriously doubt that this is an acceptable solution in the long run, we can't just pretend that the user meant append.

But, I think we should do this in the short term since it will allow a few important baseline applications to work correctly.

Please add a reference to this JIRA in the comment.

{code}
+#ifdef FUSE_CAP_ATOMIC_O_TRUNC
...
+  // Unfortunately, this capability is only implemented on Linux 2.6.29 or so.
{code}
I think we should just {{#error hdfs-fuse-dfs requires CAP_ATOMIC_O_TRUNC}} rather than implementing a mostly-broken codepath that will approximately-never get tested.  Again, add a JIRA reference.  Requiring RHEL6 (or equivalent) for FUSE is a reasonable tradeoff.

LGTM, +1., I will add a reference to the JIRA in the comment.

Do you have an idea for how to solve this problem in the "long term"?  I have thought about it and there are no really good choices.  Basically, it boils down to HDFS not supporting random writes.  We could implement some kind of caching layer that simulated random writes in fuse-dfs, but it would be complex.  The performance would also be pretty poor.

We could be a little bit more clever about when we actually open the HDFS file.  For example, we could convert {{fd = open(O_WRONLY) ; ftruncate(fd, 0)}} to a {{create(overwrite = true)}}.  That only solves one particular case, though.

I felt like this particular change was worth doing because it's a small change which simplifies things.  If we were going to write a caching layer we'd want to have a design discussion first.

bq. I think we should just #error...

We have to support RHEL5, for a little while longer at least., bq. Do you have an idea for how to solve this problem in the "long term"?

We should error out when the user asks for semantics that we can't support (overwriting part of a file without overwriting all of it).  We should not error out, and should provide correct semantics, when it is possible to do so.  Opening {{O_TRUNC}}, (user error truncated comment)

... Opening {{O_TRUNC}} and then overwriting the file is a use case we can support.  Opening {{O_APPEND}} is supportable.  Basically, we should provide the same behavior as a standard POSIX filesystem unless we cannot, and return an appropriate errno in those cases.

I haven't thought about it carefully, but {{flags |= O_APPEND}} doesn't seem like it's going to provide completely correct semantics., Using {{flags |= O_APPEND}} provides correct semantics up until the point when the user asks for something we can't provide (i.e. a write to a part of the file other than the end.)  At that point it returns an error.  So it seems strictly better than what you proposed (returning error immediately.)  Unless I am misunderstanding something., bq. provides correct semantics up until the point when the user asks for something we can't provide ... At that point it returns an error. So it seems strictly better than what you proposed (returning error immediately.)

I didn't propose anything!  I said "let's do APPEND for now, with the understanding that while it fixes the high-order-bit of our current semantic brokenness, it probably causes new breakage somewhere further down the line".  I don't know what the right answer is, I'm just fairly sure this isn't 100% right.  But moving from our current 10% right to 50% right is an improvement, so let's do it.

I suspect I was unclear when I said "we should error out when the user asks for semantics that we can't support".  You seem to have read that as "error out immediately", I didn't mean that.  I meant that we should implement as much of the semantics as we can, and error out when we can't.  The right moment to return error will depend on the specifics of a given scenario.

My whole point here is that this specific change, adding APPEND to a flags word when the user didn't ask for APPEND, is probably neither required nor sufficient nor even entirely accurate.  So it will probably get backed out at some point when we have implemented things more fully.  And that is fine.  Let's do the thing that gets a few important things working, right now, and make more progress in the future.

Anyways, +1 to the patch once the comments get updated to point to the JIRAs., OK, I misinterpreted your comment then.  I will post an updated patch shortly., add references to this jira, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12552749/HDFS-4140.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3470//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3470//console

This message is automatically generated., LGTM, +1., * refactor the 'POSIX flag to libhdfs flag' translation to a separate function

* we previously didn't handle {{O_TRUNC}} | {{O_RDWR}}, but now we do

* give an error when we try to open a nonexistent file without {{O_CREAT}}, Thanks for the cleanup in .005, Colin.  The new patch is a big improvement.

There are multiple calls to {{info = hdfsGetPathInfo(fs, path);}} and to {{hdfsFreeFileInfo}} that can be coalesced -- move hdfsGetPathInfo up into the common code before the if(O_RDWR) test, and move hdfsFreeFileInfo to a "out:" common exit path.

Other than that the new patch looks great., * address Andy's suggestions, LGTM, thanks for the cleanup!
+1 pending jenkins, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12556162/HDFS-4140.006.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestEditLog

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3604//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3604//console

This message is automatically generated., The test failure is bogus; we didn't change the edit log at all in this patch.  I filed HDFS-4283 to fix the test issue., Agreed the TestEditLog failure is unrelated.

+1, patch looks good., * refactor for clarity

* don't check length > 0; it shouldn't matter., I like the cleanup, that part LGTM.

I somewhat prefer keeping the {{int *outflags}} arg separate from the return value, but either way is fine.

This patch changes the behavior for file-exists-length-0-RDWR.  Previously it would open WRONLY presumably on the assumption that the user probably wants to put something in the file.  With .007.patch it now opens RDONLY.

Maybe we should preserve the WRONLY-if-zero-length behavior since it seems potentially useful?  If you've tested the behavior and it's not useful then no worries of course., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560655/HDFS-4140.007.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3652//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3652//console

This message is automatically generated., Thanks for cleaning up that code for clarity. I agree that it makes things clearer. I still find that function a little tough to follow, but I can't think of any way to restructure it that would make it much clearer.

bq. Maybe we should preserve the WRONLY-if-zero-length behavior since it seems potentially useful?

That makes sense to me. Opening a zero-length file for RDONLY seems odd to me., add special case for using WRONLY on 0-length files, interdiff says:
{code}
@@ -62,7 +62,9 @@
   }
   info = hdfsGetPathInfo(fs, path);
   if (info) {
-    if ((flags & O_ACCMODE) == O_RDWR) {
+    if (info->mSize == 0) {
+      ret = O_WRONLY;
+    } else if ((flags & O_ACCMODE) == O_RDWR) {
       // HACK: translate O_RDWR requests into O_RDONLY if the file already
       // exists.
       ret = O_RDONLY;
{code}
We should add a comment explaining the thinking, here.  Every other "else if" has a {{// HACK}} comment, one more is good. :)

The change looks fine. Do we have manual testing showing that the behavior is as expected?  Perhaps copy-and-paste a shell transcript into a comment here., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560910/HDFS-4140.008.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestPersistBlocks

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3661//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3661//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560910/HDFS-4140.008.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestPersistBlocks
                  org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3664//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3664//console

This message is automatically generated., I added a comment for the {{mSize == 0}} case.

With regard to testing, I used {{test_fuse_dfs}}, which includes {{testOpenTrunc}}, which tests opening a file with {{O_TRUNC}}., +1, the latest patch looks good to me. I think this not only improves the behavior, but also the code clarity, pretty substantially.

Andy: it seems to me like Colin's latest patch addresses your most recent feedback. If not, please just file a follow-up JIRA and point me to it and I'll take a look ASAP.

I'm going to commit this momentarily., I've just committed this to trunk and branch-2.

Thanks a lot for the contribution, Colin, and thanks also to Andy for the many thorough reviews., Integrated in Hadoop-trunk-Commit #3133 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3133/])
    HDFS-4140. fuse-dfs handles open(O_TRUNC) poorly. Contributed by Colin Patrick McCabe. (Revision 1423257)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1423257
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_open.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.h
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/fuse_workload.c
, Integrated in Hadoop-Yarn-trunk #69 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/69/])
    HDFS-4140. fuse-dfs handles open(O_TRUNC) poorly. Contributed by Colin Patrick McCabe. (Revision 1423257)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1423257
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_open.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.h
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/fuse_workload.c
, Integrated in Hadoop-Hdfs-trunk #1258 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1258/])
    HDFS-4140. fuse-dfs handles open(O_TRUNC) poorly. Contributed by Colin Patrick McCabe. (Revision 1423257)

     Result = FAILURE
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1423257
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_open.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.h
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/fuse_workload.c
, Integrated in Hadoop-Mapreduce-trunk #1289 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1289/])
    HDFS-4140. fuse-dfs handles open(O_TRUNC) poorly. Contributed by Colin Patrick McCabe. (Revision 1423257)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1423257
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_connect.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_impls_open.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.c
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/fuse_init.h
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/fuse_workload.c
, bq. Andy: it seems to me like Colin's latest patch addresses your most recent feedback. If not, please just file a follow-up JIRA and point me to it and I'll take a look ASAP.

LGTM, thanks for the commit.]