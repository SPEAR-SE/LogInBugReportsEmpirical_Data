[thanks for reporting.

Hope you are using 2.7.1 release. can you post jmx metrics (you can get through http:<datanodeip>:<httpport>/jmx)., Thank you [~brahmareddy], I paste a part of the datanode jmx response here, it  shown this issue.
{code}
// code json
{
  "name": "Hadoop:service=DataNode,name=FSDatasetState-null",
  "modelerType": "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl",
  "Remaining": 0,
  "StorageInfo": "FSDataset{dirpath='[/data0/dfs/current, /data1/dfs/current, /data2/dfs/current, /data3/dfs/current, /data4/dfs/current, /data5/dfs/current, /data6/dfs/current, /data7/dfs/current, /data8/dfs/current, /data9/dfs/current, /data10/dfs/current, /data11/dfs/current]'}",
  "Capacity": 6019039002624,
  "DfsUsed": 13556514861845,
  "CacheCapacity": 0,
  "CacheUsed": 0,
  "NumFailedVolumes": 0,
  "FailedStorageLocations": [],
  "LastVolumeFailureDate": 0,
  "EstimatedCapacityLostTotal": 0,
  "NumBlocksCached": 0,
  "NumBlocksFailedToCache": 0,
  "NumBlocksFailedToUncache": 25679
}
{code}, The reason is we reset the reserved to a very big value, so the capacity decrease, and lower than the used value., This situation may be amazing, but it is not a issue.]