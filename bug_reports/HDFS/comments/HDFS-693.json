[Furthermore, the exception in DataNode I see :

2009-10-12 09:43:47,261 INFO dfs.DFSClient: Could not obtain block blk_3304550638094049753 from any node: java.io.IOException: No live nodes contain current block, Furthermore, the exceptions in DataNode i see :

2009-10-12 09:43:47,261INFO dfs.DFSClient: Could not obtain block blk_3304550638094049753 from any node: java.io.IOException: No live nodes contain current block, For what it is worth I've been seeing the same problem on 0.20.2.  Specifically the java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch error., I am using the 20.1 version,I am getting following  exception

ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.3.26:50010, storageID=DS-2095362429-192.168.3.26-50010-1244004238916, infoPort=50075, ipcPort=50020):DataXceiver
java.net.SocketTimeoutException:  *{color:red}660000 millis{color}8  timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/192.168.3.26:50010 remote=/192.168.3.26:34243], I have also seen this, and believe it may go back to Java + Timezone issues. I found a possibly related thread:

http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-user/201004.mbox/%3C670102.35052.qm@web52303.mail.re2.yahoo.com%3E

With summary:

" stu@ubuntu-namenode:~/testtime$ java testDateTime 
date: Thu Apr 22 11:37:32 PST 2010
stu@ubuntu-namenode:~/testtime$ date
Thu Apr 22 12:37:34 PDT 2010
stu@ubuntu-namenode:~/testtime$ cat /etc/timezone 
America/Los_Angeles

but then says:

stu@ubuntu-hadoop-1:~/testtime$ java testDateTime
date: Thu Apr 22 12:38:11 PDT 2010
stu@ubuntu-hadoop-1:~/testtime$ date
Thu Apr 22 12:38:12 PDT 2010
stu@ubuntu-hadoop-1:~/testtime$ cat /etc/timezone 
America/Los_Angeles"


....It looks like this person is saying that testDateTime sometimes works exactly right and other times does not... We can probably get around it by using "TZ" in the environment for Java, which is what I'm trying currently. So far, haven't seen more 480s timeouts., Followup, even though I set TZ, I still get 480s timeouts in datanodes. Here's a sample, where it appears only 3ms actually pass before it complains:

2010-07-07 13:56:57,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.200.207:50010, dest: /192.168.200.207:44915, bytes: 264192, op: HDFS_READ, cliID: DFSClient_1293902555, srvID: DS-7579347-192.168.200.207-50010-1278476587314, blockid: blk_-3262154426456719253_1840

2010-07-07 13:56:57,868 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.200.207:50010, storageID=DS-7579347-192.168.200.207-50010-1278476587314, infoPort=50075, ipcPort=50020):Got exception while serving blk_-3262154426456719253_1840 to /192.168.200.207:
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/192.168.200.207:50010 remote=/192.168.200.207:44915]
    at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)
    at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)
    at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)
    at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:313)
    at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:400)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:180)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:95)
    at java.lang.Thread.run(Thread.java:619)

I am using hadoop 0.20.2 and hbase 0.20.4., Last thing to say, setting the dfs.socket.write.timeout to 0 is so far the best workaround I've discovered. I may try other values to see if I can find the upper limit, but if it is near an hour, starts to seem like timezone jumps. I'm using java 1.6.0_16. Could it be the hadoop call or the java call is relying on uninit var to decide if daylight savings hour...?, Sorry, I meant to say dfs.datanode.socket.write.timeout, Not sure how directly this is related, but since there's so much suggestions out there to use dfs.datanode.socket.write.timeout = 0, it sort of blows up in this line of code:

long writeTimeout = HdfsConstants.WRITE_TIMEOUT_EXTENSION * nodes.length +
                            datanodeWriteTimeout;

from:

hadoop-0.20.2/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java

"datanodeWriteTimeout" is -ZERO- due to the desire to have infinite write timeouts, and per much advice on the web.. however, this particular line renders that useless since it adds it to the constant (5000) * # nodes (in this case, "2" were involved.. replication maybe).
, I just wanted to re-iterate the above statement about the portions of code where the value is added in with the WRITE_TIMEOUT_EXTENSION constant * number of nodes...

If using '0' to get around write timeout problems is a bad practice or not is probably the first question. If so, is it documented somewhere? If not, then logic like I've pointed out above would break the idea of using "infinite" wait. 

I ran into timeout conditions this time starting with exception:
50010-1267539292546, infoPort=50075, ipcPort=50020):Exception writing block blk_3120944928137673159_2109400 to mirror 192.168.130.94:50010
java.net.SocketException: Broken pipe
    at java.net.SocketOutputStream.socketWrite0(Native Method)
    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
    at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
    at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
    at java.io.DataOutputStream.write(DataOutputStream.java:90)
    at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:401)
    at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:524)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:357)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:103)
    at java.lang.Thread.run(Thread.java:619)


When I look at
BlockReceiver.java:401

I see it goes back to mirrorOut, defined before the call to receiveBlock in dataXceiver, as:

(line 285)          mirrorOut = new DataOutputStream(
             new BufferedOutputStream(
                         NetUtils.getOutputStream(mirrorSock, writeTimeout),
                         SMALL_BUFFER_SIZE));

with writeTimeout from line 280:

 int writeTimeout = datanode.socketWriteTimeout +
                             (HdfsConstants.WRITE_TIMEOUT_EXTENSION * numTargets);


I have avoided almost every timeout but occasional read-side timeouts in my very slow VM environment by setting dfs.datanode.socket.write.timeout to something like 1000000, but had not yet tried this in production, where it is still zero, and I received the timeout complaint.

At the time I was writing 1M records per hour from about 8 different clients of hbase., Setting dfs.datanode.socket.write.timeout to zero decreases actual timeout both for DFS Client and Server, but for Server it also switches from NIO to Regular socket implementation.
Prooflink: http://svn.apache.org/viewvc/hadoop/common/tags/release-0.20.2/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java?revision=916569&view=markup
Lines (250,251),(307,308),(403-407) or search for socketWriteTimeout, In our observation this issue came in long run with huge no of blocks in Data Nodes . every hour Data Nodes are sending their blocks report to the Name Node. If number of blocks in Data Node are huge (3 Data Nodes with 2GB RAM, Scribe server is sending logs at 5000records/s , 4 scribe clients , block size is 64MB ) then it requires good amount of time to scan all the blocks. This block scanning causes lot of IO operations. At this time if any write request comes , then it will take long time for it to get a free io channel on the Data Node. Because of this during the blcock scan time a Data Node may not be able to acknowledge the client requests causing timeouts   on the client sockets.
 If DN1 send the data to DN2 for replication and at that time DN2 is doing the block scanning. Since DN2 is busy, it may not be able to send the ack to DN1 on time. So here timeouts can happen. 
, Folks,

What's the recommendation here. Does increasing the timeout resolve this? Does it have any other side effects.We are hitting the same on medium size cluster ~60 nodes., This issue has gone stale - resolving. The timeouts result from a variety of operations (i.e. actual timeouts due to the client disappearing - tasks getting killed, etc., hitting transceiver limits, etc.) - not due to a bug in HDFS. Or at least, not anymore.

Resolving as Not A Problem. Please file a new ticket if you are seeing unwanted timeouts on healthy clients/environments/configuration.]