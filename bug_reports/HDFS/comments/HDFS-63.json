[DF and DU sizes on the datanode match very closely with information reported by dfsadmin command. 
Lsof reports some 1000 open files in DFS data directories on the problematic datanode, but total size for open files is only about 10GB.

Here is something interesting - fsck before datanode restart reports very significant number of over-replicated blocks (~10% of blocks are over-replicated):

Status: HEALTHY
 Total size:    1472758591906 B (Total open files size: 29050588133 B)                                                      
 Total dirs:    58431                                                                                                       
 Total files:   375703 (Files currently being written: 418)                                                                 
 Total blocks (validated):      387205 (avg. block size 3803562 B) (Total open file blocks (not validated): 595)            
 Minimally replicated blocks:   387205 (100.0 %)                                                                            
 Over-replicated blocks:        38782 (10.015883 %)                                                                         
 Under-replicated blocks:       0 (0.0 %)                                                                                   
 Mis-replicated blocks:         0 (0.0 %)                                                                                   
 Default replication factor:    3                                                                                           
 Average block replication:     3.1003888                                                                                   
 Corrupt blocks:                0                                                                                           
 Missing replicas:              0 (0.0 %)                                                                                   
 Number of data-nodes:          7                                                                                           
 Number of racks:               1                                                                                           

After datanode restart - over-replicated nodes are practically gone:

Status: HEALTHY
 Total size:    1310669475298 B (Total open files size: 29535016933 B)
 Total dirs:    59431
 Total files:   377177 (Files currently being written: 387)
 Total blocks (validated):      386661 (avg. block size 3389712 B) (Total open file blocks (not validated): 607)
 Minimally replicated blocks:   386661 (100.0 %)
 Over-replicated blocks:        272 (0.070345856 %)
 Under-replicated blocks:       0 (0.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    3
 Average block replication:     3.0007036
 Corrupt blocks:                0
 Missing replicas:              0 (0.0 %)
 Number of data-nodes:          7
 Number of racks:               1
, I have the same problem on hadoop-0.18.2 + hbase-0.18.1. The one or more nodes in our cluster run out of disk once or twice a week. It gets full more frequently than before. It was less than once a week.
Is this solved in the latest version?

Our cluster have 5 data nodes. Hbase region server is running on each of them. Usually, read/write (100~1000 requests/sec) comes into hbase via thrift python interface. Heavy hbase map-reduce job runs twice a day.

Today, two nodes got almost full. Other node has 300GB free space. So, I suspect that 300 GB was used for over-replicated blocks.
After dfs was restarted, it deletes many blocks from datanode. Usually, it takes many hours to finish deleting.

The fsck status when the problem occurs:
-----
Status: HEALTHY
 Total size: 353225642094 B
 Total dirs: 38768
 Total files: 47456
 Total blocks (validated): 51146 (avg. block size 6906222 B)
 Minimally replicated blocks: 51146 (100.0 %)
 Over-replicated blocks: 108 (0.21116021 %)
 Under-replicated blocks: 0 (0.0 %)
 Mis-replicated blocks: 0 (0.0 %)
 Default replication factor: 3
 Average block replication: 3.0021117
 Corrupt blocks: 0
 Missing replicas: 0 (0.0 %)
 Number of data-nodes: 5
 Number of racks: 1
-----


Many errors are reported in datanode log. I don't know if this error is related to this problem.

----
2009-07-23 16:09:41,594 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.180.10.51:50010, storageID=DS-1052523904-10.180.10.51-50010-1247187249581, infoPort=50075, ipcPort=50020):DataXceiver: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.net.SocketInputStream.read(SocketInputStream.java:182)
        at java.io.DataInputStream.readByte(DataInputStream.java:248)
        at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:324)
        at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:345)
        at org.apache.hadoop.io.Text.readString(Text.java:410)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1270)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1076)
        at java.lang.Thread.run(Thread.java:619)

2009-07-23 16:09:42,889 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.180.10.51:50010, storageID=DS-1052523904-10.180.10.51-50010-1247187249581, infoPort=50075, ipcPort=50020):DataXceiver: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.net.SocketInputStream.read(SocketInputStream.java:182)
        at java.io.DataInputStream.readByte(DataInputStream.java:248)
        at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:324)
        at org.apache.hadoop.io.WritableUtils.readVInt(WritableUtils.java:345)
        at org.apache.hadoop.io.Text.readString(Text.java:410)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1270)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1076)
        at java.lang.Thread.run(Thread.java:619)
-----
2009-07-23 16:32:40,702 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.180.10.51:50010, storageID=DS-1052523904-10.180.10.51-50010-1247187249581, infoPort=50075, ipcPort=50020):DataXceiver: java.io.EOFException: while trying to read 65557 bytes
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.readToBuf(DataNode.java:2508)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.readNextPacket(DataNode.java:2552)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2616)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2742)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1314)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1076)
        at java.lang.Thread.run(Thread.java:619)

2009-07-23 16:32:40,706 ERROR org.apache.hadoop.dfs.DataNode: DatanodeRegistration(10.180.10.51:50010, storageID=DS-1052523904-10.180.10.51-50010-1247187249581, infoPort=50075, ipcPort=50020):DataXceiver: java.io.EOFException: while trying to read 65557 bytes
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.readToBuf(DataNode.java:2508)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.readNextPacket(DataNode.java:2552)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receivePacket(DataNode.java:2616)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(DataNode.java:2742)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1314)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1076)
        at java.lang.Thread.run(Thread.java:619)
----
, 
is this issue there in 0.20 versions????, We are experiencing the same issue in Hadoop 0.18.2. Over the time, one of the datanodes gets its disk space clogged, but on subsequent restart of the process, the disk usage evens out. Is the issue still persistent in the upgraded 0.20 versions?, This is a very old bug that has not been reported on 0.20 and later releases. Closing the bug for now. If this is still a problem, please reopen the jira.]