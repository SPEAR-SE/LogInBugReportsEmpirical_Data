[bq. The resolution is to catch the OoMException and handle it properly when calling BlockPool.offerService() in DataNode.java; like as done in 0.22.0 of Hadoop. DataNodes should not shutdown or crash but remain in a sort of frozen state until memory issues are resolved by GC
First, OOM is not an exception but an error. Catching an error and trying to handle it is a bad idea. Datanode should shutdown when this happens and staying in frozen state is incorrect., Please see some related discussion in HDFS-2911. BTW the right solution here is to configure the process with right heap size., Yes, it is an error not an exception. Yes, configuration for more memory will resolve the issue. Maybe we should specify not to run DataNodes with low memory somehow? I mean, even the simplest stress test will result in this crash.

I read your discussions in HDFS-2911 and understand why you don't want to handle OoM. If you would prefer to let the DataNode crash than I understand. To be honest, I was skeptical about raising this JIRA but I felt that this particular case can be resolved with waiting for receiver threads to free up.

If we do not want to deal with it that is fine., I think recovery from an error might be an overkill indeed. However, catching it and re-throwing with a hint about possible cause of an issue might be very helpful for the people trying to debug the problem., bq. However, catching it and re-throwing with a hint about possible cause of an issue might be very helpful for the people trying to debug the problem.
I think OOM is fairly descriptive and widely understood (hopefully!). Not sure how a cause can be pointed out - among many it could be incorrect heap size, possible memory leak or some other bug.

May be, additional information as done in HADOOP-7469 could be added to describe many reasons why it could happen. It should be done in another jira.

If no one comments back or disagrees, I will close this jira as Invalid shortly., I believe OP has came across the problem with default mem. config at 128M.
So, a descriptive hint like 
"Make sure the datanode JVM has big enough heapsize configured in... "
would be helpful. 

Not like it is a high priority ticket, but it is a usability issue., Indeed the issue why I bothered raising the JIRA was because the crash was vague and the OOM does not stand out compared to lines of "Block Pool shutting down", etc, that got printed in the logs.

Look at my log snippet I posted. That line was nowhere near the bottom of the logs by the time DataNode shutdown. It would be nice if it could be made more apparent as it was the root cause., bq. Indeed the issue why I bothered raising the JIRA was because the crash was vague and the OOM does not stand out compared to lines of "Block Pool shutting down", etc, that got printed in the logs.
Sorry I might have misunderstood. The following line made me think you want to handle OOM.

bq. DataNodes should not shutdown or crash but remain in a sort of frozen state until memory issues are resolved by GC.

I am okay if you want to add more details. But the above line from the description confuses the intent of the jira. How about creating another jira that just says improve the message printed on OOM (if you want) and close this jira as invalid?
, No you are correct. My initial intent was to handle the OOM. Your discussion post made me understand why we should not. Thank you. These were both concerns of mine, but I agree now to not handle the OOME.

Please understand, this error was reached on a freshly installed vanilla cluster of 3 DataNodes and 1 NameNode with minor changes to configs for distributed mode. This error will hit anyone using the default 128M DataNode memory configuration and trying to use a distributed cluster. Let's not handle the OOME, but maybe another path to consider is lowering the default number of receiver threads in the related default.xml so that we do not OOME so easily by default? If not, then I think the consensus is that we improve the error message handled by the try/catch around BPServiceActor.offerService().

Please let me know your final thoughts and you can close this JIRA as invalid. , I understand we are not catching OOM. But the problem still remains. People starting the cluster with default configuration ending up with dysfunctional cluster and dead DataNodes.

I propose to adjust the default configuration to avoid the problem. There is clear unbalance between the default heap size (128 MB) and number of threads we allow. Either the default heap size should increase or the # of threads should go down.
Plamen you have a reproducible configuration to crash the cluster. Could you investigate how many BP threads 128 MB can hold? You can reduce the thread count gradually until the cluster doesn't crash., bq. Either the default heap size should increase or the # of threads should go down.
Either one of them should be okay. Please do that in a separate jira.

Closing this jira as Invalid., I know I'm coming in late on this and I agree this problem is easily solved by increasing heap sizes, restricting user quotas. But perhaps inside the daemons we should be monitoring [totalMemory()|http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Runtime.html#totalMemory%28%29] and enter a "safemode" when we come too close to OOMing?, I think the agreement made was to not try to handle OOM on DataNodes, but to let them crash. If you can, you should increase the heap size of your DataNode. In HADOOP-9211, I got that 512mb was enough to not cause any issues while under stress, with a vanilla setup.]