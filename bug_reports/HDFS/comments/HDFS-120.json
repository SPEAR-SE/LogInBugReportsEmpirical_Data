[Local test passed, Submitting., -1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12380362/1549.patch
against trunk revision 645773.

    @author +1.  The patch does not contain any @author tags.

    tests included +1.  The patch appears to include 42 new or modified tests.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests -1.  The patch failed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2259/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2259/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2259/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2259/console

This message is automatically generated., {code}
org.apache.hadoop.dfs.TestDFSUpgradeFromImage.testUpgradeFromImage
org.apache.hadoop.dfs.TestDistributedUpgrade.testDistributedUpgrade 

java.lang.OutOfMemoryError: Requested array size exceeds VM limit
	at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:796)
	at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:711)
	at org.apache.hadoop.dfs.FSImage.doUpgrade(FSImage.java:303)
	at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:270)
	at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:81)
	at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:274)
	at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:255)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:133)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:178)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:164)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:782)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:264)
	at org.apache.hadoop.dfs.MiniDFSCluster.<init>(MiniDFSCluster.java:162)
	at org.apache.hadoop.dfs.TestDFSUpgradeFromImage.testUpgradeFromImage(TestDFSUpgradeFromImage.java:185)

{code}

Upgrade test failed. Is problem related with UTF8 remove?
, A problem seems to be related with hadoop-12-dfs-dir.tgz., Could you remove the un-related SuppressWarnings tags in the patch?, > A problem seems to be related with hadoop-12-dfs-dir.tgz.
This test changes the format of the namespace image on the disk. So it needs to handle upgrade of older images. TestDFSUpgradeFromImage test upgrade from hadoop 0.12 to current version which we need to support., I agree we should not use SuppressWarnings, we should define serialVersionUID instead but in a different issue.
A more serious problem here is that the patch will change the format of the image, and the consequences are not clear.
I mean that we need to know what are the changes in the image/edits file sizes, and the name-node startup time.
So I recommend just to remove UTF8 for everything but the storage classes, like Storage, FSImage, FSEditLog, DataStorage., Thanks for advice, raghu and konstantin.

{quote}So I recommend just to remove UTF8 for everything but the storage classes, like Storage, FSImage, FSEditLog, DataStorage. {quote}

I see. Then, Is there no necessity for upgrade of older image? or Should we keep old image for version migration?, I just remove UTF8 except the storage classes, Also, SuperWarnnings removed., +1 overall.  Here are the results of testing the latest attachment 
http://issues.apache.org/jira/secure/attachment/12380458/1549_v02.patch
against trunk revision 645773.

    @author +1.  The patch does not contain any @author tags.

    tests included +1.  The patch appears to include 24 new or modified tests.

    javadoc +1.  The javadoc tool did not generate any warning messages.

    javac +1.  The applied patch does not generate any new javac compiler warnings.

    release audit +1.  The applied patch does not generate any new release audit warnings.

    findbugs +1.  The patch does not introduce any new Findbugs warnings.

    core tests +1.  The patch passed core unit tests.

    contrib tests +1.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2278/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2278/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2278/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/2278/console

This message is automatically generated., The InterTrackerProtocol version needs to be bumped. SequenceFile's version will also need a bump and code handling the change to its format, right? Can this read serialized ArrayWritables written out prior to this patch?, There doesn't appear to be an easy way to "update" the Writables in this patch. If you wanted to create new Writables and deprecate these- VectorWritable instead of ArrayWritable, for example- then that might be a way to effect this, but the UTF8 and Text binaries are incompatible and their differences- at first glance- are undetectable (i.e. unsigned shorts and vints cannot be distinguished). Similarly, the change to WritableName is incompatible with SequenceFile::Reader, and- again- SequenceFiles written prior to this patch must be distinguished by a version bump and appropriate handling code. With a version bump, the changes to RPC, FileSplit, JobProfile, JobStatus, Task, TaskStatus, and TaskTrackerStatus are probably fine, since they're meant to be transient.

The tests are clearly OK, as replacements for ArrayWritable and ObjectWritable would be, but the transient stuff will require some extra testing/attention. This seems like a fairly risky incompatible change. It might be a good idea to spread some of this across several patches/JIRAs., Thank you for your review.

>> but the transient stuff will require some extra testing/attention. This seems like a fairly risky incompatible change. It might be a good idea to spread some of this across several patches/JIRAs.

I'll attention it with various test on local., I'd suggest keeping the serialization format the same and moving the old style of 
4 byte int len, bytes.

Changing the serialization format of writables is pretty much impossible at this point, because we don't have any version information. Since they can be stored in files forever, it isn't possible to update them., This duplicates HADOOP-414.]