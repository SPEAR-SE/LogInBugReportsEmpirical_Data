[In two studied cases, the pipeline succeeded but the client timedout.  Close recovery succeeded.   The problem appears to be possibly be caused by a multiple IBR race causing a delayed IBR with the prior genstamp.  Due to heavy io load and DN dataset lock contention, the belief is the block sender/receiver may not be exiting prior to completion of the close recovery.  Recovery appears to try to interrupt the existing xceiver but has a timed join before plowing ahead.  The block sender does not necessarily wait for the packet responder to exit which is what will send the IBR.  If correct: when the xceiver doesn't exit, the pending operation (close recovery in this case) must fail.  The block sender must also ensure the packet responder has exited., Here is relevant log lines of the latest example. This node was under a heavy I/O load.

{noformat}
2018-02-06 00:00:03,413 [DataXceiver for client DFSClient_XXX at /1.2.3.4:57710] INFO
 Receiving BP-YYY:blk_7654321_1234567 src: /1.2.3.4:57710 dest: /1.2.3.5:1004
2018-02-06 00:09:58,840 [DataXceiver for client DFSClient_XXX at /1.2.3.4:57710] WARN
 Slow BlockReceiver write data to disk cost:462ms (threshold=300ms)
2018-02-06 00:10:40,148 [DataXceiver for client DFSClient_XXX at /1.2.3.4:57710] WARN
 Slow BlockReceiver write data to disk cost:11155ms (threshold=300ms)
2018-02-06 00:10:46,053 [DataXceiver for client DFSClient_XXX at /1.2.3.4:57710] WARN
 Slow BlockReceiver write data to disk cost:1577ms (threshold=300ms)
2018-02-06 00:11:02,376 [DataXceiver for client DFSClient_XXX at /1.2.3.4:57710] WARN
 Slow BlockReceiver write data to disk cost:327ms (threshold=300ms)
2018-02-06 00:11:53,064 [DataXceiver for client DFSClient_XXX at /1.2.3.4:40532] INFO
 Receiving BP-YYY:blk_7654321_1234567 src: /1.2.3.4:40532 dest: /1.2.3.5:1004
2018-02-06 00:12:09,782 [DataXceiver for client DFSClient_XXX at /1.2.3.4:40532] INFO
 Recover failed close BP-YYY:blk_7654321_1234567
2018-02-06 00:12:13,081 [DataXceiver for client DFSClient_XXX at /1.2.3.7:46522] INFO
 Receiving BP-YYY:blk_7654321_1234567 src: /1.2.3.7:46522 dest: /1.2.3.5:1004
2018-02-06 00:12:13,081 [DataXceiver for client DFSClient_XXX at /1.2.3.7:46522] INFO
 Recover failed close BP-YYY:blk_7654321_1234567
2018-02-06 00:12:17,276 [DataXceiver for client DFSClient_XXX at /1.2.3.4:40532] WARN
 Lock held time above threshold: lock identifier: org.apache.hadoop.hdfs.server.datanode
 .fsdataset.impl.FsDatasetImpl lockHeldTimeMs=7492 ms. Suppressed 0 lock warnings.
 The stack trace is: java.lang.Thread.getStackTrace(Thread.java:1556)
 ... // it was recoverClose()
2018-02-06 00:12:17,276 [DataXceiver for client DFSClient_XXX at /1.2.3.7:46522] INFO
 Received BP-YYY:blk_7654321_1135832806836 src: /1.2.3.7:46522 dest: /1.2.3.5:1004 of size xx
2018-02-06 00:12:20,103 [DataXceiver for client DFSClient_XXX at /1.2.3.4:40532] INFO
 Received BP-YYY:blk_7654321_1135832805246 src: /1.2.3.4:40532 dest: /1.2.3.5:1004 of size xx
2018-02-06 00:12:38,353 [PacketResponder: BP-YYY:blk_7654321_1234567, type=LAST_IN_PIPELINE] INFO
 DataNode.clienttrace: src: /1.2.3.4:57710, dest: /1.2.3.5:1004, bytes: 134217728, op: HDFS_WRITE,
 cliID: DFSClient_XXX, offset: 0, srvID: ZZZ, blockid: BP-YYY:blk_7654321_1234567, duration: looong
{noformat}

Note the client port number to identify each writer thread. After two "successful" {{recoverClose()}}, the original writer comes around and also declares a success. This must have resulted in the reported gen stamp going backward. On disk actually was the latest one.

This clearly illustrates that it is wrong to time out on the writer termination and continue with the recovery., It looks like "trying forever" might be actually part of the problem.

{code:java}
 public Replica recoverClose(....) throws IOException {
    while (true) {
      try {
        try(AutoCloseableLock lock = datasetLock.acquire()) {
          // check replica's state
          ReplicaInfo replicaInfo = recoverCheck(b, newGS, expectedBlockLen);
          // update the replica state/gs and finalize if necessary.
          return replicaInfo;
        }
      } catch (MustStopExistingWriter e) {
        e.getReplica().stopWriter(datanode.getDnConf().getXceiverStopTimeout());
      }
    }
  }
{code}

When the I/O frees up and the original writer (normally the packet responder) and the xceiver thread doing {{recoverClose()}} can finish in non-deterministic order.  If {{recoverClose()}} finishes last, everything is good.  If the packer responder finishes last as the example above, the replica will be marked as corrupt until the next full block report., move to 2.8.5 given this jira has been quiet for a while., Note that unassigned jiras are free for the taking. Â Please. :)]