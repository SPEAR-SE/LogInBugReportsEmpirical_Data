[There are the following errors (see https://builds.apache.org/job/PreCommit-HDFS-Build/7198//testReport/):

{code} 
 org.apache.hadoop.ipc.TestSaslRPC.testErrorMessage[0]	0.6 sec	1
 org.apache.hadoop.ipc.TestSaslRPC.testErrorMessage[1]	22 ms	1
 org.apache.hadoop.ipc.TestSaslRPC.testErrorMessage[2]	18 ms	1
 org.apache.hadoop.ipc.TestSaslRPC.testErrorMessage[3]	18 ms	1
 org.apache.hadoop.ipc.TestSaslRPC.testErrorMessage[4]	15 ms	1
 org.apache.hadoop.hdfs.web.TestWebHdfsTokens.testLazyTokenFetchForSWebhdfs	5.5 sec	1
 org.apache.hadoop.hdfs.web.TestWebHdfsTokens.testLazyTokenFetchForWebhdfs
{code}

The errors belong to two categories:

1. TestWebHdfsTokens

It failed due to the following exception:
{code}
java.security.PrivilegedActionException: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (HDFS_DELEGATION_TOKEN token 5 for DummyUser) can't be found in cache.
{code}
thus causing a SecurityException thrown by UserProvider with cause InvalidToken, which is the InvalidToken listed above. The reason is "token not found in cache" and it has no cause assigned.

This means UserProvider could throw two types of exception relevant to our discussion:

T1. SecurityException with InvalidToken as cause, which has StandbyException as cause. Reported in HDFS-6475.
T2. SecurityException with InvalidToken as cause, which has no cause assigned, and the failure was because "token not found in cache".

I found that currently committeed code (including Daryn's fix HDFS-6222) appear to have handled T2, and there actually seems to be no real problem with T2, except my earlier patch to HDFS-6475 "touched" it by returning the InvalidToken, which triggerd these test failures.  I am modifying the patch of HDFS-6475 to just process T1 (and not T2) and submitting new version there. 

Hi [~daryn]  thanks a lot for your review and comments earlier at HDFS-6475. hope this change will be fine with you. I will invite you to comment there when I have the new version uploaded. Appreciate very much.

2. TestSaslRPC

{code}
expected:<[Token is invali]d> but was:<[DIGEST-MD5: IO error acquiring passwor]d>
Stacktrace

org.junit.ComparisonFailure: expected:<[Token is invali]d> but was:<[DIGEST-MD5: IO error acquiring passwor]d>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hadoop.ipc.TestSaslRPC.testErrorMessage(TestSaslRPC.java:387)
{code}

This error happened because we no longer pass the exception through getTrueCause. 

The stack when the above error happened is listed below as a reference.  We can see that the {{retriableRetrievePassword}} method is called, and this test designed it to return InvalidToken because of class {{BadTokenSecretManager}} in the test code TestSaslRPC.java. 

Hi [~jingzhao], many thanks for your review and advice in HDFS-6475. The {{getTrueCause}} method is part of the HDFS-5322 solution you worked out. Would you please comment whether the above failed tests (failed because of  getTrueCause removal) reflect some real use scenarios? If so, then the logic getgTrueCause need to be reflected in somewhere if this method is to be removed. Thanks.

{code}
TestSaslRPC$BadTokenSecretManager.retrievePassword(TestSaslRPC$TestTokenIdentifier) line: 268	
TestSaslRPC$BadTokenSecretManager(TestSaslRPC$TestTokenSecretManager).retrievePassword(TokenIdentifier) line: 1	
TestSaslRPC$BadTokenSecretManager(SecretManager<T>).retriableRetrievePassword(T) line: 91	
SaslRpcServer$SaslDigestCallbackHandler.getPassword(TokenIdentifier) line: 275	
SaslRpcServer$SaslDigestCallbackHandler.handle(Callback[]) line: 302	
DigestMD5Server.validateClientResponse(byte[][]) line: 585	
DigestMD5Server.evaluateResponse(byte[]) line: 244	
Server$Connection.processSaslToken(RpcHeaderProtos$RpcSaslProto) line: 1392	
Server$Connection.processSaslMessage(RpcHeaderProtos$RpcSaslProto) line: 1369	
Server$Connection.saslProcess(RpcHeaderProtos$RpcSaslProto) line: 1270	
Server$Connection.saslReadAndProcess(DataInputStream) line: 1244	
Server$Connection.processRpcOutOfBandRequest(RpcHeaderProtos$RpcRequestHeaderProto, DataInputStream) line: 1932	
Server$Connection.processOneRpc(byte[]) line: 1805	
Server$Connection.readAndProcess() line: 1548	
Server$Listener.doRead(SelectionKey) line: 753	
Server$Listener$Reader.doRunLoop() line: 627	
Server$Listener$Reader.run() line: 598	
{code}

BTW, I have spent quite some time to analyze this, I do believe removal of {{getTrueCause}} is a separate issue and deserve a new jira here. I hope you'd agree based on the information provided above. If you still think it should be taken care of as part of HDFS-6475, please let me know. I'm certainly open for discussion. Thanks a lot.


, HDFS-5322 should not be related to the failed test. In general, HDFS-5322 simply handles the same issue you're fixing in HDFS-6475, but for the RPC side. So please feel free to make any change you think necessary., Hi  [~jingzhao], thanks a lot for the comments. Sorry I didn't make it clear. What I wanted to say was that the getTrueCause method is part of the HDFS-5322 work, the reported tests failed here because of removing getTrueCause(). We could modify the tests to make them pass, but my worry was that the tests were set up to capture real user scenario, changing the test setup might make them no longer reflect real user scenario.

Based on your answer above, however, I guess we could just modify the tests accordingly after removing the getTrueCause() method. Please correct me if I'm wrong. Thanks.
, Yeah, also the failed tests were not introduced by HDFS-5322 actually., Thanks Jing.

I found that the getTrueCause() method interacts with the following code 
{code}
  @Override
  public byte[] retrievePassword(
      DelegationTokenIdentifier identifier) throws InvalidToken {
    try {
      // this check introduces inconsistency in the authentication to a
      // HA standby NN.  non-token auths are allowed into the namespace which
      // decides whether to throw a StandbyException.  tokens are a bit
      // different in that a standby may be behind and thus not yet know
      // of all tokens issued by the active NN.  the following check does
      // not allow ANY token auth, however it should allow known tokens in
      namesystem.checkOperation(OperationCategory.READ);
    } catch (StandbyException se) {
      // FIXME: this is a hack to get around changing method signatures by
      // tunneling a non-InvalidToken exception as the cause which the
      // RPC server will unwrap before returning to the client
      InvalidToken wrappedStandby = new InvalidToken("StandbyException");
      wrappedStandby.initCause(se);
      throw wrappedStandby;
    }
{code}
in DelegationTokenSecretManager.java introduced by HADOOP-9880.

If we remove the getTrueCause() logic, at minimum, still need to retain the logic (currently in getTrueCause) to  return the InvalidToken exception that's wrapped by SaslException.


, Upon discussion in HDFS-6475, will try to replace calls to retrievePassword with calls to retriableRetrievePassword as part of fixing this jira. They two calls throw different exceptions, the surrounding logic of the call need to be examined to handle them appropriately.

, Hi [~daryn], [~jingzhao] and [~atm],

Thanks you all for reviewing HDFS_6475 earlier. Sorry for some delay to follow up with this jira because I took some time off.  I just attached a patch. Your review and input is highly appreciated.

Some quick summary of the patch:

1. Changed AbstractDelegationTokenSecretManager.verifyToken to call retriableRetrievePassword instead of retrievePassword, so it can throw StandbyException. Changed exception specification of related methods.

2. Revised the logic in ExceptionHandler and the test I added for HDFS-6475, so now we handle the case that SecurityException that directly wraps StandbyException (no InvalidToken in between)

3. Largely dropped the logic of getTrueCause. One thing I noticed is, there is a scenario that SaslException wraps InvalidToken as cause, which need to be handled by saslProcess method of Server class. So I didn't totally remove the getTrueCause method. Instead, I left the code to handle this scenario as the only logic there.  As a result, Server.java no longer explicitly depends on StandbyException and RetriableException.

Removing getTrueCause method as a whole caused the test failure reported in this jira, because of the above described scenario.  

The stack looks like
{code}
14/07/14 10:42:28 WARN ipc.Server: Auth failed for 127.0.0.1:58082:null (DIGEST-MD5: IO error acquiring password)
javax.security.sasl.SaslException: DIGEST-MD5: IO error acquiring password [Caused by org.apache.hadoop.security.token.SecretManager$InvalidToken: Token is invalid]
	at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:594)
	at com.sun.security.sasl.digest.DigestMD5Server.evaluateResponse(DigestMD5Server.java:244)
	at org.apache.hadoop.ipc.Server$Connection.processSaslToken(Server.java:1413)
	at org.apache.hadoop.ipc.Server$Connection.processSaslMessage(Server.java:1390)
	at org.apache.hadoop.ipc.Server$Connection.saslProcess(Server.java:1289)
	at org.apache.hadoop.ipc.Server$Connection.saslReadAndProcess(Server.java:1244)
	at org.apache.hadoop.ipc.Server$Connection.processRpcOutOfBandRequest(Server.java:1953)
	at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1826)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1569)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:753)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:627)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:598)
Caused by: org.apache.hadoop.security.token.SecretManager$InvalidToken: Token is invalid
	at org.apache.hadoop.ipc.TestSaslRPC$BadTokenSecretManager.retrievePassword(TestSaslRPC.java:268)
	at org.apache.hadoop.ipc.TestSaslRPC$TestTokenSecretManager.retrievePassword(TestSaslRPC.java:1)
	at org.apache.hadoop.security.token.SecretManager.retriableRetrievePassword(SecretManager.java:91)
	at org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler.getPassword(SaslRpcServer.java:275)
	at org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler.handle(SaslRpcServer.java:302)
	at com.sun.security.sasl.digest.DigestMD5Server.validateClientResponse(DigestMD5Server.java:585)
	... 11 more
{code}

Thanks.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12655689/HDFS-6588.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.shell.TestCopyPreserveFlag
                  org.apache.hadoop.fs.TestSymlinkLocalFSFileContext
                  org.apache.hadoop.fs.shell.TestTextCommand
                  org.apache.hadoop.ipc.TestIPC
                  org.apache.hadoop.fs.TestSymlinkLocalFSFileSystem
                  org.apache.hadoop.fs.shell.TestPathData
                  org.apache.hadoop.fs.TestDFVariations
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7345//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7345//console

This message is automatically generated., It appeared to be a glitch in the testing, re-upload the same patch to trigger a new run. However, there seems to be a real problem with 
org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA.testSaveNamespace, which failed in other runs many times, filed HDFS-6683.


, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12655819/HDFS-6588.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.TestSymlinkLocalFSFileSystem
                  org.apache.hadoop.fs.TestSymlinkLocalFSFileContext
                  org.apache.hadoop.ipc.TestIPC
                  org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7350//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7350//console

This message is automatically generated., Similar failures were reported with another run for a different jira:

https://builds.apache.org/job/PreCommit-HDFS-Build/7338/#showFailuresLink

Rerunning the failed tests locally are successful. Talked with [~cmccabe] about the Symlink related failures and he indicated that symlink is not currently supported and he said these test failures can be ignored for now.


, Filed HDFS-6694 for TestPipelinesFailover.testPipelineRecoveryStress failure (it failed in other JIRA tests too).  

Patch version 001 for this jira is good for review. Thanks a lot.

, bq. Rerunning the failed tests locally are successful. Talked with Colin Patrick McCabe about the Symlink related failures and he indicated that symlink is not currently supported and he said these test failures can be ignored for now.

I said symlinks are disabled in the current *stable releases* of Hadoop.  If you want to get a patch into *trunk*, you still have to ensure that the symlink unit tests don't fail, or provide an explanation of why your patch is not at fault for any failures.  (I know Jenkins has had some issues since the machines were upgraded.), Thanks [~cmccabe]. 

In the same comment I quoted from, I was trying to say that another test without the patch here failed the same tests :
{quote}
Similar failures were reported with another run for a different jira:
https://builds.apache.org/job/PreCommit-HDFS-Build/7338/#showFailuresLink
{quote}
I believe the patch here is not related to symlink at all.  I will upload the same patch one more time when HDFS-6683/HDFS-6693 is committed.


, Upload same patch to trigger a new test.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12656378/HDFS-6588.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.shell.TestCopyPreserveFlag
                  org.apache.hadoop.fs.TestSymlinkLocalFSFileContext
                  org.apache.hadoop.fs.shell.TestTextCommand
                  org.apache.hadoop.ipc.TestIPC
                  org.apache.hadoop.fs.TestSymlinkLocalFSFileSystem
                  org.apache.hadoop.fs.shell.TestPathData
                  org.apache.hadoop.fs.TestDFVariations
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7376//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7376//console

This message is automatically generated., The symlink tests failed three times in a row here, which indicates that something is wrong. I filed HDFS-6707, investigated and found root cause (irrelevant to the patch of HDFS-6588), posted patch there pending review.  The other tests failed here appear to be flaky, but will try a run again once HDFS-6707 fix is committed.
 
, Now HDFS-6707 is committed, uploading patch 001 again for another run.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12657013/HDFS-6588.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.ipc.TestIPC
                  org.apache.hadoop.hdfs.server.balancer.TestBalancer
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7414//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7414//console

This message is automatically generated., HI [~daryn], since the fix is trying to address your comments in HDFS-6475,  I wonder if you can be the first one to review the patch. I'd really appreciate it. 

The test failures appear to be irrelevant to the fix here: the TestPipelinesFailover failure was reported as HDFS-6694. For TestIPC and TestBalancer, they are stuck at the same method at
{quote}
test timed out after 100000 milliseconds
Stacktrace
java.lang.Exception: test timed out after 100000 milliseconds
	at java.net.Inet4AddressImpl.getLocalHostName(Native Method)
	at java.net.InetAddress.getLocalHost(InetAddress.java:1374)
{quote}
I filed HADOOP-10588 and am following up from there.
, Sorry typo, the TestIPC jira is HADOOP-10888.
, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12657013/HDFS-6588.001.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10636/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12657013/HDFS-6588.001.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10651/console |


This message was automatically generated.]