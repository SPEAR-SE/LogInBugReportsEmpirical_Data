[The behavior is correct since r is required for listing the children of /user/userabc/foo.  The command "-rm -r" means "remove recursively".  It doesn't know whether foo is empty or not beforehand., The submitted patch tries to address the reported problem.
Thanks for reviewing.
, Thanks [~szetszwo]. The problem is, when the sub-dir is empty, we should be able to delete it but we can't now.
It's tried on linux and linux can delete empty dir with similar ownership/permission setting.
, > ... It's tried on linux and linux can delete empty dir with similar ownership/permission setting.

Just tried with mac.  It prompts for overriding the permission.
{noformat}
szetszwo tmp$ls -l
total 0
dr-xr-xr-x  2 szetszwo  staff   68 Mar 27 15:27 empty
dr-xr-xr-x  3 szetszwo  staff  102 Mar 27 15:27 nonempty
szetszwo tmp$ls -l empty/
szetszwo tmp$ls -l nonempty/
total 8
-rw-r--r--  1 szetszwo  staff  9 Mar 27 15:27 a.txt
szetszwo tmp$rm -r empty/
override r-xr-xr-x  szetszwo/staff for empty/? n
szetszwo tmp$rm -r nonempty/
rm: nonempty//a.txt: Permission denied
override r-xr-xr-x  szetszwo/staff for nonempty/? n
{noformat}
Which version of Linux you tried?  Could you post similar CLI output?, Thanks [~szetszwo], 

My test OS: CentOS release 6.4 (Final)
2.6.32-358.2.1.el6.x86_64

I actually used "rm -rf". I'm not aware of "-f" in HDFS FsShell. If I don't use -f, I can see same behaviour as you observed. I wonder whether we should introduce the -f switch. Below is my log:

[root@vm001 usrxyz]# ls -lrt
total 12
drwxr-xr-x 2 hdfs users 4096 Mar 25 19:56 hdfs
drwxr-xr-x 3 abc  users 4096 Mar 26 11:57 abc
drwxr-xr-x 2 abc  users 4096 Mar 26 12:28 abc-empty
[root@vm001 usrxyz]# ls abc-empty/
[root@vm001 usrxyz]# su usrxyz
[usrxyz@vm001 ~]$ rm -rf abc-empty
[usrxyz@vm001 ~]$ ls abc
abcsub
[usrxyz@vm001 ~]$ ls -lrt abc
total 4
drwxr-xr-x 2 abc users 4096 Mar 26 11:57 abcsub
[usrxyz@vm001 ~]$ rm -rf abc
rm: cannot remove `abc/abcsub': Permission denied
[usrxyz@vm001 ~]$ 
, Hey Nic, here's some output from my Ubuntu 12.04 laptop:

{noformat}
-> % whoami
andrew
-> % ls -l
total 4
drwx------ 2 root root 4096 Mar 27 15:43 empty
-> % rmdir empty  
{noformat}

I think your install is defaulting to {{rm -i}}, I get similar output if I do that instead:

{noformat}
-> % rm -ri empty
rm: descend into write-protected directory `empty'? y
rm: remove write-protected directory `empty'? n
{noformat}, > [usrxyz@vm001 ~]$ rm -rf abc-empty

[~yzhangal], could you try without -f?, > -> % rmdir empty  

[~andrew.wang], Could you try "rm -r"?, Thanks [~andrew.wang].

Hi [~szetszwo], as I stated in my last update, I did try without -f, and got the same behaviour as you observed.

In my centos system, if I use "rmdir abc-empty" , I can remove abc-copy without problem. Without "\rm -r" (added slash to use the default behavior of rm), I have the same behaviour as yours (prompted to answer yes/no). BTW, I saw dfs does have -f, I didn't try this switch that before, will try.

[yzhang@yjzsn ~]$ ls -lrt
total 12
drwxr-xr-x 2 hdfs users 4096 Mar 25 19:56 hdfs
drwxr-xr-x 3 abc  users 4096 Mar 26 11:57 abc
drwxr-xr-x 2 abc  users 4096 Mar 27 15:54 abc-empty
[yzhang@yjzsn ~]$ \rm abc-empty
rm: cannot remove `abc-empty': Is a directory
[yzhang@yjzsn ~]$ \rm -r abc-empty
rm: remove write-protected directory `abc-empty'? n
[yzhang@yjzsn ~]$ 
, [~szetszwo] My "rm -r" is the same as the "rm -ri" output I posted earlier.

We could add -f or -i modes to FsShell to mimic the GNU/BSD if you like, but do we all agree that we should be able to delete these empty directories?, The hadoop command here is "dfs -rm -r".  So let's only compare it with "rm -r" in Unix/Linux.  "rmdir" is irrelevant.

Since "rm -r" in Unix/Linux does ask for overriding permission, throwing an exception for "dfs -rm -r" seems more correct.  No?

[~daryn], any comment here?, Hey Yongjun, overall the patch looks good, I just have some small review comments:

- Isn't it simpler to use the Java APIs for the test cases rather than FsShell? If so, we could just add new tests to TestPermission.
- If you really do want shell tests, there's TestFsShell (you add new tests to the xml file in src/test/resources).
- Seems unnecessary to have both "isEmpty" and "isNonEmpty" methods where one is just the complement of the other. Let's just use the existing method., Thank you all!

I will address Andrew's comments in a another update.

There seem to be two types of behaviour on different linux:
1. "rm -r" remove empty dir without problem on Andrew's system Ubuntu 12.04
2. "rm -r" prompts question to user to answer yes/no on my CentOs6 and Tsz Wo's Mac

For CentOs, using "rm -r -f" deletes with no prompt to user.

I did try -f switch with hdfs, and see it has the same problem as reported (see blow). With linux, using -f will remove the file without prompting to ask user. I wonder whether we should fix HDFS's -f switch. 

{code}
[usrxyz@vm001 ~]$ hadoop fs -lsr /
lsr: DEPRECATED: Please use 'ls -R' instead.
drwxr-xr-x   - hdfs supergroup          0 2014-03-25 16:29 /user
drwxr-xr-x   - hdfs   supergroup          0 2014-03-25 16:28 /user/hdfs
drwxr-xr-x   - usrxyz users               0 2014-03-27 16:10 /user/usrxyz
drwxr-xr-x   - hdfs   users               0 2014-03-25 16:32 /user/usrxyz/foo
-rw-r--r--   1 hdfs   users               5 2014-03-25 16:32 /user/usrxyz/foo/test.txt
drwxr-xr-x   - abc    abc                 0 2014-03-27 16:10 /user/usrxyz/foo-empty
[usrxyz@vm001 ~]$ whoami
usrxyz
[usrxyz@vm001 ~]$ hadoop fs -rm -r -f /user/usrxyz/foo-empty
14/03/27 16:18:53 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
rm: Permission denied: user=usrxyz, access=ALL, inode="/user/usrxyz/foo-empty":abc:abc:drwxr-xr-x
{code}, Please don't commit the patch.  You may want to defer the work after we agree what to do.

IMO, "dfs -rm -r" should be the same as "rm -r".  So the current patch allows delete without prompting the user is not correct.  Possible improvement could be adding prompt to "dfs -rm -r".

BTW, we do have "dfs -rmdir".  Have you checked it?, bq. 1. "rm -r" remove empty dir without problem on Andrew's system Ubuntu 12.04

Actually, it prompts like your systems, so we're all in agreement.

It'd be nice to mimic GNU rm as much as possible. It actually explains the interactive and force modes pretty well in the man page: http://man7.org/linux/man-pages/man1/rm.1.html

The key bit is this:

{noformat}
Otherwise, if a file is unwritable, standard input is a terminal, and
       the -f or --force option is not given, or the -i or
       --interactive=always option is given, rm prompts the user for whether
       to remove the file.  If the response is not affirmative, the file is
       skipped.
{noformat}

This might be incompatible though, if we start prompting where we didn't before., What about this: 

fix the code that deals with "-f" switch, if "-f" is passed, then delete empty dir without prompting to user. If not, then prompt user or throw exception.

Prompting user would involving feedback from NN to FsShell, FsShell prompt user, and FsShell talk to NN again, a bit complex. 

What do you think?

Thanks.



, For Andrew's review comments

{quote}
Seems unnecessary to have both "isEmpty" and "isNonEmpty" methods where one is just the complement of the other. Let's just use the existing method.
{quote}

The method are IsEmptyDirecotory, isNonEmptyDirectory, they are not exactly opposite, because for file, both return false.

I will look into other review comments later.

As I posted in my last update, I'd like to look into fixing "-f" first.  Please comment on whether we prompt to user or to throw exception. Throw exception is simpler solution but not compliant with linux.

Thanks.




, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12637255/HDFS-6165.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-extras:

                  org.apache.hadoop.hdfs.TestDFSPermission

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6533//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6533//console

This message is automatically generated., By a chat with Andrew, he suggested that after we fix "-f", we might change the exception message to suggest to user to use "-f" when there is this kind of exception. I think it's a good idea. Even though this is not exactly the same semantics as Linux, it's better that what we have today. Thanks Andrew.
, > ... we might change the exception message to suggest to user to use "-f" when there is this kind of exception. ...

Sounds good.

How about "dfs -mkdir"?  Does it work as expected?, Oops, I mean "dfs -rmdir"., Revised Summary and changed Priority to minor., [~yzhangal], you may want clean up the Description.  In particular, you may want to clean up the example, e.g. remove the unrelated items in the directory listings.  Since the Description is sent out to the mailing list for every email, we should make it precise and easy to read., Thanks a lot [~szetszwo], 

The summary you changed to is one part of the picture. The other one I found during our discussion is that, "hdfs dfs -rm -r -f" doesn't work as expected. I'm working on fixing the "-f" switch which is more of a problem. 

For the "hdfs dfs -rm -r", I will change the exception message since both Andrew and you agreed.

I will address your other comments a bit later. 

, I saw my name mentioned for comment.  I'll try to grok the entire discussion and patch later today.  We need to be extremely careful how we alter the permission handling., It may be informative to refer to POSIX as a standard and GNU coreutils as a sample implementation.  Here is the documentation on rm:

http://pubs.opengroup.org/onlinepubs/9699919799/utilities/rm.html

bq. If the current file is a directory, rm shall perform actions equivalent to the rmdir() ...

Here is rmdir:

http://pubs.opengroup.org/onlinepubs/9699919799/functions/rmdir.html

{quote}
The rmdir() function shall fail if:

[EACCES]
Search permission is denied on a component of the path prefix, or write permission is denied on the parent directory of the directory to be removed.
{quote}

I see no mention of any permission requirements directly on the directory to be removed.  Looking at the GNU coreutils implementation, it appears that the "remove write-protected" prompt some of you reported seeing is not part of core file system permission enforcement.  Instead, it's just a nicety of the shell utility.

http://git.savannah.gnu.org/gitweb/?p=coreutils.git;a=blob;f=src/remove.c;h=b98f3ecb286bd79c203abca709dcfec63c55db8f;hb=HEAD#l284

I'd appreciate someone else double-checking this too.  As Daryn said, it's a sensitive area., Thanks [~daryn], I'm working on making "-f" to work. I will post new patch when I'm done. Certainly it would be helpful if you can review.  There are three issues observed now:

Running the following command as parent directory owner with WRITE permission:

1. "hdfs dfs -rm -r" behaves different than unix/linux "rm -r" when trying to delete empty directory
     hdfs throws exception; unix/linux prompt to user to ask for "yes/no"

2. "hdfs dfs -rm -r -f" doesn't delete empty directory, it also throws exception, the "-f" switch essentially appears to be ignored; however unix/linux "rm -rf" does delete.

3. "hdfs -rmdir" throws exception (thanks [~szetszwo] for suggesting to try), unix/linux "rmdir" deletes empty directory as I reported earlier

{code}
[usrxyz@vm001 root]$ hadoop fs -lsr /
lsr: DEPRECATED: Please use 'ls -R' instead.
drwxr-xr-x   - hdfs supergroup          0 2014-03-25 16:29 /user
drwxr-xr-x   - hdfs   supergroup          0 2014-03-25 16:28 /user/hdfs
drwxr-xr-x   - usrxyz users               0 2014-03-27 16:10 /user/usrxyz
drwxr-xr-x   - hdfs   users               0 2014-03-25 16:32 /user/usrxyz/foo
-rw-r--r--   1 hdfs   users               5 2014-03-25 16:32 /user/usrxyz/foo/test.txt
drwxr-xr-x   - abc    abc                 0 2014-03-27 16:10 /user/usrxyz/foo-empty
[usrxyz@vm001 root]$ su usrxyz
[usrxyz@vm001 root]$ whoami
usrxyz
[usrxyz@vm001 root]$ hdfs dfs -rm -r /user/usrxyz/foo-empty
14/03/28 16:45:22 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
rm: Permission denied: user=usrxyz, access=ALL, inode="/user/usrxyz/foo-empty":abc:abc:drwxr-xr-x
[usrxyz@vm001 root]$ hdfs dfs -rmdir /user/usrxyz/foo-empty
rmdir: Permission denied: user=usrxyz, access=ALL, inode="/user/usrxyz/foo-empty":abc:abc:drwxr-xr-x
[usrxyz@vm001 root]$ hdfs dfs -rm -r -f /user/usrxyz/foo-empty
14/03/28 16:51:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
rm: Permission denied: user=usrxyz, access=ALL, inode="/user/usrxyz/foo-empty":abc:abc:drwxr-xr-x
[usrxyz@vm001 root]$ 
{code}
, Hi [~cnauroth], just saw your comments, thanks for the info. 

Secion 2.b in http://pubs.opengroup.org/onlinepubs/9699919799/utilities/rm.html says:

{code}
If the -f option is not specified, and either the permissions of file do not permit writing and the standard input is a terminal or the -i option is specified, rm shall write a prompt to standard error and read a line from the standard input. If the response is not affirmative, rm shall do nothing more with the current file and go on to any remaining files.
{code}

I found another link http://wiki.dreamhost.com/Unix_File_Permissions that talks about deleting empty subdirectory:

{code}
For a directory, read ("r") means that the grantee has permission to see what files and directories have been placed inside of that directory. Write ("w") means that the grantee has permission to create new files within that directory and to delete the directory (when empty). Execute ("x") means that the grantee can "cd" or change into the directory. (Without "x", the user can't actually read or write either.)
{code}
, Thank you all for the comments and discussions and reviews.

Uploaded version 002 to fix all three issues summarized in my last update.

There are a few questions to ask you experienced folks:

1. This fix involves changing ClientNamenodeProtocol.proto by adding an optional bit to message DeleteRequestProto to indicate whether "-f" is specified. Is there any implication of API compatibility here?

2. This change involves both files in hadoop-common-project and hadoop-hdfs-project, is it fine to be covered by this single JIRA?

3. To address Andrew's comments on how the test is written, I discussed with him, and we had an agreement that it would be beneficial to have this new testing infrastructure for FsShelll operations (TestFsShellPrivilege.java introduced with this fix). It has the advantage of manipulating file structure, ownership, permission, and calling FsShell with command line arguments, and easy debugging with eclipse.

My test result with the fix:
{code}
[usrxyz@vm001 hadoop]$ hadoop fs -lsr /
lsr: DEPRECATED: Please use 'ls -R' instead.
drwxr-xr-x   - hdfs supergroup          0 2014-03-25 16:29 /user
drwxr-xr-x   - hdfs   supergroup          0 2014-03-25 16:28 /user/hdfs
drwxr-xr-x   - usrxyz users               0 2014-03-29 10:19 /user/usrxyz
drwxr-xr-x   - abc    users               0 2014-03-25 16:32 /user/usrxyz/foo
-rw-r--r--   1 abc    users               5 2014-03-25 16:32 /user/usrxyz/foo/test.txt
drwxr-xr-x   - abc    users               0 2014-03-29 10:18 /user/usrxyz/foo-empty
drwxr-xr-x   - abc    users               0 2014-03-29 10:18 /user/usrxyz/foo-empty1
drwxr-xr-x   - abc    users               0 2014-03-29 10:19 /user/usrxyz/foo-empty2
[usrxyz@vm001 hadoop]$ whoami
usrxyz
[usrxyz@vm001 hadoop]$ hdfs dfs -rm -r /user/usrxyz/foo
14/03/29 10:20:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
rm: Permission denied: user=usrxyz, access=ALL, inode="/user/usrxyz/foo":abc:users:drwxr-xr-x
[usrxyz@vm001 hadoop]$ hdfs dfs -rm -r /user/usrxyz/foo-empty
14/03/29 10:20:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
rm: Permission denied: user=usrxyz, access=ALL, inode="/user/usrxyz/foo-empty":abc:users:drwxr-xr-x. Consider trying again with -f switch.
[usrxyz@vm001 hadoop]$ hdfs dfs -rm -r -f /user/usrxyz/foo-empty
14/03/29 10:21:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/usrxyz/foo-empty
[usrxyz@vm001 hadoop]$ hdfs dfs -rmdir /user/usrxyz/foo-empty1

[usrxyz@vm001 hadoop]$ hdfs dfs -rmdir /user/usrxyz/foo
rmdir: `/user/usrxyz/foo is non empty
[usrxyz@vm001 hadoop]$ whoami
usrxyz
[usrxyz@vm001 hadoop]$ su abc
Password: 
[abc@vm001 hadoop]$ whoami
abc
[abc@vm001 hadoop]$ hdfs dfs -rmdir /user/usrxyz/foo-empty2
rmdir: Permission denied: user=abc, access=WRITE, inode="/user/usrxyz":usrxyz:users:drwxr-xr-x
{code}

Thanks a lot for reviewing the fix.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12637658/HDFS-6165.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6553//console

This message is automatically generated., Here's what I get on my mac (BSD based 10.9) which seems similar to the previously posted results.  It's a full recap to spare others reading this whole discussion.

Fails because no perms at all to see dir contents.
{noformat}
$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a= test/bar
$ rm -rf test
rm: test/bar: Permission denied
rm: test: Directory not empty
{noformat}

Fails because execute but no read perms to see dir contents.
{noformat}
$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a=x test/bar
$ rm -rf test
rm: test/bar: Permission denied
rm: test: Directory not empty
{noformat}

Works because of read perms and -f flag.
{noformat}
$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a=r test/bar
$ rm -rf test
{noformat}

Read perms sans -f require a prompt??
{noformat}
$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a=r test/bar
$ rm -r test
override r--r--r--  root/wheel for test/bar?
{noformat}

Works because can't prompt with no STDIN.
{noformat}
$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a=r test/bar
$ perl -e 'close(STDIN); system("rm -r test")'
{noformat}

---

Quick summary:
# the kernel will delete a non-owned directory if and only if the user has read perms which allow it to determine the directory is empty
# it's the userland rm that decides if it should prompt to delete a non-owned directory

I don't agree with any incompatible changes to the NN protocol.  Years ago I made FsShell as POSIX like as possible and have always argued for POSIX behavior since.  However, I find the prompting silly and incompatible.  It's a userland check so adding this support to the NN doesn't make sense to me.

My opinion is the only change should be the NN allows empty dirs be deleted if the user has read perms.  No changes to -f.  Let -f continue to mean "If the file does not exist, do not display a diagnostic message or modify the exit status to reflect an error".
, HI [~daryn],

Thanks a lot for your experiments and comments. The read permission thing is indeed what's not noticed earlier. It definitely need to be addressed. Several follow-up  questions here:

1. My understanding of your point is, if the empty sub-dir has read permission, then "-rm <subdir>"  and "-rmdir" would simply go ahead remove the dir, even though all linux systems we tested so far prompt user (when possible), right? If the answer is yes, then the userland won't have a choice whether it should prompt to user (as in your summary point #2). I think this solution works fine for me, but would you please confirm? The folks who commented earlier may comment on this too.

2. Not to say that we have to do this, for my better understanding, does adding -f support to the NN protocol as an optional field break compatibility? (my bad in version 002, I did miss some places to change, the scope seems to be wide because it's an additional API that I'm adding).

3. Just to clarify, I actually tried to avoid prompting with the patch. At occasions when linux prompts, the patch I did throws exception like before, however, with improved message to suggestion user to try -f switch.

BTW, your second to last experiment "Read perms sans -f require a prompt??" didn't really use "-f" switch,  but sounds like you intended to use "-f" in this test, if so,  would you please retry this one with "-f"? 

Thanks again.
, While we are still discussing about the solution, I'm uploading a new revision 003 to resolve the missed stuff from previous one.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12637675/HDFS-6165.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-extras:

                  org.apache.hadoop.fs.TestFilterFileSystem
                  org.apache.hadoop.fs.TestFilterFs
                  org.apache.hadoop.fs.TestHarFileSystem

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6555//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6555//console

This message is automatically generated., Hello [~daryn],

I did the same set of experiments on my OS and saw different results than yours. It indicates different OS implements it differently.

Platform, user and command info
{code}
[usrxyz@yjzsn ~]$ whoami
usrxyz
[usrxyz@yjzsn ~]$ groups usrxyz
usrxyz : usrxyz
[usrxyz@yjzsn ~]$ which rm
/bin/rm
[usrxyz@yjzsn ~]$ alias | grep rm
[usrxyz@yjzsn ~]$ uname -a
Linux yjzsn 2.6.32-358.2.1.el6.x86_64 #1 SMP Wed Mar 13 00:26:49 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
[usrxyz@yjzsn ~]$ cat /etc/*release*
CentOS release 6.4 (Final)
LSB_VERSION=base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
cat: /etc/lsb-release.d: Is a directory
CentOS release 6.4 (Final)
CentOS release 6.4 (Final)
cpe:/o:centos:linux:6:GA
{code}

Test 1 (no permissions)
{code}
[usrxyz@yjzsn ~]$ 
[usrxyz@yjzsn ~]$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a= test/bar
[usrxyz@yjzsn ~]$ ls -lrt -R test
test:
total 4
d--------- 2 root usrxyz 4096 Mar 30 11:56 bar
ls: cannot open directory test/bar: Permission denied
[usrxyz@yjzsn ~]$ rm -rf test
{code}

Test 2 (no read perm)
{code}
[usrxyz@yjzsn ~]$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a=x test/bar
[usrxyz@yjzsn ~]$ rm -rf test
[usrxyz@yjzsn ~]$ 
{code}

Test 3 (has read perm and -f)
{code}
[usrxyz@yjzsn ~]$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a=r test/bar
[usrxyz@yjzsn ~]$ rm -rf test
[usrxyz@yjzsn ~]$ 
{code}

Test 4 (has read perm and no -f, prompt)
{code}
[usrxyz@yjzsn ~]$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a=r test/bar
[usrxyz@yjzsn ~]$ rm -r test
rm: remove write-protected directory `test/bar'? n
[usrxyz@yjzsn ~]$ 
{code}

Test 5 (has read perm, no STDIN):
{code}
[usrxyz@yjzsn ~]$ mkdir -p test/bar; sudo chown root test/bar; sudo chmod a=r test/bar
[usrxyz@yjzsn ~]$ perl -e 'close(STDIN); system("rm -r test")'
[usrxyz@yjzsn ~]$ 
{code}

Quick summary:  on CentOs, both "rmdir" and "rm -r -f " removes empty subdir, even if the empty subdir 
doesn't have read permission. It even delete when there is no permission at all.

These experiments are just for info collection purpose. I think we should probably comply with the most stringent system 
rather than the most relaxing system.

BTW, Please ignore my earlier comment about "BTW, your second to last experiment..."), I interpreted it incorrectly.

Thanks.


, Hi [~cnauroth], 

I revisited your comments at -28/Mar/14 16:47 and studied the POSIX spec,  and understood your points better.

{quote}
rm: If the current file is a directory, rm shall perform actions equivalent to the rmdir() ...

The rmdir() function shall fail if:
[EACCES]
 Search permission is denied on a component of the path prefix, or write permission is denied on the parent directory of the directory to be removed.
{quote}

As you pointed out, there is mention of permission on the component of the path prefix, but there is no mention of permission on the directory to be removed.  What it meant seems to be "empty dir's permission can be ignored" when other permissions described in the above quote are satisfied. 
And my experiments show that the CentOs implementation does that. 

Maybe  the test results [~daryn] got on his Mac is also a "nicety of the shell utility" on mac: it checks the read permission of dir even though it's empty.

I looked at "Advanced Programming in the UNIX Environment" textbook written by W. Richard Stevens, it says:
{quote}
To delete an existing file, we need write permission and execute permission in the directory containing the file, We do not need read permission or write permission for the file itself.
{quote}
He talks about delete existing file, but not existing directory.

The link I found http://wiki.dreamhost.com/Unix_File_Permissions says
{quote}
For a directory, ..., Write ("w") means that the grantee has permission to create new files within that directory and to delete the directory (when empty).
{quote}

I wonder whether we should also ignore the empty dir's permissions in this case in HDFS.

Thanks you all again.

, Hello Guys,

Not to change the current meaning of -f switch, there are two options here:

1. "rmdir" and "rm -r" will remove empty directory, as long as the permissions described in POSIX spec is satisfied:
  {quote}
The rmdir() function shall fail if:
[EACCES]
Search permission is denied on a component of the path prefix, or write permission is denied on the parent directory of the directory to be removed.
{quote}

2. same as 1, except  in addition, the read permission of the empty directory is required.

I favor #1 based on the info collected and my understanding so far.  Would you guys please review the info posted earlier and comment again to see if we can agree on one of them? 

Thanks a lot.

--Yongjun
, Revision 004 to address Daryn's comments.
, Hi [~daryn],

Thanks for your earlier comment at 29/Mar/14 12:26. I've uploaded patch revision 004 to address them. Would you please help reviewing this version? Thanks a lot.

Some notable changes and comments:

1. No -f is introduced as in previous patch revisions.

2. When deleting empty directory, in addition to the other permissions, READ permission is required for the empty directory.

3. Prior to my change, Rmdir command uses listStatus method to check whether it's empty. Curently the listStatus method requires both READ and EXECUTE permission. My patch changed the way how to check whether a directory is empty by using PathIsNotEmptyDirectoryException.

4. As stated in 3, listStatus method requires READ and EXECUTE permission. For the purpose of checking whether a dir is empty, we probably can have another lightweight API that only requires READ permission. I did not introduce this API, but I can do it if we can agree upon.

My understanding is, listStatus is similar to command "ls -l".  I think the new API would be similar to "ls". Basically the new API should be able to tell how many entries in a directory without the need of EXECUTE permission, below is how centOs system works, when the target directory has no EXECUTE permission:
{code}
[root@yjzsn usrxyz]# ls -lrt
total 24
drwx------ 2 abc abc 4096 Apr  2 17:39 abc-notempty
drwxr--r-- 2 abc abc 4096 Apr  2 17:39 abc-notempty1
drwx------ 2 abc abc 4096 Apr  2 17:39 abc-empty
drwxr----- 2 abc abc 4096 Apr  2 17:39 abc-empty1
drwx---r-- 2 abc abc 4096 Apr  2 17:39 abc-empty2
drwxr--r-- 2 abc abc 4096 Apr  2 17:39 abc-empty3
[root@yjzsn usrxyz]# su usrxyz
[usrxyz@yjzsn ~]$ ls abc-notempty
ls: cannot open directory abc-notempty: Permission denied

[usrxyz@yjzsn ~]$ ls abc-notempty1   <== with READ perm, this one lists the file name in it (xyz.txt), prints "Permission denied" because of xyz.txt's permission
ls: cannot access abc-notempty1/xyz.txt: Permission denied
xyz.txt

[usrxyz@yjzsn ~]$ 
[usrxyz@yjzsn ~]$ ls abc-empty1
ls: cannot open directory abc-empty1: Permission denied
[usrxyz@yjzsn ~]$ ls abc-empty2
[usrxyz@yjzsn ~]$ ls abc-empty3
[usrxyz@yjzsn ~]$ 
{code}


, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12638514/HDFS-6165.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-extras:

                  org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6578//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6578//console

This message is automatically generated., The test failure appears to be irrelevant to this change, local test showed this test passed too. Upload the same patch to trigger another test run.
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12638556/HDFS-6165.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-extras.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6579//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6579//console

This message is automatically generated., Hi Yongjun, sorry about the delay in reviewing this. Thanks for the work thus far.

I think the semantics for a recursive delete via DistributedFileSystem#delete are still not quite right. The change you made will work for the shell since it does its own recursion, but we need to do the same "remove if empty dir with read" when recursing via {{recursive = true}} too. You might be able to do this by modifying {{FSPermissionChecker#checkSubAccess}} appropriately, but a new flag or new code would be safer.

More comments:
* isDirectory, can we add per-parameter javadoc rather than stacking on the {{@return}}? I think renaming {{empty}} to {{isEmpty}} would also help.
* Nit, also need a space in the ternary {{empty?}} and {{dir.isEmptyDirectory(src)?}}.
* In Delete, I think it's a bit cleaner to do an {{instanceof PathIsNotEmptyDirectoryException.class}} check instead.
* Some lines longer than 80 chars

TestFsShellPrivilege:

I gave this a quick pass, but overall it may be better to rewrite these to use the DFS API instead of the shell. We need to test recursive delete, which the shell doesn't do, and we don't really have any shell changes in the latest rev, which lessens the importance of having new shell tests.

* execCmd needs to do some try/finally to close and restore the streams if there's an exception. Also an extra commented line there.
* Could we rename this file to "TestFsShellPermission"? Permission is a more standard term.
* This file also should not be in hadoop-tools, but rather hadoop-common.
* This does a lot of starting and stopping of a MiniCluster for running single-line tests. Can we combine these into a single test? We also don't need any DNs for this cluster, since we're just testing perms.
* We have FileSystemTestHelper#createFile for creating files, can save some code
* Use of @Before and @After blocks might also clarify what's going on.
* This also should be a JUnit4 test with @Test annotations, not JUnit3.
* USER_UGI should not be all caps, it's not static final
* It's a bit ugly how we pass UNEXPECTED_RESULT in for a lot of tests. Can we just pass a boolean for {{expectSuccess}} or {{expectFailure}}, or maybe a String that we can call {{assertExceptionContains}} on?
* FileEntry looks basically like a FileStatus, can we just use that instead?, Hi Andrew, Thanks a lot for your detailed review, I will try to address them as soon as possible.

, I lost track of this jira.  I'll catch up today., Two-part comment to avoid overly lengthy response.

The FsShell logic isn't correctly.  You can't simply ignore non-empty dirs.  Removal of the parent dir will fail too.  I think the shell rm will then return 0 (success) even though it miserably failed.  Checking for RemoteException is making assumptions about hdfs semantics, and searching for a substring is very fragile.

I guess I should step back and explain how posix deletes work, although maybe everyone already knows this.  There is no recursive delete.  A userland tool has to recurse the directory structure and remove all children.  Rm prompts when it cannot traverse a directory (lack of perms).  However, the kernel will treat an empty directory just like a file - no permissions are required to delete.  Rm is prompting because it doesn't know if the remove will work.

(Sidenote: I tested on my mac and it will allow rmdir of a non-perm empty dir, so it's rm just being fussy and not trying to delete when -f is given even though it could)

, After looking over the FsShell code, I'll revise my stance.  I think the only change, for now, is the NN should only check permissions of the parent directory for deletes.  The permissions of the file/dir shouldn't matter.  FsShell isn't recursively descending like I thought...  (Explains why deletion of a dir with millions of files took down a NN)

While it's technically wrong and violates posix to allow deleting a non-empty directory to which the user has no permissions, it doesn't make anything worse.  The NN's recursive delete is already terribly broken in that it ignores permissions deeper in the tree.

I bet for the described problem, you could delete the no-perms dir if you created a new directory, moved the no-perms dir into it, and then recursively deleted the new directory., bq. I think the only change, for now, is the NN should only check permissions of the parent directory for deletes.

+1 to this proposal.  I think it's the simplest approach to fixing the current bug.  Enhancements beyond that could be addressed in separate patches if needed.

[~yzhangal], thank you for sticking with this tricky issue., Hello [~daryn] and [~cnauroth],

Thanks a lot for the comments you guys made above! I'm sorry for being late, I have been working on some more priority issues and putting aside this jira, I didn't even see your comments until now.

Will address all your comments next week.


, HI Guys, 

Thanks a lot for the review/comments you did, and sorry again for getting back late. 

Instead of posting another patch to address all comments, I'd like to summary the solution here first.  Sorry for a long posting again. Would you please help review the proposed solution below and see if you agree? I will address other comments when putting together a new revision of patch. Many thanks.

There are two commands that have problem here

1. hdfs dfs -rmdir (refer to as rmdir in later discussion)
2. hdfs dfs -rm -r (refer to as rmr in later discussion)

Both commands eventually will call FSnamesystem#deleteInternal method
{code}
  private boolean deleteInternal(String src, boolean recursive,
      boolean enforcePermission, boolean logRetryCache)
      throws AccessControlException, SafeModeException, UnresolvedLinkException,
             IOException {
{code}

The deleteInternal methods throws exception if recursive is not true and the src to be deleted is not empty; otherwise, it will check "necessary" permissions and collect all blocks/inodes to be deleted and delete them recursively. The deletion process excludes snapshottable dirs that has at least one snapshot.

Right now it requires FULL permission of the subdirs or files under the target dir to be deleted. This permission checking is also recursive, it requires all child has FULL permission), This is the place we try to fix for different scenarios.

rmr calls with the "recursive" parameter passed as true,  and rmdir calls with the "recursive" parameter with false.

Solution summary:

1. rmdir

The recursion issue in the comments you guys made is only relevant to rmr.  So the solution for rmdir is a simple:

- for nonempty directory, deleteInternal simply throws nonemptydir exception if it's nonempty, and the FsShell side catch the exception
- for empty directory, only check parent/prefix permission, ignore the target dir's permission (posix compliant), delete if the permission is satisfied, throw exception otherwise.

2. rmr

The last patch (version 004) I posted only checks whether the target dir to be deleted has READ permission (the earlier versions ignore the target dir's permission when it's empty), and I didn't change the behaviour to check subdir for non-empty target dir. For non-empty target dir, the current implementation  requires FULL permission of subdir in order to delete a subdir even if the subdir is empty. This is not quite right as [~andrew.wang] pointed out.

I'd like to try to implement what Andrew suggested with an additional/different parameter than subAccess of FSPermissionChecker#checkSubAccess
E.g. add emptyDirSubAccess
{code}
void checkPermission(String path, INodeDirectory root, boolean doCheckOwner,
      FsAction ancestorAccess, FsAction parentAccess, FsAction access,
      FsAction subAccess, FsAction emptyDirSubAccess, boolean resolveLink)
{code}
The subAccess will passed FsAction.ALL (as is currently) and emptyDirSubAccess will be passed FsAction.NONE.

And if a subdir is not empty, check against subAccess, if it's empty, check against emptyDirSubAccess.

About using FsAction.ALL for subAccess parameter, it's a bit over stringent for intemediate path, say, if we want to delete <targetDir>/a/b/c, we don't have to have WRITE permission of <targetDir>/a, but we need to have WRITE permission of <targetDir>/a/b. We might address this issue in a separate JIRA if you agree.

Hi [~daryn], the following is actually not true to me
{quote}
I bet for the described problem, you could delete the no-perms dir if you created a new directory, moved the no-perms dir into it, and then recursively deleted the new directory.
{quote}
Because the current implementation recursively requires FULL permission of subdirs/files to delete them.
If we suddenly change the implementation to allow deleting non-empty dir without checking the subdir/file permission, I'm worried about bad user impact. 

Thanks again for your time.
, I need to amend the rmr section in my last update. 

Command: "hdfs dfs -rm -r <targetDir>"

Expected result: recursively delete all components under <targetDir> and <targetDir> itself.

From POSIX quote from our earlier discussion,  for any component to be deleted,

if the component is a file or empty-dir, permissions required are:

  - Search permission of all components of the path prefix, 
  - WRITE permission of the parent directory of the directory to be removed.

if the component to delete is a non-empty-dir, recursively apply the above permissions to its child components.

The PROBLEM we are solving here is, that currently FsActions.ALL is passed to subAccess parameter:
{code}
void checkPermission(String path, INodeDirectory root, boolean doCheckOwner,
      FsAction ancestorAccess, FsAction parentAccess, FsAction access,
      FsAction subAccess, boolean resolveLink)
{code}

which disallows removal of file and empty-dir component under the <targetDir> (got exception asking for ALL permission), if the component doesn't have ALL permissions..

Given an example how to solve this, "hdfs dfs -rm -r <targetDir>", assume the <targetDir> contains:
{code}
a
a/emptyDir
a/nonEmptyDir
a/nonEmptyDir/filex
{code}

to delete <targetDir>, we need the following permissions (ALL means RWX):
{code}
Prefix components of <targetDir>: search permission (READ and EXECUTE)
Parent componet of <targetDir>:   ALL
<targetDir>:                                   ALL
a:                                                   ALL
a/emptyDir:                                    NONE
a/nonEmptyDir:                              ALL
a/nonEmptyDir/filex:                       NONE
{code}

So what we can do is to modify checkPermission. Since checkPermission used by multiple operations, the safe change for delete operation is to 

1. add a parameter leafAccess:
{code}
void checkPermission(String path, INodeDirectory root, boolean doCheckOwner,
      FsAction ancestorAccess, FsAction parentAccess, FsAction access,
      FsAction subAccess, FsAction leafAccess, boolean resolveLink)
{code}
Notice that I changed "emptyDirSubAccess" of my earlier version to "leafAccess" to cover both file and dir which are leaf (To be a leaf, the component has to be either a file or an empty dir). And we pass null to "leafAccess" parameter in deleteInternal function.

2. modify checkPermission to apply subAccess on non-leaf component, and leafAccess to leaf component.

This change is only applicable to "delete" operation (by deleteInternal method), since checkPermission function is shared across the board, need to examine how to make this change not to impact other operations.

Thanks.
, Hi,

Thanks a lot for your earlier comments, and thanks Andrew a lot for the detailed review!

 I just updated patch version 005 to address all.

For rmdir, it's the solution I described in above;
For rmr solution, I actually did
{code}
void checkPermission(String path, INodeDirectory root, boolean doCheckOwner,
      FsAction ancestorAccess, FsAction parentAccess, FsAction access,
      FsAction subAccess, boolean ignoreEmptyDir, boolean resolveLink)
{code}
The two parameters "subAccess" and "ignoreEmptyDir" work together, 
- if subAccess is not NULL, access permission of subDirs are checked, 
- when subAccess is checked, if ignoreEmptyDir is true, ignore
  empty directories.

To address Andrew's comments
{quote}
I think the semantics for a recursive delete via DistributedFileSystem#delete are still not quite right. The change you made will work for the shell since it does its own recursion, but we need to do the same "remove if empty dir with read" when recursing via recursive = true too. You might be able to do this by modifying FSPermissionChecker#checkSubAccess appropriately, but a new flag or new code would be safer.
{quote}
Thanks a lot for pointing this out, indeed it's a problem there. See above described solution, except we agreed that we don't need to check permission for empty dir.

{quote}
    isDirectory, can we add per-parameter javadoc rather than stacking on the @return? I think renaming empty to isEmpty would also help.
    Nit, also need a space in the ternary empty? and dir.isEmptyDirectory(src)?.
{quote}
These are now gone with new solution.

{code}
    In Delete, I think it's a bit cleaner to do an instanceof PathIsNotEmptyDirectoryException.class check instead.
{code}
This is handled in a better way now. I discovered a bug HADOOP-10543 (and posted a patch) when looking at this. With HADOOP-10543 committed, I would be able to do exactly what Andrew suggested. But I think what I have in this new revision should fine too.
{quote}
    Some lines longer than 80 chars
{quote}
Hopefully all addressed:-)

{quote}
TestFsShellPrivilege:
I gave this a quick pass, but overall it may be better to rewrite these to use the DFS API instead of the shell. We need to test recursive delete, which the shell doesn't do, and we don't really have any shell changes in the latest rev, which lessens the importance of having new shell tests.
{quote}
I think adding a test infra like what I added give another  option here, hopefully the new revision looks better:-)

{quote}
    execCmd needs to do some try/finally to close and restore the streams if there's an exception. Also an extra commented line there.
{quote}
This FsShell actually took care of catching exception, so the stream will not get lost. Extra comment line removed.

{quote}
    Could we rename this file to "TestFsShellPermission"? Permission is a more standard term.
{quote}
Done.

{quote}
    This file also should not be in hadoop-tools, but rather hadoop-common.
{quote}
Because it uses MiniDFSCluster, it can not be in hadoop-common. But I moved to hdfs test area now.

{quote}
    This does a lot of starting and stopping of a MiniCluster for running single-line tests. Can we combine these into a single test? We also don't need any DNs for this cluster, since we're just testing perms.
{quote}
I refactored the code to take care of this. Since we create file, I still keep DNs.

{quote}
    We have FileSystemTestHelper#createFile for creating files, can save some code
    Use of @Before and @After blocks might also clarify what's going on.
    This also should be a JUnit4 test with @Test annotations, not JUnit3.
    USER_UGI should not be all caps, it's not static final
    It's a bit ugly how we pass UNEXPECTED_RESULT in for a lot of tests. Can we just pass a boolean for expectSuccess or expectFailure, or maybe a String that we can call assertExceptionContains on?
{quote} 
All are taken care of, except I forgot @before and @After, but hoopefully it looks much better now.

{quote}
    FileEntry looks basically like a FileStatus, can we just use that instead?
{quote}
FileEntry only have the fields needed for this test, and it's easier to manage in test area. I'm worried using FileStatus would be not easy to control. So I didn't do this. Hope it's acceptable.

Thanks in advance for a further review.

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12642397/HDFS-6165.005.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6763//console

This message is automatically generated., Updated new version 006 to add a test area change missed in last one., Somehow test was not triggered by previous upload, uploaded the same version again to trigger test., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12642655/HDFS-6165.006.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6777//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6777//console

This message is automatically generated., The failed test appears to be https://issues.apache.org/jira/browse/HADOOP-10062.
, I'm struggling to catch up on jiras so could you post a short summary of why changing the behavior of subaccess is insufficient?  It appears to maybe only be used by delete so adding another parameter to permission checking might be overkill.

Also, as previously mentioned you can't catch RemoteException since it's specific to hdfs and thus FsShell won't work correctly with other filesystems., Hi [~daryn],

Thanks a lot for your comments. Adding the additional parameter is to avoid changing the behavior of any other callers to checkPermission. If subAccess is only used by deleteInternal method, then we can actually remove the additional parameter and change the behavior when we check subAccess.

About catching RemoteException, thanks for pointing that out, I will do some further study how to address that.
, HI [~daryn], 

I looked into callers of checkPermission, there are two other places in additional to delete operation:
- FSNamesystem.getContentSummary method calls checkPermission with FsAction.READ_EXECUTE passed to subAccess.
- FSNamesystem.checkSubtreeReadPermission method calls checkPermission with FsAction.READ passed to subAccess.
 so it looks like that we do need the additional parameter.

About the RemoteException,  thanks for pointing out that FsShell won't work correctly with other filesystems with the patch. Since there are so many filesystems, the scope of change to address mkdir issue will be much more wide. I'm thinking about handling this in a separate JIRA. What do you guys think? With rmr fixed, it can serve as a workaround for rmdir issue.

Thanks.



, Hi Guys, 

Thanks for your earlier review and comments.

I created subtask HDFS-6351 for rmr and HDFS-6352 for rmdir.  I submitted a patch to HDFS-6351 by taking out the rmdir part from the patch for HDFS-6165 submitted here. Your further help in reviewing it is greatly appreciated.

, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12642655/HDFS-6165.006.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10602/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12642655/HDFS-6165.006.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10612/console |


This message was automatically generated.]