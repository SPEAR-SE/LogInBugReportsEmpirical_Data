[Hey Kihwal. ATM and I had a meeting about this earlier this week and had some thoughts. I think Aaron plans to post them soon. (but yes, we agree this is important to address soon), This is very important feature. which will make upgrade under HA easier., [~tlipcon] Would you share your idea? A high-level description is fine for now., Guys, any luck? I'd like to get 2.1.1 out soon... thanks!, There are two problems, which we can address separately.  I will throw out initial ideas to start the discussion. After some discussion, we can divide and retarget the work if necessary.

1) layout upgrade support in HA mode

This is required since previous 2.0.x Apache releases have the HA support.  I think it is reasonable to incur down time for this.  We could allow ANN to come up in the layout upgrade mode. In this mode, SBN should be down and automatic failover should probably be disabled for safety. This may be checked using an admin tool or ANN in this mode may try to talk to SBN.  If this condition is met, ANN can do the layout upgrade, write out a new image and start a new edit segment (in shared edits directory too). After this point ANN can continue with the normal startup process and start serving.  SBN should do bootstrapStandby before attempting to start with the new version of NN. Once SBN is up, automatic failover can be re-enabled.


2) Online upgrade snapshot in HA mode that can be used in rolling upgrade.

It seems not everyone cares about this feature, but for those who do it is important.
One high-level idea is that NN rolls edit, saves a copy of fsimage and edits, and the cluster goes into "no block deletion" mode. Some coordination is required among NNs and DNs. Also, blocks created after this "snapshot" won't be deleted in this mode. For blocks being written or get appended afterwards, we could make a copy or ignore them.  Depending on how nodes are coordinated, it may not provide a strictly consistent global snapshot, but still be enough to survive major disasters, which is what this feature is for., Hi Kihwal,

IMO we should focus solely on #1 for now. #2 would obviously be great, but seems like it would likely be a substantially more complex undertaking, which you seem to agree not everyone feels is necessary.

As for #1, the high-level workflow Todd and I had discussed was the following:

# Shut down the NNs.
# Start one NN with the '-upgrade' flag. This will do the local metadata upgrade and trigger the JNs to do the upgrade on their nameservice-specific directories.
# Start the second NN, which upon connecting to the JNs will notice that it needs to perform an upgrade itself on its local on-disk metadata, much as the DNs currently do when connecting to an NN with a higher layout version.
# Run for a while as normal to make sure it's all working.
# Finalize the metadata upgrade.

Step #2 will likely require that we expand the internal Journal API to have a few upgrade-related methods. Right now, most (all?) of the upgrade-related functionality is baked directly into the FileJournalManager or implemented elsewhere in the NN assuming that the FileJournalManager is the one being used.

How does this look from a high level?, Hey, guys. Indeed, the scope of this jira should probably be #1. 
Not to diminish in any way the importance of rolling upgrades.

The NN upgrade happens in loadNamesystem() before RPCServer is started, so SBN wont even see this.
Then DNs are asked to upgrade before they are allowed to register. That is, Active NN is in SafeMode and there is nothing for SBN to worry about yet as the journal is not changing.

With NFS-mounted shared storage the upgrade should be pretty straightforward. We should modify the code to allow it, and then lots of testing of course.

For QJM I am not sure.
Would it be easier to let SBN checkpoint from the upgraded NN and start reading the journal from that image.
With -rollback SBN should probably do the same thing.
<Edited: I meant rollback rather than as it was finalize>, Given the options enumerated by Kihwal, I suggest marking this as Critical and not blocker so that 2.1.1 release can go out.

bq. With HA enabled, NN won't start with "-upgrade".
One of the main use case for HA is planned downtime during upgrades. This choice defeats that purpose.

As regards to option (2), [~kihwal], the purpose of "no block deletion" mode is to ensure we retain the previous upgrade snapshot behavior and avoid block deletion due to possible bugs in namenode, right?

I agree with others, we should continue this discussion and implement this feature before 2.0 GA or for the first dot release after GA.


, HDFS-5223 has some related discussion about how to do certain types of upgrades without requiring a the namedir/editdir "snapshot" stuff that we currently do. It doesn't solve this problem fully, but may be helpful. Please have a look and comment., Here's a preliminary patch for folks to take a look at to get an idea of what I'm thinking here.

The patch makes a few changes to the way upgrade works in order to support HA upgrades. In particular:
* Normal, non-HA upgrade is left unchanged.
* I've removed the "-finalize" startup option, and instead running with this will direct users to use the `hdfs dfsadmin -finalizeUpgrade' command. Supporting both styles of finalization seems unnecessary, and makes HA finalization more difficult.
* Starting the NN with the '-rollback' flag will perform the rollback just as before, but it will not then proceed to start the NN daemon. Supporting this mode also makes HA rollback more difficult, and doesn't seem to be necessary or helpful, since to perform a rollback we don't need to load the fsimage/edit log, and thus performing the actual rollback should be quick. Operators can then start the NN as normal after rolling back the FS.

To perform upgrade in the HA case, this patch does the following:
* Both NNs must be started with the '-upgrade' flag.
* On start, each one of the NNs will first try to create a special lock file, either in the shared edits dir in the NFS case or on each of the JNs in the QJM case. This lock file will contain the CTime that that NN would like to upgrade the FS to.
* One of the NNs will win the creation of this file, and the other will lose. Whichever one loses will use the CTime stored in this file as the CTime to upgrade its own local dirs.
* After either creating or reading the shared lock file, each NN will perform an upgrade just as normal of all its local storage dirs.
* At the time when either NN is transitioned to the active state, that NN will perform an upgrade of the shared log, either on NFS or on the JNs.
* To finalize an HA upgrade, an operator will just use hdfsadmin as described before. The active NN at the time this happens will perform the upgrade of the shared log. Finalization will also remove the shared log lock file previously described.
* To perform a rollback in the HA case, both NNs should first be shut down. The operator should run the roll back command on one of the NN boxes, which will perform the rollback on the local dirs there, as well as on the shared log, either NFS or on the JNs. Afterward, this NN should be started and the operator should run `-bootstrapStandby' on the other NN.

As far as how the code is structured, much of the actual local file system upgrade code is refactored out of FSImage and into a new class, NNUpgradeUtil. This is so that this code can be called both from FSImage, and from FileJournalManager. Then, a few new upgrade-related methods are added to the JournalManager interface and the QJournalProtocol, so as to support generic upgrade of the shared log, however that shared log is implemented.

Please have a look. This patch definitely needs some cleanup and more tests, but I think the majority of it should roughly work and folks can hopefully get the gist., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619955/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 4 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby
                  org.apache.hadoop.hdfs.server.namenode.ha.TestInitializeSharedEdits
                  org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer
                  org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5781//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5781//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5781//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619955/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 4 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestInitializeSharedEdits
                  org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer
                  org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby
                  org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5782//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5782//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5782//console

This message is automatically generated., Here's a patch that's rebased on trunk and should hopefully fix most of the test failures.

The failures of TestInitializeSharedEdits and TestBootstrapStandby were just due to some error message changes that those tests were looking for. I think that TestOfflineEditsViewer was just failing on trunk at the time I posted this patch, but it appears to be fixed now. At least, it runs fine on my box both with and without this patch applied now. The BKJM test appears to be failing on trunk, it's just being run as part of this patch because I had to add some no-op methods in that module., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620990/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 4 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5807//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5807//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5807//console

This message is automatically generated., Here's a (very slightly) updated patch rebased on trunk, since there have been a few conflicting commits made since I posted the last patch., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12621712/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 4 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5832//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5832//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5832//console

This message is automatically generated., This jira can use a design document. The current description in the comment covers "what is being done", but it is not clear "why it is being done that way". The subtle issues may be understood better with a design document. I would love to see a separate summary section that covers how commands worked before and how they work now and what commands are no longer supported. 

Some early comments:
bq. I've removed the "-finalize" startup option, and instead running with this will direct users to use the `hdfs dfsadmin -finalizeUpgrade' command. Supporting both styles of finalization seems unnecessary, and makes HA finalization more difficult.
Can you describe what is the difference between this vs older version of finalize? The command -finalize is fairly well know and this change will be backward compatible.

bq. Starting the NN with the '-rollback' flag will perform the rollback just as before, but it will not then proceed to start the NN daemon. Supporting this mode also makes HA rollback more difficult, and doesn't seem to be necessary or helpful, since to perform a rollback we don't need to load the fsimage/edit log, and thus performing the actual rollback should be quick. Operators can then start the NN as normal after rolling back the FS.
Sorry I am not sure I understand this. Why does HA rollback become more difficult?

bq. On start, each one of the NNs will first try to create a special lock file, either in the shared edits dir in the NFS case or on each of the JNs in the QJM case. This lock file will contain the CTime that that NN would like to upgrade the...
Why is the lock file required? Why cannot NN just write an editlog about upgrade intent, including the new layout version? During rollback we can discount the editlog starting from the upgrade intent log. Infact we can also consider requiring users to save namespace with empty editlogs? With this, perhaps we can avoid the following:
bq. At the time when either NN is transitioned to the active state, that NN will perform an upgrade of the shared log, either on NFS or on the JNs.

bq. To finalize an HA upgrade, an operator will just use hdfsadmin as described before. The active NN at the time this happens will perform the upgrade of the shared log. Finalization will also remove the shared log lock file previously described.
You mean finalize of the shared log in above?, General:
- thanks for the description in the above JIRA comment. Can you transfer this comment somewhere into the docs, perhaps hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm or a new page? Perhaps with a slightly more user-facing angle.

- what would happen if the admin called finalizeUpgrade() when neither node had yet transitioned to active? I don't see any sanity check here.. is it possible you'd end up leaving the shared in an orphaned "upgrading" state and never end up finalizing it? Similarly, what happens if you start one NN with -upgrade, and you start the other one without -upgrade. It seems to me it should check for the upgrade lock file in the shared dir and say "looks like an upgrade is in progress, please start the SBN with -upgrade".

- there are a few TODOs in the code that probably need to be addressed - nothing big, just a few things you may have missed.

JournalManager.java:
- would be good to add Javadoc on the new methods, so that JM implementors know what the upgrade process looks like. i.e what is pre-upgrade, etc?

QuorumJournalManager.java:
- "Could not perform upgrade or more JournalNodes" error message has some missing words in it.

+    throw new IOException("Failed to lock shared log.");
- this line should be unreachable, right? maybe an AssertionError("Unreachable code") would make more sense? Also this same exception message is used down below in canRollBack which isn't quite right.

Journal:
- when you upgrade the journal, I'd think you'd to copy over all the data from the PersistentLongFiles into the new dir?

FileJournalManager:
- worth considering a protobuf for the shared log lock, in case we want to add other fields to it later (instead of the custom serialization you do now)
- need try...finally around the code where you write the shared log lock. On the read side you're also forgetting to close it.
- the creation of the shared log lock file is non-atomic... I'm worried that we may hit the race in practice, since the AtomicFileOutputStream implies an fsync, which means that between the exists() check and the rename to the lock file, you may really have a decently long time window. Maybe we can use locking code like Storage does? Feel free to punt to a follow-up.


FSNamesystem.java:
- can you add a doc on doUpgradeOfSharedLogOnTransitionToActive()?

NNUpgradeUtil.java:
- why are some of the functions package-private and others are public?
- make it an abstract class or give it a private constructor so it can't be instantiated, since it's just static methods
- brief javadocs would be nice for these methods, even though they're straight refactors of existing code.

FSEditLog.java:
- in canRollBack(), you throw an exception if there is no shared log. That doesn't seem right...
- capitalization of "RollBack" vs "Rollback" is a little inconsistent. Looks like Rollback is consistently used prior to this patch, so probably best to stick with that.

FSImage.java:
- in the switch statement on the startup option, I think you should keep the ROLLBACK case, but just have it throw AssertionError -- just to make sure we don't accidentally have some case where we're passing it there but shouldn't be.GA
, Thanks a lot for the comments, Todd and Suresh. I've got some obligations during the first part of today but will try to get back to you later today or tomorrow.

Suresh - as regards to a design doc, I could potentially write up a small one if you really think it's necessary, but there really aren't all that many subtle points here, and hopefully by answering the (very good!) questions you've raised everything will become much clearer. The core of the patch isn't even all that large - there's a ton of plumbing of new RPCs, etc. that make it look more complex than it is. One of the goals I had one producing it was to leave the existing non-HA upgrade system as untouched as possible, to reduce the possibility of regressions so we can put this in a 2.x update ASAP., bq. Suresh - as regards to a design doc, I could potentially write up a small one if you really think it's necessary, but there really aren't all that many subtle points here
[~atm], you probably are right. Perhaps answering my questions will do. I may also take the answers from you and post a one pager to describe how I understand it to see I got it right. That could perhaps be the document that we can post in this jira, if you agree.

BTW have you looked at HDFS-5535. Are there anythings we can leverage from that, especially around rollback marker in editlog etc., bq. BTW have you looked at HDFS-5535. Are there anythings we can leverage from that, especially around rollback marker in editlog etc.

Yes, I have looked at that. It's a good idea, but with this patch I was explicitly trying to _not_ redo the existing upgrade/rollback system, and instead just extend the non-HA upgrade/rollback system to work in an HA setup., Hi Suresh, hopefully the below answers to your questions clears things up. Please let me know if anything is unclear, or if you have any more questions.

bq. Can you describe what is the difference between this vs older version of finalize? The command -finalize is fairly well know and this change will be backward compatible.

I'm guessing you mean "incompatible" here. There's no "this vs. older version of finalize." For as long as I can remember, we've always supported two ways of finalizing an upgrade: either by shutting down the running NN and then starting it again with the "-finalize" startup option, or by just running `hdfs dfsadmin -finalizeUpgrade' which makes an RPC to a running NN. The trouble with the startup option in an HA scenario is that an NN can't guarantee that it will be active at the time it starts, since determining who is active and who is standby is handled externally to the NN. I don't see any reason to prefer using the startup option even in a non-HA setup, so seemed like we could remove it here. I could certainly just remove support for it in the HA case, if you'd prefer.

bq. Sorry I am not sure I understand this. Why does HA rollback become more difficult?

In the case of the '-upgrade' flag it's reasonable to only do the upgrade on transition to active, since we have to load the current fsimage/edits anyway before doing the upgrade, and the act of upgrading moves the transaction ID forward. In the case of '-rollback', however, it doesn't make much sense to start up in the standby state, load the full fsimage/edits, and then roll back everything, and reload the old fsimage upon becoming active. Given that the act of rolling back does not require loading the fsimage/edits at all, just moving some directories around, seems to make sense to me that this should not be a "mode" but rather just a standalone command that runs and then exits.                

bq. Why is the lock file required? Why cannot NN just write an editlog about upgrade intent, including the new layout version? During rollback we can discount the editlog starting from the upgrade intent log. Infact we can also consider requiring users to save namespace with empty editlogs? With this, perhaps we can avoid the following:

This is again because an HA NN that is just starting up should not be writing to the shared log, but two HA NNs that are starting up need to synchronize/agree on the new CTime to use during upgrade. This needs to be known before doing the saveNamespace which is part of the upgrade process. You could imagine writing the new CTime to the edit log upon transitioning to the active state, but this would require the NNs to do the saveNamespace upon transitioning to active and/or when reading from the shared log as part of being the standby. It seems quite problematic to do the long, blocking operation of writing out a potentially large fsimage file in either of these places.
 
bq. You mean finalize of the shared log in above?
 
Yep, sure did. My bad. :), Thanks a lot for the review, Todd. Here's an updated patch which should address all of your feedback. Please have another look. Detailed responses below.

bq. thanks for the description in the above JIRA comment. Can you transfer this comment somewhere into the docs, perhaps hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm or a new page? Perhaps with a slightly more user-facing angle.

Good thinking. Done.

bq. what would happen if the admin called finalizeUpgrade() when neither node had yet transitioned to active? I don't see any sanity check here.. is it possible you'd end up leaving the shared in an orphaned "upgrading" state and never end up finalizing it?

Great point. I've now added support for running finalizeUpgrade() on each NN from DFSAdmin, and made the DFSAdmin command also check to ensure that one NN is indeed active before it will finalize the upgrade.

bq. Similarly, what happens if you start one NN with -upgrade, and you start the other one without -upgrade. It seems to me it should check for the upgrade lock file in the shared dir and say "looks like an upgrade is in progress, please start the SBN with -upgrade".

Good thinking. Fixed.

bq. there are a few TODOs in the code that probably need to be addressed - nothing big, just a few things you may have missed.

Yes, as I said, this was a prelim patch to make sure folks were OK with the direction. All of them are pretty trivial and have been addressed in the latest patch, except for this one, which I'd love to hear your thoughts on:
{code}
+    // TODO(atm): What should we do here in the case of multiple shared edits
+    // dirs, which isn't actually possible at the moment?
{code}
My suggestion is that until we do actually support having multiple shared edits dirs (perhaps never) that we should just assert that there is in fact only a single one here.

bq. would be good to add Javadoc on the new methods, so that JM implementors know what the upgrade process looks like. i.e what is pre-upgrade, etc?

Yep, good thinking. Done. Please have a look at the new javadocs.

bq. "Could not perform upgrade or more JournalNodes" error message has some missing words in it.

Eagle-eye Lipcon. Good catch. Done.

bq. this line should be unreachable, right? maybe an AssertionError("Unreachable code") would make more sense? Also this same exception message is used down below in canRollBack which isn't quite right.

Good thinking. Done.

bq. when you upgrade the journal, I'd think you'd to copy over all the data from the PersistentLongFiles into the new dir?

Good catch. Done. I've also updated the tests to ensure that this happens during upgrade of JNs.

bq. worth considering a protobuf for the shared log lock, in case we want to add other fields to it later (instead of the custom serialization you do now)

I thought about this, but concluded that with just two fields it probably wasn't worth it. Do you feel strongly about this?

bq. need try...finally around the code where you write the shared log lock. On the read side you're also forgetting to close it.

Good catch. Fixed.

bq. the creation of the shared log lock file is non-atomic... I'm worried that we may hit the race in practice, since the AtomicFileOutputStream implies an fsync, which means that between the exists() check and the rename to the lock file, you may really have a decently long time window. Maybe we can use locking code like Storage does? Feel free to punt to a follow-up.

Note that in the case the lock file is being created on a JN, that the creation of the shared lock file is within a synchronized section. I agree that this could still be an issue in the NFS case, but seems to me like we can reasonably punt that to a later JIRA.

bq. can you add a doc on doUpgradeOfSharedLogOnTransitionToActive()?

Yep, done.

bq. why are some of the functions package-private and others are public?

That was an error. You're right that the one that was public can be package-private. Fixed.

bq. make it an abstract class or give it a private constructor so it can't be instantiated, since it's just static methods

Done. Made it abstract.

bq. brief javadocs would be nice for these methods, even though they're straight refactors of existing code.

Sure. Added.

bq. in canRollBack(), you throw an exception if there is no shared log. That doesn't seem right...

It actually is correct, since we only ever call this method in the event that HA is enabled, so there really should be a shared log. In the event that HA is not enabled, the upgrade of the edits dirs will be done directly on the storage dirs, instead of going through the edit log upgrade code path. The same is true FSEditLog#doPreUpgrade, doUpgrade, doFinalize, etc. I've changed the names of these methods to have "OfSharedLog" at the end to make this clear.

bq. capitalization of "RollBack" vs "Rollback" is a little inconsistent. Looks like Rollback is consistently used prior to this patch, so probably best to stick with that.

I attempted to consistenly use "Rollback" (one word) when it's used as a noun, and "RollBack" (two words) when it's used as a verb, which I believe to be grammatically correct. Perhaps it's best to stick with just one capitalization scheme for method/variable names, for consistency, even if it is technically incorrect English. Do you have an opinion here?

bq. in the switch statement on the startup option, I think you should keep the ROLLBACK case, but just have it throw AssertionError â€“ just to make sure we don't accidentally have some case where we're passing it there but shouldn't be.GA

Good thinking. Done., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12622364/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 8 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 2 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestInitializeSharedEdits
                  org.apache.hadoop.hdfs.server.datanode.fsdataset.TestAvailableSpaceVolumeChoosingPolicy
                  org.apache.hadoop.contrib.bkjournal.TestBookKeeperAsHASharedDir
                  org.apache.hadoop.contrib.bkjournal.TestBookKeeperHACheckpoints

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5860//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5860//console

This message is automatically generated., Pretty confident that the TestAvailableSpaceVolumeChoosingPolicy failure is a spurious one. The rest seem legit, but are easily fixed by not checking for the shared log lock in the case of initializing the shared edits, since of course there can be no shared log lock if there is not yet any shared log.

The javadoc warnings are a little funny, since I don't think they're introduced by this patch at all. I think I may have fixed one, but I can't actually see what's wrong with the other. We'll see what test-patch has to say this time., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12622405/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 9 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5863//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5863//console

This message is automatically generated., +          // This is expected to happen for a stanby NN.

Typo (standby)

+      // Either they all return the same thing or this call fails, so we can
+      // just return the first result.

Would be good to assert that - eg in case one of the JNs crashed in the middle of a previously attempted upgrade sequence.

-     * @param useLock true - enables locking on the storage directory and false
-     *          disables locking
+     * @param isShared whether or not this dir is shared between two NNs. true
+     *          enables locking on the storage directory, false disables locking

I think this doc is now wrong because you inverted the sense of these booleans - we _don't_ lock the shared dir.

+  public synchronized  void doFinalizeOfSharedLog() throws IOException {
+  public synchronized  boolean canRollBackSharedLog(Storage prevStorage,
Style nit: extra space in the above two methods

+          if (!sd.isShared()) {
+            // This will be done on transition to active.
Worth a LOG.info or even warn here

Currently it seems like whichever SBN starts up first has to be the one who does the transition to active. Maybe a follow-up JIRA could be to relax that constraint? Seems like it should be fine for either one of the NNs to actually do the upgrade - the lock file is just to make sure they agree on the target ctime.

+  dfsadmin -finalizeUpgrade'>>> command while the NNs are running and one of them
+  is active. The active NN at the time this happens will perform the upgrade of
+  the shared log, and both of the NNs will finalize the upgrade in their local

I think here you mean the "finalization of the shared log"
, {quote}
+ // This is expected to happen for a stanby NN.
Typo (standby)
{quote}

Thanks, fixed.

{quote}
+ // Either they all return the same thing or this call fails, so we can
+ // just return the first result.
Would be good to assert that - eg in case one of the JNs crashed in the middle of a previously attempted upgrade sequence.
{quote}

Sure, done.

{quote}
* @param useLock true - enables locking on the storage directory and false
* disables locking
+ * @param isShared whether or not this dir is shared between two NNs. true
+ * enables locking on the storage directory, false disables locking
I think this doc is now wrong because you inverted the sense of these booleans - we don't lock the shared dir.
{quote}

Good catch. Fixed.

{quote}
+ public synchronized void doFinalizeOfSharedLog() throws IOException {
+ public synchronized boolean canRollBackSharedLog(Storage prevStorage,
Style nit: extra space in the above two methods
{quote}

Fixed.

{quote}
+ if (!sd.isShared()) {
+ // This will be done on transition to active.
Worth a LOG.info or even warn here
{quote}

Added the following:

{code}
LOG.info("Not doing recovery on " + sd + " now. Will be done on "
                + "transition to active.");
{code}

bq. Currently it seems like whichever SBN starts up first has to be the one who does the transition to active. Maybe a follow-up JIRA could be to relax that constraint? Seems like it should be fine for either one of the NNs to actually do the upgrade - the lock file is just to make sure they agree on the target ctime.

Agree this seems like a good idea, but agree it can reasonably be done in a follow-up JIRA. If you agree, I'll file it when we commit this one.

{quote}
+ dfsadmin -finalizeUpgrade'>>> command while the NNs are running and one of them
+ is active. The active NN at the time this happens will perform the upgrade of
+ the shared log, and both of the NNs will finalize the upgrade in their local
I think here you mean the "finalization of the shared log"
{quote}

Sure did. Fixed., Aaron, I understand you made -rollback an offline operation for NN, which works as -format. That is, NN makes changes in the directory structure and shuts down. How will that work with DataNodes? They also need to be started with -rollback in order to roll back to the old state. In current world you just call {{start-hdfs -rollback}} and the cluster is up and running with the previous software version and the previous data. What is the procedure in you edition?, Hi Konst, thanks for bringing this up - I should've mentioned it. The DN rollback procedure is left unchanged by this patch, so you just start up the DNs with the '-rollback' option as before. When the DN registers with an NN which has already been rolled back, the DN will perform rollback of its data dirs just like normal, i.e. all that matters is that the NN has already rolled back, not whether or not the running NN was started with the '-rollback' option., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12622729/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 10 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5866//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5866//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5866//console

This message is automatically generated., This is less intuitive than the current state of the art. Because after NN rollback you need to start NameNode as -regular, while DataNodes with -rollback startup option.
Also just mentioning there could be some collisions with the rolling upgrade design, which I just finished reading.
I think HDFS-5535 assumes current (pre-your-patch) behaviours of -rollback and -finalize. For -finalize the problem could be that you remove it as a start up option. May be Suresh can elaborate better on this., Attached patch adds an exclude directive for the findbugs warning. It was about later loading a variable which we had previously confirmed was null, but all we're doing is checking it for equality against another value which may also reasonably be null., +1 pending Jenkins results. Please don't forget to file the follow-up JIRA we discussed above. Thanks!, Thanks for the comments, Konst.

bq. This is less intuitive than the current state of the art. Because after NN rollback you need to start NameNode as -regular, while DataNodes with -rollback startup option.

It's different, but not obvious to me that it's necessary less intuitive. I've personally always found it a bit strange that to roll back you need to start the NN _once_ with the '-rollback' option, which will result in it doing some things at startup, and then starting up as normal. This might seem to imply that the NN is running in some sort of "rollback mode", when in fact the act of rolling back has already completed, and thereafter you should always start the NN without the '-rollback' option.

bq. Also just mentioning there could be some collisions with the rolling upgrade design, which I just finished reading. I think HDFS-5535 assumes current (pre-your-patch) behaviours of -rollback and -finalize. For -finalize the problem could be that you remove it as a start up option. May be Suresh can elaborate better on this.

Needing to roll back should (hopefully!) be such a rare occurrence that it doesn't seem unreasonable to me to not do that in a rolling way. Removal of the '-finalize' startup option, I would think, should make the whole thing easier, and doesn't seem to me to have any benefits vs. just using the finalizeUpgrade RPC., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12622787/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 10 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.TestClientReportBadBlock

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5868//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5868//console

This message is automatically generated., While doing some manual testing I realized that punting on that suggestion to always upgrade the shared log from either NN also meant that if you started the NN with '-upgrade', but then shut down the NN without having transitioned it to active, you could never upgrade the shared log thereafter. Doesn't seem reasonable to me to punt that to another JIRA, so here's an updated patch which adds it. Manual testing also uncovered a bug with the JN edits servlet being overly-restrictive in verifying the namespace info, so this patch relaxes that check to only check the namespace ID and cluster ID, not the CTime of the request.

This patch also attempts to fix the javadoc warning, even though it's not introduced by this patch.

Todd - could you please have another look at this updated patch?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12623056/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 10 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.TestSafeMode

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5879//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5879//console

This message is automatically generated., Attached patch is no different from the last, except for again attempting to resolve that javadoc warning (which again, is not introduced by this patch.)

I'm confident that the test failure is unrelated. The test passes on my local box and that part of the code doesn't have much to do with the code changed by this patch., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12623175/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 10 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5882//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5882//console

This message is automatically generated., +1, lgtm, Still looking through the patch. Can some of the unnecessary import changes be undone to keep the patch smaller?, bq. Can some of the unnecessary import changes be undone to keep the patch smaller?

Yes, certainly. Where'd you see them? That probably happened while resolving conflicts with other commits that have happened since I started working on this., [~atm], I am looking at this patch. As I see this, I feel this change should include design details. Some questions that come to mind:
# In documentation you say "    [[2]] Both NNs must be started with the <<<'-upgrade'>>> flag." Does this mean both the namenodes should be available during upgrade or does it just mean that namenodes must be started with -upgrade. One of the namenode can first upgrade (and possibly be finalized) and later  second NN can be upgraded?
# When active namenode is performing shared edits upgrade, if it fails, does fail over occur to the standby and  does the new active resume the upgrade? Same question for finalize and rollback.
# In documentation "The operator should run the roll back command on one of the NN boxes,...", could have issues related to which NN is chosen. It must be on the one where upgrade has been previously done right?
# Given the rollback procedure, where bootstrapStandby muste be done on one of the NNs, why not just upgrade a single namenode (without worrying about two namenodes racing to upgrade etc.) and just follow the same procedure as rollback to simplify this?, Another thing that comes mind is, the lock files are created on JNs. What if lock file was created on all but was deleted only on two. How does the presence of lock file on a JN affect the system?, bq. That probably happened while resolving conflicts with other commits that have happened since I started working on this.
FSNamesystem.java. I see that IDEs expand a.b.c.* imports to individual imports. You are changing it back to a.b.c.* in your patch., bq. In documentation you say " [[2]] Both NNs must be started with the <<<'-upgrade'>>> flag." Does this mean both the namenodes should be available during upgrade or does it just mean that namenodes must be started with -upgrade. One of the namenode can first upgrade (and possibly be finalized) and later second NN can be upgraded?

Closer to the latter. OneNN can first upgrade, and even do the upgrade of the shared log, and later the second NN can be started with the -upgrade flag. It will see that an upgrade is in progress by presence of the shared log lock  and do its local upgrade with that CTime. One cannot, however, start the second NN with the -upgrade flag after the upgrade has been finalized, since doing so removes the shared log lock.

bq. When active namenode is performing shared edits upgrade, if it fails, does fail over occur to the standby and does the new active resume the upgrade? Same question for finalize and rollback.

i.e. if it fails to upgrade the shared log? That NN woud shut down and when the other NN became active (either manually or automatically) yes, it would try to upgrade the shared log at that time. Finalization - no, failure of that procedure would require the admin to re-attempt the finalization once the system was back up, and finalization requires both NNs to be running.

bq. In documentation "The operator should run the roll back command on one of the NN boxes,...", could have issues related to which NN is chosen. It must be on the one where upgrade has been previously done right?

Well, I had been assuming that both NNs had already been upgraded, in which case no, it doesn't matter which NN does the rollback. If the NN you tried to run rollback from had not in fact already been upgraded then it won't let you start with the '-rollback' option.

bq. Given the rollback procedure, where bootstrapStandby muste be done on one of the NNs, why not just upgrade a single namenode (without worrying about two namenodes racing to upgrade etc.) and just follow the same procedure as rollback to simplify this?

That would certainly simplify the code quite a bit, since we could just assume that only one NN is running during the actual upgrade procedure, and I considered this option. Doing so means that there'd be some asymmetry between the two nodes involved in the whole HA upgrade procedure, e.g. you would then _have_ to do the rollback on the NN where you initiated the upgrade, but perhaps that's acceptable since layout version upgrades are relatively rare. If you'd be more comfortable with this approach then I can think about what it would take to rework the patch.

bq. Another thing that comes mind is, the lock files are created on JNs. What if lock file was created on all but was deleted only on two. How does the presence of lock file on a JN affect the system?

In that case the finalization would fail and would need to be re-attempted.

bq. FSNamesystem.java. I see that IDEs expand a.b.c.* imports to individual imports. You are changing it back to a.b.c.* in your patch.

Yea, that's where I recall resolving import conflicts. I'll take a look and fix those once I hear back from you on the above., Here's an updated patch which implements Suresh's suggestion of doing away with the shared log lock and instead requiring that both NNs be shut down during the upgrade and the second NN re-bootstrapped after the first NN has performed the upgrade. I think this makes the procedure a little more complex for operators, but it certainly does simplify the code.

[~sureshms] - I don't feel super strongly about it either way, so please let me know if you find this more palatable. I'd appreciate a prompt review as this JIRA has been going on for quite some time and I'd like to get it done., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12623568/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 10 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.server.namenode.TestBackupNode

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5907//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5907//console

This message is automatically generated., Attached patch should fix the TestBackupNode failure. The only difference between this and the last patch is to change the signature of BackupNode#createHAState so that it again overrides the relevant method of the superclass., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12623607/HDFS-5138.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 10 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5911//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5911//console

This message is automatically generated., +1, new version looks good., I committed this to trunk. Looks like the patch has a few conflicts against branch-2, so I didn't commit there yet. Leaving open for branch-2 commit., SUCCESS: Integrated in Hadoop-trunk-Commit #5038 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5038/])
HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers. (todd: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561381)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/dev-support/findbugsExcludeFile.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/main/java/org/apache/hadoop/contrib/bkjournal/BookKeeperJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestBookKeeperAsHASharedDir.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLoggerSet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeRpcServer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/StorageInfo.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/QJournalProtocol.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/MiniJournalCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/MiniQJMHACluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestGenericJournalConf.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestInitializeSharedEdits.java
, I cherry-picked HDFS-5721 and HDFS-5719 to branch-2, since they had some trivial changes which changed a lot of indentation (thus creating conflicts). After doing so, the backport of this JIRA was pretty clean (just imports and some changed context in DFSUtil). I ran all the modified test suites on branch-2 with this backport patch and they passed. ATM, can you take a quick look at this?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12625212/hdfs-5138-branch-2.txt
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5944//console

This message is automatically generated., @Todd, I have had some conversation about this [~atm] related to this jira. I had brought up one issue about potentially losing editlogs on JournalNode. I thought that would be addressed before this jira can be committed. I have been very busy and have not been able to provide all my comments. Reviewing this patch has been quite tricky. Here are my almost complete review comments. While some of the issues are minor nits, I do not think this patch and the documentation is ready.

I am adding information about the design, the way I understand it. Let me know if I got it wrong.
*Upgrade preparation:*
# New bits are installed on the cluster nodes.
# The cluster is brought down.

*Upgrade:* For HA setup, choose one of the namenodes to initiate upgrade on and start it with -upgrade flag.
# NN performs preupgrade for all non shared storage directories by moving current to previous.tmp and creating new current.
#* Failure here is fine. NN start up fails. Next attempt at upgrade the storage directories are recovered.
# NN performs preupgrade of shared edits (NFS/JournalNodes) over RPC. JournalNodes current moved to previous.tmp and new current is created.
#* If one of the JN preupgrade fails and upgrade is reattempted, editlog directory could be lost on the JN. Restarting the JN does not fix the issue.
# NN performs upgrade of non shared edits by writing new CTIME to current and moving previous.tmp to previous.
#* If one of the JN preupgrade fails and upgrade is reattempted, editlog directory could be lost on the JN. Restarting the JN does not fix the issue.
# NN performs upgrade of shared edits (NFS/JournalNodes) over RPC. JournalNodes current has new CTIM and previous.tmp is moved to previous.
# We need to document that all the JournalNodes must be up. If a JN is irrecoverably lost, configuration must be changed to exclude the JN.

*Rollback:* NN is started with rollback flag
# For all the non shared directories, the NN checks for canRollBack, essentially ensures that previous directory with the right layout version exists.
# For all the shared directories, the NN checks for canRollBack, essentially ensures that previous directory with the right layout version exists.
# NN performs rollback for shared directories (moving previous to current)
#* If rollback of one of the JN fails, then directories are in inconsistent state. I think any attempt at retrying rollback will fail and will require manually moving files around. I do not think restarting JN fixes this.
# We need to document that all the JournalNodes must be up. If a JN is irrecoverably lost, configuration must be changed to exclude the JN.

*Finalize:* DFSAdmin command is run to finalize the upgrade.
# Active NN performs finalizing of editlog. If JN's fail to finalize, active NN fails to finalize. However it is possible that standby finalizes, leaving the cluster in an inconsistent state.
# We need to document that all the JournalNodes must be up. If a JN is irrecoverably lost, configuration must be changed to exclude the JN.

Comments on the code in the patch (this is almost complete):
Comments:
# Minor nit: there are some white space changes
# assertAllResultsEqual - for loop can just start with i = 1? Also if the collection objects is of size zero or one, the method can return early. Is there a need to do object.toArray() for these early checks? With that, perhaps the findbugs exclude may not be necessary.
# Unit test can be added for methods isAtLeastOneActive, getRpcAddressesForNameserviceId and getProxiesForAllNameNodesInNameservice (I am okay if this is done in a separate jira)
# Finalizing upgrade is quite tricky. Consider the following scenarios:
#* One NN is active and the other is standby - works fine
#* One NN is active and the other is down or all NNs - finalize command throws exception and the user will not know if it has succeeded or failed and what to do next
#* No active NN - throws an exception cannot finalize with no active
#* BlockPoolSliceStorage.java change seems unnecessary
# Why is {{throw new AssertionError("Unreachable code.");}} in QuorumJournalManager.java methods?
# FSImage#doRollBack() - when canRollBack is false after checking if non-share directories can rollback, an exception must be immediately thrown, instead of checking shared editlog. Also printing Log.info when storages can be rolled back will help in debugging.
# FSEditlog#canRollBackSharedLog should accept StorageInfo instead of Storage
# QuorumJournalManager#canRollBack and getJournalCTime can throw AssertionError (from DFSUtil.assertAllResultsEqual()). Is that the right exception to expose or IOException?
# Namenode startup throws AssertionError with -rollback option. I think we should throw IOException, which is how all the other failures are indicated.
, SUCCESS: Integrated in Hadoop-Yarn-trunk #462 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/462/])
HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers. (todd: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561381)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/dev-support/findbugsExcludeFile.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/main/java/org/apache/hadoop/contrib/bkjournal/BookKeeperJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestBookKeeperAsHASharedDir.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLoggerSet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeRpcServer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/StorageInfo.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/QJournalProtocol.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/MiniJournalCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/MiniQJMHACluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestGenericJournalConf.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestInitializeSharedEdits.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1679 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1679/])
HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers. (todd: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561381)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/dev-support/findbugsExcludeFile.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/main/java/org/apache/hadoop/contrib/bkjournal/BookKeeperJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestBookKeeperAsHASharedDir.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLoggerSet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeRpcServer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/StorageInfo.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/QJournalProtocol.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/MiniJournalCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/MiniQJMHACluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestGenericJournalConf.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestInitializeSharedEdits.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1654 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1654/])
HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers. (todd: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561381)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/dev-support/findbugsExcludeFile.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/main/java/org/apache/hadoop/contrib/bkjournal/BookKeeperJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestBookKeeperAsHASharedDir.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLogger.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/AsyncLoggerSet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/IPCLoggerChannel.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocol.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolServerSideTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/protocolPB/QJournalProtocolTranslatorPB.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNodeRpcServer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/StorageInfo.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/QJournalProtocol.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/MiniJournalCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/MiniQJMHACluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestGenericJournalConf.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestInitializeSharedEdits.java
, Hi Suresh - thanks for the comments. Given that Todd has already committed this to trunk and very kindly created a branch-2 patch, and given that this change is quite large so rebasing it on trunk is difficult, and given that most of your review comments are rather small, I think we should address your comments in a follow-up JIRA. Is that alright with you? I'm sure I can get these addressed in a day or two at most. Please let me know ASAP and I'll go ahead and file it.

Todd - thanks a lot for taking care of dealing with the conflicts so this can be committed to branch-2. The branch-2 patch looks great to me., [~atm], please address the comments before merging to branch-2.

My main concern apart from comments on the code is, the need to have all JNs and when any of the steps related to a JN fails, the boundary conditions that arise out of it. These issues can result in loss of metadata and very involved, error prone recovery procedure. It also might need the system to be restarted (say finalize fails because one of the JNs is not up). Please look at the comments on the design and see if I understand it correctly., Hi Suresh, it's obviously fine that you're busy (we all are) but in the future please just let me know that you intend to review it and that we should hold off for committing it for a bit. I reached out to you more than once last week to ask about a review timeline and never heard back from you, so I asked Todd to commit it (I'm traveling at the moment) given the silence.

bq. I had brought up one issue about potentially losing editlogs on JournalNode.

This scenario isn't possible as you described because either the pre-upgrade or upgrade stages (depending upon when the original failure happened) will fail to rename the dir if it already exists.

That said, your points about improving the documentation and the recovery procedure in the event of partial failure of the upgrade are well taken and certainly worth addressing. Upon looking at it further, I also think we should change a few of the assertions in the code to be actual exceptions, since we shouldn't have to be running with assertions enabled to check these error conditions, which should harden all of these code paths a bit more.

bq. please address the comments before merging to branch-2.

OK, I've filed HDFS-5840 to address your latest comments. Please follow that JIRA and review it as promptly as you can. I'm going to resolve this JIRA for now with a fix version of 3.0.0 and will merge both JIRAs to branch-2 when HDFS-5840 is completed., Resolving this with a fix version of 3.0.0., {quote}
Hi Suresh, it's obviously fine that you're busy (we all are) but in the future please just let me know that you intend to review it and that we should hold off for committing it for a bit. I reached out to you more than once last week to ask about a review timeline and never heard back from you, so I asked Todd to commit it (I'm traveling at the moment) given the silence.
{quote}
[~atm], we talked about this last on Friday Jan 16th over the phone right. I did tell you that JournalNode potentially losing editlogs.

bq. This scenario isn't possible as you described because either the pre-upgrade or upgrade stages (depending upon when the original failure happened) will fail to rename the dir if it already exists.
Is that correct? Did you check it? Java File#renameTo() is platform dependent. The following code always renames the directories (on my MAC):

{code}
public static void main(String[] args) {
    File f1 = new File("/tmp/dir1");
    File f2 = new File("/tmp/dir2");
    f1.mkdir();
    f2.mkdir();
    System.out.println(f1 + (f1.exists() ? " exists" : " does not exist"));
    System.out.println(f2 + (f2.exists() ? " exists" : " does not exist"));
    f1.renameTo(f2);
    System.out.println("Renamed " + f1 + " to " + f2);
    System.out.println(f1 + (f1.exists() ? " exists" : " does not exist"));
    System.out.println(f2 + (f2.exists() ? " exists" : " does not exist"));
  }
{code}

Related question. Lets say even if the rename fails, how does user recover from that condition? I brought up several scenarios related to that in preupgrade, upgrade, and finalize. How do we handle finalize being done successfully done on one namenode and not the other?, bq. Aaron T. Myers, we talked about this last on Friday Jan 16th over the phone right. I did tell you that JournalNode potentially losing editlogs.

There must have been some misunderstanding because I'm pretty sure I told you that I didn't think that was possible. :) Anyway, see below...

bq. Is that correct? Did you check it? Java File#renameTo() is platform dependent. The following code always renames the directories (on my MAC):

I did, at least on Linux. In the code example you have above, try putting a child file or directory under the directory f2 and see if it still works. The concern is about losing edit logs by overwriting a renamed directory with some contents, so by definition there will be some files in the directory being renamed to.

bq. Related question. Lets say even if the rename fails, how does user recover from that condition? I brought up several scenarios related to that in preupgrade, upgrade, and finalize. How do we handle finalize being done successfully done on one namenode and not the other?

Finalize is actually rather easy, since it's idempotent. The preupgrade and upgrade failure scenarios should both be handled either manually or by the storage recovery process, which currently should happen on JN restart, but I agree could be improved. Let's continue discussion of this over on HDFS-5840., bq. The concern is about losing edit logs by overwriting a renamed directory with some contents, so by definition there will be some files in the directory being renamed to.
That makes sense. Thanks.

bq. The preupgrade and upgrade failure scenarios should both be handled either manually or by the storage recovery process
I do not think JN performs recovery, based on the following code from JNStorage.java
{code}
  void analyzeStorage() throws IOException {
    this.state = sd.analyzeStorage(StartupOption.REGULAR, this);
    if (state == StorageState.NORMAL) {
      readProperties(sd);
    }
  }
{code}

For JournalNode, StorageDirectory#doRecover() is not called. Is that correct? From my understanding, once it gets into this state, JournalNode restart will not work?, bq. Finalize is actually rather easy, since it's idempotent. 
Missed this. Agreed finalize is idempotent (not sure how code deals with the failures - not had time to look into it). But not being able to finalize in some cases could be problematic. Especially from storage utilization point of view., [~atm], do you have any comments on the feedback I had for this jira?

Based on that, we should look into, if we want to leave the current patch as is on trunk with a quick change that addresses any valid comments or revert this patch, if it is going to take a lot of time (if you are busy) to followup on the comments?, Sorry, Suresh, I've been distracted by other things.

I had previously filed HDFS-5840 to address the feedback regarding recovery in the case of partial upgrade failure. Let's leave this patch as-is on trunk and continue the discussion there. I should be able to get to that next week., [~atm], most of us are swamped with wrapping up rolling upgrades and testing it. Can you please look into this?, Sorry I mean the above comment to be in HDFS-5840., Reopening this jira to make sure it gets tracked as blocker for 2.4., So I did a simple test for HDFS upgrade with HA, and hit the following exception while doing rollback (with layoutversion change in the upgrade):
{code}
14/03/21 01:01:53 FATAL namenode.NameNode: Exception in namenode join
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Could not check if roll back possible for one or more JournalNodes. 1 exceptions thrown:
Unexpected version of storage directory /grid/1/tmp/journal/mycluster. Reported: -56. Expecting = -55.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:178)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:131)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:228)
	at org.apache.hadoop.hdfs.qjournal.server.JNStorage.analyzeStorage(JNStorage.java:202)
	at org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:73)
	at org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:142)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNode.getOrCreateJournal(JournalNode.java:87)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNode.canRollBack(JournalNode.java:309)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.canRollBack(JournalNodeRpcServer.java:228)
{code}

In my HA upgrade test, the new software bumped the layoutversion from -55 to -56. I stopped all the services and restarted JNs with old software. Then I run "namenode -rollback" and hit the above exception. Looks like for rollback JN with old software cannot handle future layoutversion brought by new software., Created HDFS-6135 for the issue. Also tried to use a unit test to reproduce the issue there., [~jingzhao] / [~sureshms] - Should we close this one and mark HDFS-6135 as the blocker? Thanks., [~acmurthy], I have HDFS-6135 and HDFS-5840 as blockers., Rebase the patch for branch-2., +1 the branch-2 patch looks good., Thanks for the quick review, Nicholas! I will commit the backport patch to branch-2 and 2.4. We can continue to fix remaining issues in HDFS-5840 and HDFS-6135., I've merged this to branch-2 and branch-2.4.0., SUCCESS: Integrated in Hadoop-trunk-Commit #5392 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5392/])
Move HDFS-5138 to 2.4.0 section in CHANGES.txt (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581074)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, SUCCESS: Integrated in Hadoop-trunk-Commit #5397 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5397/])
HDFS-5840. Follow-up to HDFS-5138 to improve error handling during partial upgrade failures. Contributed by Aaron T. Myers, Suresh Srinivas, and Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581260)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAStateTransitions.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #520 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/520/])
HDFS-5840. Follow-up to HDFS-5138 to improve error handling during partial upgrade failures. Contributed by Aaron T. Myers, Suresh Srinivas, and Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581260)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAStateTransitions.java
Move HDFS-5138 to 2.4.0 section in CHANGES.txt (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581074)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1737 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1737/])
HDFS-5840. Follow-up to HDFS-5138 to improve error handling during partial upgrade failures. Contributed by Aaron T. Myers, Suresh Srinivas, and Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581260)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAStateTransitions.java
Move HDFS-5138 to 2.4.0 section in CHANGES.txt (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581074)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1712 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1712/])
HDFS-5840. Follow-up to HDFS-5138 to improve error handling during partial upgrade failures. Contributed by Aaron T. Myers, Suresh Srinivas, and Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581260)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/site/apt/HDFSHighAvailabilityWithQJM.apt.vm
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAStateTransitions.java
Move HDFS-5138 to 2.4.0 section in CHANGES.txt (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581074)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
]