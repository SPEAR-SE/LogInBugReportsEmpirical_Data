[Sounds like either a JVM bug or bad hardware. Have you run memtest86 on your machine? Do you have ECC RAM?, I do have ECC RAM - but it could be bad hardware, it's pretty old. 
I've already had to put some flaky hard drives off to the side.

For now, I'm really in wait and see mode. 
I just logged this in case it repeats and I can narrow it down to a particular use of the JVM.
, FWIW we saw this on the DN under JVM 1.6.0_21 at one customer site as well. Also under heavy write load. Almost identical JVM stack trace., I filed a JVM bug for this - I'm guessing that the classloader code doesn't handle ENFILE correctly which causes the JVM to exit. Stuart: do you see "too many open files" in the DN log prior to this failure?, Todd, what's the link to the JVM ticket you filed? Interestingly enough, we are running u21, and right before the crash in Cassandra, we hit the max 'too many open files'., It has "Review ID" 1985110, but it didn't get assigned a bug ID yet that I know of., This looked to be cause jars were updated during the runtime. The mail thread http://comments.gmane.org/gmane.comp.java.openjdk.hotspot.gc.devel/2422 has some more info.

Resolving as Not A Problem cause it was not a Hadoop fault as determined above. Recorded here already for folks who may search later.]