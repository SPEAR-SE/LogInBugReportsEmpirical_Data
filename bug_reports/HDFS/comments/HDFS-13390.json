[{quote}. So why overwriting file will produce AccessControlException and not the delete method?
{quote}
[https://github.com/apache/hadoop/blob/branch-2.9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java#L162]
 For {{startFileInt witgh overWrite flage}}, it resolves to all these arguments:
{noformat}
  checkPermission(iip, false, null, null, FsAction.WRITE, null, false)
{noformat}
For {{delete}} path, it resolves to all these arguments:
{noformat}
fsd.checkPermission(iip, false, null, FsAction.WRITE, null, FsAction.ALL, true);
{noformat}
Here are all the {{checkPermission}} parameters.
{noformat}
checkPermission(INodesInPath inodesInPath, boolean doCheckOwner,
      FsAction ancestorAccess, FsAction parentAccess, FsAction access,
      FsAction subAccess, boolean ignoreEmptyDir)
{noformat}
If you notice {{access}} parameter, it is null for {{delete}} and it is {{FsAction.WRITE}} for {{startFileInt}}.
 That means it will skip the checking whether file is writable for {{delete}} and will check whether file is writable by {{currentUser}}.
 I don't have much context why the behavior is different.
 IMO it should be same (i.e you should be able to overwrite the file if you are able to delete it) but you can provide a patch and will let others review the patch.
Just FYI, this patch will bring an incompatible change in the way that some jobs will not fail when they expect it to fail.]