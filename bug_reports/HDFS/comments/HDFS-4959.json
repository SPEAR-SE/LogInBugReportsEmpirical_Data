[If stop standby NN, then run "hdfs dfsadmin -refreshNodes" on the Active NN, then decommission process is normal., Update:

if add data_node_ip:50010 in the exclude file, then decommission works, but there are no decommissioned data nodes showed on the StandbyNN's web UI., Hi Fengdong, we have already encountered the same problem, and we think the root cause is as following I described:
1. In the current implementation of dfs client, it uses a HashMap to store the NN addresses. The keys are host0 and host1, and values are the corresponding NN addresses. In Java HashMap, when traverse the HashMap, host1 is always ahead of host0.
2. The dfs client traverses the HashMap to send requests to NN, at first it will request the host1 NN, if the request on host1 NN failed or the NN on host1 is standby, then dfs client will do failover and to request the host0 NN.
3. In the current implementation of NN, nearly all the refresh work such as refreshNodes/refreshTopology/refreshUserToGroupsMappings donot check the state of NN. As a result, if the request is succeeded on the host1 NN, it will not try the host0 NN. But in our clusters, host0 NN is active and host1 NN is standby most of the time.

We have already verified this in our cluster. And we think a complete implementation should send requests such as refreshNodes/refreshTopology/refreshUserToGroupsMappings to both the active and standby NN., A simple fix may look like this, but this is not a good fix:
{code}
  public void refreshNodes() throws IOException {
    namesystem.refreshNodes();
    namesystem.checkOperation(OperationCategory.WRITE);
  }
{code}]