[1) included hadoop-version in src/contrib/hdfsproxy/build.xml file
2) changed conf to sslConf in HsftpFileSystem.java


, Won't this override the hadoop-version property defined by the main build script? Compilation will break when the core jar is updated. This is a problem for all the contrib modules and I'd rather avoid ad-hoc fixes..., Yes, I agree. how about <import file="${hadoop.root}/build.xml"/> in src/contrib/hdfsproxy/build.xml? It seems pretty clumsy though. I think it would be better to define hadoop-version in a build.properties instead of in build.xml in the hdfs trunk. That way every contrib can import this property file without importing too much.  I don't understand why it has <property file="${basedir}/build.properties" /> line in the $HADOOP_HDFS_HOME/build.xml file while there is no such file in the directory. , The only reason the version is required is for including hadoop jars, right? If the include spec takes wildcards, that might be an acceptable workaround until we get a better packaging story for intra-project dependencies, That's what I did in the beginning.  Someone changed it in the project split process to make it look prettier I guess. 
I will change it back right now., done. , Used synchronized block to address some race conditions for LdapIpDirFilter.java., fixed the bugs described above., Thanks for fixing the lib packaging. There was just one pair of changes that I wanted to ask after:
{noformat}
-    <display-name>HDFS Proxy</display-name>
+    <display-name>HDFS Proxy Forward</display-name>
{noformat}
{noformat}
-    if (dstContext == null) {
-      LOG.info("Context non-exist or restricted from access: " + version);
+    // avoid infinite forwarding.
+    if (dstContext == null
+        || "HDFS Proxy Forward".equals(dstContext.getServletContextName())) {
+      LOG.error("Context (" + version
+          + ".war) non-exist or restricted from access");
{noformat}
This is to prevent the forwarding servlet from passing requests to itself? When does this occur? Is there another way to detect/prevent it, other than (what looks like) taking a configurable string and hard-coding a check for it?

The rest of the changes look reasonable., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12414531/HDFS-481.patch
  against trunk revision 806746.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-vesta.apache.org/84/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-vesta.apache.org/84/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-vesta.apache.org/84/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-vesta.apache.org/84/console

This message is automatically generated., Hi Chris,  thanks for the comment.
right, that piece of code is to prevent the forwarding servlet from passing requests to itself. That happens if the forwarding servlet could not find the matching servlet to forward request to. 

For instance, if the user request path is /hadoop20 and hadoop20.war does not exist under the same webapps/ folder as the ROOT.war, what the forwarding servlet will do (through ServletContext.getContext()) in this case is to match the longest path. Since it could not find /hadoop20, it will match against its parent path /, which matches to ROOT.war, the forwarding servlet itself, then it causes an infinite loop, finally causing java.lang.StackOverflowError to be thrown. 

I've thought of using logic curContext.getServletContextName().equals(dstContext.getServletContextName()) to tell, but it will break the unit test.  Since cactus unit test framework won't be able to do cross context forwarding at this stage yet. All forwarding occurs in the same context.  In that case, ServletContext.getContext() would always return the same context. the unit test would stuck there. 

I couldn't think any other ways to work around this at this stage. Do you have any better ideas?

Thanks., bq. I couldn't think any other ways to work around this at this stage. Do you have any better ideas?

I'm not fluent in the servlet/Tomcat APIs, but hard-coding the (configurable) name of a component to avoid unbounded recursion seems ad hoc. Is it difficult to restrict the set of valid targets to exclude the current servlet? If nothing else, looking up (rather than hard-coding) the servlet name strikes me as a minimum requirement. This can't possibly be an issue unique to hdfsproxy; is there a canonical approach that doesn't work with its requirements?, HDFS-481 patch includes the following fixes

* LdapIpDirFilter broken into LdapIpDirFilter for authentication and AuthorizationFilter for authorization. 
* KerberosAuhoriztionFilter extends AuthorizationFilter - to be used again kerberos based hadoop secure version
* Infinite redirection addressed by checking the servlet context object instead of the names
* Acting on behalf of the requesting user using UGI.createProxyUser (hdfsproxy runs as a trusted super user)
, HDFS-481-bp-y20.patch and HDFS-481-bp-y20s.patch are backport patches. Not for commit., @Srikanth
- Is you patch still fixing the bugs stated in the description?

- Could you revert the white space changes like the following?  Otherwise, it is hard to review your patch.
{code}
-<property>
-    <name>fs.default.name</name>
-    <!-- cluster variant -->
-    <value>hdfs://localhost:54321</value>
-    <description>The name of the default file system.  Either the
-  literal string "local" or a host:port for NDFS.</description>
-    <final>true</final>
-  </property>
+    <property>
+        <name>fs.default.name</name>
+        <!-- cluster variant -->
+        <value>hdfs://localhost:54321</value>
+        <description>The name of the default file system.  Either the
+            literal string "local" or a host:port for NDFS.</description>
+        <final>true</final>
+    </property>
{code}, BTW, it seems that hdfsproxy cannot be built.  I tried to run TestLdapIpDirFilter but it failed by 
{noformat}
/home/tsz/hadoop/hdfs/h1/src/contrib/hdfsproxy/build.xml:292:
 org.codehaus.cargo.container.ContainerException: Failed to download
 [http://apache.osuosl.org/tomcat/tomcat-6/v6.0.18/bin/apache-tomcat-6.0.18.zip]
{noformat}, {quote}BTW, it seems that hdfsproxy cannot be built. I tried to run TestLdapIpDirFilter but it failed by {quote}
This is a known issue: HDFS-1046, Patch already includes changes to build.xml for pulling newer tomcat version (to run LdapIpDirFilter tests)

{noformat} 

@@ -299,7 +301,7 @@
      <containerset>
        <cargo containerId="${tomcat.container.id}" timeout="30000" output="${logs.dir}/output.log" log="${logs.dir}/cargo.log">
         <zipUrlInstaller
-            installUrl="http://apache.osuosl.org/tomcat/tomcat-6/v6.0.18/bin/apache-tomcat-6.0.18.zip"
+            installUrl="http://apache.osuosl.org/tomcat/tomcat-6/v6.0.24/bin/apache-tomcat-6.0.24.zip"
             installDir="${target.dir}/${tomcat.container.id}"/>
          <configuration type="existing" home="${tomcatconfig.dir}">
            <property name="cargo.servlet.port" value="${cargo.servlet.http.port}"/>

{noformat}

All the contrib tests including (LdapIpDirFilter, AuthorizationFilter) seems to run successfully with the revised patch (HDFS-481.patch). Attached logs from test-patch and test-contrib runs.

Nicholas, Will exclude white space changes from the patch and re-attach for review., Updated patch after removing all white space/indentation changes. Patch is identical to the earlier one otherwise., {quote}
@Srikanth

    * Is you patch still fixing the bugs stated in the description?
{quote}

Patch includes fix for all the bugs reported in this JIRA except for 
  {quote}
  ssl.client.do.not.authenticate.server setting can only be set by hdfs's configuration files, need to move this setting to ssl-client.xml.
  {quote}

Patch for this has been uploaded to HDFS-482 and maked as patch-available.
, > Patch already includes changes to build.xml for pulling newer tomcat version ...

We should fix the build problem in HDFS-1046 first since it also affects other contributors., > Patch includes fix for all the bugs reported in this JIRA except for ...

Thanks, Srikanth.  Could you update the description and the summary of this issue to reflect all the changes?  (If it is not too hard, it would be great if you could separate the divide patch to some other issues like HDFS-1009.  In general, a JIRA should only fix one issue.), Summary of Changes:

1. ProxyFileDataServlet, ProxyListPathsServlet, ProxyFileForward - Use createProxyUser instead of createRemoteUser to obtain UGI for the requesting user, name.conf - context attribute is set by LdapIpDirFilter

2. LdapIpDirFilter - Removed class members userId, groupName and Paths and these are now set for each request through LdapEntry (a private inner class)

3. KerberosAuthorizationFilter - Accessing proxy user keytab file for credentials and initializing UGI

4. LdapIpDirFilter + AuthorizationFilter -  Separated IP based authentication and path authorization into two independent filters. IP based authentication is done by LdapIpDirFilter and Path authroization is implemented through AuthorizationFilter.

5. TestLdapIpDirFilter + TestAuthorizationFilter - IP based test cases retained in TestLdapIpDirFilter and path test cases are moved to TestAuthorizationFilter

6. ProxyUtil - Added methods to create proxy user and getting namenode url from Hadoop configuration

7. hdfsproxy-default.xml - Including new security related attributes

8. tomcat-web.xml - Adding additional filter for Authroization. Allowing LdapIpDirFilter & KerberosAuthroizationFilter to be processed for forward and request methods 

9. build.xml - Including TestAuthroizationFilter for cactus based unit tests, Also increasing verbosity level for logs during build

10. ProxyForwardServlet - Fix for infinite looping by verifying if the context is same as the current and aborting

11. TestProxyUtil & TestHdfsProxy - Fixes to get the tests to run

, {quote}
If it is not too hard, it would be great if you could separate the divide patch to some other issues like HDFS-1009. In general, a JIRA should only fix one issue
{quote}

Proxy will not be fully functional and compatible with HDFS (kerberos based setup). Hence moved the fixes for 1009 into this JIRA. Related changes are 

1. Inclusion of KerberosAuthorizationFilter which extends AuthorizationFilter
2. web.xml to include KerberosAuthorizationFilter instead of the default AuthorizationFilter

Have listed the changed files in the patch along with a brief summary of what the change is meant for., Srikanth, thank you for the update.  I am looking forward to review your new patch., Revised patch for 481 (to exclude changes reported as HDFS-1074), +1 patch looks good., Output from test-patch

     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 15 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

test-contrib:


test:
BUILD SUCCESSFUL

, I also have tested it locally.  It worked fine.

I have committed this.  Thanks, Srikanth!, Integrated in Hadoop-Hdfs-trunk-Commit #230 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk-Commit/230/])
    . hdfsproxy: Bug Fixes + HdfsProxy to use proxy user to impresonate the real user.  Contributed by Srikanth
, Revised backport for yhadoop20 patch in sync with latest trunk patch, Revised patch for yhadoop20s in sync with trunk patch., Backport patch for y20.1xx, Incremental back port to fix broken unit tests in y20.1xx & y20.101. Tests are broken due to 

* Missing super user setup when Mini DFS Cluster starts
* Missing src/test/resources folder
* UserGroupInformation class depending on krb5.conf in the system. (Bypassing that through krb5.conf in ${hadoop.core}/src/test  - contrib/hdfsproxy/build.xml change)  

This patch needs to be applied incrementally over the HDFS-481-NEW.patch.]