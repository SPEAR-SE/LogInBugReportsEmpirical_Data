[This is the output from a failed test run:

{code}
Testcase: testGet took 3.826 sec
	Caused an ERROR
C:\hdp\build\test\data\dfs\data\data3\current\blk_-7731865925013663567 (The requested operation cannot be performed on a file with a user-mapped section open)
java.io.FileNotFoundException: C:\hdp\build\test\data\dfs\data\data3\current\blk_-7731865925013663567 (The requested operation cannot be performed on a file with a user-mapped section open)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:194)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:145)
	at java.io.PrintWriter.<init>(PrintWriter.java:218)
	at org.apache.hadoop.hdfs.TestDFSShell.corrupt(TestDFSShell.java:1133)
	at org.apache.hadoop.hdfs.TestDFSShell.testGet(TestDFSShell.java:1236)
{code}

The problem is that the data node's opening and closing of the block file occurs asynchronously of the JUnit test, in a separate DataXceiver thread, and there is no explicit coordination between the DataXceiver thread on the server side and the JUnit thread on the client side. Therefore, when the test tries to corrupt the block file, it's non-deterministic whether the data node still holds an open file handle.  On Windows, due to use of FileChannel.transferTo, the block file can be memory-mapped in read-only mode during the transfer to a client.  This causes the error shown above with the message about a "user-mapped section open"., With this patch, the test fully shuts down the MiniDFSCluster before corrupting the block files. Then, it restarts the MiniDFSCluster (without reformatting) and continues with the assertions. Shutting down MiniDFSCluster synchronously waits for completion of all DataXceiver threads, so there is no locking conflict.

With this version of the code, the test consistently passes for me on Mac and Windows Server 2008., +1, the patch looks good to me. Thanks for the fixing, Chris.

Nitpicking: " We must shut down the cluster so that threads...". It's not a problem on Unix, so far it's a problem only on Windows., Could you do something like sync or flush?  The tests already take long enough that hopefully we can avoid an unnecessary stop & start of the mini-cluster., Hello, Daryn.

My measurements show that stopping and starting the mini-cluster like this takes 1-2.5 seconds.  Before I coded this, I had looked for a lighter weight method: a sync or a flush like you suggested.  Unfortunately, the current data node code provides no such method.  DataNode.shutdown is the only method that synchronously waits for all DataXceiver threads to finish.  I figured it wasn't worthwhile to refactor DataNode.shutdown (and possibly weaken encapsulation) just to support one test case that needs to do some highly unusual operations on the raw block files.

If speeding up this whole test suite (which currently runs in ~50 seconds) is a goal, then I'd actually suggest refactoring so that all 14 tests in the suite reuse the same MiniDFSCluster instead of starting and stopping 14 times.  That would increase speed, but at the cost of reduced test isolation.  (This part would be tracked as a separate Jira.)

Thanks,
--Chris
, I think we need to look at changing tests not to unnecessarily restart the MiniDFSCluster. But that is a separate effort.

+1 for this change. I will commit it shortly., I committed the patch to branch-1-win. Thank you Chris.]