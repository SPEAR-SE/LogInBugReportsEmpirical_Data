[How about starting a new block while recovering the partially-failed block in background? This will cause a small block in the middle of the file, which is suboptimal. To mitigate we can create a new block with a given size -- (default block size) - (size of the partial block being recovered). Then we can merge the recovered and the new blocks., We don't support variable size blocks in HDFS now.  That would be a big project in and of itself.

I think the first step here is a design doc spelling out the approach.  Also, we might want to consider doing the async recovery only when the sync recovery fails, since that would provide the greatest protection., Updated a draft of design doc. Let me know if you think the basic approach is feasible. , Re: approach #2 in the design doc: there is some discussion of variable-length blocks ongoing at HDFS-3689., New design document to incorporate the progress on HDFS-3689. The general approach is separated from the choice of using variable sized blocks or not; so it can be assessed independent of the final outcome of HDFS-3689. , Updated design which provides strong consistency without replying on variable length blocks., [~zhezhang] and I chatted about this offline.  The main takeaway is that we might want to asynchronously search for new datanodes when a pipeline is not at full strength.  But we don't necessarily need to implement full asynchronous recovery in this JIRA (although maybe we could do that later).

Right now, clients that suffer temporary networking problems during pipeline recovery have to make an unpleasant choice: live with a short pipeline and the consequent under-replication (possibly for a long time after the networking problems have gone), or deal with an exception which puts the entire DFSOutputStream into an unpleasant error state.  It would be great if instead we could allow these clients to continue, but bring the pipeline up to full strength once the networking problems went away., [~cmccabe]: There seems to be multiple people with the same name as mine. My Apache ID is [~zhz]

Thanks for the comments. I'm updating the design doc to reflect our discussion., New design document to outline the two steps of recovering blocks in background. , This patch pauses the DataStreamer thread while waiting for RecoveryWorker to recover the pipeline., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664672/HDFS-6867-20140827.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestEncryptedTransfer
                  org.apache.hadoop.security.TestRefreshUserMappings
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.TestFileAppendRestart

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7784//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/7784//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7784//console

This message is automatically generated., RecoveryWorker should check nodes.length to make sure that an error has occurred before starting., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664708/HDFS-6867-20140827-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestSnapshotPathINodes

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestPipelines
org.apache.hadoop.hdfs.TestFileAppend2
org.apache.hadoop.hdfs.TestAppendDifferentChecksum
org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS
org.apache.hadoop.hdfs.server.namenode.TestAddBlock
org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport
org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength
org.apache.hadoop.hdfs.server.namenode.snapshot.TestINodeFileUnderConstructionWithSnapshot

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7788//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/7788//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7788//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664754/HDFS-6867-20140827-3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestFileAppend

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7794//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7794//console

This message is automatically generated., Some test classes have RepliceNodeOnFailure disabled. This new patches skips recoveryWorker invocation if that's the case., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12665006/HDFS-6867-20140828-1.patch
  against trunk revision c4c9a78.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.TestSafeMode
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7830//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7830//console

This message is automatically generated., The new patch adds a type of policy (BACKGROUND) where background recovery worker will be invoked. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12665064/HDFS-6867-20140828-2.patch
  against trunk revision c4c9a78.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.hdfs.security.token.block.TestBlockToken

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7834//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7834//console

This message is automatically generated., These 2 tests all passed on my local machine. Actually the background recovery worker didn't even start. If anyone else could verify that'd be great., [~cmccabe] Could you help review the patch? In the patch I created a new {{ReplaceDatanodeOnFailure}} policy named {{BACKGROUND}}, for the user to specify that background recovery should be used -- which we didn't cover in the offline discussion but I think is necessary., I looked at the design doc... looks good.  I wasn't able to apply the patch since there were some conflicts.  Can you rebase on current trunk?  Thanks!, A few comments on the first version (pre-rebase):

{code}
+        DFSClient.LOG.debug("RecoveryWorker for block " + block + " running");
{code}

This should be INFO, and I would also include the client name and file name.

{code}
+//    private boolean atRisk = false;
...
+//          this.atRisk = true;
{code}

I think you meant to remove this.

Please don't use Thread.interrupt on the RecoveryWorker when closing.  It gets very messy (Thread.interrupt can close sockets and FDs that the thread is listening on, etc.)  Instead, use some kind of Atomic or condition variable to communicate with this thread.

{code}
+              underBackgroundRecovery = true;
+              addDatanode2ExistingPipeline();
+              pipelineRecovered = true;
+              underBackgroundRecovery = false;
{code}

This will leave you in a bad state if there is an exception or other unexpected control flow issue.

We should think about how to manage the communication between the writer thread and recovery thread.  There are a few canonical ways:
* locks plus condition variables
* AtomicReference with a state, implementing a state machine
* locks plus polling

volatile booleans are almost always a mistake., New design doc addresses [~cmccabe]'s comments on locking between Streamer and RecoveryWorker threads., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667785/HDFS-6867-design-20140910.pdf
  against trunk revision 3072c83.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7983//console

This message is automatically generated., Does anyone know when [~hadoopqa] decides to test a newly uploaded file? I thought it checks the file extension and only tests .patch files., No, unfortunately it tests ALL files, regardless of how silly :)

No harm done, though.

The new design doc looks good.  I think having recovery done by the streamer thread will prove to be easier and more maintainable in the future, so good choice there., New patch to implement updated design., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12667965/HDFS-6867-20140910.patch
  against trunk revision 4be9517.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7995//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7995//console

This message is automatically generated., The test failure seems unrelated. The recovery worker is not enabled in {{TestPipelinesFailover}}. I tried that unit test and it passed locally.

Basically, when notified by {{RecoveryWorker}}, the {{DataStreamer}} does the following to recover the pipeline ({{setupPipelineForAppendOrRecovery}} is also modified to always try to add new DN when the state is {{UNDER_RECV}}). Since we are not following the existing recovery logic of catching DN errors from {{ResponseProcessor}} and running {{processDatanodeError}}, I want to makc sure nothing is missing. In particular, I'd like to discuss how to set the value of {{stage}}. I think it should be {{PIPELINE_SETUP_STREAMING_RECOVERY}}, but let me know if it should be something else.

{code:java}
          // Before sending each packet, check if the block is under
          // background recovery
          if (backgroundRecoveryState.get() ==
              HdfsConstants.BackgroundRecoveryState.UNDER_RECOV) {
            ...
            closeStream();
            try {
              stage = BlockConstructionStage.PIPELINE_SETUP_STREAMING_RECOVERY;
              setupPipelineForAppendOrRecovery();
            } catch (IOException e) {
              DFSClient.LOG.warn("Caught exception ", e);
            }
            ...
          }
{code}, I figured out that {{stage}} is set to the correct value if the {{recoveryFlag}} flag is passed to the {{createBlockOutputStream}}. This new patch made this change. [~cmccabe] I think the basic structure in this patch reflects all we have discussed; please take a look when you have time. Thanks!, bq. I figured out that stage is set to the correct value if the recoveryFlag flag is passed to the createBlockOutputStream. This new patch made this change. Colin Patrick McCabe I think the basic structure in this patch reflects all we have discussed; please take a look when you have time. Thanks!

Yeah, I think that's right. I don't think you should need to modify the pipeline states in this patch.

I don't think {{enum BackgroundRecoveryState}} belongs in {{HdfsConstants.java}}.  This enum is really only relevant to the {{DFSOutputStream}}-- it should be declared there (if we need it at all... see later comments).

Instead of atomic variables and ad-hoc synchronization, how about using futures here?

You could have something like this: when {{setupPipelineForAppendOrRecovery}} fails to bring the pipeline up to full strength, it creates a {{Future<DatanodeInfo[]>}}.  Then, before sending each {{o.a.h.h.DFSOutputStream.Packet}}, you check to see if the future has a new datanode for you via {{future.get(0, TimeUnit.SECONDS)}} or similar.

A future is basically a thread that runs, produces a value, and then exits.  You create it like this:
{code}
   ExecutorService executorService = 
      Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true)
          .setNameFormat(...).setUncaughtExceptionHandler(UncaughtExceptionHandlers.systemExit())
          .build());
     Future<DatanodeInfo[]> pipelineRecoveryFuture = executorService.submit(new Callable<String>() {
         public String call() {
             while (I didn't find any nodes and shutdown = false) {
                search for more datanodes
             }
             return datanodes (or null if shutdown)
         }});
{code}

You'd probably want to throw a {{close}} function in there that sets {{shutdown}}, but you get the idea.  This is a lot simpler than trying to deal with atomics all over, and {{pipelineRecoveryFuture}} only ever needs to be accessed from the streamer, so it doesn't need any locking.

Keep in mind, when you have a shutdown function that sets a boolean, that boolean needs to be volatile or atomic if another thread is reading it (that is missing from current patch).

Config key: I like the naming of the new key.  The new configuration key needs to be explained in {{hdfs-defaults.xml}}.  Especially we should document how it interacts with the best-effort configuration.  I thought about this a little more, and I think the simplest thing to do is:

{code}
best effort = false, bg recovery = false ==> current default behavior
best effort = true, bg recovery = false ==> current best effort behavior (never throw an exception)
best effort = false, bg recovery = true ==> When nodes are lost, we begin the background search for new ones.  When the number of remaining nodes drops too low, we throw an exception just like today.
best effort = true, bg recovery = true ==> When nodes are lost, we begin the background search for new ones.  As long as there is at least one node, no exception is thrown.
{code}

{code}
+            DatanodeInfo[] liveNodes =
+                dfsClient.datanodeReport(HdfsConstants.DatanodeReportType.LIVE);
{code}

I don't think this is what we want.  The cluster could have thousands of datanodes-- do we really need to get information about all of them here?   Also, it's not our job (as the client) to decide what nodes the file will be written to.  The NameNode must decide that.

I think to do this right, we'll need to modify an existing NameNode RPC like {{updatePipeline}}, or perhaps add a new RPC, that just gives us a new DataNode suitable for our pipeline, but does *not* change the state of the block on the NN.  Basically, this means breaking updatePipeline into two separate calls... one to find a good node (that RecoveryWorker will call), and another to actually update the block to be in RBR (that DataStreamer will call).

Also, you'll want to pass the NameNode the {{excludedNodes}} cache, found here:
{code}
    private final LoadingCache<DatanodeInfo, DatanodeInfo> excludedNodes =
        CacheBuilder.newBuilder()
        .expireAfterWrite(
            dfsClient.getConf().excludedNodesCacheExpiry,
...
{code}

just like we do with {{addBlock}}., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12668215/HDFS-6867-20140911.patch
  against trunk revision 17ffbe0.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 122 warning messages.
        See https://builds.apache.org/job/PreCommit-HDFS-Build/8003//artifact/trunk/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.ipc.TestCallQueueManager
                  org.apache.hadoop.ipc.TestFairCallQueue
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

                                      The following test timeouts occurred in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8003//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8003//console

This message is automatically generated., [~cmccabe] Thanks for the suggestion. I think Future is a good idea. To further simplify the design I have made {{RecoveryWorker}} more _stateless_ -- instead of keep running until full strength replication is built, it finishes as soon as at least one new DN is found. Then {{DataStreamer}} tries to recover the pipeline, and fires another {{RecoveryWorker}} if the strength is still not full.

bq. I think to do this right, we'll need to modify an existing NameNode RPC like updatePipeline, or perhaps add a new RPC, that just gives us a new DataNode suitable for our pipeline, but does not change the state of the block on the NN. Basically, this means breaking updatePipeline into two separate calls... one to find a good node (that RecoveryWorker will call), and *another to actually update the block to be in RBR (that DataStreamer will call)*.
In the second call (highlighted above) the NN needs to recalculate the good DN to use right? Since the mission of the {{RecoveryWorker}} is to provide hint to the {{DataStreamer}} suggesting good timings to trigger the real recovery process, I think we do need a new RPC, which is a lightweight _peek_ call to indicate (with reasonable accuracy) whether the recovery will succeed. E.g., the NN can quickly verify that the number of live DNs is least larger than {{nodes.length}} in the current pipeline. , Future-based implementation. Note that the aforementioned new NN RPC for quickly checking DN availability (without a full DN report) is not included yet -- I think that can be addressed in a separate JIRA., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12669520/HDFS-6867-20140917-v1.patch
  against trunk revision f230248.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8062//console

This message is automatically generated., Rebased again., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12669530/HDFS-6867-20140917-v2.patch
  against trunk revision f230248.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8063//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8063//console

This message is automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12669530/HDFS-6867-20140917-v2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10664/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12669530/HDFS-6867-20140917-v2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10666/console |


This message was automatically generated.]