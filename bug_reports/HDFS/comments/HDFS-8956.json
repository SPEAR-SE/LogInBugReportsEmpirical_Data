[[~sreelakshmi.rajula@gmail.com] thanks for interest. Jira is for tracking the issues.. Kindly post this query in [hadoop user-mailing list | https://hadoop.apache.org/mailing_lists.html]., I suggest may be you can check your DN address rather than port. When a port is occupied, you get *Caused by: java.net.BindException: Address already in use* but when a address like hostname is wrong you get *Caused by: java.net.BindException: Cannot assign requested address*. So check if the http server address mentioned for datanode is correct (as the stack tells *Cannot assign requested address*) or I guess may be a misconiguration in *hosts* file
, Just double checking, what is the value for dfs.datanode.http.address in your xml file?

Also, did you run the port checking command as root? 
#netstat -plten | grep 50010
#netstat -plten | grep 50075 , @brahma Thank you for sharing the exact place to post the query and sorry for spamming jira with unrelated issues. I am new to Hadoop and forums. I tried posting in hadoop user-mailing list as you suggested. But in that I did not find any exact place to post my queries. Do I have to send my issue to the provided e-mail ids. Please throw some light on this.

@Ajith S I am using the correct IP address, which was working earlier. I am able to do SSH from namenode to datanode. And "Node Manager" is started on my faulty Data node PC. hosts file I have cross checked and it seems to be fine, which I was using earlier and same file is being used in other data nodes, which works fine. Please let me know, if I am missing anything else.

@Rohan Rajeevan my dfs.datanode.http.address is configured as "0.0.0.0:50075" and dfs.datanode.address is configured as "0.0.0.0:50010". Same configuration is used in other data nodes which are working fine as well. And running netstat with below commands as root does not display anything, as these ports are actually not in use.
 #netstat -plten | grep 50010
 #netstat -plten | grep 50075 , [~sreelakshmi.rajula@gmail.com]

Check disk space on the box especially the folder in which the datanode writes. (example /data) 

If that is ok and if you can afford it, try to empty the contents of hadoop.tmp.dir and restart the DN., [~rrajeevan] both tmp and data directories are free and having around 900GB free space. , subscribe to user@hadoop.apache.org and post your query. while posting please provide the hdfs-site.xml also..I am thinking problem might be with {{dfs.domain.socket.path}} where already path exists.let me check your logs and configurations once you post in mailing list.., [~brahmareddy] Sorry for spamming in jira issues again. 

I have sent a request to hadoop user mailing list, but still not approved. 

I have sent request to "user-subscribe@hadoop.apache.org" and also to "user@hadoop.apache.org". 

Please let me know, if I have to follow any other procedure than sending an e-mail request for subscription. (Please bear with me that I could not find any other way than sending e-mail.)


About configurations -> I have not configured dfs.domain.socket.path in my hdfs-site.xml. 

The only configured values are 
dfs.datanode.data.dir -> file:///home/hadoop/hdfs/datanode

dfs.namenode.name.dir-> file:///home/hadoop/hdfs/namenode/

dfs.namenode.http-address-> 0.0.0.0:50070.

Thanks.

, {quote}I have sent a request to hadoop user mailing list, but still not approved.{quote}
Usually this happens immediately.can you please cross check once..?, Hello 

I got exactly same problem 
for me after uncommenting below line in /etc/hosts. datanode started 
127.0.0.1       localhost
I don't know why how...but it started

Thanks]