[Here is a patch that fixes the bug by catching the error in {{GetJournalEditServlet}}.  I considered just removing the addition of the SecondaryNameNode principal, since I've never heard of this usage in practice.  However, I suppose it would be considered a backwards-incompatible change if someone out there was running a non-HA cluster and just had chosen to offload edits to the JournalNodes for consumption by the SecondaryNameNode.  Catching it is probably the safer change.  {{TestSecureNNWithQJM}} is a new test suite that covers usage of QJM in a secured cluster.  While I was working on this, I also spotted a typo in {{TestNNWithQJM}}, which I'm correcting in this patch., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679492/HDFS-7359.1.patch
  against trunk revision 73e6012.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.web.TestWebHDFS

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8655//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8655//console

This message is automatically generated., It looks to me that simply removing the checks is equivalent to the current proposed patch, correct?, Removing that several lines means we no longer recognize SNN as a valid requestor. I guess in some scenario (maybe even in the future) we can still allow SNN to download journals from JN.
The current patch looks good to me. +1. I will commit it shortly., Here is patch v2.  We need one more change in {{ImageServlet}} to prevent the problem from happening during bootstrapStandby.

bq. It looks to me that simply removing the checks is equivalent to the current proposed patch, correct?

bq. Removing that several lines means we no longer recognize SNN as a valid requestor. I guess in some scenario (maybe even in the future) we can still allow SNN to download journals from JN.

Thanks for reviewing, Haohui and Jing.  Right, doing it this way preserves existing behavior if anyone out there is trying to use the SNN as requestor.  It would be a little odd to do this, and I haven't seen it in practice, but I think it would be a backwards-incompatible change if we dropped it.

Jing, are you still +1 for the v2 patch (pending fresh Jenkins run)?, Thanks for the update, Chris! 

For ImageServlet I have a question. Because ImageServlet is also used by Secondary NN for checkpointing. With the change in v2 is it possible that we can no longer detect wrong configuration for SNN during the startup?, That's a good question.  I believe we'll still have debugging information in that case thanks to this code in {{ImageServlet}}:

{code}
    LOG.info("ImageServlet rejecting: " + remoteUser);
{code}

{code}
    if (UserGroupInformation.isSecurityEnabled()
        && !isValidRequestor(context, request.getUserPrincipal().getName(),
            conf)) {
      String errorMsg = "Only Namenode, Secondary Namenode, and administrators may access "
          + "this servlet";
      response.sendError(HttpServletResponse.SC_FORBIDDEN, errorMsg);
      LOG.warn("Received non-NN/SNN/administrator request for image or edits from "
          + request.getUserPrincipal().getName()
          + " at "
          + request.getRemoteHost());
      throw new IOException(errorMsg);
    }
{code}

I guess another possibility would be to change the new debug log message in the catch block to warn level and include the values of {{DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY}} and {{DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY}}.

Let me know your thoughts, and if necessary, I can upload a v3.  Thanks again!, bq. I guess another possibility would be to change the new debug log message in the catch block to warn level and include the values of DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY and DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY.

Yeah, that will be helpful for debugging the issue. +1 after this change., Here is patch v3 with the improved logging.  I still retained logging of the full stack trace at debug level in case we ever need to find that.  Thanks again, Jing., +1, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679589/HDFS-7359.2.patch
  against trunk revision 1831280.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestCheckpoint
                  org.apache.hadoop.hdfs.server.balancer.TestBalancer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8658//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8658//console

This message is automatically generated., The test failures are unrelated.  {{TestBalancer}} has been flaky.  It's passing for me locally.  The {{TestCheckpoint}} failure repros on current trunk even without this patch.  We're still waiting on the Jenkins run for patch v3, which is currently in progress., The {{TestCheckpoint}} failure was introduced in HDFS-7333.  I filed HDFS-7361 to track fixing it., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679600/HDFS-7359.3.patch
  against trunk revision bc80251.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestCheckpoint

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.util.TestExactSizeInputStream
org.apache.haTests
org.apache.hadoop.hdfs.web.TestAuthFilter
org.apache.hadoop.hdfs.web.TestWebTests
org.apache.hadoop.hdfs.TesTests
org.apacheTests
org.apache.hadoop.hdfs.TestFSInputChecker
org.apache.hadoop.hdfs.serveTests
org.apache.hadoop.hdfs.server.Tests
org.apache.hadoop.hdfs.sTests
org.apache.hadoop.hdfs.server.namenode.TestNameNodeResourceChecker
org.apache.hadoop.hdfs.server.namenode.TestFsck
org.apache.hadoop.hdfs.TestClientReportBadBlock

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8661//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8661//console

This message is automatically generated., I think something confused the string parsing Jenkins does to search for timed out tests.  I reviewed the console output, and I didn't see any evidence that these tests had timed out.  I reran locally, and they were all fine.

I'll commit this later today., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12679600/HDFS-7359.3.patch
  against trunk revision bc80251.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestCheckpoint

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8662//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8662//console

This message is automatically generated., FAILURE: Integrated in Hadoop-trunk-Commit #6457 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6457/])
HDFS-7359. NameNode in secured HA cluster fails to start if dfs.namenode.secondary.http-address cannot be interpreted as a network address. Contributed by Chris Nauroth. (cnauroth: rev ba1d4ad25b301f7247f3f23df15e7f800e50feed)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestNNWithQJM.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, I committed this to trunk, branch-2 and branch-2.6.  Thank you to Haohui, Jing and Jitendra for code reviews., SUCCESS: Integrated in Hadoop-Yarn-trunk #735 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/735/])
HDFS-7359. NameNode in secured HA cluster fails to start if dfs.namenode.secondary.http-address cannot be interpreted as a network address. Contributed by Chris Nauroth. (cnauroth: rev ba1d4ad25b301f7247f3f23df15e7f800e50feed)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestNNWithQJM.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1925 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1925/])
HDFS-7359. NameNode in secured HA cluster fails to start if dfs.namenode.secondary.http-address cannot be interpreted as a network address. Contributed by Chris Nauroth. (cnauroth: rev ba1d4ad25b301f7247f3f23df15e7f800e50feed)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestNNWithQJM.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1949 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1949/])
HDFS-7359. NameNode in secured HA cluster fails to start if dfs.namenode.secondary.http-address cannot be interpreted as a network address. Contributed by Chris Nauroth. (cnauroth: rev ba1d4ad25b301f7247f3f23df15e7f800e50feed)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestNNWithQJM.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java
]