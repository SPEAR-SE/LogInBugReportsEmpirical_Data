[Hi Team,

Any Idea when this will be assigned to a developer for the Fix.
Thanks., hi [~Vinod08]
Looks like you have only one valid datanode assigned for this block it got failed. So write will fail.
bq. There are 1 datanode(s) running and 1 node(s) are excluded in this operation.

What you are suspecting as issue here ? , Hi nijel,

Thanks for replying. Can you pls tell me how many valid datanodes should be assigned for it to succeed?
There was a similar issue with PipelineForAppendOrRecovery. Then it was fixed in - https://issues.apache.org/jira/browse/HDFS-3384

Thanks., Hi Team,

Could this be a configuration issue for hadoop. Can you pls point us to the configuration that we should be looking at in order to correct this.
Thanks a lot!
, Hi Team,
Has anyone got a clue about this issue. Currently I am facing the same issue while inserting data into table hive table with ORC file formats.
ooks like this is a generic issue for below case :

1. create external table T1(col A , B , C) with partition on (col A) stored as ORC . Load table with substantial data. in my case around 85 GB data.

2. Create external table T2(col A,B,C) with partition on (Col B) stored as ORC. Load table T2 from T1 with dynamic partition.

Output :- Premature EOF exception

Thanks, I don't see a bug reported here - the report says the write was done with a single replica and that the single replica was manually corrupted.

Please post to user@hadoop.apache.org for problems observed in usage.

If you plan to reopen this, please post precise steps of how the bug may be reproduced.

I'd recommend looking at your NN and DN logs to trace further on what's happening.]