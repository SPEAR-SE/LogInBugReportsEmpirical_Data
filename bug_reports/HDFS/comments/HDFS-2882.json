[Trying to think through what the correct/expected behavior is here:

Scenario 1: user configures DN to point to a single cluster which doesn't match its storage
Possible results:
1) DN keeps running, with no block pools
2) DN shuts down once its one-and-only service fails.

I think #2 is fairly clearly correct here.

----

Scenario 2: user configures DN to point to one NN. The user adds an additional nameservice to the config and issues a -refreshNamenodes call. The newly added nameservice is from the wrong cluster.
Possible results:
1) DN tries to connect and fails. It logs a message indicating this, but keeps running with its existing service.
2) DN tries to connect and fails. It aborts the whole datanode.

I think #1 is correct here.

----
Scenario 3: user configures DN to point to two different NNs which are on different clusters, and starts up.
Possible results:
1) DN connects to cluster 1, and joins the cluster. Service to Cluster 2 fails, but the DN stays running. The admin may issue refreshNodes to try to connect again.
2) DN connects to cluster 1 and joins the cluster. When service to cluster 2 is rejected, the DN shuts down.

It's unclear what the correct results are here. My leaning is towards #2, but not certain., attaching some preliminary work for this. Fixes several of the potential NPEs and introduces a State enum for BPServiceActor, so as to distinguish failed from successful exit. the test case is still in-progress (it's not yet a real regression test), Hi Todd.
Any further updates on this ? If you want I would like to write test cases., Hi Ashish. If you want to continue work on this patch that would be great. I haven't had any time to do further work since my last update., Hi Ashish,

Do you mind if I take a look at this one?, I tried to reproduce this in the following ways:

*Changing the BPID*

I started the cluster, and then killed the datanode and ran this:
{code}
mv $rdata1/current/$old_bpid $r/data1/current/$new_bpid
sed --in-place "s/blockpoolID=$old_bpid/blockpoolID=$new_bpid/" $r/data1/current/$new_bpid/VERSION
mv $r/data2/current/$old_bpid $r/data2/current/$new_bpid
sed --in-place "s/blockpoolID=$old_bpid/blockpoolID=$new_bpid/" $r/data2/current/$new_bpid/VERSION
{code}
Then I restarted the datanode.
Result: DataNode created a new directory for {{$old_bpid}} and started up normally.

*Simulating your "disk unwritable because it's full" problem.*

{code}
[stop datanode]
rm -rf $r/data[12]/*
sudo chown root $r/data[12]
[restart datanode]
{code} 

Result: DataNode startup failed with this: 
{code}
15:59:55,978 FATAL DataNode:668 - Initialization failed for block pool Block pool BP-164367671-127.0.0.1-1358201991231 (storage id DS-226544464-  127.0.0.1-6100-1358380730839) service to     localhost/127.0.0.1:6000
java.io.IOException: All specified directories are not accessible or do not exist.
at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:137)
at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:234)
...
{code}

It seems like this is the result of {{dfs.datanode.failed.volumes.tolerated}} defaulting to 0.  Since we don't tolerate any volumes failing, and not being able to create the directories      causes the volume to fail, the DN startup aborts.

*Changing the NamespaceID of the Datanode*

{code} 
[start the cluster]
[stop the DN]
sed --in-place "s/namespaceID=$old_nsid/namespaceID=$new_nsid/" $r/data*/current/*/current/VERSION
[restart the DN]
{code}
Result: I got this exception:

{code}
17:17:01,898 FATAL DataNode:668 - Initialization failed for block pool Block pool BP-164367671-127.0.0.1-1358201991231 (storage id DS-358917359-127.0.0.1-6100-1358385257855) service to       localhost/127.0.0.1:6000
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /r/data1/current/BP-164367671-127.0.0.1-1358201991231 is in an inconsistent state: namespaceID is incompatible    with others.
        at org.apache.hadoop.hdfs.server.common.Storage.setNamespaceID(Storage.java:1091)
        at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.setFieldsFromProperties(BlockPoolSliceStorage.java:218)
{code}

So I'm not sure under what conditions the DN still starts up when the block pools fail to initialize.  I guess if you change {{dfs.datanode.failed.volumes.tolerated}} from the default of 0   to something else, you might be able to get this behavior.  But at that point, it seems like you asked for that behavior.  Is it expected that people will change this from the default, and that we should make that configuration option not apply on startup?, Attaching the patch based on todd's initial work. 
handles the scenarios mentioned in first comment of todd.

scenario 3 will be handled using #1, as no special code required for this.

Please review., Did you reproduce the problem?  If so, what were the steps to reproduce?

Also, your patch seems to make the DataNode loop endlessly trying to initialize any block pools that don't come up.  I don't think that's what we want to do here., bq. Did you reproduce the problem? If so, what were the steps to reproduce?
Please check the test. I had just reproduced cases mentioned by Todd.

bq. Also, your patch seems to make the DataNode loop endlessly trying to initialize any block pools that don't come up. I don't think that's what we want to do here.
No. In case of multiple namenodes nameservice, if any one of the namenode is able to connect and BPOS is initialized, then only retry will be infinite for the other namenode. Retry to initialize BPOS will continue until both Namenodes failed to initialize else BPOS will exit.

One more thing {{BPServiceActor#retrieveNamespaceInfo()}} is in inifinite loop, yes this can cause initialize to goto infinite loop, if namenode was down/not responding. But this is not changed in my patch., Hi [~tlipcon], could you take a look at the patch, as the patch is on top of your work. Thanks, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12597670/HDFS-2882.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
                  org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot
                  org.apache.hadoop.hdfs.security.TestDelegationToken
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration
                  org.apache.hadoop.hdfs.TestDFSRollback
                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery
                  org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks
                  org.apache.hadoop.hdfs.TestDecommission
                  org.apache.hadoop.hdfs.server.datanode.TestHSync
                  org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReplication
                  org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement
                  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart
                  org.apache.hadoop.hdfs.server.blockmanagement.TestHeartbeatHandling
                  org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner
                  org.apache.hadoop.hdfs.server.datanode.TestDiskError
                  org.apache.hadoop.hdfs.TestFileCreation
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations
                  org.apache.hadoop.hdfs.server.datanode.TestTransferRbw
                  org.apache.hadoop.hdfs.TestDFSStartupVersions
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
                  org.apache.hadoop.net.TestNetworkTopology
                  org.apache.hadoop.hdfs.TestFileCorruption
                  org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
                  org.apache.hadoop.hdfs.TestDFSUpgrade
                  org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager
                  org.apache.hadoop.hdfs.TestDatanodeConfig
                  org.apache.hadoop.hdfs.TestEncryptedTransfer
                  org.apache.hadoop.hdfs.TestReplication
                  org.apache.hadoop.hdfs.TestSafeMode
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestBackupNode
org.apache.hadoop.hdfs.server.datanode.TestBlockReport
org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4955//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4955//console

This message is automatically generated., Attaching the latest re-based patch for the issue.

Please review., Cleaned up some unnecessary changes., latest: Made branch-2 compatible, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12615082/HDFS-2882.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
                  org.apache.hadoop.hdfs.server.datanode.TestHSync
                  org.apache.hadoop.hdfs.TestFileCorruption
                  org.apache.hadoop.hdfs.TestDecommission
                  org.apache.hadoop.hdfs.TestDFSRollback
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
                  org.apache.hadoop.hdfs.TestDatanodeConfig
                  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart
                  org.apache.hadoop.hdfs.server.datanode.TestDiskError
                  org.apache.hadoop.hdfs.server.datanode.TestTransferRbw
                  org.apache.hadoop.hdfs.security.TestDelegationToken
                  org.apache.hadoop.hdfs.TestSafeMode
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.server.namenode.TestPathBasedCacheRequests
                  org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery
                  org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks
                  org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner
                  org.apache.hadoop.hdfs.TestDFSUpgrade
                  org.apache.hadoop.hdfs.server.blockmanagement.TestHeartbeatHandling
                  org.apache.hadoop.hdfs.TestReplication
                  org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReplication
                  org.apache.hadoop.hdfs.TestEncryptedTransfer
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration
                  org.apache.hadoop.hdfs.TestFileCreation
                  org.apache.hadoop.hdfs.server.datanode.TestBlockReport
                  org.apache.hadoop.net.TestNetworkTopology
                  org.apache.hadoop.hdfs.TestDFSStartupVersions
                  org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestAddBlockRetry
org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5521//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5521//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12615095/HDFS-2882.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestSafeMode
                  org.apache.hadoop.hdfs.TestEncryptedTransfer
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.server.datanode.TestFsDatasetCache
                  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart
                  org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReplication
                  org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner
                  org.apache.hadoop.hdfs.TestFileCreation
                  org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement
                  org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot
                  org.apache.hadoop.hdfs.TestDFSStartupVersions
                  org.apache.hadoop.hdfs.TestDatanodeConfig
                  org.apache.hadoop.hdfs.TestDFSRollback
                  org.apache.hadoop.hdfs.TestDecommission
                  org.apache.hadoop.hdfs.server.datanode.TestDiskError
                  org.apache.hadoop.hdfs.server.datanode.TestTransferRbw
                  org.apache.hadoop.hdfs.TestFileCorruption
                  org.apache.hadoop.net.TestNetworkTopology
                  org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks
                  org.apache.hadoop.hdfs.server.datanode.TestBlockReport
                  org.apache.hadoop.hdfs.security.TestDelegationToken
                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery
                  org.apache.hadoop.hdfs.server.blockmanagement.TestHeartbeatHandling
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
                  org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes
                  org.apache.hadoop.hdfs.TestReplication
                  org.apache.hadoop.hdfs.TestDFSUpgrade
                  org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration
                  org.apache.hadoop.hdfs.server.datanode.TestHSync

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestBackupNode
org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5523//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5523//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12615100/HDFS-2882.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
                  org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot
                  org.apache.hadoop.hdfs.security.TestDelegationToken
                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration
                  org.apache.hadoop.hdfs.TestDFSRollback
                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery
                  org.apache.hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks
                  org.apache.hadoop.hdfs.TestDecommission
                  org.apache.hadoop.hdfs.server.datanode.TestHSync
                  org.apache.hadoop.hdfs.server.blockmanagement.TestPendingReplication
                  org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement
                  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart
                  org.apache.hadoop.hdfs.server.blockmanagement.TestHeartbeatHandling
                  org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner
                  org.apache.hadoop.hdfs.server.datanode.TestDiskError
                  org.apache.hadoop.hdfs.TestFileCreation
                  org.apache.hadoop.hdfs.server.datanode.TestBlockReport
                  org.apache.hadoop.hdfs.server.datanode.TestTransferRbw
                  org.apache.hadoop.hdfs.TestDFSStartupVersions
                  org.apache.hadoop.net.TestNetworkTopology
                  org.apache.hadoop.hdfs.TestFileCorruption
                  org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
                  org.apache.hadoop.hdfs.TestDFSUpgrade
                  org.apache.hadoop.hdfs.TestDatanodeConfig
                  org.apache.hadoop.hdfs.TestEncryptedTransfer
                  org.apache.hadoop.hdfs.TestReplication
                  org.apache.hadoop.hdfs.TestSafeMode
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestBackupNode
org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5524//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5524//console

This message is automatically generated., Attaching the patch with Failure fixes. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12615119/HDFS-2882.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.net.TestNetworkTopology

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5525//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5525//console

This message is automatically generated., One more updated patch for test fix. Please review, If you look at the original description of this JIRA, by Todd, it looks like this:

{code}
I started a DN on a machine that was completely out of space on one of its drives. I saw the following:
2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-12978
42002148) service to styx01.sf.cloudera.com/172.29.5.192:8021
java.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp
at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)
but the DN continued to run, spewing NPEs when it tried to do block reports, etc. This was on the HDFS-1623 branch but may affect trunk as well.
{code}

His concern was that the block pool failed to initialize, but the the DN continued to start up anyway, leading to a system that was not functional.  I tried to reproduce this on trunk (as opposed to the HDFS-1623 branch that Todd was using).  I was unable to reproduce this behavior: every time I got the block pool to fail to initialize, the DN also did not start up.  My theory is that this was either a bug that affected only the HDFS-1623 branch, or a bug that is related to a race condition that is very hard to reproduce.  You can see how I tried to reproduce it here: https://issues.apache.org/jira/browse/HDFS-2882?focusedCommentId=13555717&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13555717

Vinay, I don't have a clear idea of what your patch is trying to do.  I can see that it adds a retry state machine.  But as you yourself commented, BPServiceActor#retrieveNamespaceInfo() already loops until the NameNode responds.  So why do we need another retry mechanism?

Also, when I asked you whether you had reproduced Todd's problem, I didn't mean in a unit test.  I meant have you started up the DN and had it fail to initialize a block pool, but continue to start?

I also wonder if any of this is addressed in the HDFS-2832 branch, which changes the way DataNode storage ID is handled, among other things., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12615147/HDFS-2882.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5527//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5527//console

This message is automatically generated., Hi colin, 
Thanks for taking a look at patch and sorry for confusing you.

Yes.. its able to reproduce easily in only HA installation.
1. Make one of the data directory unwritable
2. Restart the datanode

Here blockpool initialization will fail for first name node connected and that BPSA will exit. But for second namenode it will not try to initialize block pool. As namespace info was not null. . 
And it tries to send heartbeats and throws NPEs continously. 

Todd suggested 3 scenarios to be handled in this case. And he proposed an initial patch.  I just continued the approach. , All of the scenarios Todd outlined were misconfigurations.  Do you believe this problem can occur just because a directory is unwritable?

I will try to reproduce this tomorrow with an HA cluster., bq. All of the scenarios Todd outlined were misconfigurations.
These are not exactly mis-configurations, May be possible cases where we need to decide datanode should go down/keep running. Even if its mis-configuration then definitely datanode should go down right?

bq. Do you believe this problem can occur just because a directory is unwritable?
Yes I do. In the todd's case issue came due to disk full. Another issue HDFS-5529 raised by [~brahma] is due to disk error. , bq. These are not exactly mis-configurations, May be possible cases where we need to decide datanode should go down/keep running.

No, they are exactly misconfigurations.  Remember them again:

{code}
Scenario 1: user configures DN to point to a single cluster which doesn't match its storage
Scenario 2: user configures DN to point to one NN. The user adds an additional nameservice to the config and issues a -refreshNamenodes call. The newly added nameservice is from the wrong cluster.
Scenario 3: user configures DN to point to two different NNs which are on different clusters, and starts up.
{code}

None of those are correct configurations.

bq. Another issue HDFS-5529 raised by Brahma is due to disk error.

That is a good point.  Based on the log message he is seeing, the DN definitely is continuing on for some length of time after a block pool has failed, which seems related to this bug.

I definitely think there are issues around DataNode / block pool lifecycle, but I don't have a good handle on what they are yet.  I need to review what the expected behavior is in these scenarios., Ok. agree its a misconfiguration from user. In these cases datanode should decide whether to continue with failed state or to exit. 

bq. I definitely think there are issues around DataNode / block pool lifecycle, but I don't have a good handle on what they are yet. I need to review what the expected behavior is in these scenarios.
Sure.!, Hi [~tlipcon],  As you started the work on this jira, could you take a look at patch..? 
Thanks, Sorry that I haven't had time to review this in the last week.  I've been busy.  If someone else wants to review it, I am fine with that., Ok Colin,

Is anyone else could review the patch. ?
Thanks.
, Vinay, the patch cannot be applied anymore.  Could you update it?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12615147/HDFS-2882.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6760//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12615147/HDFS-2882.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6761//console

This message is automatically generated., Attaching the updated patch., Patch looks good.  Some minor comments:
- In BPServiceActor.isAlive(), I think we should still check if bpThread.isAlive().  No?
- In DataNode.getVolumeInfo() and getBlockLocalPathInfo(..), let's use Preconditions instead of assert.
- BPServiceActor.getRunningState() is not used.  Let's remove it., Thanks [~szetszwo] for the comments.
Updated the patch with comments fixed., +1 patch looks good., Thanks [~szetszwo] for the review. Will commit patch shortly., Committed to trunk, branch-2 and branch-2.4, SUCCESS: Integrated in Hadoop-trunk-Commit #5583 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5583/])
HDFS-2882. DN continues to start up, even if block pool fails to initialize (Contributed by Vinayakumar B) (vinayakumarb: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1590941)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSNNTopology.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMultipleRegistrations.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1746 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1746/])
HDFS-2882. DN continues to start up, even if block pool fails to initialize (Contributed by Vinayakumar B) (vinayakumarb: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1590941)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSNNTopology.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMultipleRegistrations.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #555 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/555/])
HDFS-2882. DN continues to start up, even if block pool fails to initialize (Contributed by Vinayakumar B) (vinayakumarb: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1590941)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSNNTopology.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMultipleRegistrations.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1772 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1772/])
HDFS-2882. DN continues to start up, even if block pool fails to initialize (Contributed by Vinayakumar B) (vinayakumarb: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1590941)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSNNTopology.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMultipleRegistrations.java
, I'm linking this to HDFS-7714, where I reported that a bug in this part of the code can cause a DataNode process to remain running in a "half-alive" state registered to only one NameNode with no opportunity to re-register to the other one.  I don't think this patch introduced the problem though.]