[Brahma, in the above description, I assume that you excluded DN1 by adding it to the excludes file? Did you add it to that file on both of the NN machines?

The issue is that some dsfadmin commands need to perform client-side failover and talk to the appropriate NN, while others should actually be run against both NNs., HI Aaron T.Myers,
thanks for taking look.
{quote}
Brahma, in the above description, I assume that you excluded DN1 by adding it to the excludes file? Did you add it to that file on both of the NN machines?
{quote}

Yes,I excluded DN1 by adding it to the exclude file and I added in both of the NN machines.

{quote}
The issue is that some dsfadmin commands need to perform client-side failover and talk to the appropriate NN, while others should actually be run against both NNs.
{quote}

yes,issue with dfsadmin commands which need to perform client-side failover.., {quote}
The issue is that some dsfadmin commands need to perform client-side failover and talk to the appropriate NN, while others should actually be run against both NNs.
{quote}

But if we do client-side failover also,I think BNN will not come know.
We need to execute command on both NN's., From the issue What I understud is,

We have to execute this particular command on both the nodes. 
Otherwise, even though we make it failover work here, still SNN will not know about the excluded nodes as it did not get any refresh nodes command.

Do we need to handle from DFSAdmin separate for this kind of commands separately?
ex: we can iterate all the nns which are available in list and send the refreshnodes commands.

other option might be like we have to perform refresh nodes on switch.
We have to think about the impacts on that.

 , {quote}other option might be like we have to perform refresh nodes on switch.{quote}
one problem here is, if we just configure excludes and not executed refreshNodes command yet and just switch happend then this may perform refereshNodes simply. So, This may not be the preferable way to go.

We can think about the other option in the above comment., {quote}We have to execute this particular command on both the nodes.
Otherwise, even though we make it failover work here, still SNN will not know about the excluded nodes as it did not get any refresh nodes command.
{quote}
Until SNN gets refreshnodes command SNN will not be aware of decommission of DN. But if we send the command to SNN as well which will cause the REPLICATION triggered in the SNN as well. Even though DN will not accept any requests from SNN, but SNN will be trying to replicate the blocks which will increase the load on SNN + N/W load + DN load as well.

This is my initial opinion, Please let me know if my opinion is wrong. 
, I have 2 suggestions to handle the same issue

1. Persist the NODE_DECOMMISSIONED by active so SNN will get node DECOMMISSIONED.

2. I would like to with your first opinion with SAFEMODE check in replication(or move replication to Active service)., bq. Persist the NODE_DECOMMISSIONED by active so SNN will get node DECOMMISSIONED.

I'm hesitant to go with this suggestion. How would differences be rectified between what's persisted in the edit log and what's present in the excluded hosts file?

bq. I would like to with your first opinion with SAFEMODE check in replication(or move replication to Active service).

I'm afraid I don't follow this. What do safemode or replication have to do with this issue?

I think my preference here is to require the admin to keep the excluded hosts files in sync across the machines (most deployments already have some way of keeping config files in sync), and then just make dfsadmin iterate over all configured NNs when the refreshNodes command is run. There may be other commands that should have similar support. For example, if an administrator runs `hdfs dfsadmin -safemode enter' and provides a logical HA URI, should only the active NN be put in SM? Or should both?, Oh Yesterday was my bad day @work :(

Let me correct my second opinion first 

{quote}2. I would like to with your first opinion with STANDBY check in replication(or move replication to Active service).
{quote}

Here first opinion I meant here is Uma Maheshwara Rao's opinion.
I accept that DFSAdmin command need to be sent for both NN. This can solve the problem here.

That's u also meant Arron.
And I would like to add Standby check at replication monitor to avoid load in cluster.

My First opinion

Before to support my opinion of persisting the node decommisioned consider the scenarios where 
{quote}
 Decommission command is given by admin but Standby NN is down ...etc  Scenarios where Standby NN is not available when issuing command.
{quote}

DFSAdmin will just create a DFS client and calls refreshNodes()(Client will retry for NN 10 times by default not forever if I am not wrong), in this case Standby NN will not get to know the Decommission request and when Standby is up and switches to active???

I guess It will consider the decommissioned node as well.

{quote}
I'm hesitant to go with this suggestion. How would differences be rectified between what's persisted in the edit log and what's present in the excluded hosts file?
{quote}

Initially I also thought of same thing. Since we are persisting the message in different forms, but consider 

{quote}
Admin could have just configured the exclude nodes in the file, but he wouldn't have issued refreshnode command. 
{quote}

By persisting into edit logs we can be sure of which DN is decommissioned? Not only by Standby NN but also when Standalone NN restarts. 

Or is there any way NN can identify Decommissioned node without persisting?

Here I mean to persist all the stages of recommission and decommission too.

Please correct me If I am not correct.
I will be very glad to hear more about my opinion of persisting., I would like to add some scenarios which can be handled by persisting

1. If N/W is unreachable from DFSClient issuing refreshNodes and Standby.
2. If N/W is unreachable from DFSClient issuing refreshNodes and active, refreshnodes command reached Standby NN and it marks DECOMMISSION in progress and NN N/W is reachable.

and other N/W down scenarios as well., bq. And I would like to add Standby check at replication monitor to avoid load in cluster.

Got it. This seems like a separate issue from what's being discussed here, though, and so should probably be done as a separate JIRA. Do you agree?

bq. By persisting into edit logs we can be sure of which DN is decommissioned? Not only by Standby NN but also when Standalone NN restarts.

The question that I have is still "How would differences be rectified between what's persisted in the edit log and what's present in the excluded hosts file?" Imagine that some host is not present in the excluded hosts file, but a decommission action for that host is present in the edit log. Given that edit logs are occasionally merged into an fsimage and the edit logs discarded, this would imply that we'd need to introduce a new section into the fsimage for per-host DN status. This means that we'd end up with two potentially out of sync lists of DN decommission status: one in the excludes file, the other in this new section of the fsimage file.

My point is that I think persisting DN decommission status to the edit log / fsimage is not an unreasonable idea, but it does seem like an idea that's incompatible with the excluded hosts config file. Given that, I'm still in favor of just requiring the admin keep the excluded hosts files in sync, and call refreshNodes on both NNs from DFSAdmin. I think this argument is further supported by the fact that the active/standby NN having an out of sync view of DN decommission status isn't actually that big of a problem. Yes, it might result in some unnecessary replication traffic, but it shouldn't result in data loss or unavailability, since DNs already ignore replication commands from anything but the active NN., {quote}I think my preference here is to require the admin to keep the excluded hosts files in sync across the machines (most deployments already have some way of keeping config files in sync), and then just make dfsadmin iterate over all configured NNs when the refreshNodes command is run.{quote}
Attaching a patch which this change. Please review., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12601581/HDFS-3744.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4928//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4928//console

This message is automatically generated., HDFS-6507 improved all DFSAdmin commands for the HA cluster.
 resolving as duplicate]