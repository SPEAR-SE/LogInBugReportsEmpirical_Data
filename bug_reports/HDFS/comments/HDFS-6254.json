[A lot more information is going to be needed to look at this, especially hadoop version.

But... any segment fault is invariably one of

# A sign that you're having DRAM errors. 
# a sign of a JVM problem
# a sign of an OS problem.

I'd guess for #2, with the fix being: update your JVM to version 7u51 -or the openjdk one. If it still surfaces there, this is going to have to be a problem to take up with the JVM team, because it's too removed from the Java code for the Hadoop team to address.

, Shortening description and moving backtrace to comment:

Core was generated by `./test -d'.
Program terminated with signal 6, Aborted.
#0 0x0000003735c328e5 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.107.el6_4.5.x86_64 jdk-1.7.0_21-fcs.x86_64 keyutils-libs-1.4-4.el6.x86_64 krb5-libs-1.10.3-10.el6_4.6.x86_64 libcom_err-1.41.12-14.el6_4.4.x86_64 libgcc-4.4.7-3.el6.x86_64 libselinux-2.0.94-5.3.el6_4.1.x86_64 libstdc++-4.4.7-3.el6.x86_64 libxml2-2.7.6-12.el6_4.1.x86_64 zlib-1.2.3-29.el6.x86_64
(gdb) bt
#0 0x0000003735c328e5 in raise () from /lib64/libc.so.6
#1 0x0000003735c340c5 in abort () from /lib64/libc.so.6
#2 0x00007f1beacba865 in os::abort(bool) () from /usr/java/jdk1.7.0_21/jre/lib/amd64/server/libjvm.so
#3 0x00007f1beae1ab77 in VMError::report_and_die() () from /usr/java/jdk1.7.0_21/jre/lib/amd64/server/libjvm.so
#4 0x00007f1beacbe370 in JVM_handle_linux_signal () from /usr/java/jdk1.7.0_21/jre/lib/amd64/server/libjvm.so
#5 <signal handler called>
#6 0x00007f1beaad8691 in jni_invoke_nonstatic(JNIEnv_, JavaValue, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*) () from /usr/java/jdk1.7.0_21/jre/lib/amd64/server/libjvm.so
#7 0x00007f1beaaf3ed0 in jni_CallBooleanMethodV () from /usr/java/jdk1.7.0_21/jre/lib/amd64/server/libjvm.so
#8 0x00007f1beb2a1487 in invokeMethod (env=0x2fd01d8, retval=0x7fff94dcb3f0, methType=INSTANCE, instObj=0x7f1bb8009640, 
className=0x7f1beb2a8350 "org/apache/hadoop/fs/FileSystem", methName=0x7f1beb2a8674 "exists", 
methSignature=0x7f1beb2a8e90 "(Lorg/apache/hadoop/fs/Path;)Z")
at /home/hkx/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:252
#9 0x00007f1beb2a39b5 in hdfsExists (fs=0x7f1bb8009640, path=<value optimized out>)
at /home/hkx/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.c:1366
#10 0x0000000000411561 in CChildProcess::CombineFile (this=0x7fff94dccd20, objFileName=..., vcSortInfo=<value optimized out>, 
files_map=Traceback (most recent call last):
File "/usr/lib64/../share/gdb/python/libstdcxx/v6/printers.py", line 382, in children
valuetype = self.val.type.template_argument(1)
RuntimeError: No type named hdfsFile_internal.
std::map with 0 elements, fs=0x7f1bb8009640) at CChildProcess.cpp:267
#11 0x00000000004120f0 in CChildProcess::Run (this=0x7fff94dccd20) at CChildProcess.cpp:116
#12 0x000000000040d8cb in CProcessMgr::StartAProcess (this=0x19d1820, strCustomID="(", pChildProcess=0x7fff94dccd20)
at CProcessMgr.cpp:112
#13 0x000000000041091e in CTimerService::Run (this=0x7fff94dcd280) at CTimerService.cpp:46
#14 0x000000000040db74 in main (argc=<value optimized out>, argv=<value optimized out>) at main.cpp:92
(gdb) quit
, I see a hint in the backtrace that this is 2.2.0:

{code}
#9 0x00007f1beb2a39b5 in hdfsExists (fs=0x7f1bb8009640, path=<value optimized out>)
at /home/hkx/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.c:1366
{code}

[~huangkx], do you happen to know if this happens with the recently release 2.4.0 or current trunk too?

This is our C code calling back into the Java layer for {{FileSystem#exists}}, so it's possible we're doing something wrong there., I haven't test 2.4.0 or current trunk yet.I'll attach my test result as soon as possible., Steve Loughran, #1. Updated jdk version to 7u55, also segment fault.
Chris Nauroth ,   #2. Updated hadoop version to 2.4.0, also segment fault.

No matter which version of hadoop i use, hdfsConnect() never return error or NULL, until i call hdfsCreateDirectory() or hdfsExists(), is it right ?, Lastest backtrace as followed:

#0  0x0000003735c328e5 in raise () from /lib64/libc.so.6
#1  0x0000003735c340c5 in abort () from /lib64/libc.so.6
#2  0x00007fddc4901535 in os::abort(bool) () from /usr/java/jdk1.7.0_55/jre/lib/amd64/server/libjvm.so
#3  0x00007fddc4a80457 in VMError::report_and_die() () from /usr/java/jdk1.7.0_55/jre/lib/amd64/server/libjvm.so
#4  0x00007fddc4905ebf in JVM_handle_linux_signal () from /usr/java/jdk1.7.0_55/jre/lib/amd64/server/libjvm.so
#5  <signal handler called>
#6  0x00007fddc471dac8 in jni_invoke_nonstatic(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*) () from /usr/java/jdk1.7.0_55/jre/lib/amd64/server/libjvm.so
#7  0x00007fddc4730629 in jni_CallBooleanMethodV () from /usr/java/jdk1.7.0_55/jre/lib/amd64/server/libjvm.so
#8  0x00007fddc4f59847 in invokeMethod (env=0x15de9e8, retval=0x7fffa6d2bbd0, methType=INSTANCE, instObj=0x2074998, 
    className=0x7fddc4f603d0 "org/apache/hadoop/fs/FileSystem", methName=0x7fddc4f60692 "mkdirs", 
    methSignature=0x7fddc4f61038 "(Lorg/apache/hadoop/fs/Path;)Z")
    at /home/hkx/hadoop-2.4.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:252
#9  0x00007fddc4f5affa in hdfsCreateDirectory (fs=0x2074998, path=0x7fffa6d2c308 "/tmp/root/00000629/")
    at /home/hkx/hadoop-2.4.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.c:1865
#10 0x00000000004115f3 in CChildProcess::CombineFile (this=0x7fffa6d2d510, objFileName=..., vcSortInfo=<value optimized out>, 
    files_map=Traceback (most recent call last):
  File "/usr/lib64/../share/gdb/python/libstdcxx/v6/printers.py", line 382, in children
    valuetype = self.val.type.template_argument(1)
RuntimeError: No type named hdfsFile_internal.
std::map with 0 elements, fs=0x2074998) at CChildProcess.cpp:283
#11 0x00000000004120f4 in CChildProcess::Run (this=0x7fffa6d2d510) at CChildProcess.cpp:122
#12 0x000000000040d8f3 in CProcessMgr::StartAProcess (this=0xa34f30, strCustomID="\210", pChildProcess=0x7fffa6d2d510)
    at CProcessMgr.cpp:114
#13 0x000000000041094e in CTimerService::Run (this=0x7fffa6d2da70) at CTimerService.cpp:46
#14 0x000000000040dba4 in main (argc=<value optimized out>, argv=<value optimized out>) at main.cpp:92, thanks for the details. Looking at the latest trace, it's when then native code calls back into java for a mkdirs(path) operation -it could be that something being passed down is null or otherwise uninitialized.

That the limit of my JNI knowledge -anyone else got any ideas?, The line numbers in the latest backtrace match up with the current trunk code.  Thank you for retesting.

{{hdfsConnect}} is supposed to return {{NULL}} if the connection fails.  This is covered by tests in a few places, such as test_libhdfs_read.c:

{code}
    hdfsFS fs = hdfsConnect("default", 0);
    if (!fs) {
        fprintf(stderr, "Oops! Failed to connect to hdfs!\n");
        exit(-1);
    } 
{code}

[~huangkx], are you not seeing a {{NULL}} return in your environment?  Can you post a minimal code snippet that repros the problem for you?, To clarify my earlier comment, the test is probably not actually covering the case of a connection failure, because it waits for the cluster to be running in the background before starting.  It would still be interesting to see the code sample that you're using though., test.cpp:
int main()
{
	hdfsFS fs = NULL;
	fs = hdfsConnect("172.16.19.222", 8020); // romote host is unavailable
	if (!fs)
	{
		printf("HDFS connect error!\n");
		return -1;
	}
	else 
	{
		printf("HDFS connect OK.\n");
	}
	if (hdfsCreateDirectory(fs, "/tmp/root/") != 0 )
	{
		printf("Create dir error!\n");
		return -1;
	}
	else
	{
		printf("Create dir Ok.\n");
		return 0;
	}
}

What puzzled me is following two scenarios:

#scenario 1:compile test.cpp & run
[root@datanode test]# ./a.out 
2014-04-22 09:48:34,106 WARN  util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
HDFS connect OK.
hdfsCreateDirectory(/tmp/root/): FileSystem#mkdirs error:
java.net.ConnectException: Call From datanode2/172.16.18.238 to 172.16.19.222:8020 failed on connection exception: java.net.ConnectException: Connection timed out; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
        at org.apache.hadoop.ipc.Client.call(Client.java:1351)
        at org.apache.hadoop.ipc.Client.call(Client.java:1300)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy9.mkdirs(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy9.mkdirs(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:467)
        at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2394)
        at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2365)
        at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:817)
        at org.apache.hadoop.hdfs.DistributedFileSystem$16.doCall(DistributedFileSystem.java:813)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:813)
        at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:806)
        at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1933)
Caused by: java.net.ConnectException: Connection timed out
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
        at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
        at org.apache.hadoop.ipc.Client.call(Client.java:1318)
        ... 19 more
Create dir error!

#scenario  2:compile test.cpp & gdb
[root@datanode2 test3]# gdb a.out 
GNU gdb (GDB) Red Hat Enterprise Linux (7.2-56.el6)
Copyright (C) 2010 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-redhat-linux-gnu".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /root/test3/a.out...done.
(gdb) l
1       //g++ -g test.cpp -L/home/hkx/hadoop-2.2.0-src/hadoop-dist/target/hadoop-2.2.0/lib/native -I/home/hkx/hadoop-2.2.0-src/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs -I/home/yjx/JDK/jdk1.6.0_24/include -I/home/yjx/JDK/jdk1.6.0_24/include/linux -lhdfs
2       #include <stdio.h>
3       #include <stdlib.h>
4       #include "hdfs.h"
5
6       int main()
7       {
8               hdfsFS fs = NULL;
9               fs = hdfsConnect("172.16.19.222", 8020);
10              if (!fs)
(gdb) b test.cpp:8
Breakpoint 1 at 0x40071c: file test.cpp, line 8.
(gdb) r
Starting program: /root/test3/a.out 
[Thread debugging using libthread_db enabled]

Breakpoint 1, main () at test.cpp:8
8               hdfsFS fs = NULL;
Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.80.el6_3.6.x86_64 jdk-1.7.0_55-fcs.x86_64 libgcc-4.4.6-4.el6.x86_64 libstdc++-4.4.6-4.el6.x86_64
(gdb) n
9               fs = hdfsConnect("172.16.19.222", 8020);
(gdb) p fs
$1 = (hdfsFS) 0x0
(gdb) n
[New Thread 0x7ffff23aa700 (LWP 3218)]
[New Thread 0x7ffff22a9700 (LWP 3219)]
[New Thread 0x7ffff21a8700 (LWP 3220)]
[New Thread 0x7ffff20a7700 (LWP 3221)]
[New Thread 0x7ffff0740700 (LWP 3222)]
[New Thread 0x7ffff063f700 (LWP 3223)]
[New Thread 0x7ffff053e700 (LWP 3224)]
[New Thread 0x7ffff043d700 (LWP 3225)]
[New Thread 0x7ffff033c700 (LWP 3226)]
[New Thread 0x7ffff023b700 (LWP 3227)]
[New Thread 0x7ffff013a700 (LWP 3228)]
[New Thread 0x7fffebfff700 (LWP 3229)]

Program received signal SIGSEGV, Segmentation fault.
0x00007ffff24155ca in ?? ()
(gdb) bt
#0  0x00007ffff24155ca in ?? ()
#1  0x0000000000615000 in ?? ()
#2  0x00000000f506d712 in ?? ()
#3  0x0000000000615000 in ?? ()
#4  0x00000000f5084c68 in ?? ()
#5  0x000000000000001b in ?? ()
#6  0x0000000000000000 in ?? ()
(gdb) quit
A debugging session is active.

        Inferior 1 [process 3214] will be killed.

Quit anyway? (y or n) y

In scenario 1, do return connect OK and fail in hdfsCreateDirectory(). 
In scenario 2, got sigsegv while connect to a host unavailable(not matter remote host is available or not).Seems can't debug it using gdb., [~huangkx], thanks again for the code sample.  I confirmed that the same behavior happens for me: run the program directly and I get the expected connection failure error, but run it under gdb and it dies with SIGSEGV.

I think the problem we're seeing is that the JVM actually traps SIGSEGV and runs its own signal handler to attempt recovery.  This is part of how the JVM interacts with the kernel to manage memory.  For example, see this code in OpenJDK:

http://hg.openjdk.java.net/jdk6/jdk6/hotspot/file/tip/src/os/linux/vm/os_linux.cpp#l3873

gdb's own signal handling appears to be interfering with this and choosing to suspend when it encounters a SIGSEGV.  This doesn't really represent a bug in the libhdfs code.  In fact, if I resume execution after the SIGSEGV by using the "fg" command, then it proceeds as expected, and I see the connection failure.

If you don't want gdb to suspend when it sees a SIGSEGV, then another option is to change its signal handling behavior by running this command:
{code}
(gdb) handle SIGSEGV nostop
{code}

BTW, I noticed a bug in the code sample you provided.  It does not call {{hdfsDisconnect}} on the allocated {{hdfsFS}} to free resources.  This is something that you'll want to add in your real code to avoid leaks.  Also note that the {{hdfsConnect}} function has been deprecated in favor of {{hdfsBuilderConnect}}.  For full details on this, refer to the hdfs.h header file.

At this point, I'm going to resolve this issue, because this behavior does not represent a bug in the libhdfs code.  I hope this was helpful., Chris Nauroth, thanks a lot for your explanation about gdb SIGSEGV handling & programming advice.
But <b>hdfsConnect</b> or <b>hdfsBuilderConnect</b> do return <b>not NULL</b> while namenode not connected in my test, not the same as the declaration in hdfs.h. Is it right ?, [~huangkx], yes, you're correct.  Neither {{hdfsConnect}} nor {{hdfsBuilderConnect}} actually initiates a network connection.  Instead, they "connect" an {{hdfsFS}} struct to an underlying Java {{FileSystem}}.  Basically, this stuff is all wrappers over {{FileSystem#get}} in the Java layer.  There is no actual interaction with the HDFS daemons until you use it with a function like {{hdfsCreateDirectory}}.  Our use of the word "connect" in the function names is perhaps slightly misleading., Chris Nauroth, thanks again for your explanation.I've retested my test case as followedï¼š
1. Use hdfsBuilderConnect(connect to a remote host unavailable).
2. Because of hdfsBuilderConnect not initiates a network connection, so continue to use hdfsCreateDirectory, then it failed with NoRouteToHostException.
3. But if reconnect, SIGSEGV catched.
Is it right ? test.cpp attached., Regarding this line of code:

{code}
		fs = hdfsBuilderConnect(bld);    //core dump here
{code}

This is attempting to use a {{hdfsBuilder}} that had already been used in an earlier {{hdfsBuilderConnect}} call.  This is invalid usage and could very well produce a segfault.  The docs for {{hdfsBuilderConnect}} state that it always frees the builder, regardless of the outcome:

{code}
     * The HDFS builder will be freed, whether or not the connection was
     * successful.
{code}

Related to this, the docs for {{hdfsFreeBuilder}} state that you normally wouldn't need to call it directly:

{code}
     * It is normally not necessary to call this function since
     * hdfsBuilderConnect frees the builder.
{code}

The only time you'd need to call {{hdfsFreeBuilder}} yourself directly is if you allocated one by calling {{hdfsNewBuilder}}, but then never called {{hdfsBuilderConnect}}.  I don't see a path through your code sample that can cause that to happen, so I expect you can remove the null checks on {{bld}} followed by the calls to {{hdfsFreeBuilder}}.

If there are any additional libhdfs usage questions, I recommend emailing user@hadoop.apache.org.  This is the best venue for support.  Jira is intended for reporting of bugs only, and in this case, we haven't seen any actual bugs in libhdfs., Sorry for misreport, and thank you very much for patience.]