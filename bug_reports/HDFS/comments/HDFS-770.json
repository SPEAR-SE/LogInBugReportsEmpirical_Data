[From the datanode log :

> 2009-11-13 06:18:21,965 DEBUG org.apache.hadoop.ipc.RPC: Call: sendHeartbeat 14
> 2009-11-13 06:19:38,081 DEBUG org.apache.hadoop.ipc.Client: IPC Client (47) connection to dfs.hadoop.tsukku.solatis/127.0.0.1:9000 from hadoop: closed

Note that there is no activity on DataNode for 77 seconds. There are number of possibilities, common one being GC. we haven't seen GC taking this long DN though.  

Assuming DN went to sleep for some reason, rest of the behaviour is expected. If you do expect such delays, what you need to increase is the read timeout for "responder thread" in DFSOutputStream (there is a config for generic read timeout that applies to sockets in many contexts)., I saw the same thing with hadoop 0.19 while doing heavy-weight writes.

{code}
09/10/29 01:05:18 WARN hdfs.DFSClient: DataStreamer Exception: java.net.SocketTimeoutException: 30000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.\
SocketChannel[connected local=/aa.bb.cc.dd:55040 remote=/ee.ff.gg.hh:50010]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:162)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2323)
{code}
, For what it's worth, the timeout problems I was having are gone after I have set "dfs.datanode.socket.write.timeout" to 630000, so it definitely appears as if the DN went to sleep for some reason., In my case, it seems a problem with my box.
It's running "2.6.12-1.1398_FC4smp #1 SMP Fri Jul 15 01:05:24 EDT 2005 x86_64 x86_64 x86_64 GNU/Linux" (uname -a) with 4 local hard disks.

I wrote a simple program (tried both Java and C) to write out a huge file, and the writes get blocked frequently after writing out 5~10GB of data. The block was sometimes as long as 18 seconds.

Tried it on a newer box and the problem disappeared. The block is in "FileOutputStream.write" and "fwrite". Each write takes 60KB of data.

I guess it's not a problem of the hardware, because "iostat -x -n 2" shows that the disk write speed is still non-zero (10-30MB/s) when this blocking happens., Hi Zheng,

What kernel is the newer box running? Is it a brand new kernel? 2.6.32 has significant changes to dirty page writeback

-Todd, filewriter.cpp: A simple program to test whether the box will block continuous writes for a long time. See Zheng's earlier comments for details.

, The new machine is running: "Linux hadoop0100.xxx.yyy.com 2.6.20-39_fbk #1 SMP Mon Mar 16 20:33:46 PDT 2009 x86_64 x86_64 x86_64 GNU/Linux" (uname -a) with 12 local disks.
You can use the attached filewriter.cpp to see if your box has the problem mentioned above.
, I assume the _fbk kernel doesn't have wacky patches backported from the last couple months of kernel development? Will run your test program on a couple boxes here as well, thanks for uploading it., Hey Zheng,

Any chance you can run "grep . /proc/sys/vm/*" on the system that does show the problem, and compare the results to the one that doesn't show the problem? I'm thinking this might just be a factor of system level tuning. See http://www.westnet.com/~gsmith/content/linux-pdflush.htm, bad machine:
{code}
/proc/sys/vm/block_dump:0
/proc/sys/vm/dirty_background_ratio:10
/proc/sys/vm/dirty_expire_centisecs:3000
/proc/sys/vm/dirty_ratio:40
/proc/sys/vm/dirty_writeback_centisecs:500
/proc/sys/vm/hugetlb_shm_group:0
/proc/sys/vm/laptop_mode:0
/proc/sys/vm/legacy_va_layout:0
/proc/sys/vm/lowmem_reserve_ratio:256   32
/proc/sys/vm/max_map_count:65536
/proc/sys/vm/min_free_kbytes:16383
/proc/sys/vm/nr_hugepages:0
/proc/sys/vm/nr_pdflush_threads:2
/proc/sys/vm/overcommit_memory:0
/proc/sys/vm/overcommit_ratio:50
/proc/sys/vm/page-cluster:3
/proc/sys/vm/swappiness:60
/proc/sys/vm/swap_token_timeout:0       0
/proc/sys/vm/vfs_cache_pressure:100
{code}


good machine:
{code}
/proc/sys/vm/block_dump:0
/proc/sys/vm/dirty_background_ratio:10
/proc/sys/vm/dirty_expire_centisecs:2999
/proc/sys/vm/dirty_ratio:40
/proc/sys/vm/dirty_writeback_centisecs:499
/proc/sys/vm/drop_caches:0
/proc/sys/vm/hugetlb_shm_group:0
/proc/sys/vm/laptop_mode:0
/proc/sys/vm/legacy_va_layout:0
/proc/sys/vm/lowmem_reserve_ratio:256   256
/proc/sys/vm/max_map_count:65536
/proc/sys/vm/min_free_kbytes:23004
/proc/sys/vm/min_slab_ratio:5
/proc/sys/vm/min_unmapped_ratio:1
/proc/sys/vm/nr_hugepages:0
/proc/sys/vm/nr_pdflush_threads:2
/proc/sys/vm/overcommit_memory:0
/proc/sys/vm/overcommit_ratio:50
/proc/sys/vm/page-cluster:3
/proc/sys/vm/panic_on_oom:0
/proc/sys/vm/percpu_pagelist_fraction:0
/proc/sys/vm/swappiness:60
/proc/sys/vm/vfs_cache_pressure:100
/proc/sys/vm/zone_reclaim_mode:0
{code}
, I have a suspicion this may be vaguely related to HDFS-1075 - some problems with the way write pipelines are staged.,  I also wonder whether it might be HDFS-561. I'll comment there that I think it should be put on the 20 branch]