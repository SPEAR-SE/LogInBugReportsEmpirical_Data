[{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12685778/HDFS-7489-v1.patch
  against trunk revision ffe942b.

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8952//console

This message is automatically generated., Hi, [~nlorang]. This is a very good observation and the patch looks good in general.

We had introduced DataNode hot swap feature (https://issues.apache.org/jira/browse/HDFS-1362). As it allows data volumes being added / removed during the run time, would it introduce some conflicts to the following code?

{code}
 synchronized(this) {
    // Replace volume list
   volumes = Collections.unmodifiableList(volumeList);
   FsDatasetImpl.LOG.warn("Completed checkDirs. Removed " + removedVols.size()
  + " volumes. Current volumes: " + this);
  }	
{code}

Also it would be great to address a race condition that a {{FsVolumeImpl}} being checked is being removed and re-added back. Moreover, could you re-use {{FsVolumeList#removeVolume}} ?

I would give a non-binding +1 after the above comments being addressed. Thanks again for working on this, [~nlorang]!, Thanks for looking at this, [~nlorang].  I think this is a very important fix to make.

As [~eddyxu] commented, I don't think adding {{checkDirsMutex}} is going to work here.  The {{volumeList}} can be changed by things other than {{checkDirs}}... such as the DataNode hard drive hot swap feature.

I think that what we ought to do is create an {{AtomicReference<FsVolumeImpl[]>}}.  Then, {{FsVolumeList#checkDirs}} can get a copy of the current array of volumes at the start locklessly, and simply call checkDirs on all of them.  This will fix the current problem.  I don't think there is a big disadvantage to calling {{checkDirs}} on removed volumes-- it's extra work, but shouldn't cause correctness issues.

I think there are still some correctness issues here we'll need to work, through.  For example, right now we could be calling {{FsVolumeList#getNextVolume}} right before removing a volume, and then trying to use it right after.  This is problem even right now... the {{FsVolumeList}} lock protects you, but only during the {{getNextVolume}} call.  After the call ends, the volume can be removed at any time.  We should file a follow-up JIRA for that, though.  It is critical to fix this performance issue now, and the race is an existing issue., Thanks [~eddyxu] and [~cmccabe].

If I'm not mistaken, using {{removeVolume()}} should resolve the conflict of a volume being added or removed while {{checkDirs()}} is running -- that will remove each volume one-at-a-time while locking the {{FsVolumeList}}. That doesn't do anything for the actual checking of already-removed volumes (they'll still be checked) and it won't catch newly-added volumes in that run of {{checkDirs()}}. I'll attach a reworked patch that does this.

I've left the {{checkDirsMutex}} to maintain the prior behavior of limiting one run of {{checkDirs}} at a time. This might be unnecessary -- as far as I can tell, the only pathway by which it's used is from the {{Datanode#checkDiskError}}, which is only used via a one-at-a-time thread., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12685850/HDFS-7489-v2.patch
  against trunk revision 6c5bbd7.

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8955//console

This message is automatically generated., Noah, this fix solves the current (major) problem.  I agree that calling {{removeVolume}} from {{checkDirs}} should avoid concurrent modification issues.  I still would prefer to see some refactoring, but it's critical to fix this due to its severity... so we should do that in a follow-on.  So I am +1 on the latest version, pending Jenkins.

The error message from the jenkins run is:
{code}
Detected JDK Version: 1.6.0-45 is not in the allowed range [1.7,).
{code}

This is related to the JDK6->JDK7 conversion of executors discussed upstream, not to the patch.  I will re-trigger Jenkins.

I will file follow-up issues for two remaining problems:
* If multiple threads call checkDirs at the same time, we should only do checkDirs once and give the results to all threads.  This might be easiest to accomplish with a Guava cache.
* We probably need to start reference-counting volumes so that they aren't removed while some code has a reference to them., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12685850/HDFS-7489-v2.patch
  against trunk revision ddffcd8.

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8956//console

This message is automatically generated., It seems like we are still hitting executors with jdk6 installed.  I have attached a version of the patch that sets minJdk=6, hopefully this will allow us to get a Jenkins run.

In view of how critical this patch is, I'd like to get it in today if possible.

thanks guys, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12685861/HDFS-7489-v2.patch-with-minjdk6
  against trunk revision ddffcd8.

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8958//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12685885/HDFS-7489-v2.patch-with-minjdk6-2
  against trunk revision ddffcd8.

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8959//console

This message is automatically generated., It looks like HADOOP-10530 broke the HDFS precommit job.  Hopefully it should be fixed now..., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12685901/HDFS-7489-v2.patch.1
  against trunk revision ddffcd8.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 287 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestRenameWithSnapshots

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.tools.offlineEditsVieweTests
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestRbwSTests
org.apache.hadoop.hdfs.server.datanode.fsdataset.TestTests

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8963//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8963//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8963//console

This message is automatically generated., Test failures are bogus.  I ran TestOfflineEditsViewer, TestRbwSpaceReservation,TestRenameWithSnapshots etc and got no failures.  The findbugs results are very puzzling... a bunch of "null pointer dereference" warnings for System.out and System.err (and not in code modified by this patch).  I think we can safely assume that System.out and System.err are not null-- no idea what went wrong with findbugs here.  Perhaps this is another JDK version upgrade issues.

+1.  Will commit in a few minutes.  Thanks, Noah., Ah.  The findbugs version was bumped recently in HADOOP-10476.  My uninformed guess is that we are using the old findbugs for "previous findbugs warnings" and the new one for patch findbugs, which of course makes every patch look bad?, My pleasure. Thanks for getting this upstream so quickly. , Thanks for the patch, [~nlorang]., FYI for git log greppers, this commit is missing the JIRA ID.]