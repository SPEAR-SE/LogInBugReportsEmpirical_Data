[Hi Lohit,

{{DFSClient}} currently allows configuration to specify the name of a {{SocketFactory}} class.  It will create an instance of this factory and use it to create the sockets used for reading by {{DFSInputStream}} and writing by {{DFSOutputStream}}.

{code}
  public DFSClient(URI nameNodeUri, ClientProtocol rpcNamenode,
      Configuration conf, FileSystem.Statistics stats)
    throws IOException {
    // Copy only the required DFSClient configuration
    this.dfsClientConf = new Conf(conf);
    this.shouldUseLegacyBlockReaderLocal = 
        this.dfsClientConf.useLegacyBlockReaderLocal;
    if (this.dfsClientConf.useLegacyBlockReaderLocal) {
      LOG.debug("Using legacy short-circuit local reads.");
    }
    this.conf = conf;
    this.stats = stats;
    this.socketFactory = NetUtils.getSocketFactory(conf, ClientProtocol.class);
{code}

From {{NetUtils}}:

{code}
  public static SocketFactory getSocketFactory(Configuration conf,
      Class<?> clazz) {

    SocketFactory factory = null;

    String propValue =
        conf.get("hadoop.rpc.socket.factory.class." + clazz.getSimpleName());
    if ((propValue != null) && (propValue.length() > 0))
      factory = getSocketFactoryFromProperty(conf, propValue);

    if (factory == null)
      factory = getDefaultSocketFactory(conf);

    return factory;
  }
{code}

I think you could potentially implement your requirement by writing a custom subclass of {{SocketFactory}} that calls {{Socket#setTrafficClass}}.  Then, in your configuration, set {{hadoop.rpc.socket.factory.class.ClientProtocol}} to the fully-qualified name of your class.

Does this help?
, [~cnauroth], thanks for the suggestion.

We want the ability to config different instances of DFSClient with different DSCP values. We ended up using the approach that Lohit mentioned e.g., from application point of view, they will set the configuration property dfs.client.dscp.bit.intvalue and DSClient will call setTrafficClass before establishing the connection. Your suggestion of custom SocketFactory approach will also work. Each instance of DFSClient will create its own instance of DSCPSocketFactory; DSCPSocketFactory also needs to implement Configurable so that it can access the configured DSCP value. If we want to go with custom SocketFactory approach, we can add it to hadoop-common so that applications and other hadoop projects can use it directly.

At the higher level, there are different scenarios. The protocol could be based on DFSClient/DataTransferProtocol over TCP, or Hftp/WebHDFS over HTTP.  The data transfer should be read or write.

The primary use case for us is cross DC remote read scenario with Hftp. In order to config Socket for Hftp scenario, we ended up using httpclient.Protocol.registerProtocol with custom class of ProtocolSocketFactory, given HttpURLConnection doesn't provide a mechanism for custom SocketFactory.

As we try to open source our work to support WebHDFS, it brings up the question of "how to config the socket in WebHDFS given httpclient isn't used by HDFS". We did some investigation and found some possible approaches.

1. HttpsURLConnection has setSSLSocketFactory. Need to test out if it works for regular http. 
2. Call Socket.setSocketFactroy() for the entire JVM. This requires custom SocketImplFactory to create custom socket based on webHDFS address.

We would like to get more inputs from others., bq. HttpsURLConnection has setSSLSocketFactory. Need to test out if it works for regular http. 

Hooking URLConnectionFactory in the filesystem object should be able address the problem for webhdfs., Thanks, [~wheat9]. Here are some investigations regarding the solution for webHDFS.

1. SSLSocketFactory approach doesn't work for regular http, given it requires HttpsURLConnection.
2. Socket.setSocketFactroy() for the entire JVM works if you only need to set DSCP based on remote address and port. But there is no way to config different DSCP values for different sockets.
3. There might be a way to get to the private socket instance of HttpURLConnection. But that is really hack.

So with all these, httpclient seems to be most elegant approach, specifically  org.apache.http.client. It appears HDFS chose to use URLConnection to manage connections. Appreciate if others can provide some context if httpclient has been considered.

1. Development effort at https://github.com/apache/httpclient is quite active.
2. HADOOP-10105 explained httpclient is end of life. But that seems to refer to org.apache.commons.httpclient.
, bq. So with all these, httpclient seems to be most elegant approach, specifically org.apache.http.client.

I'm a bit hesitant to take org.apache.http.client as a dependency.  My experience has been very positive with it.  It's a great library.  The problem is that it's pervasive.  I've worked in large enterprises where nearly every single application linked to it, and they didn't necessarily link to the same version.  This could set up a situation where it becomes more difficult for downstream applications to link to HDFS if they're already using org.apache.http.client.  It could turn into another dependency management challenge like Guava.  Since we publish a library rather than an application, we always need to be mindful of our dependencies and their impact on downstream applications.

bq. Socket.setSocketFactroy() for the entire JVM works if you only need to set DSCP based on remote address and port. But there is no way to config different DSCP values for different sockets.

I see your point that the {{SocketFactory}} interface is set up entirely in terms of address parameters.  What additional information would you like to use?  It sounds like you want to detect HTTP URLs with the webhdfs scheme.  Is that everything, or is there more?

Have you considered putting that information in a thread-local variable somewhere that your {{SocketFactory}} could access it?  This could end up being brittle code, but perhaps it's a way to work around the limitations of {{URLConnection}}., I might have been mistaken about something.  I actually see relevant dependencies in hadoop-common and hadoop-auth now:

{code}
    <dependency>
      <groupId>org.apache.httpcomponents</groupId>
      <artifactId>httpclient</artifactId>
      <scope>compile</scope>
    </dependency>
{code}

{code}
    <dependency>
      <groupId>commons-httpclient</groupId>
      <artifactId>commons-httpclient</artifactId>
      <scope>compile</scope>
    </dependency>
{code}

Ming, can you please clarify whether or not you're proposing adding new dependencies, or do you already have what you need?, Thanks, Chris. Really appreciate your input.

1. Each http connection might have different DSCP values. It is a good idea to use thread local variable so that webHDFS use it to set the DSCP value and custom {{SocketFactory}} can access it . But yeah, the code might not be clean.

2. For httpclient dependency, per https://issues.apache.org/jira/browse/HADOOP-10105, I assume it is on its way out. If we keep httpclient, then we have to provide a way for webHDFS to use httpclient given its dependency on URLConnection. For example, we can make webHDFS configurable to use either URLConnection or httpclient. If people are ok that, then we are good. The other option is to have webHDFS switch from URLConnection to httpclient., Yes, you're right.  I mistakenly looked at the old httpclient dependency that we're going to remove.

Then, if I understand correctly, we have this dependency now...

{code}
    <dependency>
      <groupId>org.apache.httpcomponents</groupId>
      <artifactId>httpclient</artifactId>
      <scope>compile</scope>
    </dependency>
{code}

...and in addition, you're proposing that we add this dependency:

{code}
    <dependency>
      <groupId>org.apache.httpcomponents</groupId>
      <artifactId>httpcomponents-client</artifactId>
      <scope>compile</scope>
    </dependency>
{code}

Do I have it correct now?

It's possible I'm exaggerating the impact of adding this dependency, so it would be great to get more opinions.  An email to hdfs-dev would be a good way to call attention to it., Yeah, we do have dependency on org.apache.httpcomponents' httpclient. That might be enough for HDFS. My main question is if folks are ok to have webHDFS use org.apache.httpcomponents' httpclient instead of URLConnection., If there is no net new dependency required, then I'd be +1 for the proposal overall.  It looks like it will be a cleaner way to implement it compared to trying to deal with the global {{SocketFactory}}.  It would be helpful to have a benchmark showing that there is no performance degradation compared to {{URLConnection}}., Any update on this patch? Our network infrastructure showed interest in labeling hadoop data traffic, and this jira could have been used for this purpose. I guess other companies must have similar applications that could benefit from this patch.]