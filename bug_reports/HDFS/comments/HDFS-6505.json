[This issue causes the last block is missing and the file is corrupted. But actually, the data on DataNode is correct.

I went through the code, and I think some safe check is missing when namenode receives a bad block report from datanodes.
See the following code snippet in namenode BlockManager
{code}
  public void findAndMarkBlockAsCorrupt(final ExtendedBlock blk,
      final DatanodeInfo dn, String storageID, String reason) throws IOException {
    assert namesystem.hasWriteLock();
    final BlockInfo storedBlock = getStoredBlock(blk.getLocalBlock());
    if (storedBlock == null) {
      // Check if the replica is in the blockMap, if not
      // ignore the request for now. This could happen when BlockScanner
      // thread of Datanode reports bad block before Block reports are sent
      // by the Datanode on startup
      blockLog.info("BLOCK* findAndMarkBlockAsCorrupt: "
          + blk + " not found");
      return;
    }
    markBlockAsCorrupt(new BlockToMarkCorrupt(storedBlock, reason,
        Reason.CORRUPTION_REPORTED), dn, storageID);
  }
{code} 
We should check the timestamp in reported block and stored block. If the reported block has a smaller timestamp, this block should not be marked as corrupt. It is possible that the reported block has a smaller timestamp when client has done some work on recovering pipeline.]