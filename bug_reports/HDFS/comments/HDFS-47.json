[One easy improvement: The class of DataNode.childSockets should be Set, instead Map., I found a couple of datanodes with a different exception (still OutOfMemoryError):

in the out file:
Exception in thread "org.apache.hadoop.dfs.DataNode$DataXceiver@9d54d" java.lang.OutOfMemoryError: Java heap space

in the log file:
2008-02-26 23:01:12,810 ERROR org.apache.hadoop.dfs.DataNode: xxx.xxx.xxx.101:50010:DataXceiver: java.lang.OutOfMemoryError: Java heap space
        at java.io.BufferedInputStream.<init>(BufferedInputStream.java:178)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:919)
        at java.lang.Thread.run(Thread.java:619)
or

2008-02-27 01:30:20,703 ERROR org.apache.hadoop.dfs.DataNode: xxx.xxx.xxx.136:50010:DataXceiver: java.lang.OutOfMemoryError: Java heap space
        at java.io.BufferedInputStream.<init>(BufferedInputStream.java:178)
        at org.apache.hadoop.dfs.DataNode$BlockSender.<init>(DataNode.java:1521)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.readBlock(DataNode.java:992)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:930)
        at java.lang.Thread.run(Thread.java:619)

, Edit : typos

I looked at one of these dead datanodes. OutOfMemoryErros seems to be an independent problem.  These errors (there are multiple of them) are in .out file without timestamps. On this node, .out was modified at 01:39 and log file shows DataNode seems to have functioned normally for some more time. 

The datanode seems to stuck because one of its threads is stuck forever waiting for 'df' to return while holding a central lock (FSDataset). And there is a zombied df process on the machine. The offending stacktrace : 

{noformat}

"org.apache.hadoop.dfs.DataNode$DataXceiver@dc08c3" daemon prio=10 tid=0xae45e800 nid=0x2f3d in Object.wait() [0x8cafe000..0x8caff030]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0xed78c5f8> (a java.lang.UNIXProcess$Gate)
        at java.lang.Object.wait(Object.java:485)
        at java.lang.UNIXProcess$Gate.waitForExit(UNIXProcess.java:64)
        - locked <0xed78c5f8> (a java.lang.UNIXProcess$Gate)
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:145)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:115)
        at org.apache.hadoop.util.Shell.run(Shell.java:100)
        at org.apache.hadoop.fs.DF.getCapacity(DF.java:63)
        at org.apache.hadoop.dfs.FSDataset$FSVolume.getCapacity(FSDataset.java:307)
        at org.apache.hadoop.dfs.FSDataset$FSVolume.getAvailable(FSDataset.java:311)
        at org.apache.hadoop.dfs.FSDataset$FSVolumeSet.getNextVolume(FSDataset.java:393)
        - locked <0xb6551838> (a org.apache.hadoop.dfs.FSDataset$FSVolumeSet)
        at org.apache.hadoop.dfs.FSDataset.writeToBlock(FSDataset.java:657)
        - locked <0xb6551838> (a org.apache.hadoop.dfs.FSDataset$FSVolumeSet)
        - locked <0xb653aec8> (a org.apache.hadoop.dfs.FSDataset)
        at org.apache.hadoop.dfs.DataNode$BlockReceiver.<init>(DataNode.java:1983)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(DataNode.java:1074)
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:938)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

We also need to find why there are mutlipel OutOfMemoryError. My guess is that some of the normally functioning datanodes will have these as well.
, The ProcessBuilder getting stuck is also mostly caused by DataNode running out of memory, because both the datanodes have "Exception in thread "process reaper" java.lang.OutOfMemoryError: Java heap space".  "process reaper" looks like a thread in Java Process implementation. So I guess running out memory is the main issue.
, Koji and I looked at one of the datanodes. Attached jmap-histo.txt is the output of 'jmap -histo'. The top entry takes pretty much all the memory :

{noformat}
num   #instances    #bytes  class name
--------------------------------------
  1:     25039   918792056  [B
  2:     38666     4910272  [C
  3:     23620     2674272  <constMethodKlass>
  4:     23620     1893912  <methodKlass>
  5:     38347     1549664  <symbolKlass>
{noformat}

Does "[B" refer to byte arrays? Any ideas about how get anymore info out of this process are welcome.

EDIT : Please ignore this one, I guess we need to run 'jmap -histo:live", which shows much more sober picture., Recall that running out of file handles can also throw OutOfMemoryError, as well as a few other conditions.  How much memory is the process actually using?, DataNode is allowed 1000MB (-Xmx). Top shows virtual memory of around 1.2-1.3 GB resident is around 50-150MB. I am thinking of running a temporary patch that prints some stats when any thread catches OutOfMemoryException, and probably block so that we can inspect the process further., Christian,

What is the read and write pattern like? How fast do clients read? For e.g. on one of the normal datanodes, there are around 500 connections to DataNode and it is pretty much idle. These connections need around 200MB active memory. So another DataNode only needs a few times more connections to run out of memory.

Edit: Most of these are writing new blocks., Regd buffers, for readers, HADOOP-2578 will remove one out of 3 BUFFER_SIZE buffer. We should file another jira to use a smaller buffer size for crc file. Then we will have only one large buffer per reader. Similarly while writing data.
, Every map application streams through about 40-60 DFS files and writes directly to 2 DFS files (besides sending data to map-reduce framework). None of the maps accesses the same data.

Every reduce application writes to about 70 DFS files.

Keep in mind, that the datanodes ran out of memory already during the map phase.

This access pattern did not change for a while now. But we started to see datanodes running out of memory with about nightly build #810.

I am surprised about the high number of connections. Are some of them stale?, Concerning read speed of map applications, a back-on-the-envelope calculation comes up with an estimate of 0.5-1 MB/s per map application., Raghu, do you think that the relatively slow reading speed per block poses a problem to the datanodes?, 
Slow reads are same problem as before. It hasn't changed in terms of buffering.

But slow writes have bigger penalty in terms of buffer space used. Before 16, it didn't matter how slow client wrote, DFSClient cached full block on its local disk and streamed the whole block quickly to DataNode. In 16, client has connections open pretty much as long as the output stream is open. For each client connection, 2 datanodes have 4 buffers and the last datanode has 3 buffers. Each of these buffers is of the size io.file.buffer.size. This might explain why you are seeing more OutOfMemory errors now.

We still need to find out about number of connections open. We could expect some of the datanodes to have multiple times the average load. 

Most of the time, these errors are caught by DataNode data transfer threads, we could print all the stack traces or some equally useful info once every few minutes of these exceptions to log. The stacktrace shows how many reads and writes are going on. Let me know if I should prepare patch., I am not completely convinced that the change in writing explains the problem, because:

1) since the time the reduce phase of the job started, we did not get any more dead datanodes (but they happened more frequently during the map phase)
2) we saw dead datanodes with nightly build #810 (my impression is that that release still wrote to local disk)

But I would be happy to restart a couple of datanodes with a new patch., > (my impression is that that release still wrote to local disk)
0.16. does not write to local disk. Local Disk write was remove quite some time back. But it had removed some buffers on  DataNode. HADOOP-2768 went into svn revision 618349 and you are running 618351. 
Edit: HADOOP-2768 brought buffering back to how it was before the local disk was removed. But this buffer has bigger penalty for slow writes., #810 release was a trunk release from Jan 4, running on a different cluster. Because we did not experience HADOOP-2883 with that release, I assumed that it did not write directly to DFS. Am I wrong?, 
What % of datanodes do you think logged OutOfMemory exception even once? If avg load at any time was able to cause this problem then we would see a large portion of datanodes to have this exception in their logs. I grepped on a few random datanodes and I could not seen any in last few days. Simon shows number of active reads and writes. We could check datanodes that have high numbers there., > #810 release was a trunk release from Jan 4, running on a different cluster. Because we did not experience HADOOP-2883 with that release, I assumed that it did not write directly to DFS. Am I wrong?

Right. HADOOP-1707 went in on Jan 17th. What is relation to this Jira? All the logs and times mentioned here are around Feb 19-24th or so.
, When we ran into the same problem with #810 a month ago on a different cluster it was not clear to us whether we should report it immediately because it was a trunk release and it might have been just a transient problem. We wanted to wait for a stable release and check whether it stlil happens.

BTW, I checked the logs:
Around 2% of datanodes had OutOfMemoryError exceptions. By itself, it would probably be not much of a problem, but it happened that some of the datanodes went out within a short time of period such that we lossed a few blocks., Thanks Christian. 2% is bad enough :). We need to find out if this is caused just by large number of client connections or some other bug that gets triggered., This has gone stale. FWIW, haven't seen DNs go OOM on its own in recent years. Probably a leak that was fixed?

Resolving as Not a Problem (anymore).]