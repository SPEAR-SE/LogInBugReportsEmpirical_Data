[New patches add a second assert to the test, and fix some formatting/readability issues., Marking patch available for Derek so that test-patch runs.

Derek - minor thing, but please don't set the "fix versions" field until the patch is actually committed. Before then, setting the affects/targets versions fields is sufficient., I think we still need a limit for highest priority replications. Otherwise, there could be a large number of replications schedule to a datanode and then nothing can be done.  How about adding a (hard) limit for highest priority replication?  The current conf is a soft limit.  Only highest priority replications can pass the it., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12556116/HDFS-4270.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated -6 warning messages.

    {color:red}-1 eclipse:eclipse{color}.  The patch failed to build with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestEditLog

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3600//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3600//console

This message is automatically generated., I don't fully understand this subsystem, but I'm a bit torn over limiting the "only 1 block left" replications.  This _should_ be a rare event, but it's a critical situation when it does.  I'm unclear if the max repls is compared against inflight or queued repls.  If the latter, perhaps higher prio blocks should displace already queued blocks for that DN?  If the "only 1 block left" repls are subjected to a new hard limit, is there an issue with how quickly the monitor will cycle back to schedule the critical blocks?

Based on an actual incident: we lost most of a rack, then just so happened to lose the third DN before replication occurred.  A lot of nodes were being decommissioned, which appears to have delayed replication after the first DN was lost and again after the second DN on the rack was lost.  The third DN's disk with the remaining replica died hours later, and was decommissioned with no notification that the block was lost.  There may be more bugs involved, but this seemed like an obvious fix to mitigate the risk., Hi Aaron,

Noted.  I meant to remove the entries in fixed, but I must have forgot.  I'll try not to do that in the future.



Hi Nicholas,

I do see the concern over making this case unbounded, for the sake of the NN.

I am interested, historically why was the default limit set to 2?, Canceling patch for now.

Later after discussion, I'll fix the errors and upload new patches., > ..., historically why was the default limit set to 2?

I actually don't know why the default is 2.  Let me check., Manually ran eclipse:eclipse -> works for me

Manually ran TestEditLog.testFuzzSequences -> passes for me

Javadoc error was a build error: connection timed out while fetching a dependency., Re-attaching patch, as the errors reported by the bot appear to be transient., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560296/HDFS-4270.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3632//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3632//console

This message is automatically generated., Reattaching patch again.  The balancer test failure looks like another spurious result., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560368/HDFS-4270.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3634//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3634//console

This message is automatically generated., If we consider a hard limit, what should the limit be?, How about making it configurable and setting the default to 4?, New hard-limit config, defaults to 4., New hard-limit config, defaults to 4., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561923/HDFS-4270.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3684//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3684//console

This message is automatically generated., +1 patch looks good., I have committed this.  Thanks, Derek!, Integrated in Hadoop-trunk-Commit #3174 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3174/])
    HDFS-4270. Introduce soft and hard limits for max replication so that replications of the highest priority are allowed to choose a source datanode that has reached its soft limit but not the hard limit.  Contributed by Derek Dagit (Revision 1428739)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1428739
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java
, Integrated in Hadoop-Yarn-trunk #86 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/86/])
    HDFS-4270. Introduce soft and hard limits for max replication so that replications of the highest priority are allowed to choose a source datanode that has reached its soft limit but not the hard limit.  Contributed by Derek Dagit (Revision 1428739)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1428739
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java
, Integrated in Hadoop-Hdfs-trunk #1275 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1275/])
    HDFS-4270. Introduce soft and hard limits for max replication so that replications of the highest priority are allowed to choose a source datanode that has reached its soft limit but not the hard limit.  Contributed by Derek Dagit (Revision 1428739)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1428739
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java
, Integrated in Hadoop-Mapreduce-trunk #1305 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1305/])
    HDFS-4270. Introduce soft and hard limits for max replication so that replications of the highest priority are allowed to choose a source datanode that has reached its soft limit but not the hard limit.  Contributed by Derek Dagit (Revision 1428739)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1428739
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java
, I pulled this into branch-0.23, Integrated in Hadoop-Hdfs-0.23-Build #485 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/485/])
    HDFS-4270. Replications of the highest priority should be allowed to choose a source datanode that has reached its max replication limit (Derek Dagit via tgraves) (Revision 1428883)

     Result = FAILURE
tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1428883
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java
]