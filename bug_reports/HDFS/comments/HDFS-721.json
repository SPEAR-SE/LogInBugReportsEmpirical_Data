[So looking in NN log, I see that block 1030 is what block 1029 became:

{code}
2009-10-21 04:57:02,890 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: updatePipeline(blk_6345892463926159834_1029) successfully to blk_6345892463926159834_1030
{code}

Looking up at application level, I don't see an exception up at the application level.  Maybe all is working properly... the block goes on to have a long and fruitful life?

Here is rest of log from the NN:

{code}
2009-10-21 04:57:05,728 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Inconsistent size for block blk_6345892463926159834_1030 reported from XX.XX.XX.141:51010 current size is 3056128 reported size is 67108864
2009-10-21 04:57:05,729 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: XX.XX.XX.141:51010 is added to blk_6345892463926159834_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[XX.XX.XX.139:51010|RBW], ReplicaUnderConstruction[XX.XX.XX.141:51010|RBW]]} size 67108864
2009-10-21 04:57:05,733 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: XX.XX.XX.139:51010 is added to blk_6345892463926159834_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[XX.XX.XX.139:51010|RBW], ReplicaUnderConstruction[XX.XX.XX.141:51010|RBW]]} size 67108864
2009-10-21 04:57:07,769 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask XX.XX.XX.141:51010 to replicate blk_6345892463926159834_1030 to datanode(s) XX.XX.XX.140:51010
2009-10-21 05:05:22,268 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: PendingReplicationMonitor timed out block blk_6345892463926159834_1030
2009-10-21 05:05:27,868 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask XX.XX.XX.139:51010 to replicate blk_6345892463926159834_1030 to datanode(s) XX.XX.XX.142:51010
2009-10-21 05:05:30,299 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: XX.XX.XX.142:51010 is added to blk_6345892463926159834_1030 size 67108864
2009-10-21 05:06:47,804 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_6345892463926159834_1030 to XX.XX.XX.139:51010 XX.XX.XX.141:51010 XX.XX.XX.142:51010 
2009-10-21 05:06:49,139 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask XX.XX.XX.142:51010 to delete  blk_-1883563394128357682_1221 blk_-1418192880479762585_1363 blk_-7630118490185950160_1317 blk_-8001019548793370364_1148 blk_4870116241583532381_1218 blk_2108940364850350519_1082 blk_1036824913411181109_1091 blk_-7589777526384929149_1033 blk_8923577073812832890_1027 blk_-1109307457918706344_1034 blk_-116109986645631967_1219 blk_-1553219148041957509_1367 blk_3763728034862268069_1222 blk_7960880682534006819_1267 blk_-8530634816448964605_1224 blk_666438451332088436_1103 blk_-1943198821321042756_1010 blk_3860462222387154865_1032 blk_-2887612715667758540_1342 blk_2678719195781934773_1348 blk_7531114468370112370_1018 blk_5376641334456236592_1263 blk_5786815585048721793_1025 blk_2870673990810172323_1079 blk_-5877030662574024321_1055 blk_-4584853327269407588_1021 blk_6345892463926159834_1030 blk_8408287293691629994_1268 blk_8109921405484286254_1031 blk_7335139466236556727_1320 blk_1278349007490858566_1272
2009-10-21 05:06:52,149 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask XX.XX.XX.139:51010 to delete  blk_-1883563394128357682_1221 blk_-7630118490185950160_1317 blk_-8001019548793370364_1148 blk_4870116241583532381_1218 blk_2108940364850350519_1082 blk_1036824913411181109_1091 blk_-1919844007905419086_1019 blk_8923577073812832890_1027 blk_-116109986645631967_1219 blk_7960880682534006819_1267 blk_666438451332088436_1103 blk_2067665234645028297_1017 blk_-5406582365187584682_1322 blk_3860462222387154865_1032 blk_-6377718937631289761_1223 blk_2678719195781934773_1348 blk_-2648964128352415897_1023 blk_-8349995959668148332_1051 blk_-4114818907728161248_1035 blk_568068918288377484_1294 blk_-4584853327269407588_1021 blk_6345892463926159834_1030 blk_8408287293691629994_1268 blk_-4335633782362435415_1338 blk_-4179867072599502920_1265 blk_7335139466236556727_1320
2009-10-21 05:06:52,149 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask XX.XX.XX.141:51010 to delete  blk_-1418192880479762585_1363 blk_-7589777526384929149_1033 blk_-1919844007905419086_1019 blk_-1109307457918706344_1034 blk_-1553219148041957509_1367 blk_3763728034862268069_1222 blk_-8530634816448964605_1224 blk_2067665234645028297_1017 blk_-1943198821321042756_1010 blk_-5406582365187584682_1322 blk_-6377718937631289761_1223 blk_-2887612715667758540_1342 blk_-2648964128352415897_1023 blk_7531114468370112370_1018 blk_-8349995959668148332_1051 blk_5376641334456236592_1263 blk_-3674434173543740123_1147 blk_-4114818907728161248_1035 blk_5786815585048721793_1025 blk_2870673990810172323_1079 blk_-5877030662574024321_1055 blk_568068918288377484_1294 blk_6345892463926159834_1030 blk_-4335633782362435415_1338 blk_8109921405484286254_1031 blk_-4179867072599502920_1265 blk_1278349007490858566_1272
{code}, Thanks Stack for the detailed logs. They are very useful for me to investigate the issue. Was your file's replication factor 3? Were you running your test in a very small cluster?

It seemed to me this was what happened:
1. When the block was created, the initial pipeline has datanodes 139, 140, and 141.
2. 140 failed while the block was being written to.
3. New pipeline was constructed. Only 139 and 141 remained on the pipeline.
4. The block completed. 139 & 141 reported to NN the new replica.
5. NN detetected that the block was under-replicated; therefore scheduled to replicated it to 140.
6. 140 already has a stale replica of the block; so replication failed.
7. NN scheduled to replicate it to 142. This replication succeeded.

The exception was thrown in step 6. I think this is expected., @Hairong Thanks for taking a look.  Yes small cluster -- 4 nodes, lots of hbase installs start at about this size -- and yes replication factor was 3.  Story above jibes with me.  Let me close this issue.

, Working as designed.  Closing., > The exception was thrown in step 6. I think this is expected.

If it is expected, the log message probably should not be in ERROR level.  WARN or INFO may be enough., @Tsz That makes sense...  will I reopen so this issue catches a patch to change log level?, > ... will I reopen so this issue catches a patch to change log level?

Sure.  Let's reopen this for changing the log level., Looks like this exception is printed out by the catch in DataNode#run:

{code}
    while (shouldRun) {
      try {
        startDistributedUpgradeIfNeeded();
        offerService();
      } catch (Exception ex) {
        LOG.error("Exception: " + StringUtils.stringifyException(ex));
        if (shouldRun) {
          try {
            Thread.sleep(5000);
          } catch (InterruptedException ie) {
          }
        }
      }
{code}

So, patch would add catch of ReplicaAlreadyExistsException to above try/catch and do a LOG.warn printing out the stacktrace?  Does that sound good?, Just in this particular case, the exception is expected. But in most of the cases, this is really an error. I do not think we should change the log level.

BTW, the logging is done at DataXceiver#run() lines 113:
{noformat}
    } catch (Throwable t) {
      LOG.error(datanode.dnRegistration + ":DataXceiver",t);
    } finally {
{noformat}
, > Just in this particular case, the exception is expected. But in most of the cases, this is really an error. I do not think we should change the log level.

Then, the exception for this particular case should be caught separately.  Logging expected behavior in ERROR will confuse users and ops., This appears to be fixed by HDFS-6123.]