[I'm not sure if this is actually a bug: I can see some applications requiring the semantics that moving (or deleting) a file underneath an active writer would break the writer's ability to continue to add new blocks.

Regarding the scenario of the "mixed up" blocks, I'm not sure that can happen, since {{getAdditionalBlock()}} takes the previous block as an argument. In this case, the previous block would not match up, and the request to allocate a new block on the 'wrong' file would fail, right?, One way to solve this problem is to add INode ID.  We could add INode ID without increasing memory usage since both modification time and access time are currently 8 bytes, which supports
{noformat}
2^64 / (1000 * 3600 * 24 * 365.25) = 584542046 years,
{noformat}
 which is way too much.  If only 5 bytes is used, it supports
{noformat}
2^40/(1000 * 3600 * 24 * 365.25) = 34.84 years.
{noformat}
It may already good enough.  We may also change the accuracy from milliseconds to hundredth or tenth seconds.  Then, 5 bytes supports 348.4 years and 3484 years respectively.  The INode ID is stored in the remaining 6 bytes, which supports 2^48 = 281 trillion of INodes. We may play around with the parameters more and have the following table.
|| atime/mtime || Time period supported (with milliseconds accuracy) || INode ID || \# INodes supported |
| 40 bits | 34.84 years | 48 bits | 281.47 trillions |
| 41 bits | 69.68 years | 46 bits |  70.37 trillions |
| 42 bits | 139.37 years | 44 bits |  17.59 trillions |
| 43 bits | 278.73 years | 42 bits |  4.40 trillions |
| 44 bits | 557.46 years | 40 bits |  1.10 trillions |

, > ..., the previous block would not match up, and the request to allocate a new block on the 'wrong' file would fail, right?

I think it can allocate a new block in this case that the previous block is null and the last block is full and complete., Adding INode IDs seems like a very reasonable idea to me. It can be a source for many optimizations besides fixing the problem described here.
Don't like squeezing the INode id between m&a times. Just adding a field should be fine., +1 for the idea. I prefer optimizing m&a fields than adding new field. However, we can start with adding a new field and do the optimization later. , Sounds good.  Let's start with adding a new field first., Upload the initial patch. Will split it into smaller ones later for easier review.

The basic idea is:
1. add inode id, which is a 64bit number. Root's inode id is 1 and others start from 2. We don't recycle inode id. 
2. add new addBlock interface which takes the id as an additional field. The id is checked by checkLease(). If it doesn't match current id in the found inode, NN fails the request., resubmit patch to trigger tests., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562367/HDFS-4258.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 7 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 2022 javac compiler warnings (more than the trunk's current 2015 warnings).

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer
                  org.apache.hadoop.hdfs.TestLease
                  org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer
                  org.apache.hadoop.hdfs.TestDFSClientRetries

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3690//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/3690//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3690//console

This message is automatically generated., fixed the unit tests and upload the new patch again to trigger the build., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562477/HDFS-4258.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 12 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOIVCanReadOldVersions
                  org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3697//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3697//console

This message is automatically generated., We're running into problems with parent directories being renamed and leases being held on the new pathnames.  As long as a long-running process/daemon continues to renew its lease

This is a big patch, but does the file id present any security issues?  Can I brute force access to files by guessing file ids?, I misread, didn't realize it's another sanity check, not another way to reference the file., Hey guys, given that the scope of this work seems to be expanding a bit, and there are now several sub-tasks to complete the final goal, can I ask that this work be moved to a branch? I'd also really like to see a little design doc about how all of this will fit together in the end, the extent of the changes that are required, etc., Maybe the sub-JIRA number makes it look a bigger code modification than it actually is. :-)

The esential change here is to add inode id as an additional sanity check for file modification.

Since it adds one more field in INode, it has to involve a few things like persistence and RPC changes. The idea and implementation is still very strait-forward.

A few of the sub-JIRAs are low priority incremental changes, such as HDFS-4454(enable offline-viewer to show the id) and HDFS-4398(Change WebHDFS to support file ID). 

HDFS-4434(Provide a mapping from INodeId to INode) is not closed only because I want to use it to discuss the possibility/beneifit of having such a functionality.
For HDFS-4466(include fileid in other related RPC calls), if you think it's not neceissary a sub-JIRA, I can move it out. Nevertheless, I will update its description with more details. 


If you still think we should do the rest of the work in a branch, I am OK with it too. 
, I would still prefer this work be moved to a branch.

I'd also like to see if we can address the motivation for this issue (rename of being written files) without introducing an INode ID like this. It may well be that introducing a unique INode ID as has been described here is a good architectural change, but I suspect it's not actually necessary to address the bug described by this JIRA. I say this mostly because it seems this JIRA has already concluded that the appropriate thing to do is to error out if a file which is actively being written is renamed, as mentioned in this comment:

{quote}
2. add new addBlock interface which takes the id as an additional field. The id is checked by checkLease(). If it doesn't match current id in the found inode, NN fails the request.
{quote}

Wouldn't checking the path in the lease along with the clientName be sufficient to detect that a being-written file had been renamed, and thus the NN should fail the request?, bq. I say this mostly because it seems this JIRA has already concluded that the appropriate thing to do is to error out if a file which is actively being written is renamed
Where?

bq. It may well be that introducing a unique INode ID as has been described here is a good architectural change, but I suspect it's not actually necessary to address the bug described by this JIRA.
Is the concern you are expressing that InodeID introduction is not a part of this jira? I fail to understand the concern you have. As Brandon has earlier commented, InodeID has been added and related changes are being made.

I also think HDFS-4434 is some thing that we may consider doing in the future with a switch to turn it off for folks who do not want the memory impact., {quote}Wouldn't checking the path in the lease along with the clientName be sufficient to detect that a being-written file had been renamed, and thus the NN should fail the request? {quote}
The example given in the description shows the clientName is not sufficient., I'm not sure the example given in the description can actually happen -- HDFS-3031 may have already fixed this case in its patch. The block comment that patch added describes the three possible cases on addBlock():

{code}
+        // The block that the client claims is the current last block
+        // doesn't match up with what we think is the last block. There are
+        // three possibilities:
+        // 1) This is the first block allocation of an append() pipeline
+        //    which started appending exactly at a block boundary.
+        //    In this case, the client isn't passed the previous block,
+        //    so it makes the allocateBlock() call with previous=null.
+        //    We can distinguish this since the last block of the file
+        //    will be exactly a full block.
+        // 2) This is a retry from a client that missed the response of a
+        //    prior getAdditionalBlock() call, perhaps because of a network
+        //    timeout, or because of an HA failover. In that case, we know
+        //    by the fact that the client is re-issuing the RPC that it
+        //    never began to write to the old block. Hence it is safe to
+        //    abandon it and allocate a new one.
+        // 3) This is an entirely bogus request/bug -- we should error out
+        //    rather than potentially appending a new block with an empty
+        //    one in the middle, etc
{code}

and it has code to detect case 3, which is what we seem to be talking about here.

Also, it seems like, if the main goal is to deal with this issue, you don't need anything nearly as complex -- you only would need inode IDs in the FileUnderConstruction structure - just a UUID for each open file. That wouldn't have the memory concerns, and would address the problem as it's been described, right?

To be clear, I'm not against adding inode numbers, but I agree it would be nice to write up what the use cases are and make sure that the benefits outweigh the cost. I agree we can squeeze the inum into some extra bits in our memory, but there are plenty of other things that would be nice to squeeze in as well (eg xattrs, hierarchical storage classes, etc) - if we put all of them in, it could really hurt.

Another use case that I see for inums, if they're guaranteed to not recycle (not sure if that's the case) would be for modification checks in tools like distcp. We currently can't rely on just the file name and size, since the file could have been replaced, but the combination of inum and size would be unique., bq. Where?

In [this comment|https://issues.apache.org/jira/browse/HDFS-4258?focusedCommentId=13537283&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13537283] where Brandon says the following:

bq. add new addBlock interface which takes the id as an additional field. The id is checked by checkLease(). *If it doesn't match current id in the found inode, NN fails the request.*

bq. Is the concern you are expressing that InodeID introduction is not a part of this jira?

The concern that I have is that introducing INode IDs doesn't seem like it's strictly required to address the bug described by this JIRA, and seems like a much heavier-weight solution than possible alternatives, for example something like what Todd proposed.

As I said previously, I can imagine several motivations for wanting unique INode IDs, but I don't think the bug described by this JIRA is necessarily one of them., Sorry I misunderstood the previous comment you had posted. So you are okay using INodeID in checkLease() to solve this bug.

bq. The concern that I have is that introducing INode IDs doesn't seem like it's strictly required to address the bug described by this JIRA,
If I understand it correctly, I agree with moving some of the subtasks to another umbrella jira that goes some thing like "Use InodeID as as an identifier of a file in HDFS protocols and APIs" and move some of the subtasks of this jira under that. We could also add description of what needs to be done and motivations.

Agree?, bq. Sorry I misunderstood the previous comment you had posted. 

No problem. :)

bq. So you are okay using INodeID in checkLease() to solve this bug.

Regarding this particular JIRA, given Todd's last comment, I'm not convinced that this is in fact currently a bug, since it seems like the current code may already handle this case properly. Let's investigate that separately from introducing INode IDs, perhaps by just writing up a little test case that exposes the bug.

bq. If I understand it correctly, I agree with moving some of the subtasks to another umbrella jira that goes some thing like "Use InodeID as as an identifier of a file in HDFS protocols and APIs" and move some of the subtasks of this jira under that. We could also add description of what needs to be done and motivations.

That sounds great, and I think that all of the INode ID work should be done on a branch off of trunk so that it can reasonably be done incrementally. Description of the design and motivations for the change would go into the new umbrella JIRA you propose.

Thanks a lot for your understanding, Suresh., BTW, I filed a related jira before I knew of this one, HDFS-4437, where I intended to simply revoke leases after a rename.  Even with this fileId sanity check, it doesn't address the larger issue of long running processes are left holding unusable leases for the renamed paths.  We may want to re-consider that jira., {quote}...perhaps by just writing up a little test case that exposes the bug.{quote}
The new test TestINodeFile.testWriteToRenamedFile in HDFS-4340 validate the fix to problem. , Thanks a lot for the pointer to the test, Brandon. Two things:

# It looks to me like that test doesn't actually exercise the scenario described in the description of this JIRA, since the test doesn't write to the file before the rename. It creates a file, then a parent of that empty file is renamed, then a new empty file is created with the same name, and then the first client writes to the recreated file. I suppose we can consider that a bug as well, since the first FSOutputStream can now still write to the empty file, but it's not nearly so bad as was first described by this JIRA, since we're not ending up with data in the file from two different clients.
# I think Daryn makes a really good point - to address this issue of clients still writing to renamed files, can't we just revoke the relevant leases at rename time? That seems like a way simpler way to fix this bug than adding an INode ID and threading that through a bunch of RPCs in the ClientProtocol, and has the other advantage Daryn mentioned., Linux and Unix allow users to continue writing to a file even it is renamed/moved. What do you think if we support the same behavior in HDFS?  I asked Daryn this question earlier.  What do you think?, I think FD-like behavior is a good idea, although we'd have to carefully test that MR and other components aren't intentionally or unintentionally relying on renaming a file breaking a lease.  Or if something is writing a log and assumes a broken write due to the file missing means that log rotation has occurred.  In both cases, renamed files that were assumed to become immutable will now continue to mutate.

We'll also have to carefully ensure that security cannot be bypassed by knowing or guessing fileIds.

In the meantime, can we implement HDFS-4437?  We've been running into issues with daemons holding leases to renamed files., I think I agree with Daryn. How about this:

* Let's implement HDFS-4437 and back-port that to branch-2 to fix the bug(s) that have been described here and on HDFS-4437.
* Move the INode ID-related work to a branch off of trunk.
* Once the INode ID work is completed, merge it to trunk.
* At a later date when we're fairly confident nothing is relying on the rename-breaks-leases semantics, we consider merging INode IDs to branch-2. Or, perhaps, the INode ID changes could be a good foundation for a useful Hadoop 3.x release.

Thoughts?, I've moved the INodeId related work to HDFS-4489. 

As in my comment in HDFS-4437, one concern with rename-breaks-leases semantics is the client is over-powered.
"one random client renames one of the top directories. It basically revokes all the leases granted to other clients under this directory. That is, for any client wants to gain a lease of any file, it could just rename it and then open it. Most likely it could get the lease immediately as long as it has write permission." 


, bq. As in my comment in HDFS-4437, one concern with rename-breaks-leases semantics is the client is over-powered. "one random client renames one of the top directories. It basically revokes all the leases granted to other clients under this directory. That is, for any client wants to gain a lease of any file, it could just rename it and then open it. Most likely it could get the lease immediately as long as it has write permission."

If the client has write permission on the file, couldn't it just use the {{DistributedFileSystem#recoverLease()}} API on the file, and then reopen it for append?, bq. As in my comment in HDFS-4437, one concern with rename-breaks-leases semantics is the client is over-powered. "one random client renames one of the top directories. It basically revokes all the leases granted to other clients under this directory. That is, for any client wants to gain a lease of any file, it could just rename it and then open it. Most likely it could get the lease immediately as long as it has write permission."

Isn't that behavior better than an over-powered client retaining the lease on the renamed file, blocking everyone else, yet is unable to do another with the file because it doesn't know where it is?  For daemons, it may retain the lease indefinitely.  I think it was hbase testing that was encountering  this issue., Brandon, how do you feel about my previous proposal to move the INode ID work to a branch and implement HDFS-4437 as described by Daryn in trunk in the meantime? If you're OK with it, I'll volunteer to create the branch, revert the commits from trunk, and replay the work done so far on the dev branch., bq. If the client has write permission on the file, couldn't it just use the DistributedFileSystem#recoverLease() API on the file, and then reopen it for append?
The main reason why recoverLease() was added was to essentially take over a lease from an active writer. Loosely speaking fence the other writer. That is the purpose of that method. I have hard time understanding the relationship of that with rename in this jira. In fact when recoverLease is done an active writer will not be able to write whether rename has occurred or not. That behavior is not being changed with the changes done as a part of this.

There are other ideas that have been proposed about revoking the lease on rename. I am -1 on it for the following reasons:
# Current behavior is when a rename occurs the current writer continues to write to the block that is currently allocated but fails to allocate new blocks.
# New rename behavior will be incompatible where the current writer is fenced from writing. Given that a lot of files in HDFS are less than a block in length, this could result in strange behaviors for some applications.
# I agree with the point Brandon raised above. Renaming a directory means walking through all the files open under it and revoking the leases. Rename already is a complicated operation. Doing this additional work during rename makes it even more heavier and the operation unpredictably large.

I actually like the direction Brandon and Nicholas have taken. We can continue the existing behavior. In fact with this change, we can allow current writer to continue to allocated new blocks (based on file ID) and continue to write, if we want. But that could be done in another jira.
, bq. I have hard time understanding the relationship of that with rename in this jira. In fact when recoverLease is done an active writer will not be able to write whether rename has occurred or not. That behavior is not being changed with the changes done as a part of this.

I wasn't implying that this proposed change has anything to do with the recoverLease API. I was just using it as a counterpoint to Brandon's claim that clients would be overpowered since "for any client wants to gain a lease of any file, it could just rename it and then open it. Most likely it could get the lease immediately as long as it has write permission." i.e. that should not be a concern, since a client can already do that today with the recoverLease API.

{quote}
There are other ideas that have been proposed about revoking the lease on rename. I am -1 on it for the following reasons:
# Current behavior is when a rename occurs the current writer continues to write to the block that is currently allocated but fails to allocate new blocks.
# New rename behavior will be incompatible where the current writer is fenced from writing. Given that a lot of files in HDFS are less than a block in length, this could result in strange behaviors for some applications.
# I agree with the point Brandon raised above. Renaming a directory means walking through all the files open under it and revoking the leases. Rename already is a complicated operation. Doing this additional work during rename makes it even more heavier and the operation unpredictably large.

I actually like the direction Brandon and Nicholas have taken. We can continue the existing behavior. In fact with this change, we can allow current writer to continue to allocated new blocks (based on file ID) and continue to write, if we want. But that could be done in another jira.
{quote}

I agree with Daryn's reasoning as stated in HDFS-4437:

{quote}
I think supporting file descriptor behavior is a great idea (we've internally talked about this). Until we do, I think the lease should be revoked. My concerns with fd behavior would be the ever pervasive "two wrongs make a right" where users are relying unintentionally on renames breaking writers, and ensuring we get the security right to avoid attacks probing for fileids.
{quote}

Regardless of whether or not we implement HDFS-4437 as Daryn proposed, I still think we should move the INode ID stuff to a branch. It's a fairly involved change with several sub-task JIRAs, which indicates to me that it would be better done incrementally on a branch and then merged to trunk once it's a completely functional whole., bq. I think supporting file descriptor behavior is a great idea (we've internally talked about this). Until we do, I think the lease should be revoked. My concerns with fd behavior would be the ever pervasive "two wrongs make a right" where users are relying unintentionally on renames breaking writers, and ensuring we get the security right to avoid attacks probing for fileids.

Aaron and Daryn, there is no security issue related to being able to guess fileids. It is nothing different from being able to guess valid paths in HDFS.

I posted a proposal to HDFS-4489. Please take a look., Sorry for the delay, I've been preoccupied.  There are security issues to consider: guessing a path won't bypass permission checks.  Whereas using an inode mapping will bypass permissions if the apis and implementation are not carefully considered.  I'll check out the referenced jira., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562477/HDFS-4258.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6685//console

This message is automatically generated., I think this JIRA / patch has gotten a bit stale, since INode IDs are already in, as well as HDFS-6294, which addressed the issues we had with moving files that were open for write.  I'm marking this as a dupe of HDFS-6294.  Feel free to reopen if there's something more we should do here.]