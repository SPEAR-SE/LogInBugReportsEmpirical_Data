[I check the log, find the following errors.  All the datanodes are alive, but what's the meaning of the WARN  "Failed to find datanode " ?   172.16.1.85 is the namenode,  The other IPs are for datanodes.

2016-06-28 10:44:57,995 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:44:57,996 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:44:57,996 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073744759_7794, replicas=172.16.1.143:9866, 172.16.1.92:9866, 172.16.1.87:9866 for /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.jar
2016-06-28 10:44:58,233 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.jar is closed by DFSClient_NONMAPREDUCE_1881763906_1
2016-06-28 10:44:58,239 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Increasing replication from 3 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.jar
2016-06-28 10:44:58,365 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Increasing replication from 3 to 10 for /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.split
2016-06-28 10:44:58,368 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:44:58,368 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:44:58,369 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073744760_7795, replicas=172.16.1.87:9866, 172.16.1.90:9866, 172.16.1.91:9866, 172.16.1.88:9866, 172.16.1.89:9866, 172.16.1.86:9866, 172.16.1.93:9866, 172.16.1.92:9866, 172.16.1.143:9866 for /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.split
2016-06-28 10:44:58,541 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.split is closed by DFSClient_NONMAPREDUCE_1881763906_1
2016-06-28 10:44:58,548 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:44:58,549 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:44:58,549 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073744761_7796, replicas=172.16.1.93:9866, 172.16.1.88:9866, 172.16.1.143:9866 for /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.splitmetainfo
2016-06-28 10:44:58,632 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1881763906_1
2016-06-28 10:44:58,773 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:44:58,773 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:44:58,774 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073744762_7797, replicas=172.16.1.91:9866, 172.16.1.143:9866, 172.16.1.86:9866 for /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.xml
2016-06-28 10:44:58,857 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job.xml is closed by DFSClient_NONMAPREDUCE_1881763906_1
2016-06-28 10:45:06,285 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:45:06,285 WARN org.apache.hadoop.net.NetworkTopology: Failed to find datanode (scope="" excludedScope="/default-rack").
2016-06-28 10:45:06,285 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073744763_7798, replicas=172.16.1.90:9866, 172.16.1.86:9866, 172.16.1.91:9866 for /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job_1467124628054_0001_1_conf.xml
2016-06-28 10:45:06,353 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/root/.staging/job_1467124628054_0001/job_1467124628054_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_2078921355_1
2016-06-28 10:45:12,227 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 9 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2016-06-28 10:45:12,228 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=9, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2016-06-28 10:45:12,228 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 9 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2016-06-28 10:45:12,228 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_-9223372036854736880_7799, replicas=172.16.1.90:9866, 172.16.1.87:9866, 172.16.1.93:9866, 172.16.1.86:9866, 172.16.1.88:9866, 172.16.1.143:9866, 172.16.1.92:9866, 172.16.1.89:9866 for /gaos/io_data/test_io_12
2016-06-28 10:45:12,541 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: updatePipeline(blk_-9223372036854736880_7799, newGS=7800, newLength=393216, newNodes=[172.16.1.90:9866, 172.16.1.87:9866, 172.16.1.93:9866, 172.16.1.86:9866, 172.16.1.88:9866, 172.16.1.143:9866, 172.16.1.92:9866, 172.16.1.89:9866, null:0], client=DFSClient_attempt_1467124628054_0001_m_000002_0_1932721620_1)
2016-06-28 10:45:12,542 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: updatePipeline(blk_-9223372036854736880_7799 => blk_-9223372036854736880_7800) success
2016-06-28 10:45:14,660 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_-9223372036854736864_7801, replicas=172.16.1.92:9866, 172.16.1.93:9866, 172.16.1.86:9866, 172.16.1.143:9866, 172.16.1.91:9866, 172.16.1.87:9866, 172.16.1.89:9866, 172.16.1.90:9866, 172.16.1.88:9866 for /gaos/io_data/test_io_4
2016-06-28 10:45:14,722 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_-9223372036854736848_7802, replicas=172.16.1.91:9866, 172.16.1.87:9866, 172.16.1.90:9866, 172.16.1.86:9866, 172.16.1.88:9866, 172.16.1.93:9866, 172.16.1.92:9866, 172.16.1.143:9866, 172.16.1.89:9866 for /gaos/io_data/test_io_8
2016-06-28 10:45:14,749 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_-9223372036854736832_7803, replicas=172.16.1.86:9866, 172.16.1.93:9866, 172.16.1.89:9866, 172.16.1.92:9866, 172.16.1.143:9866, 172.16.1.90:9866, 172.16.1.87:9866, 172.16.1.91:9866, 172.16.1.88:9866 for /gaos/io_data/test_io_18
............................


Also on the datanodes, ( e.g. 172.16.1.143),   there are some errors:,

java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:204)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:522)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:923)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:846)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:171)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:105)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)
	at java.lang.Thread.run(Thread.java:745)
2016-06-28 10:56:44,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-257845079-172.16.1.85-1466418599731:blk_-9223372036854736496_7847, type=LAST_IN_PIPELINE: Thread is interrupted.
2016-06-28 10:56:44,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-257845079-172.16.1.85-1466418599731:blk_-9223372036854736496_7847, type=LAST_IN_PIPELINE terminating
2016-06-28 10:56:44,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-257845079-172.16.1.85-1466418599731:blk_-9223372036854736496_7847 received exception java.io.IOException: Premature EOF from inputStream
2016-06-28 10:56:44,830 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: host-172-16-1-143:9866:DataXceiver error processing WRITE_BLOCK operation  src: /172.16.1.85:8185 dst: /172.16.1.143:9866
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:204)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:522)
................................., Other data nodes show a lot of Premature EOF errors too,  although I have set dfs.datanode.max.transfer.threads to 8192 and added  " *   -  nofile  655360,  "  in the /etc/security/limits.conf,.  

2016-06-28 10:45:16,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-257845079-172.16.1.85-1466418599731:blk_-9223372036854736697_7811
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:204)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:522)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:923)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:846)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:171)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:105)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)
	at java.lang.Thread.run(Thread.java:745)
2016-06-28 10:45:16,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-257845079-172.16.1.85-1466418599731:blk_-9223372036854736697_7811, type=LAST_IN_PIPELINE: Thread is interrupted.
2016-06-28 10:45:16,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-257845079-172.16.1.85-1466418599731:blk_-9223372036854736697_7811, type=LAST_IN_PIPELINE terminating
2016-06-28 10:45:16,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-257845079-172.16.1.85-1466418599731:blk_-9223372036854736697_7811 received exception java.io.IOException: Premature EOF from inputStream
2016-06-28 10:45:16,367 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: host-172-16-1-89:9866:DataXceiver error processing WRITE_BLOCK operation  src: /172.16.1.88:22616 dst: /172.16.1.89:9866
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:204), does the map reduce job is successfully or not. Or just the some child task is failed., The MR job failed and exited., 我也一样，写和读可以，但挂掉3个节点就读不出来，明天看看代码，哪有问题。
你那边put, get 一个文件，get的时候要挂掉3个节点，也会出这个错吧？
, put/get 在挂掉3个节点时候, 没有出错.   另外, TestDFSIO 虽然出错,但也不是每次都出错, 只是出错的几率很高, Hi guys,

Thanks for reporting this. Could you discuss in English so that others can understand.

It looks like, at least 4 blocks were lost from 9 ones in a coding group, but since only 3 datanodes were killed, it should be an issue. It could happen if the 4 blocks reside in the 3 datanodes, which means the 4 blocks were misplaced, or they should be in 4 datanodes instead. , [~gaoshbj], could you please analyse the client and datanode logs to check the possibility of poor network or datanode unreachable cases which can result in more than parity number of datanode read failures. FYI, recently HDFS-10697 issue discussed one such case., Thanks. but I feel that is not caused by network.  The cluster consists of  10 nodes ( 1 namenode and 9 datanodes ) , which are all virtual machines (15G memory for per vm) created in a same physical server machine. And IPs of these 10 nodes are assigned in a same internal network segment ( 192.168.X .X )., By running "hadoop fsck" , I find the following info.  It shows  the file /gaotest9/io_data/test_io_7  does not have 9 replicated blocks,   The machine  "192.168.122.57" is lost for this file. 

////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
/gaotest9/io_data/test_io_5 536870912 bytes, 1 block(s):  OK
0. BP-1490713436-192.168.122.69-1469603790713:blk_-9223372036854726320_8994 len=536870912 Live_repl=9  [/default-rack/192.168.122.200:50010, /default-rack/192.168.122.214:50010, /default-rack/192.168.122.43:50010, /default-rack/192.168.122.57:50010, /default-rack/192.168.122.23:50010, /default-rack/192.168.122.198:50010, /default-rack/192.168.122.92:50010, /default-rack/192.168.122.111:50010, /default-rack/192.168.122.178:50010]

/gaotest9/io_data/test_io_6 536870912 bytes, 1 block(s):  OK
0. BP-1490713436-192.168.122.69-1469603790713:blk_-9223372036854726256_8998 len=536870912 Live_repl=9  [/default-rack/192.168.122.214:50010, /default-rack/192.168.122.43:50010, /default-rack/192.168.122.57:50010, /default-rack/192.168.122.111:50010, /default-rack/192.168.122.23:50010, /default-rack/192.168.122.200:50010, /default-rack/192.168.122.178:50010, /default-rack/192.168.122.198:50010, /default-rack/192.168.122.92:50010]

/gaotest9/io_data/test_io_7 536870912 bytes, 1 block(s):  Under replicated BP-1490713436-192.168.122.69-1469603790713:blk_-9223372036854726064_9013. Target Replicas is 9 but found 8 live replica(s), 0 decommissioned replica(s) and 0 decommissioning replica(s).
0. BP-1490713436-192.168.122.69-1469603790713:blk_-9223372036854726064_9013 len=536870912 Live_repl=8  [/default-rack/192.168.122.178:50010, /default-rack/192.168.122.198:50010, /default-rack/192.168.122.92:50010, /default-rack/192.168.122.214:50010, /default-rack/192.168.122.23:50010, /default-rack/192.168.122.200:50010, /default-rack/192.168.122.111:50010, /default-rack/192.168.122.43:50010]

/gaotest9/io_data/test_io_8 536870912 bytes, 1 block(s):  OK
0. BP-1490713436-192.168.122.69-1469603790713:blk_-9223372036854726048_9012 len=536870912 Live_repl=9  [/default-rack/192.168.122.92:50010, /default-rack/192.168.122.198:50010, /default-rack/192.168.122.178:50010, /default-rack/192.168.122.200:50010, /default-rack/192.168.122.214:50010, /default-rack/192.168.122.23:50010, /default-rack/192.168.122.57:50010, /default-rack/192.168.122.43:50010, /default-rack/192.168.122.111:50010]

]