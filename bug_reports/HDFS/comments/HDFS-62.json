[This is the issue that prevented TestCheckpoint from passing in HADOOP-3694.

My patch is the same spirit as Carlos's but uses InetAddress.isAnyLocalAddress instead of a string compare. Uploading shortly., Fixes the behavior described in this ticket. Modifications to test improve the speed (100 seconds to 30 seconds on my machine where 0.0.0.0 lookup is very slow) and also verify the new behavior (this test fails with the old behavior as noted in HADOOP-3694), Todd, 

-can you use the code in DNS to get localhost reliably on more systems?  
, Steve: that would require the name of the local interface. In the default configuration, that isn't defined, unless I'm missing something.

Here's an alternate solution that I like better: I don't see any reason why the 2NN specifies its own IP address as the machine=<ip> parameter to the putimage request. There are two other options:
  - The NN could simply use the requester address of the HTTP request to determine the 2NN's IP.
  - The 2NN could simply use an HTTP PUT or POST to upload the checkpoint rather than requesting a GET callback. I don't know a lot about the Java HTTP facilities, but I assume this isn't too difficult., I think we could tease out the code that caches the local hostname and make it more public; that would save the 30s hangs whenever something tries to look it up on one of my badly configured boxes.

For Java upload, POST is good, goes through firewalls too. HttpClient is the best way to do this -it is already part of the hadoop core library dependencies. Don't waste time trying to understand java.net if you can help it., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12408757/hadoop-5626.txt
  against trunk revision 778182.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/394/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/394/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/394/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/394/console

This message is automatically generated., The test failures are unrelated (job history and capacity scheduler, both mapred whereas this only touches hdfs)

While I think the discussion above is valuable, since this blocks HADOOP-5888 I'd like for this to be committed and we can continue work on making image transfer more sane in another JIRA.

As for switching to POST rather than the "GET with a callback to another GET", I looked into this today and, while easy, I'm not sure it's a great idea. I think there are some people who are relying on the GetImageServlet on the 2NN for backup purposes, so removing the Jetty from the 2NN would break that., > As for switching to POST rather than the "GET with a callback to another GET", I 

I remember that when this was being designed the first time, I was told that there could be "permission" related issues to do a POST vs a GET. The POST is like a "write" somewhat, whereas a GET is more like a "read"., In BackupNode::getHttpServerAddress this issue also arises and is handled by using the rpcAddress, which has already been resolved to a real address.  It would be good to move your solution to NetUtils and access it from both the SecondaryNameNode and the BackupNode, to avoid code duplication., +1 to having something common

I think we also ought to spell out the minimum requirements of hadoop on a network, something like

/etc/hosts is well configured (local hostname doesnt map to 127.0.0.1 or ::1)
dns works
rDNS works
..etc
, Jakob and Steve: I definitely agree that we need a more centralized common way of doing this. I think HADOOP-4383 is a good place to suggest doing a more general overhaul of how services locate each other. I wrote up a brief proposal to Steve in a private email which I'll copy paste into that JIRA.

For now, I think making some small concrete steps is the best route. History has shown that it's difficult (and takes a lot of time) to get larger sweeping changes through the development pipeline, so we should work towards some small achievable goals while planning out the better solution., bq. For now, I think making some small concrete steps is the best route. History has shown that it's difficult (and takes a lot of time) to get larger sweeping changes through the development pipeline, so we should work towards some small achievable goals while planning out the better solution.

Service location issues are certainly worth looking at and you're right that HADOOP-4383 is a good place to start.  My comment dealt with your specific patch, which as it stands, is introducing a bit of code duplication in terms of resolving the full hostname of the backup node and secondary namenode.  I'd recommend updating your patch to avoid this by utilizing the net utils package as a repository of this rather generic function.  No need to introduce two ways of doing the same thing. :), Removing "Patch available" status to incorporate feedback and refactor the change into a static method in NetUtils., Seeing as there hasn't been any movement on this in more than a year, and that HDFS-1080 also solves this issue, I'd like to close this as won't fix.  I think I prefer the 1080 approach for two reasons: it relies on user-provided configuration to guarantee the hostname is reported as expected, rather than how we may be able to decipher, and also it's a smaller change with fewer moving parts.  

Thoughts?, Not hearing any objections, I'm resolving this issue.]