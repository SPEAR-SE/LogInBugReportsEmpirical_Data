[This is my patch to skip such problematic entries, This is patch to skip such entries. Note that it's against my "own copy" of 0.21 release tag, so revisions are from my svn, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12483166/patch.diff
  against trunk revision 1137675.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/799//console

This message is automatically generated., Looks like the checkpoint contains a record which tries to set time on a non-existing file. This should not happen. So the question is how did it happen? If it's a bug we should fix the cause. I don't see how, but if it's a legal scenario, then we can suppress NPE as you suggest., I dont know how does it happen, but it does often for me. I have a copy of namenode state that produces this problem on start I can share.
In general, as for me, it would be great to have an option to perform a checkpoint skipping invalid records of any kind. Because now any such record make namenode unusable on restart and with an option you will at most loose a bit of information., Some more details:
I am working on jobs, each produce multiple tasks. ZooKeeper is used to spread tasks over the computing cluster. 
Tasks inside single job communicate (produce result files that are used on start by another tasks) with HDFS.
The job has main control process. Before the job starts it creates directory to put communication files into and original job arguments.
After job is finished, it removes the directory with single call.
The problem was reproduced today with a number of communication files of a single job.
Note that while job is deleted when all the tasks is done, there may be some "hanging" tasks that did timeouted and was restarted. In this case, unhang may lead to access try to file that do not exists. 
I will try to check with logs if this was the case.
, I did check the job and could find nothing special. The files existed a little less then hour before the job was deleted. Nothing regarding file names in name node log.
Unfortunatelly I did not output the times being applied to the log, so I can't check if it's inside or outside (or on the edge of) the interval where files were existing., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12483166/patch.diff
  against trunk revision da2fb2b.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9475//console

This message is automatically generated., Cancelling this issue as stale since the patch no longer applies and given the number of changes to the NN in the past four years, there is a good chance the issue has been dealt with. If not, please re-open. 

Thanks.]