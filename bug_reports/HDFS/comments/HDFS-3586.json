[Attaching anlyased sheet, Thanks Brahma for filing the JIRA.

This is similar to HDFS-3493. But here the change is, DNs are available more than replication. So, ideally block should get replicated.
The problem here is, you have 2 live replicas and in remaining 2 DNs you have partial block present in RBW. So, when NN tries to replicate, DN will reject them saying block already exist in RBW. So, your replication may not happen even though you have more nodes.

Here I think the possible fix could be that, we should change the below condition 
*if (countNodes(b.stored).liveReplicas() >= bc.getReplication()) {*

to something like *if ((countNodes(b.stored).liveReplicas() + countNodes(b.stored).corruptReplicas())  > bc.getReplication()) {*

So, the extra corrupted blocks(more than replication) will get invalidated and later replication can work normally.

, Hi Konstantin,
 What is your suggestion on this issue?
Do you have any alternative idea in your mind?

Thanks
Uma, Uma, I think that the RBW replicas on DN3 and DN4 should eventually be transformed into corrupt replicas, because they should have a wrong generation stamp.
Then you are right it is similar to HDFS-3493. There is 4 nodes more than replication 3, but there are no nodes that can be used to replicate the block to, because all have a replica of the block.
I don't see how your suggestion solves the problem, as it doesn't work in case when all replicas are corrupt, if I understand it correctly., Hi Konstantin,

{code}
I don't see how your suggestion solves the problem, as it doesn't work in case when all replicas are corrupt, if I understand it correctly.
{code}

When all replicas corrupt, we need not expect it to be replicate, because there is no good replica to replicate.
But in this case we have 2 good replicas. What I am proposing is, keeping minimum of 3 replicas(replication) is fine((good + corrupt) (or) (all 3 corrupt) (or) (all 3 good replicas)), more than that if we get any replica corrupt, let's invalidate it.SO, that even if we have single good replica, it will be copied to other node in this kind of situation. Ideally having whole cluster filled with corrupt replica and good replicas is a very rare case and almost no possibility in bigger clusters. Worth considering for smaller clusters also. Because we have cluster size more than replication size here, user expectation is replica will be replicated properly., So you propose to sacrifice a corrupt replica when good ones are present. That makes sense to me. Useful for small clusters, agreed., Thanks Konstantin. 
Assigning it to Amith as he started working on this change., Hi Uma, why not provide a configuration item. Then user can decide whether to delete the corrupt replica as long as good one exists()(even (countNodes(b.stored).liveReplicas() + countNodes(b.stored).corruptReplicas()) < bc.getReplication()). This will be very helpful for small cluster., {quote}
even (countNodes(b.stored).liveReplicas() + countNodes(b.stored).corruptReplicas()) < bc.getReplication()). 
{quote}
This condition may delete the blocks when all replicas corrupted for a file. I think, our replication design don't want to delete when all replicas corrupted as anyway you can't replicate them anymore.

how about the below condition?
{quote}
if ((countNodes(b.stored).liveReplicas() + countNodes(b.stored).corruptReplicas()) >= bc.getReplication() && countNodes(b.stored).liveReplicas() > minReplication) 
{quote}, Uma, you are right. In my comments, there also be a condition (no in expression) as "as long as good one exists()". 
Your description is exactly coninsiding with my ideas. Hope it will be done., HDFS-3493 resolves the same issue. Now if the number of live replicas are more than minimum required and total replicas (live+corrupt) is more than replication factor, we invalidate the extra corrupt replica(s). Also, if the replica happens to be one that was discarded during a pipeline recovery, this will be invalidated if there are minimum number of live replicas irrespective of total replica count.

However, there is one possibility that can result in NN sending replication requests to copy a block to a DN with a write-pipeline-failed-replica. This is if the block is still being written when the reconnected DN sends a block report with an RBW/RWR replica for this block. I discussed this scenario in more detail in HDFS-2932. But for this situation, I think this jira can be closed as a duplicate to HDFS-3493. Please suggest., Hi [~usrikanth],

yes you are right.. HDFS-3493 solves the above issue. this jira can be resolved as duplicate.

as mentioned in HDFS-3493 changes, in following cases corrupt replica will be deleted.

{code}
    // case 1: have enough number of live replicas
    // case 2: corrupted replicas + live replicas > Replication factor
    // case 3: Block is marked corrupt due to failure while writing. In this
    //         case genstamp will be different than that of valid block.
    // In all these cases we can delete the replica.
    // In case of 3, rbw block will be deleted and valid block can be replicated
{code}, Closing,,Since it is addressed as part of HDFS-3493, Resolving as duplicate]