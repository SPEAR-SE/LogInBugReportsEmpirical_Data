[I did run the test in my local box before submitting patch. Don't have an idea why it fails after committed. Will look at it as soon as possible., After hours of debugging, I found the reason for the failure is that JobMonitor gets the following exception when it check the completeness of one job:
2012-06-23 19:52:18,196 ERROR [org.apache.hadoop.raid.JobMonitor@3515f1d3] raid.JobMonitor (JobMonitor.java:doMonitor(116)) - JobMonitor exception
java.io.IOException: Job status not available
  at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:315)
  at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:592)
  at org.apache.hadoop.raid.DistRaid.checkComplete(DistRaid.java:282)
  at org.apache.hadoop.raid.JobMonitor.doMonitor(JobMonitor.java:106)
  at org.apache.hadoop.raid.JobMonitor.run(JobMonitor.java:61)
  at java.lang.Thread.run(Thread.java:619)

But I don't know what causes this. I guess it's mapreduce bug because sometimes I get this exception while sometimes I don't. Anyone have an idea? Thanks!, It looks like there is no history server up and running.  In Yarn there is a race in the client.  If the client asks for status if the AM is still up and running then it will talk to the AM.  If it has exited, which it tends to do when the MR job has completed then the client will fall over to the history server.  It looks like while you are running using the minicluster there is no corresponding history server to fulfill the request., Do you mean I should use MiniMRYarnCluster instead of MiniMRCluster? Is there any example I could follow to start a job history server?, Resolving since HDFS-RAID has been removed from Hadoop.]