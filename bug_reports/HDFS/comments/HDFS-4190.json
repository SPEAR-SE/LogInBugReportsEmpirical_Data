[I bet we could get the same improvement by either:
a) increasing the "packet size" when block scanning to something like 1MB
b) using direct byte buffers in BlockSender, so that we can use the faster CRC verification.

There's already a patch floating around which starts to do (b) for the read path in general, and it would probably help in this case as well., Yes Todd, I have already tried #a, but complete block as single packet. Using memory mapped buffers gave better results than that.
But in both cases there is an improvement. I think we can do like local read with using direct byte bytebuffer with bigger size(may be equal to block size). So, here we can even reduce the buffer cleanup step when compared to memmapped buffer usage, we can reuse same buffer as we will do scanning sequentially with single thread.

{quote}
faster CRC verification
{quote}
doing fast CRC may improve scanning speed. But what I am looking the overhead here is seeks because currently it is loading 4096 size I think. correct me if I misunderstood your point.


, Could you point me the JIRA for #b ? Thanks Todd. So, that I will try to make use that in this case., Even if it's calling read wiht a 4096 byte buffer, the OS is certainly doing readahead on your behalf and coalescing the requests into larger chunks. You won't get a seek per read. You could also just use fadvise here to force readahead of larger chunks of the file -- the code's already there in BlockSender, we'd just need to make sure it's getting set properly for the scanner code path. (I think it is).

I'm generally against the idea of mmapping - in various benchmarks I've run in the past, it comes out to basically the same speed as large chunked read() calls, and sometimes slower in multi-threaded environments. The added complication of not being able to munmap easily in Java makes it even worse., bq. Could you point me the JIRA for #b ? Thanks Todd. So, that I will try to make use that in this case.

Turns out I was remembering wrong - that's what I get for going on JIRA before I've had any coffee! I was thinking of HDFS-3529, but that's for the write path. A similar improvement could be made on the read path to avoid a memcpy for the non-transferTo case., {code}
it comes out to basically the same speed as large chunked read() calls, and sometimes slower in multi-threaded environments
{code}
I am curious to know. You have already written the code for unmapping the mmapped buffers (we have to write JNI call for that right. Currently I just used cleaner class from Sun package to clean) ? If we don't clean them, I have seen bad behavior and after some time OOM errors as native memory won't be cleaned until fullGC or you used in C code directly. I have tried mmapped buffers in write path also. I have tried with 36 threads, I got around ~20% improvement. I did not try increasing them, not sure if it goes bad if i increase them.(here other improvement involved that, I have allocated complete block once, added the packet content to that buffers. So, allocating consecutive memory location would improve somewhat I guess like using fallocate). CLeaning that memmapped buffer is again heavy operation. So, I cleaned them in asynchronous thread. That gives me that improvement. If I clean them sequentially then result is in negative :( . 

I have one question here: 
fadvise options are global in DFS right? so, when I have random small reads, then readahead may create unnecessary load data? (I am not expert in fadvise options internal behaviors :-) )


{quote}
Turns out I was remembering wrong - that's what I get for going on JIRA before I've had any coffee! I was thinking of HDFS-3529, but that's for the write path. A similar improvement could be made on the read path to avoid a memcpy for the non-transferTo case.
{quote}
Ok. Yes, we can do similar.
, {quote}
I think we can do like local read with using direct byte bytebuffer with bigger size(may be equal to block size). So, here we can even reduce the buffer cleanup step when compared to memmapped buffer usage, we can reuse same buffer as we will do scanning sequentially with single thread.
{quote}
Uma, this sounds like a very good idea!  Since some setup may use a large block size, say 512MB or 1GB, it is probably good to limit the buffer to some value, say 64MB., Thanks a lot Nicholas and Todd for the comments. I am just trying to refactor BlockReadLocal class and use that in scanning. Let me check what improvement we can get.]