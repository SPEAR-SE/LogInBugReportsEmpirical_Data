[{code}
RUNNING: /usr/lib/hadoop/bin/hadoop org.apache.hadoop.fs.slive.SliveTest -rename 14,uniform -packetSize 65536 -baseDir webhdfs://ha-2-secure:50070/user/hrt_qa/ha-slive -seed 12345678 -sleep 100,1000 -duration 600 -append 14,uniform -blockSize 16777216,33554432 -create 16,uniform -mkdir 14,uniform -maps 15 -ls 14,uniform -writeSize 1,134217728 -files 1024 -ops 10000 -read 14,uniform -replication 1,3 -appendSize 1,134217728 -reduces 8 -resFile /grid/0/tmp/hwqe/artifacts/ha-slive-00006-namenode2-1394091404.out -readSize 1,4294967295 -dirSize 16 -delete 14,uniform
INFO|Initial wait for Service namenode2: 60
14/03/06 07:36:44 INFO slive.SliveTest: Running with option list -rename 14,uniform -packetSize 65536 -baseDir webhdfs://ha-2-secure:50070/user/hrt_qa/ha-slive -seed 12345678 -sleep 100,1000 -duration 600 -append 14,uniform -blockSize 16777216,33554432 -create 16,uniform -mkdir 14,uniform -maps 15 -ls 14,uniform -writeSize 1,134217728 -files 1024 -ops 10000 -read 14,uniform -replication 1,3 -appendSize 1,134217728 -reduces 8 -resFile /grid/0/tmp/hwqe/artifacts/ha-slive-00006-namenode2-1394091404.out -readSize 1,4294967295 -dirSize 16 -delete 14,uniform
14/03/06 07:36:44 INFO slive.SliveTest: Options are:
14/03/06 07:36:44 INFO slive.ConfigExtractor: Base directory = webhdfs://ha-2-secure:50070/user/hrt_qa/ha-slive/slive
14/03/06 07:36:44 INFO slive.ConfigExtractor: Data directory = webhdfs://ha-2-secure:50070/user/hrt_qa/ha-slive/slive/data
14/03/06 07:36:44 INFO slive.ConfigExtractor: Output directory = webhdfs://ha-2-secure:50070/user/hrt_qa/ha-slive/slive/output
14/03/06 07:36:44 INFO slive.ConfigExtractor: Result file = /grid/0/tmp/hwqe/artifacts/ha-slive-00006-namenode2-1394091404.out
14/03/06 07:36:44 INFO slive.ConfigExtractor: Grid queue = default
14/03/06 07:36:44 INFO slive.ConfigExtractor: Should exit on first error = false
14/03/06 07:36:44 INFO slive.ConfigExtractor: Duration = 600000 milliseconds
14/03/06 07:36:44 INFO slive.ConfigExtractor: Map amount = 15
14/03/06 07:36:44 INFO slive.ConfigExtractor: Reducer amount = 8
14/03/06 07:36:44 INFO slive.ConfigExtractor: Operation amount = 10000
14/03/06 07:36:44 INFO slive.ConfigExtractor: Total file limit = 1024
14/03/06 07:36:44 INFO slive.ConfigExtractor: Total dir file limit = 16
14/03/06 07:36:44 INFO slive.ConfigExtractor: Read size = 1,4294967295 bytes
14/03/06 07:36:44 INFO slive.ConfigExtractor: Write size = 1,134217728 bytes
14/03/06 07:36:44 INFO slive.ConfigExtractor: Append size = 1,134217728 bytes
14/03/06 07:36:44 INFO slive.ConfigExtractor: Block size = 16777216,33554432 bytes
14/03/06 07:36:44 INFO slive.ConfigExtractor: Random seed = 12345678
14/03/06 07:36:44 INFO slive.ConfigExtractor: Sleep range = 100,1000 milliseconds
14/03/06 07:36:44 INFO slive.ConfigExtractor: Replication amount = 1,3
14/03/06 07:36:44 INFO slive.ConfigExtractor: Operations are:
14/03/06 07:36:44 INFO slive.ConfigExtractor: LS
14/03/06 07:36:44 INFO slive.ConfigExtractor:  UNIFORM
14/03/06 07:36:44 INFO slive.ConfigExtractor:  14%
14/03/06 07:36:44 INFO slive.ConfigExtractor: READ
14/03/06 07:36:44 INFO slive.ConfigExtractor:  UNIFORM
14/03/06 07:36:44 INFO slive.ConfigExtractor:  14%
14/03/06 07:36:44 INFO slive.ConfigExtractor: APPEND
14/03/06 07:36:44 INFO slive.ConfigExtractor:  UNIFORM
14/03/06 07:36:44 INFO slive.ConfigExtractor:  14%
14/03/06 07:36:44 INFO slive.ConfigExtractor: CREATE
14/03/06 07:36:44 INFO slive.ConfigExtractor:  UNIFORM
14/03/06 07:36:44 INFO slive.ConfigExtractor:  16%
14/03/06 07:36:44 INFO slive.ConfigExtractor: RENAME
14/03/06 07:36:44 INFO slive.ConfigExtractor:  UNIFORM
14/03/06 07:36:44 INFO slive.ConfigExtractor:  14%
14/03/06 07:36:44 INFO slive.ConfigExtractor: DELETE
14/03/06 07:36:44 INFO slive.ConfigExtractor:  UNIFORM
14/03/06 07:36:44 INFO slive.ConfigExtractor:  14%
14/03/06 07:36:44 INFO slive.ConfigExtractor: MKDIR
14/03/06 07:36:44 INFO slive.ConfigExtractor:  UNIFORM
14/03/06 07:36:44 INFO slive.ConfigExtractor:  14%
14/03/06 07:36:44 INFO slive.SliveTest: Running job:
14/03/06 07:36:44 WARN hdfs.DFSClient: dfs.client.test.drop.namenode.response.number is set to 1, this hacked client will proactively drop responses
14/03/06 07:36:45 WARN hdfs.DFSClient: dfs.client.test.drop.namenode.response.number is set to 1, this hacked client will proactively drop responses
14/03/06 07:36:45 WARN hdfs.DFSClient: dfs.client.test.drop.namenode.response.number is set to 1, this hacked client will proactively drop responses
14/03/06 07:36:45 ERROR slive.SliveTest: Unable to run job due to error:
java.lang.IllegalArgumentException: java.net.UnknownHostException: ha-2-secure
at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:377)
at org.apache.hadoop.security.SecurityUtil.buildDTServiceName(SecurityUtil.java:258)
at org.apache.hadoop.fs.FileSystem.getCanonicalServiceName(FileSystem.java:299)
at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:521)
at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:505)
at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121)
at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:127)
at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:460)
at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:343)
at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:833)
at org.apache.hadoop.fs.slive.SliveTest.runJob(SliveTest.java:205)
at org.apache.hadoop.fs.slive.SliveTest.run(SliveTest.java:118)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
at org.apache.hadoop.fs.slive.SliveTest.main(SliveTest.java:315)
Caused by: java.net.UnknownHostException: ha-2-secure
... 30 more
{code}, This is actually a similar issue with HDFS-5339: SecurityUtil#buildTokenService tries to resolve the name service id as a host name. Since HDFS-5339 already figures out the token service name for webhdfs filesystem in the initialization, we can simply override the getCanonicalServiceName method and return the tokenServiceName., A simple patch to fix., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12633480/HDFS-6077.000.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6348//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6348//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12633480/HDFS-6077.000.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6362//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6362//console

This message is automatically generated., +1, I've committed this to trunk, branch-2 and branch-2.4.0., SUCCESS: Integrated in Hadoop-trunk-Commit #5299 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5299/])
HDFS-6077. running slive with webhdfs on secure HA cluster fails with unkown host exception. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1576076)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #506 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/506/])
HDFS-6077. running slive with webhdfs on secure HA cluster fails with unkown host exception. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1576076)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1698 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1698/])
HDFS-6077. running slive with webhdfs on secure HA cluster fails with unkown host exception. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1576076)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1723 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1723/])
HDFS-6077. running slive with webhdfs on secure HA cluster fails with unkown host exception. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1576076)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java
]