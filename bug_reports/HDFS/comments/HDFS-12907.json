[[~andrew.wang], does this sound reasonable?, SGTM, we locked it down originally since we didn't know of a usecase besides distcp (which often runs as a superuser). The contents of the FEInfo are already accessible to anyone who has permissions to read the file if they write a custom DFSClient., bq.  Allowing non-superusers to easily read the raw bytes will be extremely useful for regular users, esp. for enabling webhdfs client-side encryption.
I am wondering why only read access ?
If the user has access to write in the encrypted directory, we should not block them to access /.reserved/raw/ directory structure.
Any thoughts ?, It's so they don't accidentally write data without xattrs or with the wrong xattrs, which would be essentially corrupt. We also don't want plaintext getting written accidentally.

The NameNode does a fair amount of work at create time to provision an EDEK for the file. This logic is beyond the ability of most clients., Attaching a simple patch., bq. It's so they don't accidentally write data without xattrs or with the wrong xattrs, which would be essentially corrupt. 
This morning, Rushabh and I discussed/debated non-superuser write access, and my position too is non-superusers should not have access to raw attrs like feinfo.  I see no use case except allowing users to (un)intentionally lose access to their data.  Except...

I am inclined to believe that create should have been extended to allow an optional feinfo.  NN verifies the key name is correct, output stream doesn't encrypt.  Or creating a raw path requires a mandatory feinfo.  Lots of messy backwards compat issues to consider which is why it's a topic for another jira., I think that would work, though of course I'd prefer not to open up internal state representation if it can be avoided.

On the topic of webhdfs client-side encryption, could you talk a little more about your usecase? We discussed this internally before in the context of Hue, and there didn't seem to be a great solution. They have a very simple Python WebHDFS client built around effectively curl, and they'd need to add their own KMS client and encryption routines. Really though, we'd want to move this all the way to the browser, and write the KMS client and encryption routines in Javascript. Ouch.

A way of scoping the KMS delegation token to limit what keys could be accessed would also be an improvement, e.g. a "key token" similar to the HDFS block token. It addresses some of the issues with webhdfs and encryption., bq. On the topic of webhdfs client-side encryption, could you talk a little more about your usecase? 
The use case is read/write to encrypted directory via WebhdfsfileSystem.
If we follow the current community implementation, we have to create a new user to run datanode as (lets say {{dn}} separate from the user with which the namenode runs: {{hdfs}}).
Also we need to whitelist {{dn}} user to be able to decrypt {{edek}}.
If someone gets access to {{dn}} user, they can easily decrypt all client's edek., Solving this for WebHdfsFileSystem is a lot more tractable than Hue, so this makes sense to me.

FYI [~xiaochen] also, since he's a KMS expert and was also involved in the internal discussions., HDFS-12355 is tracking all the efforts for adding EZ support to WebhdfsFileSystem., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 11m 56s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 16m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 20s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 50s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 51s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 92m 42s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 23s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}154m 38s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.fs.TestUnbuffer |
|   | hadoop.hdfs.qjournal.server.TestJournalNodeSync |
|   | hadoop.hdfs.web.TestWebHDFSXAttr |
|   | hadoop.hdfs.server.namenode.TestFileContextXAttr |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.server.namenode.TestNameNodeXAttr |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-12907 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12901126/HDFS-12907.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 67f1b5e640eb 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / acb9290 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/22316/artifact/out/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22316/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22316/testReport/ |
| Max. process+thread count | 4140 (vs. ulimit of 5000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22316/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks Andrew for the ping and Rushabh / Daryn for discussions.

Sorry I did not fully understand the intent here, and probably misunderstood some part of HDFS-12355. Could you help elaborating?

My understanding is after HDFS-12355, webhdfs eventually works with encryption by:
- user gets the DT from hdfs and kms.
- user read / write a file, auths with HDFS using DT, get file status, then gets redirected to a DN
- user passes the DTs along to the DN, where read/write a file with the crypto streams happens.
- CryptoStreams auths with KMS using kms DT. The data is then read, decrypted and returned.
- user cancels the DT.

Is this remotely correct? Why do we need to run datanode as a separate user?
(I think I understood Daryn's comment, and agree it would be another jira. Not 100% sure I see the relation here, are we trying to write raw bytes to the DN and decrypt at the client-side instead of on the DN on HDFS-12355?), Following are the tests that failed.
{noformat}
TestUnbuffer
TestJournalNodeSync
TestDataNodeVolumeFailureReporting
TestNameNodeXAttr
TestWebHDFSXAttr
TestFileContextXAttr
{noformat}

Last 3 tests {{TestNameNodeXAttr,TestWebHDFSXAttr,TestFileContextXAttr}} are failed by my patch.
Remaining are just flaky tests.
{{TestUnbuffer}} -- Tracked by HADOOP-15056. This jira is almost closed to being resolved.
Remaining 2 tests passed locally.
{noformat}
[INFO] Running org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeSync
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 52.376 s - in org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeSync
[INFO] Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
[INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 86.746 s - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting
{noformat}
Will attach a new patch soon which fixes the 3 test failures.


Regarding the findbugs warning, it is not related to my patch.
, [~daryn], [~andrew.wang]: can you please review., bq. My understanding is after HDFS-12355 \[...\] , Is this remotely correct?

No, the goal is all encryption/decryption is done on the client.  The DN will not be given KMS tokens.  Ever.  It will not talk to the KMS.  Ever.  The DN will never encrypt/decrypt.

<rage level="furious">
The KMS client completely breaks all the ugi-semantics to enable DNs to do the encrypt/decrypt.  How?  Why?  First off, the KMS client morphs based on the caller's ugi context.  Clients are expected to always be who they were when created.  Imagine if an IPC client was user1, disconnected, and magically became user2 just because it was in another context.  That's the KMS client.

It gets worse. If the current ugi is a proxy user, the KMS client will try to authenticate as the real user.  That's fine when the ugi real user  is the login user of a service (ex. oozie).  But if there are _no credentials_, ex. proxy ugi from a token, the client willfully decides to use the login user's credentials!  And for good measure, let's proxy as the effective ugi!  That's super bizarre.

So let's put it together with the DN.  I use webhdfs as "daryn (token)", but the DN connects to the KMS as  "daryn via dn (kerberos)".  Or I submit a job with oozie, so I'm "daryn via oozie (token)" to the DN but it connects to the KMS as "daryn via dn (kerberos)".  Wow, that would never work right?  It would it you told users to make all their DNs be proxy users on the KMS!  And since most people map their DNs to the hdfs superuser, which is a really bad idea, you now have let admins have the ability to decrypt any file.

Both Cloudera and Hortonworks actually documented this security insanity.  Cloudera's docs appear to be gone now, but used to acknowledge with a yellow box like "this is a bad idea, but if you really want to...".  HortonWorks docs still exist with a footnote like "oh yeah, if you are still paying attention after clicking all the ui buttons, all your nodes now have access to all your keys, might want to consider changing your superuser".  

If you allow every node in your cluster the ability to decrypt everything on your cluster.  Why did you even enable security let alone EZ?  It's a rotten idea that should have never been implemented or passed a review.  It's what happens when a feature is rushed.
</rage>
––
Phew.  I value my security and data.  I'm sure as hell not making my DNs be proxy users, but we're stuck not breaking all the people that go to sleep with a false sense of cluster security.  So in the new design in progress, the DN is used as a dumb passthrough of encrypted bytes.  Never encrypts/decrypts or even talks to the KMS.  It can be done in a compatible way by the client sending a header to the NN that it knows how to handle EZ.  A new NN gives back the feinfo and prefaces the redirect path with /.reserved/raw.  That works across both old and new nodes and clusters.  Should be beautiful.  Stay tuned., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 10m 20s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m  5s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m  6s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  2m 14s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 46s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}102m  7s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 27s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}167m 17s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestErasureCodingPolicies |
|   | hadoop.hdfs.TestFileChecksum |
|   | hadoop.fs.TestUnbuffer |
|   | hadoop.hdfs.TestSafeModeWithStripedFileWithRandomECPolicy |
|   | hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure050 |
|   | hadoop.hdfs.server.namenode.TestSecurityTokenEditLog |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure080 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
|   | hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure160 |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-12907 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12901178/HDFS-12907.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 3d02135f23b4 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d6c31a3 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/22324/artifact/out/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22324/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22324/testReport/ |
| Max. process+thread count | 3630 (vs. ulimit of 5000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22324/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks for the explanation Daryn.

The rage block is what's painful for us as well, when supporting different downstream components and partners. I hope the kms clients were designed / documented / implemented / reviewed perfectly too. Interestingly if you see the history of KMSCP class you'll see quite a few attempts to make it 'work for the case of xxx'. Maybe what you described can be fixed in a new jira, and consider some of the past behaviors simply wrong so we don't worry about compatibility. 

Agree letting the DN to pass through raw bytes would be great. I hope this can be rolled into a simple design doc for HDFS-12355 so other people can figure out easily., bq.  I hope the kms clients were designed / documented / implemented / reviewed perfectly too. Interestingly if you see the history of KMSCP class you'll see quite a few attempts to make it 'work for the case of xxx'.
Sadly, no.  The kms client, specifically KSMCP, is the source of the detailed problems.

bq. consider some of the past behaviors simply wrong so we don't worry about compatibility.
That would be fantastic but I suspect this was done as a rush to support components like Hue, HttpFs. Jupyter, maybe Knox, etc when means it's going to be a hard sell to break them...
, Test failures from #001 patch.
It fixed the actual test failures from previous patch.
I have no idea when  EC related test failures will be fixed.
{noformat}

[INFO] Running org.apache.hadoop.fs.TestUnbuffer
[ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.3 s <<< FAILURE! - in org.apache.hadoop.fs.TestUnbuffer
[ERROR] testUnbufferException(org.apache.hadoop.fs.TestUnbuffer)  Time elapsed: 0.125 s  <<< FAILURE!
java.lang.AssertionError: Expected test to throw (an instance of java.lang.UnsupportedOperationException and exception with message a string containing "this stream org.apache.hadoop.fs.FSInputStream$$EnhancerByMockitoWithCGLIB$$67df4153 does not support unbuffering")
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.rules.ExpectedException.failDueToMissingException(ExpectedException.java:184)
	at org.junit.rules.ExpectedException.access$100(ExpectedException.java:85)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:170)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:369)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:275)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:239)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:160)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:373)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:334)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:119)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:407)

[INFO] Running org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 27.369 s - in org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean
[INFO] Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes
[INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 69.555 s - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes
[INFO] Running org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.583 s - in org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog
[INFO] Running org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure
[WARNING] Tests run: 18, Failures: 0, Errors: 0, Skipped: 10, Time elapsed: 180.385 s - in org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure
[INFO] Running org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure050
[WARNING] Tests run: 18, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 166.602 s - in org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure050
[INFO] Running org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure080
[WARNING] Tests run: 18, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 268.573 s - in org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure080
[INFO] Running org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure160
[WARNING] Tests run: 18, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 226.743 s - in org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure160
[INFO] Running org.apache.hadoop.hdfs.TestErasureCodingPolicies
[INFO] Tests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 89.64 s - in org.apache.hadoop.hdfs.TestErasureCodingPolicies
[INFO] Running org.apache.hadoop.hdfs.TestFileChecksum
[INFO] Tests run: 30, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 293.067 s - in org.apache.hadoop.hdfs.TestFileChecksum
[INFO] Running org.apache.hadoop.hdfs.TestSafeModeWithStripedFileWithRandomECPolicy
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.744 s - in org.apache.hadoop.hdfs.TestSafeModeWithStripedFileWithRandomECPolicy

[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR]   TestUnbuffer.testUnbufferException Expected test to throw (an instance of java.lang.UnsupportedOperationException and exception with message a string containing "this stream org.apache.hadoop.fs.FSInputStream$$EnhancerByMockitoWithCGLIB$$67df4153 does not support unbuffering")
[INFO] 
[ERROR] Tests run: 145, Failures: 1, Errors: 0, Skipped: 16
{noformat}
{{TestUnbuffer#testUnbufferException}} should be fixed by HADOOP-15056.

[~daryn]: can you please give your blessings ?
, bq. KSMCP
FWIW I also dreamt the KMS uses Hadoop RPC instead of restful APIs over tomcat, when I was dealing with KMSCP auth issues and perf issues. Though that has its own pros and cons for sure...

At least fixes like HADOOP-12787 should work after your webhdfs jiras.

FWIW, I think the initial KMSCP design _was_ to make it to be when they were created. But it didn't work for some cases with the stir of client cache, proxy and token. HADOOP-13749 is where this was changed. , Minor things:
# The case statements should be indented within the switch block.  It's like writing an if or while w/o indenting the body.
# Should change the test {{testAdminAccessOnly}} to something like {{testUserReadAccessOnly}} to reflect what it's now testing.
# There's no need to try-catch just to call Assert.fail.  All it does is swallow the exception that caused the unexpected test failure.  Get rid of the try blocks and just let the exception itself fail the test., Thanks [~daryn] for the reviews.
All the comments are addressed in the patch v#002 except one.
bq. The case statements should be indented within the switch block. It's like writing an if or while w/o indenting the body.
Didn't I do the same thing ?
The checkstyle also didn't complain., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 15m 11s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 29s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 56s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 50s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}135m 55s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 46s{color} | {color:red} The patch generated 42 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}207m  0s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure150 |
|   | hadoop.hdfs.TestAppendSnapshotTruncate |
|   | hadoop.hdfs.TestDFSStorageStateRecovery |
|   | hadoop.hdfs.crypto.TestHdfsCryptoStreams |
|   | hadoop.hdfs.TestParallelShortCircuitLegacyRead |
|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |
|   | hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerWithStripedBlocks |
|   | hadoop.hdfs.TestQuota |
|   | hadoop.hdfs.TestDFSStartupVersions |
|   | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |
|   | hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerForXAttr |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
|   | hadoop.hdfs.TestRollingUpgrade |
|   | hadoop.hdfs.TestWriteRead |
|   | hadoop.hdfs.tools.TestDFSAdminWithHA |
|   | hadoop.hdfs.TestErasureCodingMultipleRacks |
|   | hadoop.hdfs.TestDFSStripedOutputStream |
|   | hadoop.hdfs.TestErasureCodingPolicyWithSnapshotWithRandomECPolicy |
|   | hadoop.hdfs.TestDFSInputStream |
|   | hadoop.hdfs.TestHDFSTrash |
|   | hadoop.hdfs.TestReplication |
|   | hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |
|   | hadoop.hdfs.qjournal.TestSecureNNWithQJM |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.qjournal.client.TestQJMWithFaults |
|   | hadoop.hdfs.TestReadStripedFileWithDNFailure |
|   | hadoop.hdfs.qjournal.TestNNWithQJM |
|   | hadoop.hdfs.TestFileChecksum |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-12907 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12901528/HDFS-12907.002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux d8ccce144fc9 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 312ceeb |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/22349/artifact/out/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22349/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22349/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/22349/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 2756 (vs. ulimit of 5000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22349/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Difference between v001 and v002 patch is only in test code namely {{TestReservedRawPaths, FSXAttrBaseTest}}.
Both of the tests passed.
So I think all the failures are not related to the latest patch.
Many of them failed with {{unable to create new native thread}}.
[~daryn] please review., The xattr test change brings up an interesting case.  Unless [~andrew.wang] objects, I think it's also ok to allow users to see raw xattrs if they have read access.  That gets us 1/2 to supporting distcp in backwards compatible manner.  The test should actually verify not just that it passes, but that it actually returned the expected xattr.

The switch/case on the same ident level is still bothering me..., xattr change makes sense based on the scope of this change.

We should also still validate the case that users that don't have read access can't access the raw xattrs, if we aren't., Attaching a new patch.
Following comments are addressed.
1. Allowing user to see raw xattrs if they have read access.
2. Added test to verify that user who don't have access are not allowed to getattr.
3. Fixed the switch statement indentation., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 10m  3s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 17m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 44s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  2m  0s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 38s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 8 new + 101 unchanged - 0 fixed = 109 total (was 101) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 58s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 92m 33s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 21s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}155m 17s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.TestSetrepIncreasing |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithRandomECPolicy |
|   | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure200 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure150 |
|   | hadoop.hdfs.TestFileChecksum |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-12907 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12901789/HDFS-12907.003.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux e84c5296390b 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2abab1d |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/22374/artifact/out/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/22374/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22374/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22374/testReport/ |
| Max. process+thread count | 4338 (vs. ulimit of 5000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22374/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, All of the test failures are due to {{unable to create new native thread}} except the ones listed below.
1. TestUnderReplicatedBlocks#testSetRepIncWithUnderReplicatedBlocks: The test timed out. Tracked by HDFS-9243
2. TestDecommissioningStatus#testDecommissionLosingData: It failed with following error message.
{noformat}
Problem binding to [localhost:60134] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
{noformat}

It passes locally on my machine.
{noformat}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus
[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.331 s - in org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus
{noformat}

Regarding checkstyle issues.
6 of 8 warnings are due to identation of switch statements.
Will fix the remaining 2 in next revision.

As always, findbugs warning is not related to the patch.
Tracked by HDFS-12915

[~daryn]: please review. This patch is blocking HDFS-12574., Feeling bad to waste jenkins resources for fixing checkstyle warnings.
Diff with the previous patch.
{noformat}
 diff ~/patches/jira/HDFS-12907.003.patch  ~/patches/jira/HDFS-12907.004.patch 
117c117
< index 8aa5dc9b92d..834e570b291 100644
---
> index 8aa5dc9b92d..43eeadf0e50 100644
230c230
< +          .createUserForTesting("fakeUser", new String[] { "fakeGroup" });
---
> +          .createUserForTesting("fakeUser", new String[] {"fakeGroup"});
{noformat}, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 40s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 31s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  2m  5s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 36s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 7 new + 101 unchanged - 0 fixed = 108 total (was 101) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 52s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}120m 29s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 26s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}177m 33s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |
|   | hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-12907 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12901914/HDFS-12907.004.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 493c01a47cc6 3.13.0-135-generic #184-Ubuntu SMP Wed Oct 18 11:55:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 10fc8d2 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/22382/artifact/out/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/22382/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22382/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22382/testReport/ |
| Max. process+thread count | 3208 (vs. ulimit of 5000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22382/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Th failed test has nothing to do with my patch. The only change from previous patch is fixing whitespace warnings.
TestRetryCacheWithHA#testUpdatePipeline is tracked by HDFS-7524.
, +1. Will check in tomorrow morning if there are no objections, [~andrew.wang]?, Attaching branch-2 patch., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 16m  9s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
|| || || || {color:brown} branch-2 Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m  2s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 50s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 31s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  1s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 11s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 10s{color} | {color:green} branch-2 passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 27s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 7 new + 98 unchanged - 0 fixed = 105 total (was 98) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 21s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  7s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 76m  1s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 59s{color} | {color:red} The patch generated 121 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}121m 15s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Unreaped Processes | hadoop-hdfs:17 |
| Failed junit tests | hadoop.hdfs.TestFileAppendRestart |
|   | hadoop.hdfs.web.TestWebHDFS |
|   | hadoop.hdfs.web.TestFSMainOperationsWebHdfs |
| Timed out junit tests | org.apache.hadoop.hdfs.TestLeaseRecovery2 |
|   | org.apache.hadoop.hdfs.TestHdfsAdmin |
|   | org.apache.hadoop.hdfs.TestSetrepDecreasing |
|   | org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure |
|   | org.apache.hadoop.hdfs.TestDatanodeDeath |
|   | org.apache.hadoop.hdfs.TestPread |
|   | org.apache.hadoop.hdfs.TestSafeMode |
|   | org.apache.hadoop.hdfs.TestDFSFinalize |
|   | org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS |
|   | org.apache.hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer |
|   | org.apache.hadoop.hdfs.TestBlockStoragePolicy |
|   | org.apache.hadoop.hdfs.TestRollingUpgrade |
|   | org.apache.hadoop.hdfs.TestDFSInputStream |
|   | org.apache.hadoop.hdfs.TestRollingUpgradeRollback |
|   | org.apache.hadoop.hdfs.TestEncryptedTransfer |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:17213a0 |
| JIRA Issue | HDFS-12907 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12901962/HDFS-12907.branch-2.004.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux d256c6a8bd5a 3.13.0-129-generic #178-Ubuntu SMP Fri Aug 11 12:48:20 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2 / d69b735 |
| maven | version: Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-10T16:41:47+00:00) |
| Default Java | 1.7.0_151 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/22393/artifact/out/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| Unreaped Processes Log | https://builds.apache.org/job/PreCommit-HDFS-Build/22393/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs-reaper.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22393/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22393/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/22393/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 4867 (vs. ulimit of 5000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22393/console |
| Powered by | Apache Yetus 0.7.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Almost all the tests in branch-2 failed due to {{java.lang.OutOfMemoryError: unable to create new native thread}}.
The difference between branch-2 patch and trunk patch is as follows:
{noformat}
199c199
< +      Path parentPath = new Path("/foo");
---
> +      final Path parentPath = new Path("/foo");
205c205
< +      Path childPath = new Path("/foo/bar");
---
> +      final Path childPath = new Path("/foo/bar");
215c215
< +      Path rawChildPath =
---
> +      final Path rawChildPath =
{noformat}
Also branch-2 patch applies cleanly to branch-2.8.
I compiled and ran relevant test cases in branch-2.8 after the patch and all looks clean., Committed this to trunk, branch-3.0, branch-2, branch-2.9 and branch-2.8. I enjoyed reading the discussion in this jira. Thanks everyone., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #13378 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/13378/])
HDFS-12907. Allow read-only access to reserved raw for non-superusers. (kihwal: rev f5a72424c0009c454aab6759c30f74b397a7e935)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/XAttrPermissionFilter.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java
]