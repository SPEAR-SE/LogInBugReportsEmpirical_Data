[A bit more details. {{FSNamesystem.getAdditionalBlock()}} consists of two parts both surrounded by {{writeLock}}. First part validates different conditions on the file. The second actually adds a new block. The two parts are separated by {{chooseTarget()}}, which is performed outside the the {{writeLock}}.
In the error scenario the are two threads trying to perform the same operation {{getAdditionalBlock()}} with the same input parameters.
- First thread goes through the first {{writeLock}} section checking the file state and releases control somewhere in {{chooseTarget()}}.
- Then the second thread starts working on the same first {{writeLock}} section and will get the same results as the first thread, because the file state hasn't changed.
- Then both threads execute the second {{writeLock}} section in some order, which doesn't matter, and create two different blocks with different ids and targets.
- The client will receive response only from one of the competing threads and will have no idea about the other block. The client proceeds with sending data to DataNodes.
- When the client tries to create yet another block it will receive {{NotReplicatedYetException}}, because the penultimate block in the file has 0 replicas and will never have more., Hi Konstantin, HDFS-4212 is for the same issue and I am working on the patch.
Basically, the idea is to add offset as additional input parameter to addBlock() to make it real idempotent. , Here is a simple test I used to catch the race. You just need to run it in debugger and set a breakpoint in FSNamesystem.getAdditionalBlock() on the line that calls chooseTarget()., Running few benchmarks on 2.0.2-alpha I did hit this once. Should this be marked blocker? , Marked it critical. Saw it happening on a large cluster under load. 
Up to RM to make it a blocker at this point. Lohit you can always lobby for it.

Brandon, adding the offset parameter to addBlock() is the right direction. I was also thinking making the first synchronized section as a read-only and use writeLock only when actually adding the block. Under this approach it may be acceptable to move chooseTarget under the readLock of the first section., Linking related issues.
HDFS-4212 and HDFS-4208 are dealing with the corrupt files after the extra block was created.
HDFS-3031 recognized the problem and attempted to fix it under the assumption that the block has already been created by getAdditionalBlock(). 
Unfortunately, the two attempts can race inside getAdditionalBlock(). And this can only be fixed by adding offset to addBlock RPC call. As stated in HDFS-4212 discussion., I was going back and forth with this. Finally realised that the block offset parameter does not help the problem.
Suppose we are creating the first block and there are two thread executing getAdditionalBlock() simultaneously. That is, thread 1 is stuck in chooseTarget() and thread 2 is just starting getAdditionalBlock(). Then thread 2 has no way to determine if it is a retry or not, because thread 1 has not changed the file. And offset does not help anything, because it is the same in both threads.

The solution is to repeat the full analysis in the second part within the second writeLock section. When thread 2 reaches the second section, thread 1 is guaranteed to already create the block. So we can simply return that block., Here is the patch.
I wrapped the analysis in a separate function, which is called in both parts of getAdditionalBlock() - before and after chooseTarget().
- The patch returns previously allocated block if this is a retry rather than removing and then recreating it as in current code.
- The patch moves all updates to the file into the second locking section, which makes it possible to hold only readLock for the first section.

If people could review this quickly, I think we can commit it to make into the upcoming release. This is a long standing  rather annoying bug.
And it does not introduce incompatible changes., I tested it locally. It passes all tests including TestDFSClientRetries, which models the case when the retry happens after the first attempt completes getAdditionalBlock() fully.
I'll write another test which models simultaneous execution of  getAdditionalBlock() by two threads shortly., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12567527/getAdditionalBlock.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3933//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3933//console

This message is automatically generated., The same chamges now with the test case which succeeds with the patch and fails on trunk with the expected error:
{code}
java.lang.AssertionError: Must be one block expected:<1> but was:<2>
{code}, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12567572/getAdditionalBlock.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3935//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3935//console

This message is automatically generated., The patch looks mostly good. A few minor comments:
- the logger in the class should be initialized for {{TestAddBlockRetry}}, not {{TestFSDirectory}}
- formatting only change
{noformat}
   LocatedBlock getAdditionalBlock(String src,
-                                         String clientName,
-                                         ExtendedBlock previous,
-                                         HashMap<Node, Node> excludedNodes
-                                         ) 
+                                  String clientName,
+                                  ExtendedBlock previous,
+                                  HashMap<Node, Node> excludedNodes)
{noformat}

Test is failing without the corresponding change in the code, so it seems right on the money.
+1, Patch for branch 2. Somebody "conveniently" renamed local variables and changed types., Patch for trunk. Incorporated Cos's comments. Corrected couple comment and log messages, minor code cleanup., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12567681/getAdditionalBlock.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3940//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3940//console

This message is automatically generated., I just committed this., Integrated in Hadoop-trunk-Commit #3314 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3314/])
    HDFS-4452. getAdditionalBlock() can create multiple blocks if the client times out and retries. Contributed by Konstantin Shvachko. (Revision 1441681)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1441681
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java
, Integrated in Hadoop-Yarn-trunk #115 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/115/])
    HDFS-4452. getAdditionalBlock() can create multiple blocks if the client times out and retries. Contributed by Konstantin Shvachko. (Revision 1441681)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1441681
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java
, Integrated in Hadoop-Hdfs-trunk #1304 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1304/])
    HDFS-4452. getAdditionalBlock() can create multiple blocks if the client times out and retries. Contributed by Konstantin Shvachko. (Revision 1441681)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1441681
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java
, Integrated in Hadoop-Mapreduce-trunk #1332 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1332/])
    HDFS-4452. getAdditionalBlock() can create multiple blocks if the client times out and retries. Contributed by Konstantin Shvachko. (Revision 1441681)

     Result = SUCCESS
shv : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1441681
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java
]