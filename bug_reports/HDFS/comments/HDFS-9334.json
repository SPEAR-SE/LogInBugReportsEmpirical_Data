[It shouldn't return -1. If it did, it means the linked list is broken. Hi, [~hexiaoqiao]! Does it happen in real case? Could you attach the stack trace? Thanks., hi [~walter.k.su], the case happens at SBN when do editlog tailer. Stack trace:
{code:xml}
2015-10-28 23:14:36,737 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation DeleteOp [length=0, path=$PATH, timestamp=1446044851382, RpcClientId=42f62f2c-2d6a-4bfd-8988-e1ae9fe15b34, RpcCallId=53195, opCode=OP_DELETE, txid=26131111424]
java.lang.ArrayIndexOutOfBoundsException: -2
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.setPrevious(BlockInfo.java:140)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.listRemove(BlockInfo.java:333)
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.removeBlock(DatanodeStorageInfo.java:227)
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.removeBlock(DatanodeDescriptor.java:288)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.removeBlock(BlocksMap.java:125)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.removeBlock(BlockManager.java:3212)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeBlocksAndUpdateSafemodeTotal(FSNamesystem.java:3473)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removePathAndBlocks(FSNamesystem.java:3445)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedDelete(FSDirectory.java:1418)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:502)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:224)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:133)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:805)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:786)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:230)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:324)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:282)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:299)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1536)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:413)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:295)
2015-10-28 23:14:36,739 FATAL org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unknown error encountered while tailing edits. Shutting down standby NN.
java.io.IOException: java.lang.ArrayIndexOutOfBoundsException: -2
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:234)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:133)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:805)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:786)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:230)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:324)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:282)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:299)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1536)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:413)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:295)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -2
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.setPrevious(BlockInfo.java:140)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.listRemove(BlockInfo.java:333)
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo.removeBlock(DatanodeStorageInfo.java:227)
        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.removeBlock(DatanodeDescriptor.java:288)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.removeBlock(BlocksMap.java:125)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.removeBlock(BlockManager.java:3212)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeBlocksAndUpdateSafemodeTotal(FSNamesystem.java:3473)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removePathAndBlocks(FSNamesystem.java:3445)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedDelete(FSDirectory.java:1418)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:502)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:224)
        ... 12 more
{code}, the following scenario may occur ArrayIndexOutOfBoundsException:
1. more than one replica of Block located at same DN, then part circular are included in Double linked list,
2. delete one replica when DN hb/other,
3. delete File of Block belong to when do editlog tailer .
not find proof through log yet. 
[~walter.k.su] FYI., In the latest code from thunk, {{removeStorage()}} is always called together with {{listRemove()}}.
I think HDFS-6830 probably fixed this.
, HDFS-6830 has fixed this.]