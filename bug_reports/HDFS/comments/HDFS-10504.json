[Hi [~sebyonthenet], jira is not appropriate for such discussion/troubleshooting. The hadoop-user mailing list is the right place to discuss user issues.

In this instance you may look for BlockPlacementPolicyDefault errors in the NameNode logs as a starting point., Thank you Arpit for your feedback. 

I did not complain about the temporary error, just the fact that the client holds on to resources when this happen, so it can cause the application that's using the client to die with Out of memory. I see this as a bug in the current hadoop client code.  , Just to make it clear again. When that exception happens, the DFSClient filesBeingWritten map does not get cleaned. It seems to still keep a ref to that stream even though close was called., Hi [~arpitagarwal] ,

Please let me know if you need more details about this or if you want me to open a new ticket while being more explicit about the problem.

Thanks, Hi [~sebyonthenet], I haven't got a chance to look into what you described, but yes please create another Jira. Also please mention your Apache Hadoop version and the output of the 'hadoop version' command for completeness., Thanks [~arpitagarwal]. I've just created HADOOP-13264 with a simple test on how to reproduce the problem]