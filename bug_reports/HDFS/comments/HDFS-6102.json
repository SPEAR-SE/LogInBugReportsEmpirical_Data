[Doing some back of the envelope math while looking at INodeDirectorySection in fsimage.proto, we save a packed uint64 per child. These are varints, but let's assume worst case and they use the full 10 bytes. Thus, with the 64MB default max message size, we arrive at 6.7 million entries.

There are a couple approaches here:

- Split the directory section up into multiple messages, such that each message is under the limit
- Up the default from 64MB to the maximum supported value of 512MB, release note, and assume no one will realistically hit this
- Enforce a configurable maximum on the # of entries per directory

I think #3 is the best solution here, under the assumption that no one will need 6 million things in a directory. Still needs to be release noted of course., I also did a quick audit of the rest of fsimage.proto, and I think the other repeated fields are okay. INodeFile has a repeated BlockProto of up to size 30B, but we already have a default max # of blocks per file limit of 1 million so this should be okay (30MB < 64MB)., It might be sufficient to putting it into the release note. I agree with you that realistically it is quite unlikely to see someone put 6.7m inode as the direct children into a single directory.

I'm a little hesitate to introduce a new configuration just for this reason. I wonder, is the namespace quota offering a super set of this functionality? It might be more natural to enforce this in the scope of the namespace quota., bq. Enforce a configurable maximum on the # of entries per directory
I think this is reasonable. Recently we changed the default max length of file name allowed. We should also add reasonable limit to the number of entries in a directory., Thanks for the comments, Haohui and Suresh. I think this is actually easier than I thought, since there's already a config parameter to limit directory size (dfs.namenode.fs-limits.max-directory-items). If we just change the default to 1024*1024 or something, that might be enough.

I'm currently reading through the code to make sure it works and doing manual testing, will post a (likely trivial) patch soon., Patch attached. It's dead simple, just ups the default in DFSConfigKeys and hdfs-default.xml, and adds some notes. I also took the opportunity to set the max component limit in DFSConfigKeys, since I noticed that HDFS-6055 didn't do that.

I manually tested by adding a million dirs to a dir, and we hit the limit. NN was able to startup again afterwards, and the fsimage itself was only 78MB (most of that probably going to the INode names). I think this is best case, not worst case, since IIRC the inode numbers start low and count up, but if someone wants to verify my envelope math I think it's good to go., It looks mostly good to me. The only comment I have is that the code should no longer support unlimited number of children in a directory, which is in {{FSDirectory#verifyMaxDirItems()}}.

{code}
    if (maxDirItems == 0) {
      return;
    }
{code}

Otherwise users might run into a problem that the saved fsimage cannot be consumed. Do you think it is a good idea to enforce a maximum limit, say, 6.7m based on your calculation?
, Good idea Haohui, new patch adds some precondition checks and removes that if statement. Also a new test for the preconditions., +1 pending jenkins, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12634587/hdfs-6102-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6399//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6399//console

This message is automatically generated., Thanks for the review Haohui, will commit shortly., Committed through to branch-2.4, thanks all., SUCCESS: Integrated in Hadoop-trunk-Commit #5327 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5327/])
HDFS-6102. Lower the default maximum items per directory to fix PB fsimage loading. Contributed by Andrew Wang. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1577426)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/fsimage.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/hdfs.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsLimits.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #509 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/509/])
HDFS-6102. Lower the default maximum items per directory to fix PB fsimage loading. Contributed by Andrew Wang. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1577426)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/fsimage.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/hdfs.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsLimits.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1701 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1701/])
HDFS-6102. Lower the default maximum items per directory to fix PB fsimage loading. Contributed by Andrew Wang. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1577426)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/fsimage.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/hdfs.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsLimits.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1726 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1726/])
HDFS-6102. Lower the default maximum items per directory to fix PB fsimage loading. Contributed by Andrew Wang. (wang: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1577426)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/fsimage.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/hdfs.proto
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsLimits.java
, This is preventing log aggregation for jobs of users who run very many jobs. e.g. in the NodeManager logs:
{code}The directory item limit of <yarn.nodemanager.remote-app-log-dir>/<user>/logs is exceeded: limit=1048576 items=2144288{code}, Hey Ravi, we could probably up this to ~6.7mil, but it seems like you'd probably run into this limit soon enough too. Do you mind filing a new JIRA to chunk up large directories? That's the only future-proof fix., I'm flagging this as backwards-incompatible, because existing clusters that used 0 in their configuration now fail during startup.  They'd need to change their configuration to use a non-zero value., Thanks [~andrew.wang]! I'm sorry I just saw your comment. I have filed https://issues.apache.org/jira/browse/HDFS-7482]