[One thought : Could it be that the failover-controller is responsible for killing the namenode before it's finished with the task because it doesn't respond properly ?


I had one situation now, where one datanode had only 120.000 blocks without file-referende, and when I restarted this datanode, it connected to the active namenode. This then recieved those block-id's and invalidated them. When finished, I had 120.000 bocks 'pending deletion', but since the name-node kept logging blocks 'does not belong to any file' it took very long time before it managed to tell the datanode to start deleting. When the number of blocks decreased, it took shorter and shorter time between delete-commands. In the end that datanode was cleaned up, but when I tried the samme approach for a datanode with more unattached blocsk, the namenode died before all blocks was marked as invalid. I suspect that it might have beed the failover-controller that actually killed the namenode. And of course; when the namenode died, it lost all information about blocks 'pending deletion' and had to start over when restarted...

For the moment, I have killed the failover-controller, but it seems that the number of invalid blocks that constantly is bombarding the name-server prevents it from ever getting around to tell the datanode to delete the blocks. (It's taking forever between deletes in the beginning)

The bug in this case must be that the namenode/datanode-communication repeats the loop of non-attached-blocks, the second bug must be that the name-node get's so busy recieving those messages that it's unresponsive to anything else..., Looks similar to HDFS-7503. Is it fixed by it?, I will test when 2.6.1 is released.., 2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.
Athe the delete-moment, I don't think that the logging is that problematic.
The logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2., 2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.
Athe the delete-moment, I don't think that the logging is that problematic.
The logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2., 2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.
Athe the delete-moment, I don't think that the logging is that problematic.
The logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2., 2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.
Athe the delete-moment, I don't think that the logging is that problematic.
The logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2., 2.6.1 is not out yet, but one thought; This fix might resolve the issue when namenodes are started with a lot of incoming information about 'loose' data-blokcs, but it probably won't resolve the issue that causes the namenodes to be killed by zookeeper when I delete a lot of files.
Athe the delete-moment, I don't think that the logging is that problematic.
The logging-issue, I believe, is secondary. I believe that the active namenode gets busy calculating/distributing delete-orders to datanodes when I drop 500.000 files at once, and that this is the causer fo the zookeeper-shutdown. When the namenode gets overloaded with caclulating/distributing those delete-orders, it doesn't keep up with responses to zoo-keeper, which the kills the namenode in order to failover to NN2.]