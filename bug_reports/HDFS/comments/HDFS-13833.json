[Since you only have one DataNode, it is expected due to the stress put to that DataNode.

For the purpose of PoC, you can consider set dfs.namenode.replication.considerLoad = false at NameNode hdfs-site.xml to disable this logic., *Context:* We are migrating from HortonWorks Hadoop v2.3 to this one. The POC is clucial since we decomissioned the HW nodes for installing this Cloudera's POC. With this Random error we cannot accept the solution. At least without knowing the real cause.

We already tried turning that off (dfs.namenode.replication.considerLoad) and it works, but it is only hiding the problem.


It is not because of the load. Our load is really really low across all the cluster - 2 NN and one DN.
Disks, CPU, Memory are all sleeping, we do not have network issues, nor disk issues; we are getting around 1 GBits per second between all the 3 machines.

It seems to me that the node is being excluded by some reason that we cannot find in the logs and then the total load becomes equal to 0 and the message:
{code:java}
load: 8 > 0.0{code}
Shows off. Sometimes that load is 2 other times is 10, but the total load (number on the right) is always zero which seems like a consequence of the only DN being excluded.

Do you know some other crucial classes I can activate DEBUG logs on, in order to find more about this?

Any Help is appreciated, we already tried so many configurations, including raising the Cloudera CDH version (it is now the one in description box), even tried raising our Flink version from 1.3.2 to 1.6.0, and the same happens.

Flink is our client, and this exception only happens with the Flink Checkpoints pointing to HDFS.

 

Best Regards,

Barros, I think the total load == 0.0 issue is a corner case when running with very small number of datanodes. [~shwetayakkali] was looking at a test failure earlier, and the 0.0 case seems should be fixed for the consider load logic., IIUC I think inconsistent {{stats}} is the main reason about this corner case:
a. {{chooseTarget}} at {{BlockPlacementPolicyDefault}} is not under global lock, and it can been invoke with {{sendHeartbeat}} at same time.
b. when {{chooseTarget}} has choose the target node, it's stat (of course include {{load}}) is immutable, because {{DatanodeStorageInfo}} instances is created by {{DatanodeDescriptor#getStorageInfos}}.
c. On the other hand, when compare load about target node to average load (* factor) of the whole cluster, it get the real time value.
d. if update heartbeat between step b and step c, and the latest load of the only node is zero unfortunately, chooseTarget will fail, and exception stack show as depict above. 
e. the good news is the average load will update when next heartbeat at the latest, and chooseTarget will work as expect. 

{code:java}
    // check the communication traffic of the target machine
    if (considerLoad) {
      final double maxLoad = 2.0 * stats.getInServiceXceiverAverage();
      final int nodeLoad = node.getXceiverCount();
      if (nodeLoad > maxLoad) {
        logNodeIsNotChosen(storage, "the node is too busy (load: " + nodeLoad
            + " > " + maxLoad + ") ");
        return false;
      }
    }
{code}
my suggestion is try to add more datanode (for instance: 3) in cluster.
[~jojochuang],[~xiaochen] branch trunk also has this problem., Yes, [~hexiaoqiao] and [~xiaochen] it appears your explained logic is happening. The only thing that conflicts with it is the 'message of the load' being out of order with the message that informs about the excluded the node. It appears it exclude it first and then print the load message.

However, you could reproduce it and your analysis makes total sense, it could only be inconsistent stats - chooseTarget and  {{sendHeartbeat}} being invoking at same time. But if it is, the stats should be saved someway till the next heartbeat, I think.
Your explanation still can explain why it happens so randomly.

I will maintain 'considerLoad' deactivated for now and will check if it happens with more than one dataNode.

 

Thank you very much for your fastest response and help.
 I will come back with some conclusion to this issue soon to see if we can close it or recheck.

 ]