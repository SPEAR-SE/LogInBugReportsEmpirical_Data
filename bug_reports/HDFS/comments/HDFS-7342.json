[Some details I've been able to gather from the logs on a cluster running Hadoop 2.2.0:
The client logs. 
{noformat}
2014-10-27 19:46:54,952 INFO [Thread-60] org.apache.hadoop.hive.ql.exec.FileSinkOperator: Writing to temp file: FS hdfs://<NAMENODE>:8020/<FILENAME>
..... nothing related to this file.......
2014-10-28 01:18:26,018 INFO [main] org.apache.hadoop.hdfs.DFSClient: Could not complete <FILENAME> retrying...
2014-10-28 01:18:26,419 INFO [main] org.apache.hadoop.hdfs.DFSClient: Could not complete <FILENAME> retrying...
...goes on for 10 mins.....
2014-10-28 01:28:24,481 INFO [main] org.apache.hadoop.hdfs.DFSClient: Could not complete <FILENAME> retrying...
2014-10-28 01:28:24,883 INFO [main] org.apache.hadoop.hdfs.DFSClient: Could not complete <FILENAME> retrying...
{noformat}

The Namenode Logs grepping for <FILENAME>
{noformat}
2014-10-27 19:46:58,041 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_A{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE1>:50010|RBW], ReplicaUnderConstruction[<SLAVE8>:50010|RBW]]}
2014-10-27 20:13:26,607 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_B{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE2>:50010|RBW], ReplicaUnderConstruction[<SLAVE9>:50010|RBW]]}
2014-10-27 20:47:52,422 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_C{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE3>:50010|RBW], ReplicaUnderConstruction[<SLAVE10>:50010|RBW]]}
2014-10-27 21:23:13,844 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_D{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE4>:50010|RBW], ReplicaUnderConstruction[<SLAVE11>:50010|RBW]]}
2014-10-27 22:02:33,405 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_E{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE5>:50010|RBW], ReplicaUnderConstruction[<SLAVE12>:50010|RBW]]}
2014-10-27 22:42:49,227 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_F{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE6>:50010|RBW], ReplicaUnderConstruction[<SLAVE15>:50010|RBW]]}
2014-10-27 23:25:58,555 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_G{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE7>:50010|RBW], ReplicaUnderConstruction[<SLAVE12>:50010|RBW]]}
2014-10-28 00:07:36,093 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_H{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE2>:50010|RBW], ReplicaUnderConstruction[<SLAVE13>:50010|RBW]]}
2014-10-28 01:13:50,298 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: <FILENAME>. <BlockPoolID> blk_A_I{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[<CLIENT_IP>:50010|RBW], ReplicaUnderConstruction[<SLAVE1>:50010|RBW], ReplicaUnderConstruction[<SLAVE14>:50010|RBW]]}
2014-10-28 01:18:20,868 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: <FILENAME> is closed by DFSClient_attempt_X_Y_r_T_U_V_W
2014-10-28 01:18:21,272 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: <FILENAME> is closed by DFSClient_attempt_X_Y_r_T_U_V_W
....This keeps going interspersed with other logs until ....
2014-10-28 01:28:24,483 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: <FILENAME> is closed by DFSClient_attempt_X_Y_r_T_U_V_W
2014-10-28 01:28:25,615 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: <FILENAME> is closed by DFSClient_attempt_X_Y_r_T_U_V_W
2014-10-28 02:28:17,569 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1], src=<FILENAME>
......BOOM.... NN IS IN INFINITE LOOP...... Only the following two messages keep getting repeated:
2014-10-28 02:28:17,568 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1] has expired hard limit
2014-10-28 02:28:17,569 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1], src=<FILENAME>
2014-10-28 02:28:17,569 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1] has expired hard limit
2014-10-28 02:28:17,569 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1], src=<FILENAME>
2014-10-28 02:28:17,569 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1] has expired hard limit
2014-10-28 02:28:17,569 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1], src=<FILENAME>
2014-10-28 02:28:17,569 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1] has expired hard limit
2014-10-28 02:28:17,569 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_attempt_X_Y_r_T_U_V_W, pendingcreates: 1], src=<FILENAME>
{noformat}, Here's a patch which triggered the infinite loop in the Namenode. (You'll have to comment out this line in FSNamesystem.internalReleaseLease
{code}
assert false : "Already checked that the last block is incomplete";
{code}, Its also worth noting that I spent quite a few cycles trying to get the file itself in a state where the penultimate block was marked as COMMITTED and last as COMPLETE (by stopping the nodes with the penultimate block while writing various blocks) but wasn't successful. Also, there's no easy way for me to iterate through values of minReplication (true/false) because BlockManager.minReplication is a final member.
, Hi Yongjun!

Here's a patch with your suggested fix. I converted the assertAllBlocksComplete() method into assertAllBlocksCompleteOrCommitted() . This is the only place it is ever used, so we should be fine., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12682370/HDFS-7342.2.patch
  against trunk revision 5bd048e.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8780//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8780//console

This message is automatically generated., HI Ravi,

Thanks a lot for your continued effort to work on this!

I had some concern of changing {{assertAllBlocksComplete()}} to {{assertAllBlocksCompleteOrCommitted()}}, because {{finalizeINodeFileUnderConstruction}} checks to make sure all blocks are complete (which means all blocks need to have minimal replication) before closing a file. Your suggested change would change the behavior here, and I think it may not be safe.

Good thing is, the place that we are fixing does check the minimal replication requirement before calling {{finalizeINodeFileUnderConstruction}}. So I was looking into addressing this issue in a different approach on top of my earlier suggested fix at https://issues.apache.org/jira/browse/HDFS-4882?focusedCommentId=14215703&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14215703. 

Here is the complete snapshot of my approach:

{code}
    // If penultimate block doesn't exist then its minReplication is met
    boolean penultimateBlockMinReplication = penultimateBlock == null ? true :
        blockManager.checkMinReplication(penultimateBlock);
    BlockUCState penultimateBlockState = penultimateBlock == null ?
        BlockUCState.COMPLETE : penultimateBlock.getBlockUCState();
    String blockStateStr = "(penultimateBlockState=" + penultimateBlockState +
        " lastBlockState="+lastBlockState + ")";

    switch(lastBlockState) {
    case COMPLETE:
      // Getting here means the penultimate block is COMMITTED, fallthrough
      // to handle the same way as when the final block is COMMITTED
    case COMMITTED:
      blockStateStr = "(penultimateBlockState=" + penultimateBlockState +
        " lastBlockState="+lastBlockState + ")";
      // Close file if committed blocks are minimally replicated
      if(penultimateBlockMinReplication &&
          blockManager.checkMinReplication(lastBlock)) {
        if (penultimateBlock != null &&
            penultimateBlockState == BlockUCState.COMMITTED) {
          getBlockManager().forceCompleteBlock(pendingFile,
              (BlockInfoUnderConstruction)penultimateBlock);
        }
        getBlockManager().forceCompleteBlock(pendingFile,
            (BlockInfoUnderConstruction) lastBlock);
        finalizeINodeFileUnderConstruction(src, pendingFile,
            iip.getLatestSnapshotId());
        NameNode.stateChangeLog.warn("BLOCK*"
          + " internalReleaseLease: Committed blocks are minimally replicated,"
          + " lease removed, file closed " + blockStateStr + ".");
        return true;  // closed!
      }
      // Cannot close file right now, since some blocks
      // are not yet minimally replicated.
      // This may potentially cause infinite loop in lease recovery
      // if there are no valid replicas on data-nodes.
      String message = "DIR* NameSystem.internalReleaseLease: " +
          "Failed to release lease for file " + src +
          ". Committed blocks are waiting to be minimally replicated " +
          blockStateStr + ". Try again later.";
      NameNode.stateChangeLog.warn(message);
      throw new AlreadyBeingCreatedException(message);
    case UNDER_CONSTRUCTION:
{code}

That is, add the {{getBlockManager().forceCompleteBlock}} for the neede blocks before calling
{code}
        finalizeINodeFileUnderConstruction(src, pendingFile,
            iip.getLatestSnapshotId());
{code}

However, there is a problem because of the way your test is written, the {{BlockInfoUCDesired}} in your test is not allowed to be converted to {{(BlockInfoUnderConstruction}}. I was looking a bit more into fixing that.

Make sense to you? If you agree, maybe you can look further into this direction?

BTW, notice I had the block state added to the messsage printing, which I think is important. Also I added comments in between the two {{case}} statements.

Thanks again!
, HI Ravi,

I worked out a version toward the direction I described in my last comment (use {{getBlockManager().forceCompleteBlock}}).

Below are the main changes I made:

* I forgot one condition check in the snapshot of code change in my last comment, revised as 
{code}
  if (lastBlockState == BlockUCState.COMMITTED) {
    getBlockManager().forceCompleteBlock(pendingFile,
        (BlockInfoUnderConstruction) lastBlock);
   }
{code}
*  Replace your original {{BlockInfoDdesired}} with the following
{code}
  class BlockInfoUcDesired extends BlockInfoUnderConstruction {
    BlockUCState desiredState;
    BlockInfoUcDesired(BlockInfo blk, BlockUCState state) {
      super(blk, (short)3);
      desiredState = state;
      setBlockCollection(blk.getBlockCollection());
    }
    @Override
    public BlockUCState getBlockUCState() {
      return desiredState;
    }
  }
{code}
*  Replace
{code}
  assert lm.getLeaseByPath(testFile.toString()).getHolder().equals(
      HdfsServerConstants.NAMENODE_LEASE_HOLDER) : "Lease not recovered";
{code}
with
{code}
  Lease lease = lm.getLeaseByPath(testFile.toString());
  String leaseHolder = (lease == null) ? "" : lease.getHolder();
  assert leaseHolder == "" ||
      leaseHolder == HdfsServerConstants.NAMENODE_LEASE_HOLDER :
        "Lease not recovered";
{code}

Would you please revise the patch with the above changes I suggested?

Notice that even though the tests now pass, there are some ERROR messages in the test log file after the test finished. I studied a bit, and figured out that it's because DFSClient tries to close all files at the very end of the test, but the files were already closed by the leaseManager. So this kind of ERROR msgs are expected based on my understanding.

BTW, there are some format changes needed in the test code you did, such as shorten lines that's over 80, when splitting one line into two, the second part need to have 4 spaces difference in the beginning etc. 

Thanks a lot.
, Hi Yongjun! Sorry for the delay, but I got to thinking
1. What is special about the penultimate block that it could be COMMITTED when the last block is COMPLETE? Could that happen to any of the preceding blocks?
2. Do other callers of finalizeINodeFileUnderConstruction have the same expectations that internalReleaseLease did (i.e. even when the penultimate/non-last block is !COMPLETE) it should finalize the file.
, Hi Ravi,

No problem, actually you responded pretty fast and I really appreciate it!

Good thoughts! 

To answer your comment#1: for the case that blocks even before penultimate block that are COMMITTED, current code handles it as
{code}
    // Only the last and the penultimate blocks may be in non COMPLETE state.
    // If the penultimate block is not COMPLETE, then it must be COMMITTED.
    if(nrCompleteBlocks < nrBlocks - 2 ||
       nrCompleteBlocks == nrBlocks - 2 &&
         curBlock != null &&
         curBlock.getBlockUCState() != BlockUCState.COMMITTED) {
      final String message = "DIR* NameSystem.internalReleaseLease: "
        + "attempt to release a create lock on "
        + src + " but file is already closed.";
      NameNode.stateChangeLog.warn(message);
      throw new IOException(message);
    }
{code}
which means the lease will be released right away because of the IOException. The exception message there is a bit misleading though. I'm actually not so sure about the effect of releasing the lease without closing the file (e.g., my guess is, there might be some bad effect, and it's not uncovered because this code path is not really exercised). But  I guess this kind of case would be more rare than penultimate block being COMMITTED and last block being COMPLETE (which I refer to as caseOfInterest).  So we could possibly live with the current code.

My suggested approach was to handle caseOfInterest is to do it similar like penultimate block being COMPLETE and last block being COMMITTED. Another approach is to treat them the same as the above pasted code. But since more people are hitting caseOfInterest problem, that means the chance it happens is relatively high. And since we are checking the minimal replication before calling finalizeINodeFileUnderConstruction, it looks safer to close the file before releasing the lease to me (as my suggested fix does).

To answer your comment#2, there are two other callers of the method {{finalizeINodeFileUnderConstruction}}, {{FSNamesystem#closeFileCommitBlocks}} and {{FSNameSystem#completeFileInternal}}. But the requirement is the same: {{finalizeINodeFileUnderConstruction}} expects all blocks are complete and throw an exception otherwise. Since we check minimal replication in {{internalReleaseLease}} before calling  {{finalizeINodeFileUnderConstruction}} , that's why I think we should call {{getBlockManager().forceCompleteBlock}} before calling  {{finalizeINodeFileUnderConstruction}} in the suggested fix. This sounds a safer solution than the pasted code above.

Comments?

Thanks.



, I got the reason, why [~raviprak] is facing the problem and I am not. 

You have mentioned affected version as 2.0.0-alpha, which came to help.
The possibility of making the last block COMPLETE when the penultimate block still in COMMITTED state was before the fix of HDFS-5558 which was fixed in 2.3.0 version, though the problem faced at that time was not infinite loop, but crash of the lease monitor thread.

After the fix, last block cannot be in COMPLETE state when others blocks are not COMPLETED, and infinite loop never occurs.

I hope this clears the confusion. And I think, there is no change required in this Jira. In that case, can this be closed as Duplicate?

Hi  [~yzhangal], any thoughts?, Hi [~vinayrpet],

Thanks for the good catch of yours! 

Yes, I examined the version of the s/w that caused my case, it doesn't have HDFS-5558. And so is Ravi's case.

If HDFS-5558 avoids the case that penultimate block is COMMITTED and last block is COMPLETE, I had the following thoughts.

With HDFS-5558 fix, I assume the case that both the penultimate and the last block are COMMITTED could be possible, which case the code pasted in https://issues.apache.org/jira/browse/HDFS-4882?focusedCommentId=14213992&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14213992 would be executed.

Even though the lease can be removed in this case based on current trunk code,  in scenario#1 described in that comment, if both blocks have minimal replication number of blocks,  there would be an exception thrown because the method {{finalizeINodeFileUnderConstruction}} that calls:
{code}
      Preconditions.checkState(blocks[i].isComplete(), "Failed to finalize"
          + " %s %s since blocks[%s] is non-complete, where blocks=%s.",
          getClass().getSimpleName(), this, i, Arrays.asList(blocks));
{code}
Thus the file won't be closed.

I proposed a solution here 

https://issues.apache.org/jira/browse/HDFS-7342?focusedCommentId=14218085&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14218085
(and the one that follows it)

would avoid this by forcing the penultimate and last block to complete if they already have minimal replication number of replicas, and the file will be closed successfully.

Any comments/thoughts on this proposed solution?

Hi Ravi, to help further discussion about the fix here, would you please help consolidating your testcase with the solution I suggested above?

Thanks.




, bq. With HDFS-5558 fix, I assume the case that both the penultimate and the last block are COMMITTED could be possible
I agree, this is possible only in case of {{commitBlockSynchronization()}} and finalize fails. But as of now this also removes the lease, and which will not be added back immediately. Lease will be present in the Standby NN, or it will come back after NN restart. Note that at that time also last Block state will be UNDER_CONSTRUCTION not COMMITTED. COMMITTED state is only at ACTIVE NN side.

bq. in scenario#1 described in that comment, if both blocks have minimal replication number of blocks, there would be an exception thrown because the method finalizeINodeFileUnderConstruction that calls:
If any COMMITTED blocks reaches minReplication, state will be automatically changed to COMPLETE while processing that IBR itself. Need not be user call. So there is no chance of COMMITTED block state with minReplication met. right?

One more thing I have observed was, In this state, where penultimate was not having minReplication(=2) replicas, but had only one replica, restarted the Namenode, and NN scheduled replication for penultimate block and replicated successfully even before lease recovery was triggered. Later lease recovery was successful as everything was OK.

How about scheduling replication during the lease recovery for such penultimate blocks with atleast one replica available to satisfy min-replication, then go ahead for lease recovery. Till now this situation might not have experienced as minReplication itself by default was 1. 

Any thoughts...?, Hi Yongjun! Thanks a lot for your very helpful and detailed explanations and patience while we work through this. Much appreciated.
Here's the patch with what I have so far. Is my understanding of the conversation with Jing and Colin correct?
I was also wondering what happens when the penultimate block is COMMITTED but last block is UNDER_RECOVERY/UNDER_CONSTRUCTION ?, bq. How about scheduling replication during the lease recovery for such penultimate blocks with atleast one replica available to satisfy min-replication, then go ahead for lease recovery. Till now this situation might not have experienced as minReplication itself by default was 1. 

Let's first think about the meaning of min-replication. It is the level of degradation that is allowed before being considered critical in terms of data durability. Falling below this level does not necessarily mean a failure (i.e. data not available) unless min-replica is 1. For synchronous or semi-synchronous operations such as {{addBlcok()}} and {{complete()}}, this is *enforced* to maintain the healthy steady state. Clients also do their best to meet this, but any failures on datanodes between finalizing a block and sending the IBR are beyond their control.  For asynchronous recovery activities such as lease recovery and replication, min-replica should be advisory.  Since replication is already doing the right thing, let's focus on lease recovery.

Dealing with COMMITTED blocks is simpler. Being committed means the client thought enough number of replicas were finalized. If a lease is expired, the block can simply turn in to COMPLETE. If it has at least one live replica, it will be replicated soon after closing the file. If it doesn't, the block will be considered missing.  I think it is better to report the committed but missing data early rather than hiding it in the infinite lease recovery cycle.  Also, recovery will be faster this way.  If all blocks in a file are in either complete or committed state, lease recovery may force complete all committed blocks and close the file. The rest will be up to the replication monitor.

{{recoverLeaseInternal()}} and {{internalReleaseLease()}} will need to be made to distinguish the on-demand recovery from normal lease expiration.  For on-demand recovery, we might want it to fail if there is no live replicas, as a file lease is normally recovered for subsequent append or copy(read). If there is no data, they will fail.

For recovering blocks in the UNDER_CONSTRUCTION state, we can make {{commitBlockSynchronization()}} to force commit when there is at least one replica, ignoring min-replication. It will allow the recovery to make progress and eventually the file to be closed if there is at least one replica per block. Then the blocks can be replicated.  This is far better than getting stuck in recovery., bq. If all blocks in a file are in either complete or committed state, lease recovery may force complete all committed blocks and close the file. The rest will be up to the replication monitor.
bq.  we can make commitBlockSynchronization() to force commit when there is at least one replica, ignoring min-replication. It will allow the recovery to make progress and eventually the file to be closed if there is at least one replica per block. Then the blocks can be replicated. This is far better than getting stuck in recovery.
Makes sense., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12683334/HDFS-7342.3.patch
  against trunk revision 555fa2d.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestLeaseRecovery2
                  org.apache.hadoop.hdfs.server.namenode.TestLeaseManager

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8819//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8819//console

This message is automatically generated., Hi Guys, 

Thanks a lot for the comments and new rev. Please see my comments below, one for each of you:-)

{quote}
If any COMMITTED blocks reaches minReplication, state will be automatically changed to COMPLETE while processing that IBR itself. Need not be user call. So there is no chance of COMMITTED block state with minReplication met. right?
{quote}
Hi [~vinayrpet], indeed the following code in {{BlockManager::addStoredBlock}} may be called when IBR is processed, that matches what you were saying:
{code}
  if(storedBlock.getBlockUCState() == BlockUCState.COMMITTED &&
        numLiveReplicas >= minReplication) {
      storedBlock = completeBlock(bc, storedBlock, false);
  }
{code}
But the block has to be COMMITTED to be made COMPLETE. If it's not COMMITTED yet (changing to COMMITTED is a request from client and it's asynchronous) , even if it has min replication number of replications, it won't be changed to COMPLETE. So I think we may still need to take care of changing block's state to COMPLETE in {{FSNamesystem#internalReleaseLease}}. Right?

Hi [~kihwal], 

Summary of my understanding of your comment is, there are two paths, one is the regular write, the other is recovery. 
* for regular write path, we need to enforce minimal replication
* for the recovery patch, we just need to enforce 1 replica and let replication monitor to take care of the rest.
* we can make commitBlockSynchronization() to change a block to COMMITTED when there is at least one replica, ignoring min-replication. Currently only client can inform NN asynchronously to make a block COMMITTED.

I think it makes sense. Am I understanding you correctly?

Hi Ravi,
Thanks for the new rev. While we are still discussing the final solution, I noticed couple of things in your rev3 per my original suggested solution:
 
1. Change 
{code}
4471	   * <li>If the penultimate/last block is COMMITTED or COMPLETE -> force the 
4472	   * block to be COMPLETE even if it is not minimally replicated</li>
{code}
To
{code}
4471	   * <li>If the penultimate/last block is COMMITTED  -> force the 
4472	   * block to be COMPLETE if it is minimally replicated</li>
{code}

2. you forgot to add {{setBlockCollection(blk.getBlockCollection());}} in BlockInfoDesired constructor, thus Null pointer exception will happen. 

Let's not rush into addressing those, but see if we can work out a solution toward the direction Kihwal stated.

Thank you all again.
, {quote}But the block has to be COMMITTED to be made COMPLETE. If it's not COMMITTED yet (changing to COMMITTED is a request from client and it's asynchronous) , even if it has min replication number of replications, it won't be changed to COMPLETE. So I think we may still need to take care of changing block's state to COMPLETE in FSNamesystem#internalReleaseLease. Right?{quote}
I agree that client request and Datanode's IBR are asynchronous. But both will update the block state under writelock.
penultimate block will  be COMMITTED in the {{getAdditionalBlock()}} client's request.

Here there are 3 possibilities,
1. All IBRs comes before even block is COMMITTED. At this time, if the block is FINALIZED in DN, replica will be accepted.
{code}    if (ucBlock.reportedState == ReplicaState.FINALIZED &&
        !block.findDatanode(storageInfo.getDatanodeDescriptor())) {
      addStoredBlock(block, storageInfo, null, true);
    }{code}
2. If client request comes after receiving 2 (=minReplication) IBRs, then client request only will make the state to COMPLETED immediately after making it COMMITTED in following code of {{BlockManager#commitOrCompleteLastBlock()}}
{code}    final boolean b = commitBlock((BlockInfoUnderConstruction)lastBlock, commitBlock);
    if(countNodes(lastBlock).liveReplicas() >= minReplication)
      completeBlock(bc, bc.numBlocks()-1, false);
    return b;{code}
  At this time, if the IBRs received are not enough, then block will be just COMMITTED.

3. If the IBRs received after client request. i.e. after COMMITTED, then while processing the second IBR block will be COMPLETED in below code.
{code}    if(storedBlock.getBlockUCState() == BlockUCState.COMMITTED &&
        numLiveReplicas >= minReplication) {
      storedBlock = completeBlock(bc, storedBlock, false);{code}

So I couldnt find the possibility of the Block in COMMITTED state with minReplication met.

{quote}{{recoverLeaseInternal()}} and {{internalReleaseLease()}} will need to be made to distinguish the on-demand recovery from normal lease expiration. For on-demand recovery, we might want it to fail if there is no live replicas, as a file lease is normally recovered for subsequent append or copy(read). If there is no data, they will fail.{quote}
I understood [~kihwal]'s suggestions as below.
{{recoverLease()}} call from client passes a {{force}} flag to {{recoverLeaseInternal()}}. Based on this flag, we can check the block's states (excluding last block) and # of replicas and decide to go ahead for recovery or not even initiating request to DataNode. 
So we need not worry this case in commitBlockSynchronization. In {{commitBlockSynchronization()}} directly complete all blocks and close the file.
Am I right [~kihwal] ?, Thanks a lot for your detailed explanation [~vinayrpet].
{quote}
2. If client request comes after receiving 2 (=minReplication) IBRs, ...
{quote}
It seems that lease recovery could happen before the client request comes here, when this happens, the block state would be COMMITTED with minReplication met, right?

, bq. It seems that lease recovery could happen before the client request comes here, when this happens, the block state would be COMMITTED with minReplication met, right?
We are talking about the state of the penultimate block not the last block, which is the cause found for this issue.
1. For the penultimate block, only client request (request for another block) will make it COMMITTED, as client will be still alive and adds one more block.
2. And for the last block, client makes it COMMITTED during normal closure, else {{commitBlockSynchronization()}} during the lease recovery closure. 

I see no other places, block getting COMMITTED., Hi [~vinayrpet],

Thanks for further explanation. I studied your latest two comments, and I agree with your analysis: the case that penultimate block being in COMMITTEED sate with minimal replication met may not exist. In other words, when penultimate block is in COMMITTED state, the minimal replication is not met.

Going back to the source that triggerd this discussion, it's the scenario#1 described in:
 https://issues.apache.org/jira/browse/HDFS-4882?focusedCommentId=14213992&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14213992

Here is what I think now:

scenario#1 exists for last block but not penultimate block
scenarion#2 exists for both penultimate and last block

My suggested solution for this jia is to share the same code that handles the case when last block is COMMITTEED, with some modifications:

* For scenarion#1 (last block), my suggested solution is to forceComplete the block before calling  {{finalizeINodeFileUnderConstruction}}:
{code}
        if (lastBlockState == BlockUCState.COMMITTED) {
          getBlockManager().forceCompleteBlock(pendingFile,
              (BlockInfoUnderConstruction) lastBlock);
        }
        finalizeINodeFileUnderConstruction(src, pendingFile,
            iip.getLatestSnapshotId());
{code}
thus avoiding the exception thrown from {{finalizeINodeFileUnderConstruction}} caused by
{code}
Preconditions.checkState(blocks[i].isComplete(), "Failed to finalize"
          + " %s %s since blocks[%s] is non-complete, where blocks=%s.",
          getClass().getSimpleName(), this, i, Arrays.asList(blocks));
{code}

* For scenarion#2, the same code can be shared to handle the case that when penultimate block is COMMITTED with minimal replication NOT met. the lease will be recovered, even if it's this kind of state.

On top of this suggested solution, we can add what Kihwal suggested.

What do you think?

Thanks.
, I found another\(?) instance in which the lease is not recovered. This is reproducible easily on a pseudo-distributed single node cluster
# Before you start it helps if you set. This is not necessary, but simply reduces how long you have to wait 
{code}
  public static final long LEASE_SOFTLIMIT_PERIOD = 30 * 1000;
  public static final long LEASE_HARDLIMIT_PERIOD = 2 * LEASE_SOFTLIMIT_PERIOD;
{code}
# Client starts to write a file. (could be less than 1 block, but it hflushed so some of the data has landed on the datanodes) (I'm copying the client code I am using. I generate a jar and run it using $ hadoop jar TestHadoop.jar)
# Client crashes. (I simulate this by kill -9 the $(hadoop jar TestHadoop.jar) process after it has printed "Wrote to the bufferedWriter"
# Shoot the datanode. (Since I ran on a pseudo-distributed cluster, there was only 1)

I believe the lease should be recovered and the block should be marked missing. However this is not happening. The lease is never recovered. I am going to check what happens when only the primary datanode is shot. {color:red}Please let me know if I shouldn't hijack this JIRA. By default I will{color}

{code:title=TestHadoop.java|borderStyle=solid}
import java.io.BufferedWriter;
import java.io.IOException;
import java.io.OutputStreamWriter;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

public class TestHadoop {
  public static void main(String args[]) throws IOException, InterruptedException {
    Path path = new Path("/tmp/testHadoop");
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(conf);
    System.out.println("DefaultFS: " + conf.get("fs.defaultFS"));
    System.out.flush();
    FSDataOutputStream hdfsout = fs.create(path,true);
    BufferedWriter br=new BufferedWriter(new OutputStreamWriter(hdfsout));
    System.out.println("Created the bufferedWriter" );
    System.out.flush();
    br.write("Some string");
    br.flush();
    hdfsout.hflush();
    System.out.println("Wrote to the bufferedWriter" );
    System.out.flush();
    
    Thread.sleep(120000); //KILL THE PROCESS DURING THIS SLEEP
    br.close();
    System.out.println("Closed the bufferedWriter" );
    System.out.flush();
  }
}
{code}

, I checked with 2 datanodes, and the correct thing happens. i.e. the lease is recovered properly. , Here's a patch with a unit test and a fix. Could people please review it?

The effect of this bug for us was that nodes could not be decommissioned cleanly. Although we knew that the client had crashed, the Namenode never released the leases (even after restarting the Namenode) (even months afterwards). There are actually several other cases too where we don't consider what happens if ALL the datanodes die while the file is being written, but I am going to punt on that for another time., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 32s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 28s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 31s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   2m 17s | The applied patch generated  4 new checkstyle issues (total was 538, now 525). |
| {color:green}+1{color} | whitespace |   0m  1s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 34s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   3m  3s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | native |   3m 14s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests | 163m 56s | Tests failed in hadoop-hdfs. |
| | | 206m 35s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.tracing.TestTraceAdmin |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12731032/HDFS-7342.04.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 4c7b9b6 |
| checkstyle |  https://builds.apache.org/job/PreCommit-HDFS-Build/10839/artifact/patchprocess/diffcheckstylehadoop-hdfs.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/10839/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/10839/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10839/console |


This message was automatically generated., Hi [~raviprak],

Thanks for reporting the new case and patch. I think it may be better to have a new jira for this case, so we don't mix the discussion. What do you think? Thanks..

, Hello [~raviprak], I have a quick question, will the above test reproduce the infinite loop of recovery by lease manager ? I am trying to reproduce this issue in Hadoop 2.0.0.

Thanks, A lot of work went into LeaseRecovery from my collection. Although I have not verified that its been fixed, the chances are high that it has. Hence closing this.]