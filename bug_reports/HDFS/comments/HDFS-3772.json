[After survey the source code, I found that after modifying the minimum replication, the HDFS persistent storage has no information about the former minimum replication. So we can not get the it after restart. So we need to think out of box.
I have found the blockThreshold(dfs.namenode.safemode.threshold-pct) specifies the percentage of blocks that should satisfy the minimal replication requirement defined by dfs.namenode.replication.min. I think we can change the sementics of this parameter to the percentage of blocks that satisfy the real replication of each file. And then we compare the NN received replication with the real replication of each file. If they are equal, we increment blockSafe.
Anyone who have some opinions? If this scenario is ok, I will fix it., yes, we will not persist min replication parameter and also this is not a per file config item. It will be common in cluster for all files.

{quote}
And then we compare the NN received replication with the real replication of each file. If they are equal, we increment blockSafe.
{quote}
Client's write will succeed if it meets min replication. ( So, it is possible that replication might not met the replication factor for that file yet). At that moment, if cluster restarts , this check may not allow NN to come out of safemode right? because real replication and file replication is not equal yet.

I am not sure we have scenarios to increasing the min-replication factor. Because that will be safe replication count. And we have actual replication count separately which will be modified by users.

Why can't you come out of safe mode explicitely by giving admin command and let replication happen. After this, restarts will not have issue.
, {quote}
Client's write will succeed if it meets min replication. ( So, it is possible that replication might not met the replication factor for that file yet).
{quote}
Indeedï¼Œthe min replication is used as this.
I also suspect whether we need to restart cluster with a bigger minimum replication. But the parameter in hdfs-site.xml is not limited to only smaller. Whether we need to add this limitation in the parameter description? 
{quote}
Why can't you come out of safe mode explicitely by giving admin command and let replication happen. After this, restarts will not have issue.
{quote}
It can resolve this problem when we know the hang is blocked by this issue clearly. But if the cluster just hang in safe mode and users can not judge whether is blocked by this issue. It's dangerous to exit safe mode explicitly and may cause HDFS metadata corruption. , {quote}
It can resolve this problem when we know the hang is blocked by this issue clearly. But if the cluster just hang in safe mode and users can not judge whether is blocked by this issue.
{quote}
But admin only modified this min replication property. In this case, deffinitely it will hang right. It should be clear for the admins I feel, when they modify it.

persisting this parameter at file level will be really unnecessary and this parameter is common for all. 

So, the other option would be to persist this parameter in Image along with the LAYOUT_VERSION..etc. We can bump the leyout version number and while loading image, if the version number is this new version number, then read that min replication from image file and use for the safemode validations.

But I am not sure about the case, whether we need this really or not.
So, Before moving let's take others opinion as well.
, {quote}
So, the other option would be to persist this parameter in Image along with the LAYOUT_VERSION..etc. We can bump the leyout version number and while loading image, if the version number is this new version number, then read that min replication from image file and use for the safemode validations.
{quote}
Agree with this solution and waiting for others opinion!, I was writing this when Jira went down. Will try to reproduce that comment.

> files created with the old replication count will expected to bump up to the new minimum upon restart automatically

This is not an expected behavior.
{{dfs.namenode.replication.min}} has two purposes:
# Counting the blocks satisfying the new minimum replication during startup.
# Controls the minimal number of replicas that must be created during pipeline in order to call the data transfer successful.

Setting {{replication.min}} to higher value does not mean NN replicates blocks to that min. 
It means NN will wait for that many replicas to be reported during startup before exiting SafeMode.
If you set it too high, this is one of the ways to never let NN go out of SafeMode automatically.

SafeMode prohibits replication or deletion of blocks or modification of the namespace,
so block replication will not happen until NN leaves SafeMode.
If you are trying to increase block replication for all files in your file system you should use
{{setReplication()}} on the root recursively. But replication will start only after SafeMode is OFF.

> I think we can change the semantics of this parameter to the percentage of blocks that satisfy the real replication of each file.

Not a good idea. In general, changing semantics of existing parameters is confusing. 
And in particular, because this will make NN stay in SafeMode forever if some DataNodes don't come up.

I think the question here is what you are trying to achieve with this? , Change semantics of the existing parameter or persistent the replication.min parameter to disk, we can choose one from the two alternatives., What problem are you solving?, the hang in safe mode.
we have the two alternative solution you can reference the above comments., It is not the hang. It works as designed. You increase {{replication.min}} and SafeMode waits for replicas to reach that safe replication limit.
You seem to assume that bumping up {{replication.min}} will result in more replicas, but it is not the case. You can use setReplication() for that., And you can use 
{code}hadoop dfsadmin -safemode leave{code}
command to manually exit safemode whenever it "hangs".]