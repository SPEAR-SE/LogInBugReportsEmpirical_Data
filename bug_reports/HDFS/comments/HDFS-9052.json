[Thanks for the report, Alex.

I'm thinking whether the exception may be caused by HDFS-6908, which has been fixed in release 2.6. It is possible that a stale INode is left in the deleted list due to HDFS-6908, which caused this conflicts (there happened to be another deleted INode with the same local name in the previous deleted list).

Because 2.3 is a released version, I suggest you to try the latest version (2.7.1) and see whether the same issue can still be reproduced. But the corruption may have to be manually fixed., I saw [https://issues.apache.org/jira/browse/HDFS-6908] but since the exception was different, NullPointerException, I opened this Jira.
I'm not sure how this corruption can be manually fixed since it stems from the FsImage. Do you have a workaround in mind or were you thinking of a patch? Also, maybe you can explain something that I struggle to understand since it's not documented anywhere: how come when a snapshot is deleted, its Diffs are merged with the PRIOR snapshot, and not the subsequent one? I appreciate your help!, A possible scenario is like this:
1. We deleted a file "foo" but it belongs to snapshot s1 so it is in s1's deleted list
2. Under the same directory, we did all the steps listed in the description in HDFS-6908, and the file created in step 3 is also named "foo". And we suppose the snapshots created in step 2&4 are named s2 and s3.
3. Because of the bug reported in HDFS-6908, we may have the later "foo" left in s2's deleted list. Then when you try to delete s2 you will hit the above exception.

The bug fixed in HDFS-6908 is that an INode which should be cleared is wrongly left in the deleted list. In HDFS-6908, because we changed the fsimage format in release 2.4, in the fsimage we only record INode ID in deleted list, and use the id to lookup the inode map. Since the real INode has been cleared from the INode Map, the lookup will hit NPE. You will not see NPE when loading fsimage in 2.3. And this conflict happens only when you have files with the same name ("foo" in the above example).

But the above example is just one possible scenario. It's still possible that the issue is caused by some other bug. To bypass the issue, you may need to apply a temporary patch to ignore the INode in the later snapshot's delete list.
, I've confirmed that the CDH 5.0.0/5.0.5 code actually contains the Hadoop 2.4 change you're referring to:
[Grepcode FSImageFormatPBSnapshot.java|http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hadoop/hadoop-hdfs/2.3.0-cdh5.0.5/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java/]

Let me elaborate a little on my scenario. We have a log file in HDFS (useraction.log.crypto), which we overwrite, and then do a snapshot on a parent folder. So the snapshots look as follows in the XML translation of the FsImage (abridged):
{code}
SNAPSHOT ID=5063; NAME=s2015022606
<diff><inodeid>3432626</inodeid>				// useractionlogs folder
  <dirdiff><snapshotId>5063</snapshotId><isSnapshotroot>false</isSnapshotroot><childrenSize>3</childrenSize><name>useractionlogs</name>
    <created><name>useraction.log.crypto</name></created>
    <deleted><inode>3564976</inode></deleted>
  </dirdiff>
  ... 
</diff>
<diff><inodeid>3564976</inodeid>
  <filediff><snapshotId>5063</snapshotId><size>1388</size><name>useraction.log.crypto</name></filediff>
</diff>
...

SNAPSHOT ID=5065; NAME=s2015022607
<diff><inodeid>3432626</inodeid>				// useractionlogs folder
  <dirdiff><snapshotId>5065</snapshotId><isSnapshotroot>false</isSnapshotroot><childrenSize>3</childrenSize><name>useractionlogs</name>
    <created><name>useraction.log.crypto</name></created>
    <deleted><inode>3565860</inode></deleted>
  </dirdiff>
  ...
</diff>
<diff><inodeid>3565860</inodeid>
  <filediff><snapshotId>5065</snapshotId><size>1388</size><name>useraction.log.crypto</name></filediff>
</diff>
...
{code}

As you see, each snapshot deletes and re-creates the _useraction.log.crypto_ file. It seems that there are 2 ways for me to run into the AssertionError, and I don't think the fix for [HDFS-6908|https://issues.apache.org/jira/browse/HDFS-6908] addresses Case 2.

*Case 1 - Do a snapshot diff using the hdfs client*
{code}
[root@node1075]# hdfs snapshotDiff /data/tenants/pdx-svt.baseline84/wddata s2015022606 s2015022607
snapshotDiff: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]
{code}

Here's the _tail_ from the Namenode logs:
{code}
2015-09-11 01:44:59,558 WARN  [IPC Server handler 89 on 8020] ipc.Server (Server.java:run(2002)) - IPC Server handler 89 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getSnapshotDiffReport from 10.52.209.75:60609 Call#0 Retry#1: error: java.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]
java.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]
	at org.apache.hadoop.hdfs.util.Diff.insert(Diff.java:193)
	at org.apache.hadoop.hdfs.util.Diff.delete(Diff.java:239)
	at org.apache.hadoop.hdfs.util.Diff.combinePosterior(Diff.java:462)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.initChildren(DirectoryWithSnapshotFeature.java:293)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.iterator(DirectoryWithSnapshotFeature.java:303)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.computeDiffRecursively(INodeDirectorySnapshottable.java:441)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.computeDiffRecursively(INodeDirectorySnapshottable.java:446)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.computeDiffRecursively(INodeDirectorySnapshottable.java:446)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.computeDiff(INodeDirectorySnapshottable.java:390)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.diff(SnapshotManager.java:378)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getSnapshotDiffReport(FSNamesystem.java:7123)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getSnapshotDiffReport(NameNodeRpcServer.java:1261)
...
{code}

It seems that the fix for the other Jira is likely to address this issue. However, the next case doesn't seem to follow the same exception stack.

*Case 2 - Deleting a snapshot*
{code}
[root@node1075]# hdfs dfs -deleteSnapshot /data/tenants/pdx-svt.baseline84/wddata s2015022607_maintainer_soft_del
deleteSnapshot: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]
{code}

Here's the exception from the Namenode logs this time:
{code}
2015-09-11 03:25:16,403 WARN  [IPC Server handler 10 on 8020] ipc.Server (Server.java:run(2002)) - IPC Server handler 10 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.deleteSnapshot from 10.52.209.75:61662 Call#1 Retry#0: error: java.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]
java.lang.AssertionError: Element already exists: element=useraction.log.crypto, DELETED=[useraction.log.crypto]
	at org.apache.hadoop.hdfs.util.Diff.insert(Diff.java:193)
	at org.apache.hadoop.hdfs.util.Diff.delete(Diff.java:239)
	at org.apache.hadoop.hdfs.util.Diff.combinePosterior(Diff.java:462)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.initChildren(DirectoryWithSnapshotFeature.java:293)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2.iterator(DirectoryWithSnapshotFeature.java:303)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDeletedINode(DirectoryWithSnapshotFeature.java:531)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDirectory(DirectoryWithSnapshotFeature.java:823)
	at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtree(INodeDirectory.java:714)
	at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtreeRecursively(INodeDirectory.java:684)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature.cleanDirectory(DirectoryWithSnapshotFeature.java:830)
	at org.apache.hadoop.hdfs.server.namenode.INodeDirectory.cleanSubtree(INodeDirectory.java:714)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectorySnapshottable.removeSnapshot(INodeDirectorySnapshottable.java:341)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.deleteSnapshot(SnapshotManager.java:238)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteSnapshot(FSNamesystem.java:7173)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.deleteSnapshot(NameNodeRpcServer.java:1222)
{code}

As you see, the method where the fix is, _computeDiffBetweenSnapshots_, is never called. I'd assume that some kind of check is needed in the ELSE clause of the following method in _Diff.java_ before _insert_ is called:
{code}
public UndoInfo<E> delete(final E element) {
    final int c = search(created, element.getKey());
    E previous = null;
    Integer d = null;
    if (c >= 0) {
      // remove a newly created element
      previous = created.remove(c);
    } else {
      // not in c-list, it must be in previous
      d = search(deleted, element.getKey());
      insert(ListType.DELETED, element, d);
    }
    return new UndoInfo<E>(c, previous, d);
  }
{code}, [~jingzhao], please let me know if you have any additional comments on this since we're trying to figure out how to work around this problem in our production clusters., Hi Alex, so the issue here is not about {{computeDiffBetweenSnapshots}} or deleting a snapshot. These are just possible cases that can expose the corrupted snapshot diff list. Let me try to provide more context information about snapshot diff list. In our current snapshot implementation, we record newly created files in create list and deleted files in the delete list. So let's suppose we take a snapshot s1, and then delete the file "useraction.log.crypto", since the file exists before creating snapshot s1, we have:
{noformat}
s1: deleted list: [INodeFile_1(useraction.log.crypto)]
{noformat}
Now we take another snapshot s2, and then create the new log file with the same name. s2's diff list looks like:
{noformat}
s2: created list: [INodeFile_2(useraction.log.crypto)]
{noformat}
We then take snapshot s3, and delete the log file. Now we have:
{noformat}
s1: created list:[], deleted list: [INodeFile_1(useraction.log.crypto)]
s2: created list: [INodeFile_2(useraction.log.crypto)], deleted list: []
s3: created list: [], deleted list: [INodeFile_2(useraction.log.crypto)]
{noformat}
Let's say we now delete s3. The diff lists of s2 and s3 should be combined and because INodeFile_2(useraction.log.crypto) is created after taking s2, the correct diff lists should look like:
{noformat}
s1: created list: [], deleted list: [INodeFile_1(useraction.log.crypto)]
s2: created list: [], deleted list: []
{noformat}
But before HDFS-6908 we have a bug which caused INodeFile_2(useraction.log.crypto) still stayed in s2's deleted list. Then we have:
{noformat}
s1: deleted list: [INodeFile_1(useraction.log.crypto)]
s2: deleted list: [INodeFile_2(useraction.log.crypto)]
{noformat}
Now we have a corrupted diff list state. No matter we compute snapshot diff between s1 and the current state, or delete the snapshot s2, in case that we have to combine s1 and s2, we will get the AssertionError.

Because the corruption has been persisted in your fsimage, to fix the issue you may have to use a patched jar to remove the INodeFile_2(useraction.log.crypto) from s2's deleted list when loading the fsimage.
, Thank you for the detailed explanation, Jing. I had not seen the following change in _cleanDirectory_ method in [HDFS-6908|https://issues.apache.org/jira/browse/HDFS-6908], which threw me off:
{code}
+      counts.add(currentINode.cleanSubtreeRecursively(snapshot, prior,
+          collectedBlocks, removedINodes, priorDeleted, countDiffChange));
+
       // check priorDiff again since it may be created during the diff deletion
       if (prior != Snapshot.NO_SNAPSHOT_ID) {
         DirectoryDiff priorDiff = this.getDiffs().getDiffById(prior);
{code}

I will follow your suggestion to fix the fsimage. Should I link this Jira to [HDFS-6908|https://issues.apache.org/jira/browse/HDFS-6908] and resolve it?, Yeah, we can resolve this jira as duplicated with HDFS-6908., Sounds good. Thanks again, Jing, appreciate your help!]