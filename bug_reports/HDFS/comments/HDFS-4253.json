[This patch changes pseudoSortByDistance to randomize more thoroughly -- it will randomly choose nodes on the local rack, and randomly order the remote-rack nodes if there are multiple of them.

This means changing some of the TestNetworkTopology unit tests which assert that the high replicas are not reordered.

I also added an {{@VisibleForTesting derandomizeRandom}} helper function to make the test deterministic.  Without this determinism, the "check that we get varying results" test would occasionally get the same random permutation twice in a row and fail., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12555568/hdfs4253.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3581//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3581//console

This message is automatically generated., Good find, Andy!  This issue could certainly kill performance for highly replicated files on a cluster where there was no topology.

{code}
+      @Override
+      public int compare(Node a, Node b) {
{code}

I find it a little bit concerning that this function will not return 0 (equals) even when a and b are identical but distinct objects.  It seems to me that you should not use {{==}}, but instead simply do all the comparisons and return 0 when all fields are identical.  {{==}} might be an optimization, but that hardly seems relevant here., bq. I find it a little bit concerning that this function will not return 0 (equals) even when a and b are identical but distinct

Is the correct fix to simply
{code}
-        if (a == b) return 0;
+        if (a.equals(b)) return 0;
{code}
?

Thanks for the review!, Uploading new patch using .equals rather than ==., More about that {{compare}} method:  I re-read this method, with an eye to seeing if it satisfied all the comparator requirements outlined in Effective Java, listed here: http://www.javapractices.com/topic/TopicAction.do?Id=10  I don't think that it does.  In particular, you need to satisfy the "if a.compare(b) < 0, b.compare(a) > 0" requirement.

I understand now why you special-cased a == b; it's so that the next few lines, where you test that a == rdr and b == rdr, they can't both be true at that point.  While this is correct, it's also non-obvious, so I would suggest either adding a comment or simply testing {{if (aIsLocal && bIsLocal)}} directly.

The bug comes later, where you always return 1 if neither Node is on the local rack.  This is wrong; it violates anticommutation (see link).  I would suggest something along these 
lines:

{code}
salt = Random.nextInt();
...
@Override
public int compare(Node a, Node b) {
  return ComparisonChain.start()
     .compare(a != rdr, b != rdr)
     .compare(!isOnSameRack(a, rdr), !isOnSameRack(b, rdr))
     .compare(a.getHash() ^ salt, b.getHash() ^ salt)
     .compare(a.getName(), b.getName());
     .result();
}
{code}, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12555845/hdfs4253-1.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3593//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3593//console

This message is automatically generated., bq. The bug comes later, where you always return 1 if neither Node is on the local rack. This is wrong; it violates anticommutation (see link).

But that's not what the code does.  If neither Node is on the local rack, then {{aIsLocalRack == bIsLocalRack}} and we use the shuffle for a total ordering, right here:
{code}
858         if (aIsLocalRack == bIsLocalRack) {
859           int ai = shuffle.get(a), bi = shuffle.get(b);
860           if (ai < bi) {
861             return -1;
862           } else if (ai > bi) {
863             return 1;
864           } else {
865             return 0;
866           }
{code}
The final {{else}} is only reached when {{bIsLocalRack && !aIsLocalRack}}. So I'm pretty sure this implementation does satisfy anticommutation., Avoid extra a.equals(b) by checking {{aIsLocal && bIsLocal}} instead.  On average for a given sort this will result in fewer calls to {{.equals()}}., Thanks for clarifying that.  I still think there's a problem, though-- I don't see any reason why shuffle(a) could not be equal to shuffle(b), for two completely unrelated DatanodeIDs a and b.  This could be fixed by checking something that's supposed to be unique in the case where the two agree-- like the name field.  It also seems better to just use {{hashCode}}, rather than creating your own random set of random ints associated with objects., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561062/hdfs4253-2.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3667//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3667//console

This message is automatically generated., bq. I don't see any reason why shuffle(a) could not be equal to shuffle(b), for two completely unrelated DatanodeIDs a and b.

That's true, equality is possible.  It's very unlikely given that we're choosing N items (where N is the replication count of a block, so nearly always 3, sometimes 10, possibly as absurdly high as 50) from the range of {{Random#NextInt}} which is about 2**32.  The algorithm does something reasonable in the case that the shuffle has a collision (it puts the items in some order, either stable or not, and either result is fine for the rest of the algorithm). It would be possible to remove the possibility of collisions, but I don't know how to do that quickly with minimal code.  So the current implementation seemed to strike a nice balance between the desired behavior, efficient and easily understandable code, and low algorithmic complexity.

bq. It also seems better to just use hashCode, rather than creating your own random set of random ints associated with objects.

It's important that we get a different answer each time {{pseudoSortByDistance}} is invoked; that randomization is what spreads the read load out across the replicas. So using a stable value like hashCode would defeat that goal of this change.  (Possibly it might be true that hashCode ordering would be different in different {{DFSClient}} instances on different nodes, but I see no guarantee of that, and even if it's true, depending on such a subtle implementation detail would be dangerous. And it still doesn't resolve the issue that a single DFSClient should pick different replicas from a given class, for various reads of a given block.), bq. It would be possible to remove the possibility of collisions, but I don't know how to do that quickly with minimal code.

Just compare the name., Instead of generating random numbers, put all datanodes in an array, shuffle it and use the index.  Then, there is no collisions., bq. Instead of generating random numbers, put all datanodes in an array, shuffle it and use the index. Then, there is no collisions.

That seems like more code for little benefit, but OK.  Here's the incremental diff:
{code}
@@ -831,8 +831,14 @@ public void pseudoSortByDistance( Node reader, Node[] nodes ) {
     final Node rdr = reader;
     final HashMap<Node, Integer> shuffle = new HashMap<Node, Integer>(nodes.length);
 
+    // Fisher-Yates shuffle, http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle
+    Node [] nodeshuffle = Arrays.copyOf(nodes, nodes.length);
+    for (int i = nodes.length - 1; i > 0; i--) {
+      int j = r.nextInt(i + 1);
+      swap(nodeshuffle, i, j);
+    }
     for (int i = 0; i < nodes.length; i++) {
-      shuffle.put(nodes[i], r.nextInt());
+      shuffle.put(nodeshuffle[i], i);
     }
{code}, There are shuffle methods in java.util.Collections., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561823/hdfs4253-3.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3683//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3683//console

This message is automatically generated., bq. There are shuffle methods in java.util.Collections.

Thanks!  that works very nicely., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12562189/hdfs4253-4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.ha.TestZKFailoverController
                  org.apache.hadoop.hdfs.TestPersistBlocks

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3687//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3687//console

This message is automatically generated., That looks better.  I think this can be improved even a little bit more, though. You don't need a HashMap for this.  Just shuffle the array and then call Arrays.sort with your custom comparator.  Since Arrays.sort is a stable sort (doesn't re-order equal elements) the randomization will still be there.

(See http://docs.oracle.com/javase/6/docs/api/java/util/Arrays.html#sort%28T[],%20java.util.Comparator%29), bq. Just shuffle the array and then call Arrays.sort with your custom comparator. Since Arrays.sort is a stable sort (doesn't re-order equal elements) the randomization will still be there.

OK, I think I see your point after squinting at it for a while.  I would find that code quite impenetrable and surprising without a comment explaining why, though.  And I've taken a shot at writing a good comment and not succeeded.  Could you suggest wording to explain the subtle approach you've suggested, so that others can understand this code in the future?, New patch implementing cmccabe's suggestion to presume stable search and shuffle first., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12563303/hdfs4253-5.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3733//console

This message is automatically generated., Attaching new patch -6 with improved comment., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12563809/hdfs4253-6.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDFSShell

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3799//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3799//console

This message is automatically generated., "System.out.println" should be replace with "LOG.debug"., please tell me how to unsubscribe from this mailing list?

, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12729962/HDFS-4253.06.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 6ae2a0d |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10721/console |


This message was automatically generated., Closing as duplicate of HDFS-6840.
If that doesn't solve the issue. Feel free to re-open.]