[Following trace is logged..
{noformat}
012-07-04 03:17:28,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1438589468-HOST**.233-1341244293218:blk_-46591813407025079_624958, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2012-07-04 03:17:28,937 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: HOST-10-18-40-168:50010:DataXceiver error processing WRITE_BLOCK operation  src: /10.18.40.168:13129 dest: /DN**.168:50010
java.lang.OutOfMemoryError: Direct buffer memory
	at java.nio.Bits.reserveMemory(Bits.java:632)
	at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:97)
	at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:288)
	at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:155)
	at sun.nio.ch.IOUtil.read(IOUtil.java:169)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:243)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:54)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:154)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:127)
	at java.io.FilterInputStream.read(FilterInputStream.java:116)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at java.io.DataInputStream.read(DataInputStream.java:132)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readToBuf(BlockReceiver.java:405)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.readNextPacket(BlockReceiver.java:452)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:511)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:748)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:464)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:98)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:66)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:189)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

If we increase XX:MaxDirectMemorySize,i think OOME wn't come.currently I configured 1GB..But even then there is chance it will throw..May be native memory leak I am suspecting..
, bq. -XX:+DisableExplicitGC
This could be the possible culprit.

Generally I noticed these kind of problems in DataNode and RegionServer processes.
In these processes, native memory used heavily used via NIO and I have seen RegionServer(HBase) process consuming around 20+ GB of memory although its max heap is configured to 4GB (-Xmx)

So, in order to keep the memory footprint(VIRT & RES values) in control, we need to configure MaxDirectMemorySize. At the same time, I observed that this direct memory is not part of heap and is getting collected with FullGC (When it reaches the limit or rmi server dgc interval) only.

To conclude, configure MaxDirectMemorySize but DONT use DisableExplicitGC.

@Brahma, can you please post your findings after removing this flag (DisableExplicitGC)., Hi Laxman,

Thanks for taking a look

I will remove the flag and perform the test again and let you know the results, I removed DisableExplicitGC and performed test..I did not seen any outofMemoryError..thanks laxman, I removed DisableExplicitGC and performed test..I did not seen any outofMemoryError..thanks laxman]