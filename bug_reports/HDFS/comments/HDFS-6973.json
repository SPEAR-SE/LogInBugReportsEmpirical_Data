[Hi [~stevenxu], thanks for finding the reporting the issue. HBASE-9393 reported same issue as you reported, however, this jira is still unresolved. I saw [~cmccabe]  did some anaylsis and gave suggestion there (Colin Patrick McCabe added a comment - 11/Oct/13 19:25), which makes sense to me. I wonder if you can try what he recommended there? Thanks.


, I have the same problem,too many tcp status wait_close to datanode 50010. According to issue HBASE-9393 ，set dfs.client.socketcache.capacity to 0 and dfs.datanode.socket.reuse.keepalive to 0, but not take effect, How did you solve this problem?, forbid IPv6 function in  all nodes, I  meet the problem，you try it, I can relate to that. We're using a org.apache.hadoop.fs.FSDataInputStream for reading multiple files continuously 2 times an hour from a 2.7.1 cluster. 

I've added "-Djava.net.preferIPv4Stack=true" and "-Djava.net.preferIPv6Addresses=false" but it only changed that the sockets are now ipv4 instead of ipv6.

After 12 hours usage, 1.4K open sockets:

java    10486 root 2233u  IPv4           28226850      0t0      TCP 10.134.160.9:55927->10.134.160.28:50010 (CLOSE_WAIT)
java    10486 root 2237u  IPv4           28223758      0t0      TCP 10.134.160.9:37363->10.134.160.17:50010 (CLOSE_WAIT)
java    10486 root 2240u  IPv4           28223759      0t0      TCP 10.134.160.9:48976->10.134.160.41:50010 (CLOSE_WAIT)
java    10486 root 2248u  IPv4           28222398      0t0      TCP 10.134.160.9:55976->10.134.160.28:50010 (CLOSE_WAIT)
java    10486 root 2274u  IPv4           28222403      0t0      TCP 10.134.160.9:53185->10.134.160.35:50010 (CLOSE_WAIT)
java    10486 root 2283u  IPv4           28211085      0t0      TCP 10.134.160.9:56009->10.134.160.28:50010 (CLOSE_WAIT)

10.134.160.9 ip of the host with the driver programm, dst-ips are the hadoop-nodes., Sorry here, but i found out that our driver programm forgot to close the FSDataInputStream at some place, which fixed the behaviour above., HBASE-9393, is due to unclosed streams maintained in hbase for later reads.
number of CLOSE_WAITs is same as number of streams kept open. 
When the stream is re-used for reading, corresponding CLOSE_WAIT will get closed and read will happen by opening new connection.

So IMO, this is not a problem. As already suggested in HBASE-9393, if want to keep the stream open, without keeping the socket open, FSDataInputStrean#unbuffer() can be called after reading to close the block readers., Disable IPv6 cannot kill the problem. The number of opened socket in IPv4 keep increase., I also meet a similar issue. We use parquet file in a 2.7.1 hdfs cluster to store result from spark. In my part, I need to read the parquet file by java. But I found the lots of CLOSW_WAIT connection to 50010 port.

tcp        1      0 192.168.1.83:50437      192.168.1.83:50010      CLOSE_WAIT  46462/java          
tcp        1      0 192.168.1.83:50539      192.168.1.83:50010      CLOSE_WAIT  46462/java          
tcp        1      0 192.168.1.83:50520      192.168.1.83:50010      CLOSE_WAIT  46462/java          
tcp        1      0 192.168.1.83:50540      192.168.1.83:50010      CLOSE_WAIT  46462/java          
tcp        1      0 192.168.1.83:50513      192.168.1.83:50010      CLOSE_WAIT  46462/java          
tcp        1      0 192.168.1.83:50440      192.168.1.83:50010      CLOSE_WAIT  46462/java          
tcp        1      0 192.168.1.83:50530      192.168.1.83:50010      CLOSE_WAIT  46462/java

And in my cod I have closed the parquet reader. Here is my code

        List<Map<String, Object>> result = new ArrayList<>();
        Path path = new Path(filePath);
        GroupReadSupport readSupport = new GroupReadSupport();
        ParquetReader.Builder<Group> builder = ParquetReader.builder(readSupport, path).withConf(configuration);
        ParquetReader<Group> reader = builder.build();
        Group record = null;

        while ((record = reader.read()) != null) {
            List<Type> columnDescriptors =  record.getType().getFields();

            Map<String, Object> recordMap = genRecordMap(record, columnDescriptors);

            result.add(recordMap);
        }

        reader.close();
        reader = null;

        return result;

Any one have ideas on how to fix it?, Yaolong Zhu (JIRA won't let me tag you for some reason), I just ran into the same issue when using ParquetReader. For us it happened after we upgraded parquet-hadoop and parquet-column to version 1.9.0. When we downgraded back to 1.8.1 the issue went away. I haven't had time to search for known issues or report a new issue., [~robreeves] Hi Rob, I found the root cause of this issue which lies in the close method of ParquetFileReader.

@Override
  public void close() throws IOException {
    try {
      if (f != null) {
        f.close();
      }
    } finally {
      if (codecFactory != null) {
        codecFactory.release();
      }
    }
  }

The f.close() is actually calling the close() method of InputStream which is an empty method rather than H2SeekableInputStream or H1SeekableInputStream. So I update this close method to 
@Override
  public void close() throws IOException {
    try {
      if (f != null) {
        if(f instanceof H2SeekableInputStream) {
          ((H2SeekableInputStream)f).close();
        } else if(f instanceof H1SeekableInputStream) {
          ((H1SeekableInputStream)f).close();
        } else {
          f.close();
        }

      }


    } finally {
      if (codecFactory != null) {
        codecFactory.release();
      }
    }
  }

And the problem is solved. ]