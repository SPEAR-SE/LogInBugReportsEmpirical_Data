[My proposal is to apply some limit to number of blocks scanned per iteration of {{BlockPoolSliceScanner#scan()}}. So rolling will remove stale entries from verification logs.

Any thoughts..?, Attaching the proposed patch. Please review, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12635270/HDFS-6114.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestSafeMode

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6422//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6422//console

This message is automatically generated., Hi,
Can someone take a look at the patch?

Thanks in advance., Hi Vinayakumar,

Good find.

This proposed configuration parameter seems difficult to tune.  Most people don't keep careful track of how many blocks they scan each time the scanner runs, and would not have a clear idea of how to adjust it.  I can also see some people setting this incorrectly and ending up with a cluster where the block scanner falls further and further behind, and 99% of the blocks on the DN are never scanned.

Rather than introducing another configuration parameter, how about simply not adding the new blocks to the existing scan?  Perhaps {{BlockPoolScanner#addBlock}} could add the block to a secondary map that would get dumped into the main map when the current scan had terminated.  This way, scan() will always return at some point, when the current blocks are done, even if new blocks have been created.  The main point of the block scanner is to scan older blocks we haven't touched in weeks, not to re-read stuff we just wrote, so this seems like a better behavior anyway., Thanks colin, Sorry for the late response. I will try implementing your suggestion and post a new patch soon., Attaching the patch as per [~cmccabe] suggestion, I don't really see a good reason to separate {{delBlockInfo}} and {{delNewBlockInfo}}.  It seems like this could just lead to scenarios where we think we're deleting a block but it pops back up (because we deleted, but did not delete new)

I guess maybe it makes sense to separate {{addBlockInfo}} from {{addNewBlockInfo}}, just because there are places in the setup code where we're willing to add stuff directly to {{blockInfoSet}}.  Even in that case, I would argue it might be easier to call {{addNewBlockInfo}} and then later roll all the {{newBlockInfoSet}} items into {{blockInfoSet}}.  The problem is that having both functions creates confusion and increase the chance that someone will add an incorrect call to the wrong one later on in another change.

{code}
  private final SortedSet<BlockScanInfo> blockInfoSet
      = new TreeSet<BlockScanInfo>(BlockScanInfo.LAST_SCAN_TIME_COMPARATOR);

  private final Set<BlockScanInfo> newBlockInfoSet =
      new HashSet<BlockScanInfo>();
{code}
It seems like a bad idea to use {{BlockScanInfo.LAST_SCAN_TIME_COMPARATOR}} for blockInfoSet, but {{BlockScanInfo#hashCode}} (i.e. the HashSet strategy) for {{newBlockInfoSet}}.  Let's just use a {{SortedSet}} for both so we don't have to ponder any possible discrepancies between the comparator and the hash function.  Another problem with {{HashSet}} (compared with {{TreeSet}}) is that it never shrinks down after enlarging... a bad property for a temporary holding area., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12645954/HDFS-6114.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7340//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7340//console

This message is automatically generated., Thanks [~cmccabe], I will recheck all your points and post an updated patch soon., bq. I don't really see a good reason to separate delBlockInfo and delNewBlockInfo. It seems like this could just lead to scenarios where we think we're deleting a block but it pops back up (because we deleted, but did not delete new)
Here, both are working on different set. {{delBlockInfo}} is used in someother places as well while updating the scantime and resort the blockInfoSet.
{{delNewBlockInfo}} is only needs to be called while deleting the block itself, as intermediate updates will not happen on this set data.
So {{delBlockInfo}} and {{delNewBlockInfo}} serves separate purposes and both are required.

bq. I guess maybe it makes sense to separate addBlockInfo from addNewBlockInfo, just because there are places in the setup code where we're willing to add stuff directly to blockInfoSet. Even in that case, I would argue it might be easier to call addNewBlockInfo and then later roll all the newBlockInfoSet items into blockInfoSet. The problem is that having both functions creates confusion and increase the chance that someone will add an incorrect call to the wrong one later on in another change.
As I am seeing, both these methods are private and acts on different sets. since method name itself suggests {{addNewBlockInfo}} is only for the new blocks. I am not seeing any confusion here.

bq. It seems like a bad idea to use BlockScanInfo.LAST_SCAN_TIME_COMPARATOR for blockInfoSet, but BlockScanInfo#hashCode (i.e. the HashSet strategy) for newBlockInfoSet. Let's just use a SortedSet for both so we don't have to ponder any possible discrepancies between the comparator and the hash function.
{{blockInfoSet}} is required to be sorted based on the lastScanTime, as oldest scanned block will be picked for scanning, which will be the first element in this set always. BlockScanInfo.LAST_SCAN_TIME_COMPARATOR is used because {{BlockScanInfo#hashCode()}} is default which will sort based on the blockId rather than scan time. 
Do you suggest me to update this {{hashCode()}} itself?

bq. Another problem with HashSet (compared with TreeSet) is that it never shrinks down after enlarging... a bad property for a temporary holding area
Yes, this I agree, will update in the next patch., bq. blockInfoSet is required to be sorted based on the lastScanTime, as oldest scanned block will be picked for scanning, which will be the first element in this set always. BlockScanInfo.LAST_SCAN_TIME_COMPARATOR is used because BlockScanInfo#hashCode() is default which will sort based on the blockId rather than scan time.  Do you suggest me to update this hashCode() itself?

I was suggesting that you use a {{TreeSet}} or {{TreeMap}} with the same comparator as {{blockInfoSet}}.  All the hash sets that I'm aware of do not shrink down after enlarging.

bq. So delBlockInfo and delNewBlockInfo serves separate purposes and both are required.

I can write a version of the patch that only has one del function and only one add function.  I am really reluctant to put in another set of add/del functions on top of what's already there, since I think it will make things hard to understand for people trying to modify this code later or backport this patch to other branches., Attached the updated patch
1. Used TreeSet for newBlockInfoSet
2. Merged add/del methods,

Please review, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12656002/HDFS-6114.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA
                  org.apache.hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage
                  org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7357//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7357//console

This message is automatically generated., {code}
  // add new blocks to scan in next iteration
  private synchronized void rollNewBlocksInfo() {
    for (BlockScanInfo newBlock : newBlockInfoSet) {
      blockInfoSet.add(newBlock);
    }
  }
{code}
I think we need to clear the {{newBlockInfoSet}} here.

{code}
+    boolean exists = newBlockInfoSet.remove(info);
+    exists = exists || blockInfoSet.remove(info);
{code}
I guess this is a nit, but I'd prefer just another "if" statement, to the {{||}} construct., Updated patch with above comments. Please review, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12656200/HDFS-6114.patch
  against trunk revision .

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7368//console

This message is automatically generated., Seems like there was a compilation error in this build even before applying the patch. Later builds dont have that problem. So triggered the jenkins again., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12656200/HDFS-6114.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7373//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7373//console

This message is automatically generated., Hi [~cmccabe], could you please take a look at the updated patch whenever you find time?
Thanks in advance., +1.  Thanks, Vinayakumar., FAILURE: Integrated in Hadoop-trunk-Commit #5954 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5954/])
HDFS-6114. Block Scan log rolling will never happen if blocks written continuously leading to huge size of dncp_block_verification.log.curr (vinayakumarb via cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1612943)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
, Thanks a lot [~cmccabe] for reviews and commit., FAILURE: Integrated in Hadoop-Yarn-trunk #622 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/622/])
HDFS-6114. Block Scan log rolling will never happen if blocks written continuously leading to huge size of dncp_block_verification.log.curr (vinayakumarb via cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1612943)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1814 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1814/])
HDFS-6114. Block Scan log rolling will never happen if blocks written continuously leading to huge size of dncp_block_verification.log.curr (vinayakumarb via cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1612943)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1841 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1841/])
HDFS-6114. Block Scan log rolling will never happen if blocks written continuously leading to huge size of dncp_block_verification.log.curr (vinayakumarb via cmccabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1612943)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java
]