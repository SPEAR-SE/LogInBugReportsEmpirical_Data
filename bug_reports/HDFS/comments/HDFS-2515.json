[Hi,

This happens when you have not configured "fs.default.name" in core-site.xml and attempt to start your NameNode without a proper hdfs://host:port schema. The default FS is file:/// and hence you see this.

Its always a better idea to use the Apache Hadoop HDFS mailing lists in case you are not sure its a bug, before reporting!

Marking as resolved as am able to run the security branch NN just fine with proper configs on the classpath.

Regards,
Harsh, hi,

I have configured "fs.default.name" in core-site.xml,still it can't work sucessfully.

example 1:
 <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:8020</value>
  </property>

example 2:
 <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost</value>
  </property>
, still not work, 程国强,

Ok, could you exactly tell me the steps you're following?

Here's what I do, seems to work just fine:

* Download a tarball. Extract to HADOOP_HOME
* vim HADOOP_HOME/conf/core-site.xml
* Add fs.default.name as a property name-value pair, under the configuration tags.
* HADOOP_HOME/bin/hadoop namenode -format
* HADOOP_HOME/bin/start-dfs.sh

All done. Can upload/download files.

Also, I'd appreciate if this were mailed to hdfs-user@hadoop.apache.org instead, until we have determined a bug., I have extracted HADOOP_HOME,but it's always prompt "Warning: $HADOOP_HOME is deprecated."
, Are you sure the correct config directory is being used when the namenode is launched?  I've seen this issue before, but it was only when the wrong configs were used.  Try checking all your HADOOP_* envs.  Perhaps try "strace -fe open start-dfs.sh" to see where the configs are being read., And are you sure if it is 0.20.205 you are using?, The wrong configs were used,I use the "/usr/conf/core-site.xml",but the correct config directory is "/usr/etc/hadoop/core-site.xml"

thanks very much, The wrong configs were used,I use the "/usr/conf/core-site.xml",but the correct config directory is "/usr/etc/hadoop/core-site.xml"

thanks very much, User cited wrong configs were used, so the observed behavior is correct., Hi, I got this error too. I am using hadoop-1.0.0 on cygwin with the correct entries in /etc/hadoop/core-site.xml

Log is as below - 
2012-02-12 01:55:41,455 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = calvin-PC/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.0.0
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1214675; compiled by 'hortonfo' on Thu Dec 15 16:41:31 UTC 2011
************************************************************/
2012-02-12 01:55:41,911 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2012-02-12 01:55:41,951 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2012-02-12 01:55:41,954 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2012-02-12 01:55:41,954 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2012-02-12 01:55:42,058 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: file:///
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:185)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)

2012-02-12 01:55:42,059 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at calvin-PC/127.0.0.1
************************************************************/

Pls let me know where am i going wrong.

Thanks,
Manish, Hi Manish,

Please configure correctly "fs.default.name" in core-site.xml and *make sure the same configurations are cumming into classpath*. 
Also please look at previos discussions already discussed about the same issue., Hello,
I downloaded Hadoop 1.0 and tried to start. It give the same error as above.
Is it mandatory to edit and enter properties core-site.xml even when running in standalone mode?

I was going through this site. (Doc for 1.0)
http://hadoop.apache.org/common/docs/r1.0.0/single_node_setup.html

It says it is required to edit core-site.xml only for Pseudo-Distributed mode.

Please clarify.

, Same problem here, according to the documentation it should work on standalone mode in case core-site.xml, mapred-site.xml and hdfs-site.xml only contain an empty configuration element. Once added the pseudo-distributed mode configuration values I am able to start the system., Hi,

This was qualified to be a user issue and was closed out. The JIRA project is for Hadoop development and bug reports. For user-queries, please send a mail out to the mailing lists instead (hdfs-user at hadoop.apache.org).

Thanks :)]