[We has implement a prototype which cover 4-TODO before, we use quicklz as compress codec.

During compressing, we get some statistics like below:
dfs.block.compress.chunk.size   1M
dfs.block.compress.ratio.min    1.2  
block.compressor.thread.num     1

compress block 132744, before size 8008031402428, after size 2910562972347. compress ratio 2.75136
total block 300060, before size 14813811957857, after size 9716343527776. compress ratio 1.52462.
compress block/total block : 0.44239 
compress save space : 4.63612 T

, Efficiency of compression seems like something that varies depending on the codec and available hardware. Do we need some kind of metric to guide the tool in deciding when to compress? ...or just provide appropriate controls to the user with reasonable defaults?

IN any event, I don't think it is a given that compression of hot data will always be inefficient in all codecs for all hardware for all users at all times., I think the detection of hot vs. cold data is something important that should be added in with or without transparent compression.  It opens up a number of possibilities for trying to improve storage efficiencies.  In addition to compression we could keep hot data in a cache of some sort SSD, Ram Disk, etc.  We could also migrate cold data to spinning disks, or even to something else even slower and cheaper per GB if it is really never accessed.  I am sure others will have ideas as well about what to do with hot vs cold data.  It could be tied into some of the work for fsync and fadvise behind the scenes so that we try to keep hot data in the disk cache and only run fadvise to flush out the cache if it is not really hot., To Tim:
   Absolutely, efficiency of compression depend on codec and data to be compress. In the first step, we can use a specify codec on prototype. In the future, we can use right codec for different data in an self-adaption way, but I have no idea in implement it in an effective way yet.
   In our prototype,we decide "when to compress" in two ways. 
   1. data xceiver number and number of compressing task. 
      When a datanode has a high data xceiver number, it always means it've to serve for many client request(include balance/block replication).At this time, I think, compression is not a very urgent task, so it can be slow down, and release resource for computing task.
   2. We make compression as a single process, and make it running as idle CPU class. In this way, when some CPU-intensive job coming, compression task can release CPU slice to job, and when our cluster idle, compression can work in full speed.

 >> IN any event, I don't think it is a given that compression of hot data will always be inefficient in all codecs for all hardware for all users at all times.

  It's right, compression before upload can save bandwidth and reduce transmission cost, but it'll slow down running job. It's a trade off. In our cluster, CPU utilization isn't mean in every time, so use idle time to make compression is valuable. To reduce transmission cost, we'll support compression on client as while.

To Robert:
   Absolutely, detection of hot/cold data is really an important thing.To distinguish them, we add atime in block level."Atime" will be updated only when any DFSClient read it, and block replication,block scanner or re-balance should not modify it.This value will be store in disk, to avoid atime loss when datanode restart.
   Back to cold/hot data topic, we can make many improvements for different application. For example, if we use hdfs as an image storage, and hot image can be accessed for thousands of time in a second, we can use SSD to reduce latency, and use sata disk for cold data for cost-effective. 
   Currently, in our hadoop cluster, low latency is not a very important thing, so for cost-effective, we have not made any improvements for hot data.But for cold data, I think , compression + RaidNode + cheaper  disk is a feasible way to limit storage cost. , To Jinglong:
I agree completely with you, I just wanted to be sure that any final solution provided a generic solution.  Something that can cleanly separate out classification of hot vs. cold vs really cold data from any extra processing that might happen when data goes from one classification to another.  Access time is a great start, but I can imagine a lot of potential innovation and experimentation in this area.  I can also see lots of different groups wanting to do something when the classification changes.  Like you said, compress the data, possibly move it to a different disk, possibly apply RAID to it.  What ever we do it should be something that is also pluggable. , I agree with you.
To classify cold/hot data and store them in lower cost is a generic issue, and it's always related to application characteristic, so I think we should make strategy pluggable, and provider a default implement., HDFS-2115 had lot smaller scope than the problem being solved here.

While the description of the jira starts off the discussion, there are lot of details to be covered. Some of the questions I am left with is:
# Post compression, the block files have completely different length. The length tracked at NN for the blocks is no longer valid.
# What is the state of the file during compression?
# How do you deal with data that was deemed cold, that could become hot at a later point?
# How does Datanode block scanner and directory scanner, internal datanode data structures that track block length, Append interact with this feature?

Given that, based on the approach taken, this could result in changes to some core parts of HDFS, please write a design document. Alternatively should we look at an external tool that can do this analysis and compress the files, based on HDFS-2115 mechanism proposed by Todd, to minimize the impact to HDFS core code?
, Adding to Suresh's comments, one of the key goals of compression is space reclamation. Given the hdfs has rigid notions of block sizes, compression could leave the filesystem with varied hdfs block sizes and NN has to be aware of the varied block sizes. NN needs to be able to reclaim the storage. 

The other problem is that when data becomes hot again sometime in the future, filesystem needs to have space to store uncompressed version of the block.

Data deduplication is another approach that can be combined with compression to reduce the storage footprint.

, bq. Data deduplication is another approach that can be combined with compression to reduce the storage footprint.

Dedup seems a strategy contrary to the basic rationale of HDFS providing reliable storage. Instead of one missing block corrupting one file, it may impact many, perhaps hundreds, thousands.

, Dedup blocks would be stored in a hdfs filesystem with 3 replicas. In fact, if the deduped block is a "hot" block with lots of references, replica count can be increased for those blocks as a policy setting., bq. Dedup blocks would be stored in a hdfs filesystem with 3 replicas. 

This was implied even so in my comment, obviously.

, Sorry for so late design document and my poor English.And patch will be commit latter. 
, I recall looking for this feature 2-3 years ago while at a large financial and it's just come up again with another large financial client I'm working for right now.

I see I actually already upvoted this jira the last time I looked at it but there has been no movement on this in years.

Having transparent compression on a directory tree would be a very useful feature.

Is there any chance of this being implemented?]