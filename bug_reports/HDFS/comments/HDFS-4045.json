[Exception in SecondaryNameNode logs:

11:58:21.069 PM	ERROR	org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode	
Exception in doCheckpoint
java.io.IOException: Found no edit logs to download on NN since txid 1374
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.downloadCheckpointFiles(SecondaryNameNode.java:361)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:465)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)
	at java.lang.Thread.run(Thread.java:662), The issue is that the GetImageServlet doesn't yet know about generalized journal managers. We need to switch it over to use journal manager APIs to find the edits and stream them out., It took a fair bit of work to generalize the getRemoteEdits and streaming code, but this implementation seems to work OK and passes my initial sniff test. I'm skeptical that streaming the edits from the JournalNode through the NameNode to the SNN is a robust solution both in error-resilience and performance, but I think it's time to post a rough cut and see how it looks., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12550359/hdfs-4045.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3380//console

This message is automatically generated., Build failed in contrib/bkjournal.  Fixed, and I ran a full {{mvn clean test -DskipTests}} locally to ensure I didn't miss anything else.  New diff uploaded., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12550534/hdfs-4045-2.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3390//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12551255/hdfs4045-3.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 5 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3422//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3422//console

This message is automatically generated., {code}
+  @Override
+  public InputStream getInputStream() throws IOException {
+    throw new NotImplementedException();
+  }
+
{code}

We generally don't use this class from commons-lang. Instead, UnsupportedOperationException is probably to be preferred. This is in a few places. Should we also add TODOs here?
----

Tests:

- The two test cases are identical except that one of them has both a local edits dir and a QJM configured, whereas the second has only QJM, right? Instead of duplicating all the code, can you make them both call through to a method with a {{boolean alsoIncludeLocalEdits}} or something? That would make it much easier to tell the difference between the two cases, and cleaner code.

- Also, can you add some assertions that a new checkpoint is actually uploaded by the 2NN to the NN? ie that it successfully checkpointed something instead of being a no-op?

----
Nits:

- A bunch of changes in TestFileJournalManager don't seem to be necessary for this patch: why switch out FileJournalManager for JournalManager?
- Why is the test called TestSnapshotWithQJM? It probably should be TestCheckpointWithQJM -- otherwise it looks related to HDFS Snapshots
- Some indentation is off in testSecondaryCheckpoint
- the new startSecondaryNameNode function in TestNNWithQJM doesn't seem to be used. There are also some new unused imports. In general, please try to review the patch files so that they only make the minimum changes to fix the issue at hand.
- {{getStreamServer}} seems to share most of its code with {{getFileServer}}. Can you refactor them to share code? I'd also pick a different name, like {{copyInputStreamToResponse}} or {{streamServletOutput}} or something since {{get*}} should be reserved for functions that return a value.

----
General thought:
I'm not wild about exposing the underlying input stream via {{getInputStream}}, for a few reasons:

1) It means that we don't get the benefit of the RedundantInputStream stuff that Colin did -- if there's an error in the middle of serving a stream, we won't fail over to another option.
2) It seems like the {{serveEdits}} API that you added to JournalManager overlaps quite a bit with {{getInputStream}}.

Instead, what about adding only {{serveEdits}}, and adding {{getInputStream}} as necessary to the JM-specific subclasses of EditLogInputStream where appropriate? This makes more sense because, for example in the case of QJM, we could have serveEdits issue a 302 redirect to one of the JournalNodes and avoid the 'proxying' through the NN. It also maintains the abstraction of EditLogInputStream only being an iterator for edit log ops, and not a bytewise interface.
, I guess we have to think about whether the overhead of avoiding an extra "deserialize + serialize" pair on the JN is worth violating the EditLogInputStream abstraction.  Unfortunately, I can't think of a good way to get the benefits of {{RedundantEditLogInputStream}} without using its intended interface-- {{readOp}}.

If you do choose to violate the abstraction, it would be best to do so by creating FileInputStream objects for the underlying edit log files, when appropriate.  Then just read directly from the files.  You'll still have a hard time dealing with IOExceptions in the middle of reading the file.  It wouldn't be *too* hard to do that failover yourself, but you run into sticky situations.  What if you get an IOExecption, fail over to the next file, and then discover that it's actually fairly different from the first one?  You might have sent bytes over the wire that completely fail the edit log operation checksums.

I think it's best to punt on the issue of zero-copy {{serveEdits}} for now.  Maybe open up a follow-up JIRA for that.  Just do the simple thing of streaming it for now.  I also wonder whether serveEdits should serve a range of edits, rather than discrete edit log segments?  The API feels very segment-oriented, and I thought we were trying to get away from that when possible., hdfs4045-4.txt: new approach; streams edits from any JM via the NN GetImageServlet., Note for reviewers: I will add the missing javadoc.  Please review the architecture and refactoring in hdfs4045-4 and let me know if you'd like to see a different approach.  Also, there are a few open-coded constructs that I haven't yet found a relevant implementation for, specifically
{code}
+  @Override
+  public void writeRaw(FSEditLogOp op) throws IOException {
+    Checksum checksum = new PureJavaCrc32();
+    checksum.reset();
+    byte [] bytes = op.getRawBytes();
+    checksum.update(bytes, 0, bytes.length);
+    writeBytes(bytes);
+    byte [] b = new byte[4];
+    int sum = (int)checksum.getValue();
+    b[0] = (byte)((sum >> 24) & 0xff);
+    b[1] = (byte)((sum >> 16) & 0xff);
+    b[2] = (byte)((sum >> 8 ) & 0xff);
+    b[3] = (byte)((sum      ) & 0xff);
+    writeBytes(b);
+  }
{code}
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12553876/hdfs4045-4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 4 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3529//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/3529//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3529//console

This message is automatically generated., {{EditLogFileOutputStream#EditLogServletOutputStream}}: I don't think that a servlet output stream "is-a" file output stream.  There's just so many things that the base class does that don't make sense at all in the derived class.    Like what does it mean to preallocate space in a servlet output stream?  What  does it mean to fsync a servlet output stream?  It's meaningless.  I think you'll find that if you create {{EditLogServletOutputStream}} as a top-level class, you will share little, if any, code with {{EditLogFileOutputStream}}.  It's usually better to favor composition over inheritance, and here is a great example of why.

For the benefit of people reading this, we had a short discussion outside of this JIRA where we basically concluded that we needed to expose the raw bytes of the opcodes.  The reasoning was that in some cases, the FSEditLog processes opcodes differently depending on the version of the edit log.  Since {{FSEditLogOp#writeFields}} can only serialize the opcode to the latest version, serialization followed by deserialization would thus change the semantics of the opcode.

The problem with this is that in a lot of cases, "the raw bytes" doesn't have a very well-defined meaning.  What should {{getRawHeader}} return for {{RedundantEditLogInputStream}}?  Just picking the header of the current stream is not really correct.  I think that if we are going to do this, we should at least make sure that the header is the same for every stream managed by the {{RedundantEditLogInputStream}}.  As far as I can see, this should be true anyway., Took a look through the patch. I generally think this approach is better than the first. Here are some more specific comments:

- I agree with Colin above that EditLogServletOutputStream doesn't seems like it should inherit from EditLogFileOutputStream. There's no need to double-buffer in this case, so it really reuses very little of its superclass. Instead, I think you could just operate directly on the ServletOutputStream itself, pushing the raw bytes directly to the output. This means you also wouldn't need the new {{writeRaw(FSEditLogOp)}} API -- that API doesn't make a lot of sense to me either, since it seems kind of like a contradiction in terms.
- Instead of the trick in {{decodeOp()}} where you mark/reset/re-read to get the raw bytes, did you consider adding a {{readRawOp()}} API? I agree conceptually with Colin that the EditLogInputStream is a stream of operations, but if the API were still "give me the raw bytes corresponding to the next op", that doesn't seem to break the abstraction as much as the previously proposed API which had {{getInputStream}} in it.

With the above, serveEdits could be implemented something like:
{code}
os = response.getOutputStream();
byte[] op;
while ((op = es.readOpRaw()) != null) {
  os.write(op, 0, op.length);
}
{code}

(assuming that the "readOpRaw" includes both the txid and the checksum.

Even if the second suggestion above doesn't work out, I think avoiding the EditLogServletOutputStream would make the code a little cleaner as well.

Colin, any thoughts on this?, I think that Todd's idea of simply having a {{readOpRaw}} and writing the resulting bytes to the HTTP[S] response stream directly makes a lot of sense.  That way, you don't have to create a new kind of {{EditLogOutputStream}} at all.

I see that the current change adds a rawBytes field to every {{FSEditLogOp}}, which can be accessed by clients of the {{EditLogInputStream}}.  This seems like a viable alternative approach to {{readOpRaw}}, though it's unfortunate that it adds some overhead to the regular loading path.  I would not make this field protected, though-- perhaps make it private and add an accessor.  This is just a possible idea-- I think you could equally well create {{readOpRaw}}., bq. Instead of the trick in decodeOp() where you mark/reset/re-read to get the raw bytes, did you consider adding a readRawOp() API?

Yes, I did consider that.  The parsing code in FSEditLogOp makes it remarkably difficult to do anything other than parse the Ops using the expected codepath. In particular, the way that each subclass such as SymlinkOp has only implicit knowledge in readFields of how many bytes it consumes, makes it a serious undertaking to refactor., Here's a proposal that might make this a little nicer:
* augment the {{StreamLimiter}} interface with a {{getRemainingBeforeLimit}} call.  This call will return how many bytes are left before the limit hits.
* add a field, {{decodedSize}}, to {{FSEDitLogOp.}}  At the end of {{FSEditLogOp#decodeOp}}, set {{this.decodedSize = limiter.getRemainingBeforeLimit()}}.
* add a {{FSEditLogOp#decodeRawOp}} function which returns a byte array.  It does so by calling readOp in a loop, and noticing the decoded opcode sizes.  Then it just pulls the bytes from the stream itself.

I think this is nicer because it doesn't burden the main opcode loading path with the need to re-read every byte twice or create an array with all the opcode bytes each time.  Calculating the remaining bytes before the limit is really cheap, just a subtraction.  {{decodeRawOp}} still does re-read everything twice, but this only affects that one code path, not all of them.  For extra efficiency, decodeRawOp could just fill an array you pass, rather than creating a lot of garbage all the time.  Allocating a single 1.5 MB array is not a big deal.

Thoughts?, Thanks to Todd and Colin for the suggestions, this is much cleaner.  I'm still looking for pointers on where to go for my Checksum byte-wrangling (now in FSEditLogOp#getRawChecksum)., New patch: Improve handling of checksum in FSEditLogOp and remove vestigal EditsDoubleBuffer#writeRaw., Attaching new patch, -7, which makes the new FSEditLogOp#rawBytes member optional and turns it on only for streams of interest. 
This removes the performance penalty of always allocating a {{byte []}} for every Op, which [~cmccabe] pointed out is a serious problem for namenode startup time on large edit logs., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12556137/hdfs4045-6.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.server.namenode.TestEditLog

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3605//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/3605//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3605//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12556199/hdfs4045-7.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.server.namenode.TestEditLog

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3609//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/3609//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3609//console

This message is automatically generated., New patch, -8.
- Use readFully rather than read to address Findbugs warnings.
- make supportRawBytes final, like isInProgress
- fix a wrapping issue

The failure in TestEditLog is HDFS-4282., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12559784/hdfs4045-8.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal:

                  org.apache.hadoop.hdfs.server.namenode.TestEditLog

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3618//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3618//console

This message is automatically generated., New patch, -9.
- throw IOException in EditLogInputStream#getRawHeader if called on a non-raw stream
- only store rawHeader if supportRawBytes==true
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560678/hdfs4045-9.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3654//console

This message is automatically generated., Mark RedundantEditLogInputStream#getRawHeader as "throws IOException"., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560870/hdfs4045-10.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3658//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3658//console

This message is automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12729959/HDFS-4045.10.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 6ae2a0d |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10719/console |


This message was automatically generated.]