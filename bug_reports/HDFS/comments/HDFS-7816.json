[This appears to be broken after HDFS-7279.  What's happening is the URI is being properly encoded by the client but the datanode is not decoding it properly when it tries to act as a DFS client.

Looks like netty's QueryStringDecoder#path method, despite the javadoc stating it returns a decoded path, is not returning a decoded path from the URI.  It doesn't use a URI object to decode it, rather it just performs substrings on the URI string.  Even if you pass it a URI it uses the raw path, not the decoded path, for the URI and then returns a substring of that as the path.

As a result the datanode ends up using a non-decoded path and we have a path mismatch between what the client requested and what the datanode tries to open on their behalf., Fixed in HDFS-6662., Thanks, Haohui!  Sorry for the duplicate noise, I missed HDFS-6662 when filing this., HDFS-6662 only partially fixes the problem. The decoding is done by calling {{decodeComponent()}}, which decodes "\+" as a space. A "\+" sign in query string should be decoded as a space, but not if in the path.

For proper decoding of path, we can create a {{URI}} object with the encoded string and then call getPath(), which only decodes "%", not "\+".

, Attaching a patch with the updated test case.
The description and the link have also been updated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12699933/HDFS-7816.patch
  against trunk revision 02e7dec.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9629//console

This message is automatically generated., The previous one was a broken patch. Attaching the correct one., My impression is that {{WebHdfsFileSystem}} is passing the wrong url, thus it looks like the fixes should be done in {{WebHdfsFileSystem}}., I don't think we can rely on clients changing the way the URL is encoded, otherwise we break compatibility with older clients.

I think Kihwal's patch will work even with older clients.  My main concern is that we're relying on QueryStringDecoder#path to give us a raw path so URI can decode it properly.  The javadoc for that method says it returns a decoded path, and if that were ever fixed to match the javadoc then we'd end up double-decoding which will break for some paths.  It also seems weird to me that we're using a QueryStringDecoder to obtain parts of the URL that aren't query strings.  I think it would be safer to avoid QueryStringDecoder altogether for the path computation and just pass the original request URI string to the URI constructor for path decoding., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12699945/HDFS-7816.patch
  against trunk revision 3c5ff07.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestFileTruncate
                  org.apache.hadoop.hdfs.tools.TestDFSHAAdminMiniCluster

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9631//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9631//console

This message is automatically generated., It was broken from HDFS-6662...
Patch looks good to me, +1..Hope you manually also verified in webUI... (I thought of give patch,by the time I woke up it was in patch available state :(  )

, Testcase failures are unrelated to this patch.., bq. I don't think we can rely on clients changing the way the URL is encoded, otherwise we break compatibility with older clients.
I think Kihwal's patch will work even with older clients. My main concern is that we're relying on QueryStringDecoder#path to give us a raw path so URI can decode it properly

Speaking about compatibility, note that we need to consider the compatibility of other clients as well. For example, python clients expects WebHDFS server strictly follows URIs encoding scheme, that is, the URI that are sent over the wire strictly follows RFC 3986. This is well-defined. If the WebHDFS client happens to diverge from it, it should be considered as a bug instead of a feature that needs to be backward-compatible., bq. Patch looks good to me, +1

Reviews from non-committer are welcome, but it would be helpful for other people if you clarify this is a non-binding +1 to avoid confusion. Thanks., Thanks a lot for your suggestion,will do same as you told.., bq.  This is well-defined. If the WebHDFS client happens to diverge from it, it should be considered as a bug instead of a feature that needs to be backward-compatible.
This is not the first time this problem was discovered and discussed. The original webhdfs design is flawed, but we cannot ignore the compatibility.  IMO, maintaining compatibility takes precedence over being standard compliant, because things stop working and business are impacted.

I think we should definitely fix webhdfs to be RFC3986 compliant, but it must be done in a way that avoids any breakages due to incompatibility. However, this jira is intended for making things continue to work in the short term, as such I suggest we open a new jira and continue discussion there.

, Filed HDFS-7822., [~wheat9], are you okay with restoring the compatibility here and address the standard-compliance in HDFS-7822?, bq. maintaining compatibility takes precedence over being standard compliant, because things stop working and business are impacted.

IMO this is a bug of implementing the spec instead of a feature. One thing to note is that the scope of the compatibility of WebHDFS is much larger. There are many other investments that rely on the fact that the WebHDFS server is compliant with RFC 3986. Just take a few examples:

* Microsoft .NET SDK For Hadoop (http://hadoopsdk.codeplex.com/wikipage?title=WebHDFS%20Client)
* Python (https://pypi.python.org/pypi/WebHDFS/0.2.0)
* Proxy-based solutions, like Knox and HttpFS
* Reverse proxy standing in front of WebHDFS

This bug prevents all of the above libraries working properly. I think that in this case we also need to consider the use cases of the above libraries as well., The server will still decode per-cent encoded "+" and other special characters. So it won't break clients who are encoding according to the standard. , Take a closer look of the RFC -- the syntax of HTTP URLs are specified in RFC 1738.

{code}
; HTTP

httpurl        = "http://" hostport [ "/" hpath [ "?" search ]]
hpath          = hsegment *[ "/" hsegment ]
hsegment       = *[ uchar | ";" | ":" | "@" | "&" | "=" ]
search         = *[ uchar | ";" | ":" | "@" | "&" | "=" ]

safe           = "$" | "-" | "_" | "." | "+"
extra          = "!" | "*" | "'" | "(" | ")" | ","

reserved       = ";" | "/" | "?" | ":" | "@" | "&" | "="
hex            = digit | "A" | "B" | "C" | "D" | "E" | "F" |
                 "a" | "b" | "c" | "d" | "e" | "f"
escape         = "%" hex hex

unreserved     = alpha | digit | safe | extra
uchar          = unreserved | escape
{code}

So it looks like it is okay to not encode "+". However, the current webhdfs client is still broken as it does not encode the '%' character:

{code}
    final URL url = new URL(getTransportScheme(), nnAddr.getHostName(),
          nnAddr.getPort(), path + '?' + query);

scala> new java.net.URL("http", "localhost", 80, "/+%asdlkf")
res3: java.net.URL = http://localhost:80/+%asdlkf
{code}

So it looks like that we need to fix the client anyway., bq. So it looks like that we need to fix the client anyway.
I am not disputing that. I just want it to be done in a way that does not affect existing users in  HDFS-7822., Use the force, I mean Javadoc, Luke!  It needs to be fixed, but as Kihwal says, we must maintain compatibility.

{code}
The URL class does not itself encode or decode any URL components according to the escaping mechanism defined in RFC2396. It is the responsibility of the caller to encode any fields, which need to be escaped prior to calling URL, and also to decode any escaped fields, that are returned from URL. Furthermore, because URL has no knowledge of URL escaping, it does not recognise equivalence between the encoded or decoded form of the same URL. For example, the two URLs:

    http://foo.com/hello world/ and http://foo.com/hello%20world

would be considered not equal to each other.
Note, the java.net.URI class does perform escaping of its component fields in certain circumstances. The recommended way to manage the encoding and decoding of URLs is to use java.net.URI, and to convert between these two classes using toURI() and URI.toURL().
{code}, The URI RFC does not specify whether to encode the path but the RFC of HTTP URL does (RFC 1738). What it means is that HTTP proxies (e.g., Apache / Nginx / Knox) can reject webhdfs URLs like "/webhdfs/v1/asdf%asdklf".

I have no problems on compatibility, but the changes must allow WebHdfs requests to survive over HTTP proxies. Any ideas?, I may be being completely naive here, but how about we add a parameter or something which specifies RFCCompatibility and turn that off by default for older clients?, Discussed with [~vinodkv] offline. we can go back to the old behavior in branch-2 and fix it in trunk. Thoughts?, {code}
+      throws IOException {
+    URI uri;
+    try {
+      uri = new URI(decoder.path());
+    } catch (java.net.URISyntaxException e) {
+      throw new IOException("Invalid path:", e);
+    }
{code}

For GC reasons it might make more sense to (1) take {{QueryStringDecoder.decodeComponent()}} and make some tweaks based on it, and it is okay to throw {{IllegalArgumentException}} directly (which will be translated to 400)., Bump. [~daryn] / [~kihwal] / [~wheat9], can we get this going for 2.7? Tx., [~daryn] / [~kihwal] do you have time to take a look at this? I can provide a patch if you guys don't have time., [~wheat9] I would appreciate if you can come up with a patch., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12704261/HDFS-7816.002.patch
  against trunk revision bf3275d.

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9902//console

This message is automatically generated., This jira was a test subject for fixing the precommit build. Deleted whole bunch of jenkins postings., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12704261/HDFS-7816.002.patch
  against trunk revision a89b087.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA
                  org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager
                  org.apache.hadoop.hdfs.server.balancer.TestBalancer

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9930//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9930//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12704261/HDFS-7816.002.patch
  against trunk revision 5b322c6.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestCrcCorruption

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9954//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9954//console

This message is automatically generated., +1 lgtm. The test failures don't seem related., I've committed this to trunk, branch-2 and branch-2.7. Thanks for the fix, [~wheat9]., FAILURE: Integrated in Hadoop-trunk-Commit #7372 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7372/])
HDFS-7816. Unable to open webhdfs paths with "+". Contributed by Haohui Mai (kihwal: rev e79be0ee123d05104eb34eb854afcf9fa78baef2)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #138 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/138/])
HDFS-7816. Unable to open webhdfs paths with "+". Contributed by Haohui Mai (kihwal: rev e79be0ee123d05104eb34eb854afcf9fa78baef2)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #872 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/872/])
HDFS-7816. Unable to open webhdfs paths with "+". Contributed by Haohui Mai (kihwal: rev e79be0ee123d05104eb34eb854afcf9fa78baef2)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2070 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2070/])
HDFS-7816. Unable to open webhdfs paths with "+". Contributed by Haohui Mai (kihwal: rev e79be0ee123d05104eb34eb854afcf9fa78baef2)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #129 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/129/])
HDFS-7816. Unable to open webhdfs paths with "+". Contributed by Haohui Mai (kihwal: rev e79be0ee123d05104eb34eb854afcf9fa78baef2)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #138 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/138/])
HDFS-7816. Unable to open webhdfs paths with "+". Contributed by Haohui Mai (kihwal: rev e79be0ee123d05104eb34eb854afcf9fa78baef2)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2088 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2088/])
HDFS-7816. Unable to open webhdfs paths with "+". Contributed by Haohui Mai (kihwal: rev e79be0ee123d05104eb34eb854afcf9fa78baef2)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java
, Tx for the 2.7 push, [~wheat9] and [~kihwal]!]