[Attaching a patch which solves the current issue. 

Please review and suggest if any improvements needs to be done. Thanks, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12610312/HDFS-5428.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5277//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5277//console

This message is automatically generated., Attaching the patch for the following

*Scenario 1:*
Stores the complete snapshot path in the leases whenever a dir/file which is having a snapshot is deleted.
 ex:
1. /foo/bar is a dir containing /foo/bar/f1 and /foo/bar/f2 which is having a snapshot /foo/.snaphost/s1
2. Now if /foo/bar is deleted, then there will be two leases (/foo/.snapshot/s1/bar/f1 and /foo/.snapshot/s1/bar/f2)  added with the holder "HDFS_snapshot" to leaseManager, these leases will be present till the snapshot is deleted. Will not be considered for lease recovery.
3. Now on checkpoint, these leases also will be stored as underconstruction files with snapshot path.
4. These INodes will be reloaded as under construction files replacing the last block as underconstruction. 
5. While considering the namenode safemode threshold these underconstruction blocks will be excluded.
6. NameNode startup will be success.

*Scenario 2:*
 Renaming a file/dir inside a snapshot will also be maintained using leases.
ex: 
1. /foo/bar is a dir containing /foo/bar/f1 and /foo/bar/f2 which is having a snapshot /foo/.snaphost/s1
  2. Now /foo/bar is renamed to /foo/bar-renamed
    3. then two leases will be added with snapshot paths.
    4. Again while checking pointing these will be written
    5. While counting for the namenode threshold if there are two leases for the same file then only the original file lease will be considered and threshold will be correct

*Scenario 3:*
Deleting a snapshot of which contains a file with multiple snapshots.
ex: 
1. /foo/bar is a dir containing /foo/bar/f1 and /foo/bar/f2 which is having two snapshots /foo/.snaphost/s1 and /foo/.snaphost/s2
2. Now if the /foo/bar is deleted then leases will be created with latest snapshot paths. (/foo/.snapshot/s2/bar/f1 and /foo/.snapshot/s2/bar/f2)
3. After this if the latest snapshot (s2) is deleted, then leases will be replaced with prior snapshot path for those files which are present in both these snapshots


Please review, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12611933/HDFS-5428-v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5327//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5327//console

This message is automatically generated., Continue the discussion in HDFS-5443 here..

So HDFS-5428.000.patch is a simple patch that implements similar idea mentioned in HDFS-5443: 
1) Record extra information in fsimage to indicate INodeFileUC that are only in snapshots. To keep the compatibility we keep the information in the "under-construction-files" section in fsimage, and just use ".snapshot" as their paths.
2) Identify these snapshot files while loading fsimage, and temporarily store them in a map in SnapshotManager.
3) When calculating total block number when starting NN, besides the files recorded in the lease map, also deduct the number of files recorded in 2).

In general the idea is very similar to Vinay's patch. The difference is that we do not keep and maintain records in the lease map and only handle these files when starting the NN. We can even clear the records in SnapshotManager after computing the total number of blocks.

One more thing we may need to handle is that if we remove the 0-sized blocks (HDFS-5443), it is possible that we can have an under-construction file in snapshot while there is no corresponding blockUC for the file. In that case we should not record extra information in fsimage for this kind of INodeFileUC. 

The current patch is just for demonstration. It can pass the new unit tests in Vinay's patch. If folks think the general idea is ok, we can continue our work based on this patch.
, Continue the discussion ( HDFS-5443) here.
As we discussed yesterday to verify this patch for (HDFS-5443).With this patch the issue is still reproducing i..e After restart NN is going to safemode.I am not sure where the flow is missing., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12612477/HDFS-5428.000.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5348//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5348//console

This message is automatically generated., Hi Jing, thanks for posting the simplified patch.

Patch looks quite good, making all unit test in my patch pass.

Small improvements required to satisfy below points as well.
bq. (From issue Description) So when the Datanode reports RBW blocks those will not be updated in blocksmap. Some of the FINALIZED blocks will be marked as corrupt due to length mismatch.
This problem is still there, because while loading the fsimage, snapshot inodes are not replaced with an UCInode and last block is COMPLETE. In this case after reloading from fsimage we will not be able to read the last block. 
Replacing such inodes with UCInode is required.
, {quote}
But I am little uncomfortable for managing leases for snapshotted files as they are readonly files, no need of leases. If all others ok on that point, I will not object.
{quote}

After this point ,Uma and me discussed the same points what Jing has mentioned in the HDFS-5428-000.patch.
It is better way to maintain the leases regarding the snapshot files in snapshot manager,As the responsibility of lease manager is to maintain the leases for open files for write.with the current implementation snapshots are read only,so there is no need to maintain the leases for snapshotted files in lease manager.so it is better to maintain the leases regarding the snapshotted files in snapshot manager.

+1 patch looks good
I will verify this patch in my env once.




































, I think, current updated patch HDFS-5428.000.patch can solve HDFS-5443. I mean NN will exit from safemode even without removing the 0-sized blocks.
But removing 0-sized blocks will be an added advantage., I tried to update the patch according to my previous comment.
But to replace the exact inode we need to have the full snapshot path. in the current case since the full snapshot path is not tracked anywhere we cannot replace the INode.
Need a way to track the full path of the snapshot INode and replace the INode with INodeFileUC., I just tried the following to read the file after restart. It failed with BlockMissingException
{code}
  @Test
  public void testWithCheckpoint() throws Exception {
    Path path = new Path("/test");
    doWriteAndAbort(fs, path);
    fs.delete(new Path("/test/test"), true);
    NameNode nameNode = cluster.getNameNode();
    NameNodeAdapter.enterSafeMode(nameNode, false);
    NameNodeAdapter.saveNamespace(nameNode);
    NameNodeAdapter.leaveSafeMode(nameNode);
    cluster.restartNameNode(true);
    // read snapshot file after restart
    String test2snapshotPath = Snapshot.getSnapshotPath(path.toString(),
        "s1/test/test2");
    DFSTestUtil.readFile(fs, new Path(test2snapshotPath));
    String test3snapshotPath = Snapshot.getSnapshotPath(path.toString(),
        "s1/test/test3");
    DFSTestUtil.readFile(fs, new Path(test3snapshotPath));
  }{code}, vinay as i observed when debugging the scenario along with your patch,
There is some path mismatch,when counting the blocks of snapshotfile under construction,due to this it is not removing that blocks from block threshold.
{code}
String fileSnapshotPath = StringUtils.replaceOnce(
          file,
          snapshottableDir,
          Snapshot.getSnapshotPath(snapshottableDir,
              Snapshot.getSnapshotName(snapshot)));
{code}
String util is not replacing the correct path.
logs for this 
2013-11-07 01:05:15,103 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.RuntimeException: java.io.FileNotFoundException: File does not exist: /.snapshot/snap_6ran/_temporary/0/_temporary/attempt_local1866843415_0001_m_000000_0/part-m-00000	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startCommonServices
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getCompleteBlocksTotal(FSNamesystem.java:5068)
(FSNamesystem.java:853)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:540)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:482)
, Hi [~sathish.gurram] , You are right. replacement is wrong if the snapshottable dir is "/". I will update the patch if necessary. ;), bq. But to replace the exact inode we need to have the full snapshot path. in the current case since the full snapshot path is not tracked anywhere we cannot replace the INode.

Yeah, in our current implementation it's hard (sometime impossible) to get the full path for a given snapshot inode. Thus it will be hard to replace the whole INodeFile. So here my question is whether it's possible that we just replace the last block of the snapshot INode with a BlockInfoUC (but without replacing the INodeFile with an INodeFileUC)?
, Upload a new patch that replaces the block but without replacing the inodefile. , bq. So here my question is whether it's possible that we just replace the last block of the snapshot INode with a BlockInfoUC (but without replacing the INodeFile with an INodeFileUC)?
If we replace the problem is, if the same INode is referring to a completed file [  might be due to rename and leaserecovery ] in normal path and replacing a last block in this INode may not be correct.

And one more problem here is the snapshotUCMap will not always contains the latest snapshot inode which will be written to fsmage as underconstruction file.
for ex:
    1. when the file is being written, after allocating block b1, take snapshot "s1"
    2. File is renamed.
    3. Now the file is closed by lease recovery. and appended again one more block b2, and before closing one more snapshot is taken "s2"
    4. and finally file is deleted.
    5. Now while writing the inode tree to fsimage, inode in s2 comes first and then s1 , then only INode in s1 will be marked as underconstruction. but actual underconstruction is INode in S2 snapshot, bq. if the same INode is referring to a completed file [ might be due to rename and leaserecovery ] in normal path 

We will replace the whole Inode if it is in normal path. We only replace its last block if the file is only in snapshot. But next time when we do the checkpoint again, we may need to check a file's last block to decide whether it's a fileUC.

Another option here is that we replace the inode for all the cases. To cover the challenge that we cannot get the full snapshot path, we can use the inode id to get the inode first, then scan the diff list of its parent to do the replacement. This will be inefficient but might be ok in case that we do not have a lot of snapshots and inodeUC.

bq. Now while writing the inode tree to fsimage, inode in s2 comes first and then s1 , then only INode in s1 will be marked as underconstruction. but actual underconstruction is INode in S2 snapshot

For rename, we will only have one INode here, which is referenced by two INodeReference instances stored in s1 and s2. And since we only record inode id in snapshotUCMap, this scenario might be fine?, bq. We will replace the whole Inode if it is in normal path. 
Here we will replace whole Inode only if its underconstruction. What if the same file is closed and present in some other path.?
bq.  Another option here is that we replace the inode for all the cases. To cover the challenge that we cannot get the full snapshot path, we can use the inode id to get the inode first, then scan the diff list of its parent to do the replacement. This will be inefficient but might be ok in case that we do not have a lot of snapshots and inodeUC.
To what level of scanning we can do..? And how we can find out the all previous locations of the inode. same INode might be renamed to different locations in snapshot

bq. For rename, we will only have one INode here, which is referenced by two INodeReference instances stored in s1 and s2. And since we only record inode id in snapshotUCMap, this scenario might be fine?
I am not sure about this. As far as I have seen while debugging if there is any modification done (such as adding one more block) on snapshotted node, a new inode instance will be saved inside snaphot diffs, not the INodeReference. INodeReference  will be used only if there is no modification between two inodes attributes other than name. 
Actually I got this point, because I have already faced these problems while preparing my patch. , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12612548/HDFS-5428.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5353//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5353//console

This message is automatically generated., From HDFS-5443:
bq. Patch will not clear the blocks in this case.

So I checked the rename case. Looks like we have a bug there and we fail to clean the blocks for INodeFile/INodeFileUnderConstruction in some cases after rename. I will fix it in a new jira. , Created HDFS-5476 for the rename issue., One more case needs to be handled. 
1. Renaming of file inside snapshot
2. Checkpoint
3. restart

Changes needs to be done : 
in {{INodeDirectoryWithSnapshot.ChildrenDiff.replace(ListType, INode, INode)}} 
precondition check should removed after replace in below part of the code.
{code}      final INode removed = list.set(i, newChild);
      Preconditions.checkState(removed == oldChild);{code} 
Old node will INodeFile instance and new node will be INodeFileUnderConstruction instance.
, Vinay, I think this precondition issue with rename was filed by Sathish also sometime back. HDFS-5425. Is that issue same as you are also seeing?, bq. One more case needs to be handled. 

Vinay, could you please provide more detailed steps to reproduce this (e.g., a unit test)? I think we can try to fix this in HDFS-5425., bq. Vinay, I think this precondition issue with rename was filed by Sathish also sometime back. HDFS-5425. Is that issue same as you are also seeing?
Yes Uma and Jing,
This issue is same as HDFS-5425. I will post a patch with a unit test soon in HDFS-5425. Thanks for pointing me to that jira., Since HDFS-5425 is committed, this latest patch needs re-base on trunk.

I verified the rename+append scenario mentioned in previous comments. I found no issues with that.

+1, from my side once the patch is rebased.
, Update the patch. I tried to solve the following issue in the patch:

bq. (From issue Description) So when the Datanode reports RBW blocks those will not be updated in blocksmap. Some of the FINALIZED blocks will be marked as corrupt due to length mismatch.

The current solution is to scan the parent's diff list to replace INodeFile with INodeFileUC when loading under-construction files from fsimage. This is a temporary solution, which can be removed when we have a chance to bump the layout version and change the fsimage format.

Let's see if the patch can pass Jenkins., Patch looks pretty good. 
Anyway I did not observe the mentioned issue in previous patch also. Need to check more.

Here are some small nits

1. BlockInfoUnderConstruction could be replaced with another simple constructor with only 2 parameters
{noformat}
+              BlockInfo lastBlk = blocks[blocks.length - 1]; 
+              blocks[blocks.length - 1] = new BlockInfoUnderConstruction(
+                  lastBlk, replication, BlockUCState.UNDER_CONSTRUCTION, null);
+            }{noformat}


2. changes in FSImageSerialization.java are only whitespace and format, unrelated to this patch.  I think it can be removed from current patch
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12613809/HDFS-5428.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestRenameWithSnapshots

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5433//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5433//console

This message is automatically generated., Test failures are related to this line addition
{noformat}          // replace oldnode with cons
          parentRef.setReferredINode(cons);{noformat}

Is this required.?, Thanks Vinay for the review! Update the patch to address the comments.

bq. parentRef.setReferredINode(cons);

You're right. I should call this only for the snapshot case., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12613937/HDFS-5428.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestCrcCorruption

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5437//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5437//console

This message is automatically generated., +1, latest patch looks good. 
test failure looks unrelated., - In loadINode(..), we should also check if lastBlk.getNumBytes() < blockSize.  If it is a full block, we should not convert it to BlockInfoUnderConstruction.

- How about using inode id path (/.reserved/.inodes/<inodeid>) instead of .snapshot?

- There are some white space changes in SnapshotManager and other places.
, Thanks for the review, Nicholas!

bq. In loadINode(..), we should also check if lastBlk.getNumBytes() < blockSize. If it is a full block, we should not convert it to BlockInfoUnderConstruction.

I also thought about it. So here is it possible that the client just writes a full block and call the sync(update-length), but has not tried to get an additional block or close the file? In that case, we will have a full-size block which is under construction? If we treat this block as a complete block and write it into fsimage, later when the NN restarts and receives a block report from DN, we may miss the special process added in HDFS-5283 in BlockManager#processFirstBlockReport. And this may cause NN to stay in SafeMode. 
, Update the patch to address Nicholas's comments., +1 patch looks good., +1 patch looks good. Thanks Jing, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12614513/HDFS-5428.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5471//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5471//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12614513/HDFS-5428.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5477//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5477//console

This message is automatically generated., Thanks for the review, Nicholas and Vinay! I will commit the patch shortly., I've committed this to trunk and branch-2., Thanks Jing for the patch., SUCCESS: Integrated in Hadoop-trunk-Commit #4758 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4758/])
HDFS-5428. Under construction files deletion after snapshot+checkpoint+nn restart leads nn safemode. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1543329)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeDirectoryWithSnapshot.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #396 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/396/])
HDFS-5428. Under construction files deletion after snapshot+checkpoint+nn restart leads nn safemode. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1543329)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeDirectoryWithSnapshot.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1613 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1613/])
HDFS-5428. Under construction files deletion after snapshot+checkpoint+nn restart leads nn safemode. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1543329)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeDirectoryWithSnapshot.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1587 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1587/])
HDFS-5428. Under construction files deletion after snapshot+checkpoint+nn restart leads nn safemode. Contributed by Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1543329)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/INodeDirectoryWithSnapshot.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java
, SUCCESS: Integrated in Hadoop-trunk-Commit #4859 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4859/])
Move HDFS-5257,HDFS-5427,HDFS-5443,HDFS-5476,HDFS-5425,HDFS-5474,HDFS-5504,HDFS-5428 into branch-2.3 section. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1550011)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, I've merged this to branch-2.3, FAILURE: Integrated in Hadoop-Yarn-trunk #418 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/418/])
Move HDFS-5257,HDFS-5427,HDFS-5443,HDFS-5476,HDFS-5425,HDFS-5474,HDFS-5504,HDFS-5428 into branch-2.3 section. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1550011)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1609 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1609/])
Move HDFS-5257,HDFS-5427,HDFS-5443,HDFS-5476,HDFS-5425,HDFS-5474,HDFS-5504,HDFS-5428 into branch-2.3 section. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1550011)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1635 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1635/])
Move HDFS-5257,HDFS-5427,HDFS-5443,HDFS-5476,HDFS-5425,HDFS-5474,HDFS-5504,HDFS-5428 into branch-2.3 section. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1550011)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
]