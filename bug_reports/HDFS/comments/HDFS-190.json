[> Consider splitting the constructor from the start() operation.
I think this can be done with simple changes. An option is to split {{startDataNode()}} into {{initDataNode()}} and {{startDataNode()}} that starts the threads. The constructor would call initDataNode, and makeInstance() would invoke startDataNode() after constructing a DataNode. , Having done a bit more coding, its worth noting I've just done what I shouldnt -subclassed DataNode and NameNode in the org.apache.hadoop.dfs package itself, in order to

-get at the package scoped constructor of DataNode
-get at some internal package-scoped fields
-add the synchronisation I want around shutdown
-run DataNode in my own thread
-add the ability to ping() either node type and check their health. For NameNode, that means getting at the filesystem and checking it is running, for a datanode, the only check I can patch in is that the worker thread is running.

This is all pretty nasty stuff, and does need fixing...this bug lays down what is wrong. What I don't yet know is what is the right thing to do. Give me a few more weeks to experiment, then I will probably propose something like

-a base lifecycle interface for server-side nodes that includes start(),stop() and ping() methods
-a base thread class that catches and caches exceptions thrown during their operation
-retrofit of this into NameNode,DataNode and others.


, We're now in a situation where the current codebase uses subclassing of {{DataNode}} for some tests.  There has been no activity on this issue for many years, so it looks unlikely that it would be implemented.  I'm closing it as won't fix., you are right...but we should really move the DNs to the yarn service model (some other JIRA of mine there) and only start threads in the startService() call, as it is the DN starts threads in its ctor, which totally makes a mess of any subclass overriding methods those threads call., Yes, the YARN-style service model is a good idea., ...it was deliberately put into hadoop-common for this reason]