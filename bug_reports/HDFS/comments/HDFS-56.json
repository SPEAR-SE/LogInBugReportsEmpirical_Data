[Does the current behavior actually cause any problems?  Or is the message just disconcerting?, Assuming default replication is 3, setting mapred.submit.replication to 2 should not result in this exception. Relevant log from namenode from original poster would be helpful.

The problem that Hairong described hasn't cause real failures yet as far as I know. It only slows down increasing replication.. not sure by how much., Until I saw the explanation above, it was disconcerting as it looked like there was something wrong with the DFS, especially since the log message is an ERROR. If it was an INFO, I would have assumed it was mostly harmless., This is from Datanode log. As far as datanode is concerned, it is an error. Agreed, it is confusing to the user. We should fix this issue., We can try to gather a (new) log but if I remember even the unit test had the exception. (see the thread on dev) With my still limited knowledge of the codebase and comparing to what is going on the cluster it seems it does not really cause huge problems. So it was mainly disconcerting. But - at some stage we have had so many of these exception in the logs that they were essentially spamming the logs. (More than hundred exceptions per minute!) ...which made it a bit more than just disconcerting. Maybe the word "panic" was more suited ;) At least OPS stopped believing this is not a problem and doesn't add to a good reputation of hadoop. It also make you lose the eye for real problems as it just drowns them in a see of information.

So yeah ...we should really fix this., > We can try to gather a (new) log but if I remember even the unit test had the exception. 

The unit test failure was for a different reason. If there are many of this kind, we can fix that too. The proposed fix in this jira should fix the most commonly observed cause of these exceptions.
, This looks like a recent addition. Could you guys post with which versions of hadoop you can/started to see that behavior
so that we could track what could have caused it., We have been seen this in 0.10 and are still seeing it in 0.14, Hi Torsten, how many datanodes do you have in your cluster? The reason I asm asking this question is because the probability of these exception occuring increases if the cluster size is very small (or if only a very small number of nodes have avilable disk space), I am running a 2-node cluster and see this exception all the time.
Here is the log from the name-node that explains the behavior:

{code}
07/09/18 12:08:05 INFO dfs.StateChange: BLOCK* NameSystem.allocateBlock: /Dir0/file20. blk_-4934058791921230875 is created and added to pendingCreates and pendingCreateBlocks
07/09/18 12:08:06 INFO dfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: node.x.x.111:50077 is added to blk_-4934058791921230875
07/09/18 12:08:08 INFO dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask node.x.x.111:50077 to replicate blk_-4934058791921230875 to datanode(s) node.x.x.222:50017
07/09/18 12:08:09 INFO dfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: node.x.x.222:50017 is added to blk_-4934058791921230875
{code}

What we see here is a race condition between blockReceived()  and sendHeartbeat().
The client writes 2 block replicas to 2 data-nodes. When finished writing replicas to their disks the data-nodes send blockReceived() to the name-node.
The following sequence of events causes the exception:

# node1.blockReceived(): the block is added to the list of under-replicated blocks, since it is supposed to have replication 2;
# node1.sendHeartbeat(): node1 starts replicating block to node2.
   In respond node2 throws "block is valid, and cannot be written to" exception.
# node2.blockReceived(): everything goes back to normal.

On a 2-node cluster there is always one choice where the other replica can be placed. That is why the exception is inevitable if replication is requested.
On a bigger clusters the exception is rather rare, because replication most probably will be scheduled to a data-node that does not contain the block.
Which makes it even worse, because the actual transfer happens, the block becomes over-replicated so that now one of the replicas needs to be removed.

I think this decreases the overall performance of the cluster  - unnecessary transfers of large blocks can be costly.
, HADOOP-1946 is the main reason for that behavior.
The data-nodes were very slow so that blockReceived()  were in fact coming late after the file had been closed.
Do we still want to remove the exception mentioned in the initial description?
, I feel that fundamentally namenode does not handle blockReceived well. HADOOP-1946 simply "amplied" the problem. BlockReceived should not trigger re-replication. We should put block placement decisions at file creation time in the PendingReplicationBlocks queue as well. ]