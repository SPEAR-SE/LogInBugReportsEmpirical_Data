[We had a situation where the data node could send packets out, but packets coming in were dropped.  So while jobs were unable to write blocks to that data node, the name node thought it was perfectly healthy.  There should be some sort of 'double check'  to make sure the data node is telling the truth about its health status., (FWIW, I haven't had a chance to see if this issue impacts the job/task trackers, so in HDFS this bug goes for now...), Do you have a proposal on how this can be detected? What would be the scenario when there is a network partition whereby some cluster nodes can write to a datanode while many other cluster nodes cannot write to this datanode, would you still want it to be flagged?, Just to put what I suspect is the basic concern to rest :), I don't want a double check on every block report/heartbeat.   But I think it might be useful if the name node attempted to connect to the data node over a long period of time [probably another configurable :( ].  

I'm trying to think of a use case where it would be beneficial/useful if data node/name node had one way communication and coming up empty.

As to the network partitioning problem (where data nodes lose connectivity to each other, but name node still has connectivity), it may be worth while to have an algorithm such that if x% percent cannot communicate, then we enter safe mode.  From a practical perspective, chances are good the job tracker is going to go down in flames in those sorts of situations anyway since the tasktrackers should end up on the dead pile.  Even in a pure HDFS setup, at some point the replication list is going to get very large if we start declaring nodes dead based upon %... so probably better off to just safemode ourselves and alert the admin that the network is horked.]