[Observed this on 1.0.3 as well. , The issue is that the NN passes the rpc listener address (which may be the wildcard) directly to NAMENODE_ADDRESS_ATTRIBUTE_KEY (name.node.address). This value is then used to populate the nnaddr parameter which is used in the web UIs and servlets. name.node.address is also used in some cases to get the UGI when nnaddr is not passed in the URL and we're not running in the NN context, eg DN jsp when security is enabled.

Seems like the simplest and most compatible near term fix is to have the NN HTTP server set name.node.address to one of the IPs serving RPCs rather than the wildcard. We could make a self rpc and ask for the remote address or just resolve the IP of the NN hostname and assume there's an RPC server listening there. Tokens may use a different IP, or use the hostname if hostname based tokens are enable, though I think these already broken when the NN RPC servers bind to the wildcard because we'll currently be doing UGI lookups using the wildcard IP.

A perhaps better but more involved approach would be to:
# Separate out the "name.node.address" usage so it's only used when necessary (outside NN context, in NN context we can always use the "name.node" attribute to get the NN object directly)
# In the cases where name.node.address is used outside the NN, think of these like clients and use the fs.defaultFs authority to determine the address of the NN. Unlike the NN (which will clobber fs.defaultFS with the wildcard) they can use it to determine the IP of the NN to talk to. This should work with security since we'll use the same IP clients would use.
, This is a simple patch which just uses the first available local hostname in lieu of 0.0.0.0.

I tried the RPC-based approach, but it didn't quite work out, because the {{NameNodeHttpServer}} needs to be initialized before the RPC stuff is quite prepared.

The weakness of this approach is that it doesn't distinguish between external and internal hostnames.  However, this should not be a problem in most scenarios.  I think the full solution, which we should do later, is the refactoring Eli proposed in HDFS-3946., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12545636/HDFS-3932.001.patch
  against trunk revision .

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3204//console

This message is automatically generated., rebase on trunk, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12545637/HDFS-3932.002.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3205//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3205//console

This message is automatically generated., {code}
+  /**
+   * Determine the name.node.address.
+   *
+   * @return               the name.node.address to use
+   * @throws IOException
+   */
+  @VisibleForTesting
+  public static InetSocketAddress determineHttpAddress(Configuration conf)
+      throws IOException {
+    InetSocketAddress nnAddress = NameNode.getServiceAddress(conf, true);
+    return new InetSocketAddress(InetAddress.getLocalHost().getHostName(),
+        nnAddress.getPort());
+  }
{code}

We should only do this if the HTTP bind address is configured to a wildcard. Otherwise, we should use the bind address. Keep in mind that this isn't necessarily the same as the service address. Also, you're using the port from the service address, which is an IPC server, not HTTP.
, * use NameNodeHttpServer#bindAddress, unless it's 0.0.0.0

* use port from bindAddress (i.e., http port), not RPC port, when constructing new http address/port pair., Looks good to me. I'll wait on Eli to take a quick look tomorrow morning before commit, though, since he understands the issue better than I., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12545664/HDFS-3932.003.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner
                  org.apache.hadoop.hdfs.web.TestWebHDFS

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3208//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3208//console

This message is automatically generated., - determineHttpAddress would be better named something like getNameNodeRpcAddressForHttpServer since we're getting the NN RPC address not the HTTP address (which is a different port)
- Did you consider resolving the hostname to an IP rather than passing the hostname verbatim, eg use getHostAddress instead of getHostName or resolve the hostname returned here if getHostAddress is not equivalent?  If we pass the hostname do we need to URL encode it?
- In the javadoc would replace of "the name.node.address to use" with something more descriptive, eg 
{noformat}
Determine the NameNode RPC address to pass in the name.node.address Http server attribute
for use by the Web UI and servlets. If the NameNode has bound the RPC server to the
wildcard then we use the hostname (which we assume has an active RPC server).
{noformat}
- The test should also check for the loopback IP for sanity  
- Test the broken web UI links and hftp (eg using distcp) on a pseudo cluster that binds the rpc addr to the wildcard?
 , From wikipedia:

bq. The Internet standards (Request for Comments) for protocols mandate that component hostname labels may contain only the ASCII letters 'a' through 'z' (in a case-insensitive manner), the digits '0' through '9', and the hyphen ('-'). The original specification of hostnames in RFC 952, mandated that labels could not start with a digit or with a hyphen, and must not end with a hyphen. However, a subsequent specification (RFC 1123) permitted hostname labels to start with digits. No other symbols, punctuation characters, or white space are permitted.

So no URL encoding is necessary.

More documentation sounds like a good idea; I'll add that.

bq. The test should also check for the loopback IP for sanity

OK., bq. The test should also check for the loopback IP for sanity

Actually, I don't think that's possible with this approach.  Some people configure their computer to have a hostname simply by adding an entry like this to {{/etc/hosts}}:

{code}
127.0.0.1       keter localhost
{code}

If there is no DNS server configured with a non-loopback address, then {{/etc/hosts}} really is the only authority, and {{127.0.0.1}} really is the only IP address bound to that hostname.  I think a lot of developer boxes are set up this way.

On the other hand, if there is a real DNS setup, there should be some way of iterating through the IP addresses associated with our hostname.  I'll see if I can find some way of ensuring we use the non-loopback IP if it's available., * Rename {{NameNodeHttpServer#determineHttpAddress}} to {{NameNodeHttpServer#getNameNodeRpcAddressForHttpServer}}

* Add longer JavaDoc

* search through all IP addresses associated with the hostname, trying to find a non-localhost one (this may not exist, though.), +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12545799/HDFS-3932.005.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3215//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3215//console

This message is automatically generated., I'm doing some testing with the web UI, - Rather than loop through IPs I think it would be better to use DNS#getDefaultIP("default"). This may return the loopback IP eg on a developer local install but that's OK because that would be a valid IP to use on a developer local install anway. IIUC on real clusters we require this not be the loopback. We should add a wrapper so you can just call DNS#getDefaultIP().
- The javadoc doesn't match the implementation (it says use the hostname)
- The test should check all the cases the code checks (multicast, link-local and loopback here, but see above about not checking loopback), Latest patch uses DNS#getDefaultIP("default"), and updates the Javadoc.

I dropped the testing for multicast stuff since I don't think that it matters here-- we're strictly fixing the 0.0.0.0 issue here., +1 pending jenkins. I sanity tested using the Web UI and distcp using hftp on a system with the rpc addr bound to the wildcard., +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12545839/HDFS-3932.006.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3218//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3218//console

This message is automatically generated., {{NetUtils.getConnectAddress(nnAddr)}} is a better approach since it already has the necessary logic., NetUtils#getConnectAddress claims it returns the loopback when the wildcard is passed, but that's not what we want here. We only want the loopback on dev machines when the hostname doesn't map to a non-loopback IP. However perhaps the javadoc is wrong because looking at the code I don't think it always returns the loopback when the wildcard is passed, so I think you're right. Colin, want to confirm and update the patch?, * Use {{NetUtils.getConnectAddress}}

* Update JavaDoc for {{NetUtils.getConnectAddress}}

Testing: I ran the unit test.  I also deployed this on a machine with a hostname that could be looked up (did not only resolve to 127.0.0.1), and observed that the external IP address got used to make URLs in the web UI.

(Just FYI, it still displays 0.0.0.0 in some places... but that is purely cosmetic I believe.  It will be fixed with HDFS-3946.), -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12545937/HDFS-3932.007.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3220//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3220//console

This message is automatically generated., getNameNodeRpcAddressForHttpServer can be replaced entirely with getConnectAddress right since it already checks isAnyLocalAddress?, Yes, the javadoc is wrong.  If the given address is the wildcard address, it tries to use the address of the first active interface.  If there are no active interfaces, it will use the loopback address., This latest patch replaces getNameNodeRpcAddressForHttpServer entirely with getConnectAddress.

testing: I ran this patch on a server with correct DNS setup, and observed that the nnaddr was no longer 0.0.0.0 but instead a routable IP when looking at "Live Nodes", -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12545985/HDFS-3932.008.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3221//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3221//console

This message is automatically generated., +1 to the latest patch. Thanks for the suggestion Daryn., I've committed this and merged to branch-2. Thanks Colin!, Integrated in Hadoop-Common-trunk-Commit #2749 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2749/])
    HDFS-3932. NameNode Web UI broken if the rpc-address is set to the wildcard. Contributed by Colin Patrick McCabe (Revision 1388321)

     Result = SUCCESS
eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1388321
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java
, Integrated in Hadoop-Hdfs-trunk-Commit #2812 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2812/])
    HDFS-3932. NameNode Web UI broken if the rpc-address is set to the wildcard. Contributed by Colin Patrick McCabe (Revision 1388321)

     Result = SUCCESS
eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1388321
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java
, Integrated in Hadoop-Mapreduce-trunk-Commit #2772 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2772/])
    HDFS-3932. NameNode Web UI broken if the rpc-address is set to the wildcard. Contributed by Colin Patrick McCabe (Revision 1388321)

     Result = FAILURE
eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1388321
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java
, BTW, has anyone tested this on
# box w/ multiple addresses
# box with totally broken networking that returns null instead of a hostname/IP addr. One of mine with a buggy resolv.conf had this. Such a box is beyond hope -the goal should just be to bail out with a message saying "your network is broken", instead of a stack trace.

Networking is one area that has higher variance across installations than any other -and a high variance in understanding, especially for people setting up pseudo clusters. Hadoop needs to fail in a way that lets them know it's their problem, not the Hadoop codebase., Integrated in Hadoop-Hdfs-trunk #1172 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1172/])
    HDFS-3932. NameNode Web UI broken if the rpc-address is set to the wildcard. Contributed by Colin Patrick McCabe (Revision 1388321)

     Result = SUCCESS
eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1388321
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java
, Integrated in Hadoop-Mapreduce-trunk #1203 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1203/])
    HDFS-3932. NameNode Web UI broken if the rpc-address is set to the wildcard. Contributed by Colin Patrick McCabe (Revision 1388321)

     Result = SUCCESS
eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1388321
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java
, bq. box w/ multiple addresses

Well, I tested it on a box that had 127.0.0.1 and another IP, and verified that it got the non-localhost IP.

In general, however, we should implement HDFS-3946, which will make it unnecessary to guess the IP in the way we're doing here.  This is just a quick fix in the mean time.

bq. box with totally broken networking that returns null instead of a hostname/IP addr. One of mine with a buggy resolv.conf had this. Such a box is beyond hope -the goal should just be to bail out with a message saying "your network is broken", instead of a stack trace.

{{NetUtils#getConnectAddress}} has code for this case.  I haven't tested it-- I like to keep all my boxes from getting "beyond hope," and also, I doubt any part of Hadoop will work in this scenario., Steve, wrt #1, yes, I tested on a machine with three interfaces/IP and RPC binding to the wildcard., With respect to Steve's scenario #2 ("box with totally broken networking"):

bq. ... a box with totally broken networking that returns null instead of a hostname/IP addr. One of mine with a buggy resolv.conf had this. Such a box is beyond hope -the goal should just be to bail out with a message saying "your network is broken", instead of a stack trace.

I guess I should mention that {{NetUtils#getConnectAddress}} will return {{127.0.0.1}} if no local hostnames can be resolved.  I think that's pretty much as best as we can handle it, although I'm open for other ideas.]