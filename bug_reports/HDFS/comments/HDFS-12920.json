[CC [~andrew.wang], [~linyiqun], [~chris.douglas]., Particularly since there's a workaround, let's bump this to 3.0.1., Hi  [~djp], thanks for reporting this.
bq. A quick workaround is to add values in hdfs-site.xml with removing all time unit. But the right way may be to revert HDFS-10845 (and get rid of noisy warnings).
I think we don't need to removing all time unit values in hdfs-default file. HDFS configurations support time unit suffix was implemented in HDFS-9847. That change was only committed in trunk not include branch-2. So the new settings with time unit suffix are only making sense in 3.x.x versions. So the right way should be to revert HDFS-10845 and get rid of noisy warnings as you suggested.
If we are all agreed on on this way, I will attach the patch to make this changed.
Thanks., /cc [~arpitagarwal]

If we're not going to [change the config properties|https://issues.apache.org/jira/browse/HDFS-9847?focusedCommentId=15211227&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15211227], and the client needs to load 3.x config files during rolling upgrades, then this isn't worth the hassle., bq. So the right way should be to revert HDFS-10845 and get rid of noisy warnings as you suggested. If we are all agreed on on this way, I will attach the patch to make this changed.
Thanks [~linyiqun] for quick response. I am +1 on ongoing this way.

bq. If we're not going to change the config properties, the client needs to load 3.x config files during rolling upgrades, then this isn't worth the hassle.
Actually, it could be a serious issue for old client (provided by MR tarball via distributed cache) to work with new 3.x config which include incompatible default value. Old client cannot recognize the new value for the known property (end up with "s", etc.) and throw exception to end the job which means app cannot run successful during upgrade from 2.9 to 3.0.0., I understand the issue, Junping. An alternative to reverting the change is to deprecate the old property and create a new one that understands time units, as was raised in that JIRA. If specifying units breaks rolling upgrades, then what is the point of adding units, ever?, This only occurs if the job submitter is using 3.x jars and the submitted job is using 2.x jars.  If the job submitter is using the same jars as the code then this does not happen, since the values copied from hdfs-default.xml into job.xml as part of job submission are compatible with the parsing code.

So another workaround is to have at least two tarballs on HDFS, one that uses 3.x and one that uses 2.x.  The 3.x site configs request the 3.x tarball and the 2.x site configs request the 2.x tarball.  When the job submitter client upgrades to use 3.x jars, it can also upgrade to 3.x configs to start running the job with 3.x as well.
, [~djp], does presence of any unit-suffixed values in the config file cause this failure?, bq. An alternative to reverting the change is to deprecate the old property and create a new one that understands time units, as was raised in that JIRA. If specifying units breaks rolling upgrades, then what is the point of adding units, ever?
That is also a possible approach. We can either keep the default value for existing properties or start to using new properties and deprecated previous properties.

bq. So another workaround is to have at least two tarballs on HDFS, one that uses 3.x and one that uses 2.x. The 3.x site configs request the 3.x tarball and the 2.x site configs request the 2.x tarball. When the job submitter client upgrades to use 3.x jars, it can also upgrade to 3.x configs to start running the job with 3.x as well.
As we discussed offline, if we explicitly packaging these configs into tarball, then we may not hitting this issue as different version tar ball and configuration will match each other in the end. However, some users may not follow this practice before and after. Also, managing configurations in different places (cluster setup, MR tar ball, job submission, etc.) is also complicated. May be it is more easier to fix issue here instead of tarball configuration?

bq. Junping Du, does presence of any unit-suffixed values in the config file cause this failure?
Hi [~arpitagarwal], the unit-suffixed values is by default (in hdfs-default.xml) now in 3.x. Job submit against old version MR tar ball will load new default values provided by new hadoop deployment which will get stuck with exception I put above.]