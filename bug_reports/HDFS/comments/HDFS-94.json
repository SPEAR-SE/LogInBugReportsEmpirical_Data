[The codes use [Runtime.getRuntime().totalMemory()|http://java.sun.com/javase/6/docs/api/java/lang/Runtime.html#totalMemory()] to obtain the first number.  However, totalMemory() does not represent the usage of the heap as described in the javadoc., What is the most appropriate call then? , I think this is a presentation problem.  The text "10.01 GB / 10.01 GB (100%)" seems saying that 100% of memory is being used but it may not be the case.  Unfortunately, there is no easy way to obtain accurate memory usage.  How about we change it to "maxMemory= 10.01 GB, totalMemory = 10.01 GB, freeMemory = xx".  Then it will be less confusing., I have a machine on which the namenodeis running with an -Xmx20480m but the nameode ui shows:

  xxx files and directories, yyy blocks = zzz total. Heap Size is 15.27 GB / 17.78 GB (85%) 

I wonder why it shows a total of 17.78GB instead of 20GB, > I wonder why it shows a total of 17.78GB instead of 20GB

Would it be the case that you have hit some limit?  The following is quoted from [java man page|http://java.sun.com/javase/6/docs/technotes/tools/solaris/java.html]:

{quote}
On Solaris 7 and Solaris 8 SPARC platforms, the upper limit for this value is approximately 4000m minus overhead amounts. On Solaris 2.6 and x86 platforms, the upper limit is approximately 2000m minus overhead amounts. On Linux platforms, the upper limit is approximately 2000m minus overhead amounts. 
{quote}, I agree. Do you plan to do anything to this JIRA to make the reporting more accurate?, Currently, the code uses
{quote}
    long totalMemory = Runtime.getRuntime().totalMemory();
    long maxMemory = Runtime.getRuntime().maxMemory();
    long used = (totalMemory * 100)/maxMemory;

{quote}

Is it better to use :

{quote}
    MemoryMXBean memoryMXBean = ManagementFactory.getMemoryMXBean();
    MemoryUsage status = memoryMXBean.getHeapMemoryUsage();
    usedMemory = status.getUsed();
    maxMemory = status.getMax();
{quote}, It makes to replace the current codes by MemoryMXBean since it provides more information.  I think it is better to show more numbers like non-heap usage, init, used, committed, max, etc., This patch uses JMX Memory beans instead of runtime.
I was talking to Dhruba, he mentioned we might want to do this JIRA as a bugfix, so that it can be pulled into 21 and displaying more information can be a separate 'improvement' JIRA.
, Thanks Dmytro for the patch.

@Nicholas: please let me know if you agree that the additional metrics (e.g. non heap usage, used, committed) be done via another JIRA. Also, I do not see the requirement of a unit test for this case., +1 patch looks good.  Thanks, Dmytro., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12428685/HDFS-94.patch
  against trunk revision 893066.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/156/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/156/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/156/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/156/console

This message is automatically generated., The failed tests are 
org.apache.hadoop.hdfs.server.datanode.TestFiDataTransferProtocol2.pipeline_Fi_18 
 org.apache.hadoop.hdfs.TestReadWhileWriting.pipeline_02_03 

and are not related to this patch.

i will commit this patch soon., I filed HDFS-849 against the first error org.apache.hadoop.hdfs.server.datanode.TestFiDataTransferProtocol2.pipeline_Fi_18 ., I just committed this. Thanks Dmytro.

, Integrated in Hdfs-Patch-h5.grid.sp2.yahoo.net #159 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/159/])
    . The Heap Size printed in the NameNode WebUI is accurate.
(Dmytro Molkov via dhruba)
, Integrated in Hadoop-Hdfs-trunk #182 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk/182/])
    , Integrated in Hadoop-Hdfs-trunk-Commit #158 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk-Commit/158/])
    , Integrated in Hdfs-Patch-h2.grid.sp2.yahoo.net #94 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/94/])
    ]