[Scenario is as follows:
---------------------
1. Cluster is having 4 DNs.
2. File is written to 3 DNs DN1->DN2->DN3 with genstamp of 1001
3. Now DN3 is stopped.
4. Now append is called.
5. For this append Client will try to create the pipeline DN1->DN2->DN3
During this time following things will happen
1. The Generation stamp will be updated in volumeMap to 1002
2. Now datanode will try to connect to next DN in pipeline.
If Next DN in pipeline is down, then exception will be thrown and client will try to reform the pipeline.

Now since DN3 is down, in DN1 and DN2 genstamp is already updated to 1002. But client doesnot know about this.
6. Now client is trying to add one more datanode to append pipeline. i.e. DN4. and ask DN1 or DN2 to transfer block to DN4. But Client will ask to transfer block with genstamp 1001.
7. Since DN1 and DN2 dont have block with genstamp 1001, so transfer will fail and Client write also will fail.

Proposed solution
------------------
In DataXceiver#writeBlock(), before creating the BlockReceiver instance, if we try to create mirror connection, then this solves the problem., Good catch!  I think the bug is in the following:
{code}
//DataNode.transferReplicaForPipelineRecovery(..)
    synchronized(data) {
      if (data.isValidRbw(b)) {
        stage = BlockConstructionStage.TRANSFER_RBW;
      } else if (data.isValidBlock(b)) {
        stage = BlockConstructionStage.TRANSFER_FINALIZED;
      } else {
        final String r = data.getReplicaString(b.getBlockPoolId(), b.getBlockId());
        throw new IOException(b + " is neither a RBW nor a Finalized, r=" + r);
      }

      storedGS = data.getStoredBlock(b.getBlockPoolId(),
          b.getBlockId()).getGenerationStamp();
      if (storedGS < b.getGenerationStamp()) {
        throw new IOException(
            storedGS + " = storedGS < b.getGenerationStamp(), b=" + b);        
      }
      visible = data.getReplicaVisibleLength(b);
    }
{code}
It should first call getStoredBlock(..) and then use the stored block to call isValidRbw(..) and isValidBlock(..).  It expects GS to be updated but does not handle it correctly., Thanks Nicholas, that works. I will upload a patch for that., Thanks a lot Vinay for taking this issue. Please edit the issue title as this will come in recovery flow also, not only in Append., Attaching the Patch for same, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12528560/HDFS-3436.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2506//console

This message is automatically generated., The patch file is corrupted.
{noformat}
patching file hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend.java
patch: **** malformed patch at line 75: }
{noformat}
, Hi Nicholas, I am working on it. Thanks., Submitting  the patch for trunk, test-patch.sh result for the latest patch.

{noformat}+1 overall.  

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version ) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.{noformat}, +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12528719/HDFS-3436-trunk.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2510//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2510//console

This message is automatically generated., +1 patch looks good., I have committed this.  Thanks, Vinay!, Integrated in Hadoop-Hdfs-trunk-Commit #2355 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2355/])
    HDFS-3436. In DataNode.transferReplicaForPipelineRecovery(..), it should use the stored generation stamp to check if the block is valid.  Contributed by Vinay (Revision 1341961)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1341961
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppendRestart.java
, Integrated in Hadoop-Common-trunk-Commit #2282 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2282/])
    HDFS-3436. In DataNode.transferReplicaForPipelineRecovery(..), it should use the stored generation stamp to check if the block is valid.  Contributed by Vinay (Revision 1341961)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1341961
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppendRestart.java
, Thanks a lot Nicholas., Integrated in Hadoop-Mapreduce-trunk-Commit #2300 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2300/])
    HDFS-3436. In DataNode.transferReplicaForPipelineRecovery(..), it should use the stored generation stamp to check if the block is valid.  Contributed by Vinay (Revision 1341961)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1341961
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppendRestart.java
, I saw this happening when the second DN in a pipeline becomes very slow due to hardware issues. It will cause all downstream dn to fail in writes and pipeline recoveries and eventually itself will fail. Adding a new dn and transferring block fails since recoverRbw() in previous failed recovery attempt modified the gen stamp.  I am glad it's already fixed in branch-2. I am pulling into branch-0.23.

HDFS-4257 would have made writes to succeed, but at the same time this bug might have gone undetected. It's fortunate this was fixed before HDFS-4257., Integrated in Hadoop-Hdfs-0.23-Build #600 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/600/])
    svn merge -c 1341961 Merging from trunk to branch-0.23 to fix HDFS-3436. (Revision 1478889)

     Result = SUCCESS
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1478889
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppendRestart.java
]