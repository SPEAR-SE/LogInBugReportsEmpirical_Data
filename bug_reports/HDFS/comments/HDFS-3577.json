[Is this true on branch-1 as well?, @Eli, it doesn't look like, doing a quick check to the *ByteRangeInputStream.getInputStream()* method the impl is different, no exception begin thrown:

{code}
      final String cl = connection.getHeaderField(StreamFile.CONTENT_LENGTH);
      filelength = (cl == null) ? -1 : Long.parseLong(cl);
      in = connection.getInputStream();
{code}

The *ByteRangeInputClass* seems that had some significant rewriting in Hadoop2., I verfied this is an issue on branch-1 as well:

{noformat}
2012-07-03 18:43:23,150 INFO org.apache.hadoop.tools.DistCp: FAIL CentOS-5.8-x86_64-netinstall.iso : java.io.IOException: Content-Length header is missing
	at org.apache.hadoop.hdfs.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:125)
	at org.apache.hadoop.hdfs.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:103)
	at org.apache.hadoop.hdfs.ByteRangeInputStream.read(ByteRangeInputStream.java:158)
	at java.io.DataInputStream.read(DataInputStream.java:83)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:424)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:547)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:314)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:266)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1178)
	at org.apache.hadoop.mapred.Child.main(Child.java:260)
{noformat}, Marking as a blocker since this prevents any reasonable distcp using webhdfs from working., Two potential fixes:

# change WebHdfsFileSystem to support chunked transfer encoding;
# change server to set content length.

Which one is better?, AFAIK you don't want to do #2 as that triggers in-memory client full buffering of the response. Note that to do #1 you don't have to do much, in Hadoop 2 just don't throw an exception if content length is not set., Alejandro, thanks for the hint.  I will try #1 then., h3577_20120705.patch: do not throw exceptions when Content-Length is missing., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12535316/h3577_20120705.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestHDFSTrash
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2746//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2746//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12535316/h3577_20120705.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.hdfs.TestHDFSTrash

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2747//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2747//console

This message is automatically generated., The file size was 1MB in the test but the block size was only 1kB.  Therefore, it created a lot of local files and failed with "java.net.SocketException: Too many open files"., h3577_20120708.patch: use a larger block size., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12535614/h3577_20120708.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.hdfs.TestDFSClientRetries

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2761//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/2761//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2761//console

This message is automatically generated., No, no, no!  This is reverting a fix for > 32-bit file transfers.  I think the correct fix is to require content-length unless chunked encoding is being used., Sorry, I freaked out before studying the whole patch.  I still think a chunked encoding check should be present unless I'm misunderstanding something.  There's also not much use in instantiating a {{BoundedInputStream}} w/o a limit., bq. The file size was 1MB in the test but the block size was only 1kB. Therefore, it created a lot of local files and failed with "java.net.SocketException: Too many open files".

Does this mean there's a fd leak?  Or at least a leak during the create request?  If so, is the test at fault?, It turns that the size of each chunk is very small (tens of bytes) when it uses chunked transfer encoding.  The client keeps opening new socket for each chunk and then it leads to SocketException.  The problem disappear if StreamingOutput is replaced with MessageBodyWriter since it can specify the response size., h3577_20120714.patch: uses MessageBodyWriter in DatanodeWebHdfsMethods., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536511/h3577_20120714.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestBackupNode
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2824//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2824//console

This message is automatically generated., How about a test that copies > 1 block of data so we can detect regressions?, Tucu, do we have an HttpFs test that checks that it can read files big enough to require chunked transfer encoding?, The failed TestBlocksWithNotEnoughRacks is not related to this.

> How about a test that copies > 1 block of data so we can detect regressions?

Hi Eli, thanks for taking a look.  The modified test in the patch already has writing and reading > 1 blocks of data.  It will fail if the fix is not applied., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536511/h3577_20120714.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestBackupNode

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2830//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2830//console

This message is automatically generated., The {{BoundedInputStream}} is a no-op when the ctor w/o a length is used, so I think this:
{code}final InputStream is = cl == null? new BoundedInputStream(in)
    : new BoundedInputStream(in, Long.parseLong(cl));{code}
can be:
{code}final InputStream is = cl == null? in
    : new BoundedInputStream(in, Long.parseLong(cl));{code}

A chunk size can be specified for a {{HttpURLConnection}} and we should be able to enable keep-alive on the socket (I thought it was the default?) to avoid new connections for every chunk.  I don't know anything about {{MessageBodyWriter}} et al, so if my suggestion isn't feasible and someone else oks the {{MessageBodyWriter}}, I'm fine with it., This impacts branch-0.23 as well.  I really would like to see whatever fix happens go into branch-0.23 as well.  I applied the latest patch and it looks to apply fairly cleanly.  If it does not apply cleanly when checking in the file fix I will be happy to port it., @Eli, chunked encoding is enabled by default in modern servlet containers, it normally kicks in after a threshold (I believe in Tomcat this threshold is 8K), don't know what is the threshold of jetty.

Another thing to keep in mind, is that the HttpURLConnection fully caches in memory the outputstream (uploading data) and this seems hardcoded in the JDK. This would pose a limitation in writing files through webhdfs/httpfs. A solution may be to refactor WebHdfsFileSystem to use HttpClient (arghh, we may need to tweak hadoop-auth to work with HttpClient)., Added link to HDFS-3318: fix of that issue led to the issue described in HDFS-3577, bq. Another thing to keep in mind, is that the HttpURLConnection fully caches in memory the outputstream

Isn't that only the case for non-chunked responses since it has to compute the content-length?  But to avoid that, I think you can use {{setFixedLengthStreamingMode(length)}}., h3577_20120716.patch: do not use BoundedInputStream when content-length is not found., +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536761/h3577_20120716.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2838//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2838//console

This message is automatically generated., @daryn, http://docs.oracle.com/javase/6/docs/api/java/net/HttpURLConnection.html#setFixedLengthStreamingMode(int), works only if you know the length in advance, when doing a FS create() you don't know how many bytes will be written., It seems that using http://docs.oracle.com/javase/6/docs/api/java/net/HttpURLConnection.html#setChunkedStreamingMode(int) would do the trick, if the server side (jetty & tomcat) support it., If the content-length check is removed and content-length is not present, transfers >2GB will fail unless the client's read timeout is greater than the server's connection idle timeout (200s), and partial downloads will be considered successful.

I'd suggest the check be left in place on this jira, but it only generates an exception if it's not a chunked download.  We can then decide what to do on HDFS-3671. , Okay, let's revert the changes in ByteRangeInputStream.  Here is a new patch.

h3577_20120717.patch, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12536946/h3577_20120717.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
                  org.apache.hadoop.hdfs.TestPersistBlocks

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2851//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2851//console

This message is automatically generated., +1 Looks good to me!  Might want to alter the description to indicate it's fixing the problem of making new connections for every chunk and exhausting fds., Integrated in Hadoop-Common-trunk-Commit #2495 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2495/])
    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java
, Integrated in Hadoop-Hdfs-trunk-Commit #2560 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2560/])
    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java
, Thanks Daryn for the review.

I have committed this., Integrated in Hadoop-Mapreduce-trunk-Commit #2517 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2517/])
    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java
, Thanks Nicholas.  This needs to be fixed in branch-1 as well, mind posting a patch?  IMO this is a 1.1 blocker as w/o it we've broken compat with earlier 20 releases and you can't distcp with webdhfs., Hi Eli, the Content-Length exception was introduced by HDFS-3318 which was committed to 0.23 but not branch-1.  So branch-1 does not have the problem on transferring files > 24kB.  However, it may have problem on transferring files > 2GB since it does not have HDFS-3318.  If it is the case, we should backport both HDFS-3318 and this., I will test > 2GB files in branch-1., Integrated in Hadoop-Hdfs-trunk #1108 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1108/])
    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java
, Integrated in Hadoop-Hdfs-0.23-Build #318 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/318/])
    svn merge -c 1362976 from trunk for HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362981)

     Result = SUCCESS
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362981
Files : 
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java
* /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java
, Integrated in Hadoop-Mapreduce-trunk #1141 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1141/])
    HDFS-3577. In DatanodeWebHdfsMethods, use MessageBodyWriter instead of StreamingOutput, otherwise, it will fail to transfer large files. (Revision 1362976)

     Result = FAILURE
szetszwo : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362976
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/OpenEntity.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java
, Thanks Nicholas, I forgot 3318 was not in branch-1. I pinged HDFS-3671 since we're not removing the Content-Length check here., I merged this to branch-2.1.0-alpha., Hey Nicholas,

Did you test this with distcp? Trying to distcp from a recent trunk build with this change still fails with *Content-Length header is missing*. Hadoop fs -get using webhdfs with the same file works.

{noformat}
12/07/19 23:56:43 INFO mapreduce.Job: Task Id : attempt_1342766959778_0002_m_000000_0, Status : FAILED
Error: java.io.IOException: File copy failed: webhdfs://eli-thinkpad:50070/user/eli/data1/big.iso --> hdfs://localhost:8020/user/eli/data4/data1/big.iso
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:262)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:229)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:45)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:726)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:333)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:154)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:149)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying webhdfs://eli-thinkpad:50070/user/eli/data1/big.iso to hdfs://localhost:8020/user/eli/data4/data1/big.iso
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:258)
	... 10 more
Caused by: org.apache.hadoop.tools.mapred.RetriableFileCopyCommand$CopyReadException: java.io.IOException: Content-Length header is missing
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.readBytes(RetriableFileCopyCommand.java:201)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:167)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToTmpFile(RetriableFileCopyCommand.java:112)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:90)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:71)
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
	... 11 more
Caused by: java.io.IOException: Content-Length header is missing
	at org.apache.hadoop.hdfs.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:125)
	at org.apache.hadoop.hdfs.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:103)
	at org.apache.hadoop.hdfs.ByteRangeInputStream.read(ByteRangeInputStream.java:158)
	at java.io.DataInputStream.read(DataInputStream.java:132)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at java.io.FilterInputStream.read(FilterInputStream.java:90)
	at org.apache.hadoop.tools.util.ThrottledInputStream.read(ThrottledInputStream.java:70)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.readBytes(RetriableFileCopyCommand.java:198)
	... 16 more
{noformat}, Per offline conversation with Daryn the reason this isn't an issue is that HDFS-3166 (Add timeout to Hftp connections) isn't in branch-1 (w/o the timeout we just get a 200s lag, not a failure) and 3166 exposed HDFS-3318 (Hftp hangs on transfers >2GB) which introduces the check that will fail if the content length header is not set (ie 20.x).

, Yes, HDFS-3166 (add timeouts) exposed the 200s tail on >2GB files caused by a java bug.  The content-length has to be known in order to workaround the java bug, and thus avoid the read timeout.

What I think can be done:
* If chunked, eliminate the content-length requirement
* If not chunked, and no content-length, obtain the length from a file stat or a HEAD, etc, Shouldn't be an issue in branch-1 I mean. I verified that on a branch-1 build from today distcp using webhdfs of a directory that contains a 3gb file works.  So looks like this is just an issue on trunk/2.x, though per my comment above the same distcp does not work even with 3577 applied., Btw webhdfs is still broken for large files when using distcp. I confirmed on the latest trunk build and filed HDFS-3788.]