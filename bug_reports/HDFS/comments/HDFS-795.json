[Attached patch {{toreproduce-5796.patch}} helps illustrate the problem. How to reproduce :

Create an HDFS cluster with 2 datanodes. For one of them set "dfs.datanode.address" to "0.0.0.0:50013". Now try to write a 5MB file. You will notice that when ever the 50013 is the last datanode in the pipeline, write is aborted.

The hunk from patch for HADOOP-1700 that reverts the earlier fix :
{noformat}
@@ -2214,10 +2218,15 @@
               /* The receiver thread cancelled this thread. 
                * We could also check any other status updates from the 
                * receiver thread (e.g. if it is ok to write to replyOut). 
+               * It is prudent to not send any more status back to the client
+               * because this datanode has a problem. The upstream datanode
+               * will detect a timout on heartbeats and will declare that
+               * this datanode is bad, and rightly so.
                */
               LOG.info("PacketResponder " + block +  " " + numTargets +
                        " : Thread is interrupted.");
               running = false;
+              continue;
             }
             
             if (!didRead) {
{noformat}

I don't think the added justification is always correct.

Suggested fix : 
============
- the loop should 'continue' if write to the local disk fails.
- it should not if write to downstream mirror fails. (this test case)



, Not certain if what I"m seeing is the exact same cause, but I have another reproducible case in which the write pipeline recovery decides the first node is dead every time, when in actuality it's the last node that's dead. In my case, I've set up a 3-node HDFS cluster with replication 3, and each DN having one 100G volume and one 2G volume. The 2Gs fill up, throw DiskOutOfSpaceExceptions, and the write pipeline recovers incorrectly when the node that runs out of space is the last. It first ejects pipeline[0], fails again when trying to continue the write on the dead node, ejects the second, then tries again writing only to the failed node. Of course that fails too, and the whole write is aborted.

I'll try applying this patch (and thinking it through a bit further) and seeing if it resolves the issue., Silly me, now I see that this patch is just to reproduce, not to fix :) Will investigate a fix since I have a good petri dish in which to reproduce this issue., Upgrading to critical since this is reproducible and causes complete pipeline failure for writers., I will get this issue fixed in HDFS-101., Great, thanks Hairong. FYI this is happening in 0.20.1, and I think should be fixed in the branch as well. Let me know if you need help testing a patch - I can reproduce it pretty reliably., HDFS-101 duplicates this, and fix is under way there.]