[We can work around it by setting dfs.datanode.failed.volumes.tolerated to zero so that as long as there is one disk failure, NN will remove that DN. For the fix, there are several possible approaches.

1. Have DN notify NN via DatanodeProtocol.reportBadBlocks for these blocks.
2. Modify DatanodeProtocol.errorReport so that DN can pass storage id to NN.
3. Have DN send blockReport for this failed storage so that NN can detect that.

Appreciate any suggestions., I think #3 will be the minimally invasive change.  Should just need to send an empty report for the failed storage., Thanks, Daryn. We can do #3, but want to put the approaches in the following ways, in part to clarify the design of heterogeneous storage. [~arpitagarwal] and others might have more input here. Note that dfs.datanode.failed.volumes.tolerated > 0 in the discussion.

1. Have DN eventually deliver failed storage notification. Prior to heterogeneous storage, NN detects missing replicas on the failed storage via BR. So if we use BR to report failed storage, we are on par in terms of time to detect metrics. However, we have to make sure DN eventually deliver the failed storage notification in all cases. hotswap is one scenario. Here is another scenario, a) A storage fails. b) DN restarts prior to the next BR. c) DN couldn't send BR after restart as it excluded the failed storage during startup. To address this, we can persist storage ids that DN need to BR on, perhaps on other healthy storages.

2. Have DN timely deliver failed storage notification. DN provides StorageReport via HB. With this NN could detect failed storage much faster. This will greatly improve time to detect metrics. But this requires HB to take the FSNS write lock. We can make it async without FSNS write lock. This can be done in a separate jira.

3. Time out on DN storage notification. Similar to how NN use HB to manage DN, we can have HB for each storage. There should be some max time out value of notification for any given storage. But if the design of heterogeneous storage is to allow a DN to use different BR intervals for different storages, we could potentially have much larger value of BR for a given storage., Here is the initial patch based on heartbeat notification approach, the assumption is DN will report all healthy storages in the heartbeat. This approach is simpler than the blockReport approach which needs to have DN persist the info to cover some failure scenarios. It also makes storage failure detection faster.

1. NN detects failed storages during HB processing based on the delta between DN's reported healthy storages and the storages NN has. Marked the state of those missing storages DatanodeStorage.State.FAILED.

2. HeartbeatManager will remove blocks on those DatanodeStorage.State.FAILED storages. This will cover some corner scenarios where new replicas might be added to BlocksMap afterwards.

3. It also covers the case where admins reduce the number of healthy volumes on DN and restart DN., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12674633/HDFS-7208.patch
  against trunk revision 178bc50.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8413//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8413//console

This message is automatically generated., Hi Ming, thanks for working on this.  The patch looks pretty good.  Some comments:

- For heartbeatedSinceRegistration == false, let's check failed storage anyway, i.e. no need to compare storageMap.size() > reports.length.
- The method removeBlocksOnDatanodeStorage(..) does not use anything in DatanodeManager.  We may move the code to BlockManager.removeBlocksAssociatedTo(..).
- In HeartbeatManager.heartbeatCheck(), allAlive should be changed to allAlive = dead == null && failedStorage == null.
- In DatanodeDescriptor.updateFailedStorage(..), check if a storage was already failed.  Log and update the state only if it was not already failed.
- HeartbeatManager.register(..) also calls DatanodeDescriptor.updateHeartbeat(..).  So setting heartbeatedSinceRegistration = true in updateHeartbeat(..) is wrong.  Need to fix it.
, Thanks Nicholas for the review.

The latest patch addresses all your comments, except for the allAlive one. The reason is the patch handles deadnode separately from the failedStorage.

, What is the version of the bug occurred?
My cluster version is 2.4.1.
Can I apply the patch without service down-time ?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12674917/HDFS-7208-2.patch
  against trunk revision 0260231.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The test build failed in hadoop-hdfs-project/hadoop-hdfs 

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8429//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8429//console

This message is automatically generated., > The latest patch addresses all your comments, except for the allAlive one. The reason is the patch handles deadnode separately from the failedStorage.

We need to change allAlive.  Otherwise, the while loop won't work if there is only failed storage.  Of course, we also need to update the if-condition for dead datanode.  Here is my suggestion:
{code}
    while (!allAlive) {
      ...
      allAlive = dead == null && failedStorage == null;
      if (dead != null) {
        ...
      }
      ...
    }
{code}

We should also call namesystem.checkSafeMode() in removeBlocksAssociatedTo(..)., Ha, thanks, Nicholas. Here is the new patch., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12675133/HDFS-7208-3.patch
  against trunk revision 0af1a2b.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.mover.TestStorageMover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8435//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8435//console

This message is automatically generated., +1 patch looks good., FAILURE: Integrated in Hadoop-trunk-Commit #6271 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6271/])
HDFS-7208. NN doesn't schedule replication when a DN storage fails.  Contributed by Ming Ma (szetszwo: rev 41980c56d3c01d7a0ddc7deea2d89b7f28026722)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerTestUtil.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeStorage.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java
, I have committed this.  Thanks, Ming!, Thanks Daryn for the input and Nicholas for the review and the commit., FAILURE: Integrated in Hadoop-Yarn-trunk #713 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/713/])
HDFS-7208. NN doesn't schedule replication when a DN storage fails.  Contributed by Ming Ma (szetszwo: rev 41980c56d3c01d7a0ddc7deea2d89b7f28026722)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeStorage.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerTestUtil.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1903 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1903/])
HDFS-7208. NN doesn't schedule replication when a DN storage fails.  Contributed by Ming Ma (szetszwo: rev 41980c56d3c01d7a0ddc7deea2d89b7f28026722)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerTestUtil.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeStorage.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1928 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1928/])
HDFS-7208. NN doesn't schedule replication when a DN storage fails.  Contributed by Ming Ma (szetszwo: rev 41980c56d3c01d7a0ddc7deea2d89b7f28026722)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerTestUtil.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/DatanodeStorage.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java
, The new test cannot work correctly on Windows.  See HDFS-7355 for a full explanation and a trivial patch to skip the test on Windows., We meet the same problem, but we have a very simple path that works.  We can treat it as the datanode deleted the related blocks, so we only need one line to fix it.


diff --git a/hadoop/adh/src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java b/hadoop/adh/src/hadoop-hdfs-proje
index 3320c65..7a10072 100644
--- a/hadoop/adh/src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
+++ b/hadoop/adh/src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
@@ -1332,6 +1332,8 @@ public void checkDataDir() throws DiskErrorException {
                   + " on failed volume " + fv.getCurrentDir().getAbsolutePath());
               ib.remove();
               removedBlocks++;
+              datanode.notifyNamenodeDeletedBlock(new ExtendedBlock(bpid, b.getBlockId()), b.getStorageUuid());
             }
           }
         }, AdMaster patch, Code in comment seems ugly, so I have added a patch file.]