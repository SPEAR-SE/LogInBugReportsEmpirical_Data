[Here's the relevant portion of the log:

Exception in thread "Thread-2125" java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum error: /block-being-written-to at 1072128 exp: 1082174632 got: -132500175
	at org.apache.hadoop.hdfs.TestFileConcurrentReader$4.run(TestFileConcurrentReader.java:383)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: /block-being-written-to at 1072128 exp: 1082174632 got: -132500175
	at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:297)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.verifyPacketChecksums(RemoteBlockReader2.java:221)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:191)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:130)
	at org.apache.hadoop.hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:578)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:632)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:673)
	at java.io.DataInputStream.read(DataInputStream.java:83)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader.tailFile(TestFileConcurrentReader.java:440)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader.access$200(TestFileConcurrentReader.java:54)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader$4.run(TestFileConcurrentReader.java:379)
	... 1 more
Exception in thread "Thread-2124" java.lang.RuntimeException: java.io.InterruptedIOException: Interrupted while waiting for data to be acknowledged by pipeline
	at org.apache.hadoop.hdfs.TestFileConcurrentReader$3.run(TestFileConcurrentReader.java:367)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.InterruptedIOException: Interrupted while waiting for data to be acknowledged by pipeline
	at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:1649)
	at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:1633)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:1718)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:71)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:99)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader$3.run(TestFileConcurrentReader.java:363)

And this as well..

2012-08-06 23:38:14,373 INFO  hdfs.StateChange (FSNamesystem.java:reportBadBlocks(4727)) - *DIR* NameNode.reportBadBlocks
2012-08-06 23:38:14,374 INFO  hdfs.StateChange (CorruptReplicasMap.java:addToCorruptReplicasMap(66)) - BLOCK NameSystem.addToCorruptReplicasMap: blk_4844811661965065785 added as corrupt on 127.0.0.1:33823 by /127.0.0.1 because client machine reported it
2012-08-06 23:38:14,375 ERROR hdfs.TestFileConcurrentReader (TestFileConcurrentReader.java:run(381)) - error tailing file /block-being-written-to
org.apache.hadoop.fs.ChecksumException: Checksum error: /block-being-written-to at 1072128 exp: 1082174632 got: -132500175
	at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:297)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.verifyPacketChecksums(RemoteBlockReader2.java:221)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:191)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:130)
	at org.apache.hadoop.hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:578)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:632)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:673)
	at java.io.DataInputStream.read(DataInputStream.java:83)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader.tailFile(TestFileConcurrentReader.java:440)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader.access$200(TestFileConcurrentReader.java:54)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader$4.run(TestFileConcurrentReader.java:379)
	at java.lang.Thread.run(Thread.java:662)
2012-08-06 23:38:14,376 ERROR hdfs.TestFileConcurrentReader (TestFileConcurrentReader.java:run(393)) - error in tailer
java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum error: /block-being-written-to at 1072128 exp: 1082174632 got: -132500175
	at org.apache.hadoop.hdfs.TestFileConcurrentReader$4.run(TestFileConcurrentReader.java:383)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.hadoop.fs.ChecksumException: Checksum error: /block-being-written-to at 1072128 exp: 1082174632 got: -132500175
	at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:297)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.verifyPacketChecksums(RemoteBlockReader2.java:221)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:191)
	at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:130)
	at org.apache.hadoop.hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:578)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:632)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:673)
	at java.io.DataInputStream.read(DataInputStream.java:83)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader.tailFile(TestFileConcurrentReader.java:440)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader.access$200(TestFileConcurrentReader.java:54)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader$4.run(TestFileConcurrentReader.java:379)
	... 1 more
2012-08-06 23:38:14,377 INFO  FSNamesystem.audit (FSNamesystem.java:logAuditEvent(267)) - allowed=true	ugi=jenkins (auth:SIMPLE)	ip=/127.0.0.1	cmd=append	src=/block-being-written-to	dst=null	perm=null
2012-08-06 23:38:14,377 DEBUG hdfs.DFSClient (DFSOutputStream.java:computePacketChunkSize(1329)) - computePacketChunkSize: src=/block-being-written-to, chunkSize=442, chunksPerPacket=1, packetSize=475
2012-08-06 23:38:14,378 DEBUG hdfs.DFSClient (DFSOutputStream.java:queueCurrentPacket(1342)) - Queued packet 0
2012-08-06 23:38:14,378 DEBUG hdfs.DFSClient (DFSOutputStream.java:waitForAckedSeqno(1638)) - Waiting for ack for: 0
2012-08-06 23:38:14,378 DEBUG hdfs.DFSClient (DFSOutputStream.java:run(466)) - Append to block BP-402451742-67.195.138.20-1344296267847:blk_4844811661965065785_2099
2012-08-06 23:38:14,378 ERROR hdfs.TestFileConcurrentReader (TestFileConcurrentReader.java:run(365)) - error in writer
java.io.InterruptedIOException: Interrupted while waiting for data to be acknowledged by pipeline
	at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:1649)
	at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:1633)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:1718)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:71)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:99)
	at org.apache.hadoop.hdfs.TestFileConcurrentReader$3.run(TestFileConcurrentReader.java:363)
	at java.lang.Thread.run(Thread.java:662)
2012-08-06 23:38:14,379 INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1317)) - Shutting down the Mini HDFS Cluster
, HDFS-3719 recently re-enabled this test btw., I don't think the interruption is related -- I think we have some kind of subtle race on the DN side with the "recomputing last chunk" when appending to a partial chunk - basically the same issue as HDFS-1057 but specific to right as the file is reopened for append., Basically: when the reader opens the file, the file is closed, so it's not instanceof {{ReplicaBeingWritten}}. So, it doesn't do the special re-computation of the last checksum chunk at EOF. But, then, after the file is open, another writer comes along and appends. Then the BlockSender doesn't do the workaround for this race, and we end up with the same issue as described in that other JIRA., Integrated in Hadoop-Hdfs-trunk-Commit #2637 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2637/])
    Revert HDFS-3719. See discussion there and HDFS-3770 for more info. (Revision 1372544)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372544
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java
, Integrated in Hadoop-Common-trunk-Commit #2572 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2572/])
    Revert HDFS-3719. See discussion there and HDFS-3770 for more info. (Revision 1372544)

     Result = SUCCESS
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372544
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java
, Integrated in Hadoop-Mapreduce-trunk-Commit #2593 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2593/])
    Revert HDFS-3719. See discussion there and HDFS-3770 for more info. (Revision 1372544)

     Result = FAILURE
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372544
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java
, Integrated in Hadoop-Hdfs-trunk #1135 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1135/])
    Revert HDFS-3719. See discussion there and HDFS-3770 for more info. (Revision 1372544)

     Result = FAILURE
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372544
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java
, Integrated in Hadoop-Mapreduce-trunk #1167 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1167/])
    Revert HDFS-3719. See discussion there and HDFS-3770 for more info. (Revision 1372544)

     Result = FAILURE
atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372544
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java
]