[The issue is the following:

When the last "good" replica block report comes in, the number of good live replicas has reached the correct replication count, and it calls {{invalidateCorruptReplicas}}:
{code}
    if ((corruptReplicasCount > 0) && (numLiveReplicas >= fileReplication))          
      invalidateCorruptReplicas(storedBlock);                                        
{code}

This function then calls {{invalidateBlock}} on the corrupt replicas (correctly). However, since it is the first block report after being active, the node which is block-reporting is still considered to have "stale block information", so this corruption invalidation is postponed:

{code}
2013-04-30 17:21:40,945 INFO BlockStateChange: BLOCK* invalidateBlock: blk_-XXX_5512300(same as stored) on XXX:1004
2013-04-30 17:21:40,945 INFO BlockStateChange: BLOCK* invalidateBlocks: postponing invalidation of blk_XXX_5512300(same as stored) on XXX:1004 because 1 replica(s) are located on nodes with potentially out-of-date block reports
{code}

However, this code path in {{invalidateCorruptReplicas}} still runs:

{code}
    // Remove the block from corruptReplicasMap
    if (!gotException)
      corruptReplicas.removeFromCorruptReplicasMap(blk);
{code}

So, after this function, all of the replicas of the block are considered live, rather than corrupt.

Then, at the end of block report processing, it marks this node as no longer stale, re-processes the postponed invalidations, and sees that the block is over-replicated (6 replicas instead of 3). Since nothing is in {{corruptReplicasMap}} anymore, {{chooseExcessReplicates}} may mistakenly choose all of the good replicas for removal instead of the bad replicas.

I'm working on a unit test and fix:
- invalidateCorruptReplicas should not remove the replicas from the corruptReplicasMap unless the replicas were also removed from the blocks map.
- separately, I think that the block-reporting node should not be considered stale during its block report -- i.e it's incorrect to postpone blocks in the above context, though I will target this as a separate patch., Here's a hacky unit test with which I'm able to reproduce the issue (at least some of the time). The eventual unit test will be cleaned up from here, just wanted to post this in case anyone wanted to look at the issue., Here's a patch which should fix the issue.

I took the least invasive route I could to fix the problem, by just having {{invalidateBlock}} return a boolean indicating whether it was actually invalidated. Only if it was invalidated do we remove from the corruptReplicas set.

I'm not certain that the call to corruptReplicas.removeFromCorruptReplicasMap is even necessary here at all, since {{removeStoredBlock}} itself will call that when removing from the blocksMap. But, I didn't want to make the larger change here, lest I accidentally introduce a leak in the corruptReplica tracking structure (something we don't have good automated tests for).

The patch also includes a unit test which reliably fails on the unpatched code
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582224/hdfs-4799.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4363//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4363//console

This message is automatically generated., Great sleuthing, Todd. Test and fix look great.

+1, the patch looks good to me., Thanks for the review, Aaron. I'm going to leave this open for another day before committing, just because it's an important bug and a subtle area of the code. If some of the other folks watching this ticket want to take a look, would be very much appreciated., This is really minor: {{removedFromBlockMap}} - use {{BlocksMap}} instead of {{BlockMap}} for the consistency. Otherwise the patch looks okay., renamed to {{removedFromBlocksMap}}, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582364/hdfs-4799.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4367//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4367//console

This message is automatically generated., +1 for the latest patch. , +1 for the patch.  Thanks, Todd!, I've committed this to trunk and branch-2. Thanks for the thorough analysis and the patch, Todd. , Integrated in Hadoop-trunk-Commit #3736 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3736/])
    HDFS-4799. Corrupt replica can be prematurely removed from corruptReplicas map. Contributed by Todd Lipcon. (Revision 1481084)

     Result = SUCCESS
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1481084
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java
, Integrated in Hadoop-Yarn-trunk #206 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/206/])
    HDFS-4799. Corrupt replica can be prematurely removed from corruptReplicas map. Contributed by Todd Lipcon. (Revision 1481084)

     Result = SUCCESS
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1481084
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java
, Integrated in Hadoop-Hdfs-trunk #1395 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1395/])
    HDFS-4799. Corrupt replica can be prematurely removed from corruptReplicas map. Contributed by Todd Lipcon. (Revision 1481084)

     Result = FAILURE
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1481084
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java
, Integrated in Hadoop-Mapreduce-trunk #1422 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1422/])
    HDFS-4799. Corrupt replica can be prematurely removed from corruptReplicas map. Contributed by Todd Lipcon. (Revision 1481084)

     Result = SUCCESS
kihwal : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1481084
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java
]