[Hi [~dmtucker],

This issue looks interesting. Have you got any updates on this? Do you mind I work on it?, [~vagarychen], please, by all means! I have nothing substantial to add at the moment., Hi [~dmtucker],

After checking into the code, it turns out that if the use is a super user, the code will simply bypass the permission check. So I wonder does this issue happen if the user is not a super user?, Indeed, I am able to reproduce with an internal Snakebite-like client and with the regular client:

{code:none}
>>> import pydoofus
>>> super = pydoofus.namenode.v9.Client('namenode', 8020, auth={'effective_user': 'hdfs'})
>>> client = pydoofus.namenode.v9.Client('namenode', 8020, auth={'effective_user': 'nobody'})
>>> super.mkdirs('/test', 0777)
True
>>> client.mkdirs('/test/empty', 0222)
True
>>> client.get_listing('/test/empty')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/lib/python2.7/site-packages/pydoofus/namenode/v9.py", line 666, in get_listing
    self.invoke('getListing', request, response)
  File "/usr/lib/python2.7/site-packages/pydoofus/namenode/v9.py", line 490, in invoke
    blob = self.channel.receive()
  File "/usr/lib/python2.7/site-packages/pydoofus/namenode/v9.py", line 310, in receive
    raise exceptions.create_exception(err_type, err_msg, call_id, err_code)
pydoofus.exceptions.AccessControlException: ERROR_APPLICATION: Permission denied: user=nobody, access=READ_EXECUTE, inode="/test/empty":nobody:supergroup:d-w--w--w-
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1728)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1712)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1686)
        at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getListingInt(FSDirStatAndListingOp.java:76)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4486)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:999)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getListing(ClientNamenodeProtocolServerSideTranslatorPB.java:634)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

>>> client.delete('/test/empty', can_recurse=True)
True
{code}

{code:none}
[hdfs@rketcherside-hdponlynext-1 ~]$ hdfs dfs -mkdir /test
[hdfs@rketcherside-hdponlynext-1 ~]$ hdfs dfs -chmod 777 /test
[hdfs@rketcherside-hdponlynext-1 ~]$ hdfs dfs -ls /
Found 9 items
drwxrwxrwx   - yarn   hadoop          0 2017-03-27 10:20 /app-logs
drwxr-xr-x   - hdfs   hdfs            0 2017-03-27 10:20 /apps
drwxr-xr-x   - yarn   hadoop          0 2017-03-27 10:20 /ats
drwxr-xr-x   - hdfs   hdfs            0 2017-03-27 10:20 /hdp
drwxr-xr-x   - mapred hdfs            0 2017-03-27 10:20 /mapred
drwxrwxrwx   - mapred hadoop          0 2017-03-27 10:20 /mr-history
drwxrwxrwx   - hdfs   hdfs            0 2017-03-28 14:55 /test
drwxrwxrwx   - hdfs   hdfs            0 2017-03-28 09:21 /tmp
drwxr-xr-x   - hdfs   hdfs            0 2017-03-28 09:21 /user

[ambari-qa@rketcherside-hdponlynext-1 ~]$ hdfs dfs -mkdir /test/empty
[ambari-qa@rketcherside-hdponlynext-1 ~]$ hdfs dfs -chmod 222 /test/empty
[ambari-qa@rketcherside-hdponlynext-1 ~]$ hdfs dfs -ls /test/empty
ls: Permission denied: user=ambari-qa, access=READ_EXECUTE, inode="/test/empty":ambari-qa:hdfs:d-w--w--w-
[ambari-qa@rketcherside-hdponlynext-1 ~]$ hdfs dfs -rm -r /test/empty
17/03/28 14:57:45 INFO fs.TrashPolicyDefault: Moved: 'hdfs://rketcherside-hdponlynext-1.west.isilon.com:8020/test/empty' to trash at: hdfs://rketcherside-hdponlynext-1.west.isilon.com:8020/user/ambari-qa/.Trash/Current/test/empty
[ambari-qa@rketcherside-hdponlynext-1 ~]$ hdfs dfs -ls /test
[ambari-qa@rketcherside-hdponlynext-1 ~]$ 
{code}, Thanks [~dmtucker] for showing the steps! Very helpful, I did manage to reproduce the issue following these steps.

After checking the code, I think this is because the code is written that, if the directory to be deleted is empty, the permission check is (again) bypassed. I created a file under the directory, it then prevented me from deleting the directory:
{code}
$ ./bin/hdfs dfs -ls /test/dir
ls: Permission denied: user=someone2, access=READ_EXECUTE, inode="/test/dir":someone2:supergroup:d-w--w--w-
$ ./bin/hdfs dfs -chmod 777 /test/dir
$ ./bin/hdfs dfs -touchz /test/dir/file
$ ./bin/hdfs dfs -chmod 222 /test/dir
$ ./bin/hdfs dfs -ls /test/dir
ls: Permission denied: user=someone2, access=READ_EXECUTE, inode="/test/dir":someone2:supergroup:d-w--w--w-
$ ./bin/hdfs dfs -rm -r /test/dir
rm: Permission denied: user=someone2, access=ALL, inode="/test/dir":someone2:supergroup:d-w--w--w-
{code}, [~vagarychen], so this is a feature, not a bug?, [~dmtucker], I'm no expert on this code path, but given that it looks like delete dir is the only place where ignoreEmptyDir is set to true, I think this must be intentionally written (which makes this a feature), otherwise there is simply no need to even have this field.

[~wheat9], do you have any comments on this? Since this code is introduced in HDFS-7573 where you are the author., [~dmtucker] I did another quick, I think this is consistent to the POSIX syntax. Specifically, if you just do the same thing on any POSIX OS, it would have the same behavior of bypassing the permission check for empty directory. In this case, this is definitely a feature I think., [~vagarychen], I believe macOS is POSIX-compliant, and this is what I see on my MacBook Pro:

{code:none}
david@macbook:/tmp $ ls -ld .
drwxrwxrwt  12 root  wheel  408 Mar 30 22:46 .
david@macbook:/tmp $ mkdir foo
david@macbook:/tmp $ chmod 222 foo
david@macbook:/tmp $ ls -l foo
ls: foo: Permission denied
david@macbook:/tmp $ rm -r foo
rm: foo: Permission denied
david@macbook:/tmp $ rmdir foo
david@macbook:/tmp $ 
{code}

FWIW, I have observed similar behavior on OneFS., Interesting to see there seems to be a difference between "rm -r" and "rmdir"...But it seem to work in my CentOS environment....
{code}
[testuser@c6401 ~]$ mkdir test/empty
[testuser@c6401 ~]$ chmod 222 test/empty
[testuser@c6401 ~]$ ls test/empty/
ls: cannot open directory test/empty/: Permission denied
[testuser@c6401 ~]$ rm -r test/empty/
[testuser@c6401 ~]$ ls test/
{code}
So I guess this behaviour might be undefined even for Linux systems...I tracked the code a bit further, this behaviour in HDFS has been there for several years, changing this might cause compatibility issues. Also this should not cause any security concerns because this happens only when the directory is already empty. So I would prefer to just leave it like what it is now, and just be aware of this behaviour.]