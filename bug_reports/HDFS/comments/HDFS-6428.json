[Submitted simple patch to address the issue.

Hi [~cmccabe], 

Thanks for the advice earlier. Would you please help reviewing this one? Thanks.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12645441/HDFS-6428.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6931//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6931//console

This message is automatically generated., Do we know what else is modifying {{bpSlices}} and causing the CME?  Hopefully we aren't masking another bug., bq. Do we know what else is modifying bpSlices and causing the CME? Hopefully we aren't masking another bug.

it seems like {{FsDatasetImpl#shutdownBlockPool}} calls {{FsVolumeList#removeBlockPool}}, which then calls {{FsVolumeImpl#shutdownBlockPool}}.  That last function removes the entry from {{bpSlices}}.  The key to understanding this change is to realize that {{FsVolumeImpl#shutdownBlockPool}} is always done under the {{FsDatasetImpl}} lock, since the first function in the chain (inside {{FsDatasetImpl}}) is synchronized.

So to sum up: if we don't want to get CMEs, we need to add {{synchronized (dataset)}} blocks around the stuff that is accessing {{bpSlices}}.  It looks like this has been done in a few places, but not in others, and there is a TODO in the code to fix this.

Yongjun, can you put a {{synchronized (dataset)}} block inside {{FsVolumeImpl#shutdownBlockPool}}?  Otherwise, it is confusing to realize that this must be done under the lock.  Since Java monitors are re-entrant, this will work fine.

+1 once this is addressed., Hi [~daryn] and [~cmccabe], 

Thanks for your comments. 

For Daryn's, I attempted to figure out exactly what else is modifying bpSlices by adding some debug printing, and the CME went away. The nature of the problem is the intermittency. I think Colin's analysis of what could have happened by looking at the source code makes sense. I did have samilar observation and actually had one attempt to add synchronized syntax to ALL methods that access bpSlices before posting the patch, it turned out the run was really slow, and it looked an overkill. So I ended up adding only the place that showed CME. 

I will take Colin's suggestion to add one to shutdownBlockPool, to see if it cause any significant run time increase, hopefully not because it's at shutting down stage. Still in theory we should synchronize all access. Thanks.

, bq. I will take Colin's suggestion to add one to shutdownBlockPool, to see if it cause any significant run time increase, hopefully not because it's at shutting down stage. Still in theory we should synchronize all access. Thanks.

It is not going to cause any change in run-time, because the method is already synchronized.  Did you understand the analysis I posted above?

bq. I did have samilar observation and actually had one attempt to add synchronized syntax to ALL methods that access bpSlices before posting the patch, it turned out the run was really slow, and it looked an overkill.

Why don't you try making it a reader-writer lock?  The number of times we modify bpSlices is truly tiny... it should happen when the DataNode is starting up and shutting down, but not otherwise., HI Colin, 

Yes, I think I understood what you were talking about. My thinking is that adding synchronized syntax to all the methods that access {{bpSplices}} seem to have slowed down things (in one attempt I did), and I suspect it is the other accesses (not the shutdownBlockPool) that caused the run to be slow.

What about I will post result after testing out my original change + the addition you suggested (the test has been running for some time), and we then decide whether we want to change to reader-writer lock?  Thanks.





, Thanks Colin for the off-line discussion. He suggested to find out what caused the runtime increase, and I figured out it's the highlighted line in the following block that caused long runtime after adding synchronize statement, which is understandable because multi-threads are synchronizing here. Actually without the synchronization, it would not be safe.

{code}
  void addBlockPool(final String bpid, final Configuration conf) throws IOException {
    ...
    List<Thread> blockPoolAddingThreads = new ArrayList<Thread>();
    for (final FsVolumeImpl v : volumes) {
      Thread t = new Thread() {
        public void run() {
          try {
            ...
            v.addBlockPool(bpid, conf); <=====================if synchronized, caused slow performance.
            ...
{code}

Why we did not run into problem easily at the above highlighted line? this question made me realize that bpSlices is {{ConcurrentHashMap}}, which is designed to take care of most of concurrency issue:
{code}
The allowed concurrency among update operations is guided by the optional concurrencyLevel constructor argument (default 16), which is used as a hint for internal sizing. The table is internally partitioned to try to permit the indicated number of concurrent updates without contention. Because placement in hash tables is essentially random, the actual concurrency will vary. Ideally, you should choose a value to accommodate as many threads as will ever concurrently modify the table. 
{code}
So I think adding another level of synchronization for the addBlockPool and some of the other operations are not necessary (though some may really need). The real fix should be based on the ConcurrentHashMap requirements.

The other day when I worked out the patch, it was very reproducible in my env, but now it's not unfortunately (because I cleaned my build, and the nature of this issue), so I can't verify whether a new fix resolve the problem. I will keep watching if I can see this issue again.

BTW [~daryn], thanks for your "Do we know what else is modifying bpSlices and causing the CME? Hopefully we aren't masking another bug." comments. The discussion we had so far is along this line.

Thanks., bq. So I think adding another level of synchronization for the addBlockPool and some of the other operations are not necessary (though some may really need). The real fix should be based on the ConcurrentHashMap requirements.

I think the issue might be this code:

{code}
    Set<Entry<String, BlockPoolSlice>> set = bpSlices.entrySet();
    for (Entry<String, BlockPoolSlice> entry : set) {
      entry.getValue().shutdown();
    }
{code}

An iterator on a {{ConcurrentHashSet}} is thread-safe, but is an iterator on an {{EntrySet}} derived from a {{ConcurrentHashSet}} thread-safe?  I think it may not be.

I think our choices here are getting rid of the {{ConcurrentHashMap}} and just having a reader-writer lock and a normal hash map, or taking a look to see if we can fix issues like the one above., HI Colin, 

I agree that the problematic code is as you described.  There is the option of using the patch I posted (which addresses this piece of problematic code, and now we have better understanding), and the other option you suggested to use readwrite lock. Right?  Since I was not able to reproduce this problem now, I can not easily verify the fix.

Thanks.

, In my experience, CHM's entry set is thread-safe per the javadocs: The view's iterator is a "weakly consistent" iterator that will never throw ConcurrentModificationException, and guarantees to traverse elements as they existed upon construction of the iterator, and may (but is not guaranteed to) reflect any modifications subsequent to construction.

In my experience, the iterator tends to return items already removed by other threads.

A cursory scan of the code makes it look like {{bpSlices}} is going to be a highly contended resource.  Adding a RW lock is better than sync, but both are undesirable.  Is this actually a faulty shutdown procedure?  Ie. Should the shutdown only be performed after other threads that might mutate the CHM have exited?  Again, I haven't studied the code so maybe I'm way off., bq. In my experience, CHM's entry set is thread-safe per the javadocs: The view's iterator is a "weakly consistent" iterator that will never throw ConcurrentModificationException, and guarantees to traverse elements as they existed upon construction of the iterator, and may (but is not guaranteed to) reflect any modifications subsequent to construction.

That's what I thought, too.  I guess what we need to explain is this stacktrace:

{code}
org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes  Time elapsed: 3.771 sec  <<< ERROR!
java.util.ConcurrentModificationException: null
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
        at java.util.HashMap$EntryIterator.next(HashMap.java:934)
        at java.util.HashMap$EntryIterator.next(HashMap.java:932)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:251)
{code}

Perhaps the base class' iterator implementation is getting used instead of ConcurrentHashMap's?, I only have JDK 7 handy and the debugger shows it's using concurrent classes for the set and iterator. Maybe JDK6 is flawed. It's probably better to iterate the values since the keys aren't used, and perhaps that will correctly use a concurrent iterator., FYI, all my runs were using JDK 7 (jdk1.7.0_45 for one of the runs that had the problem).
, Very sorry guys my bad! The failure stack was created with another source base, I just noticed the diff between trunk and the other source base, which uses regular HM rather than CHM. See HDFS-5705. The explains the "mystery" about the stack, and why I never reproduced the issue on trunk.

, bq. Very sorry guys my bad! The failure stack was created with another source base

That explains it.  If there's nothing left to do here, can you close this as invalid?  thanks, Thanks you guys for the discussion, just closed it as Colin suggested.
, BTW, the synchronized blocks described in the comment below existed before HDFS-5705, or before HM was replaced with CHM. These synchronized blocks are for updating data and collecting data from the map (not add, remove entry), so there is the need to have them to block other threads from changing the data.
{quote}
So to sum up: if we don't want to get CMEs, we need to add synchronized (dataset) blocks around the stuff that is accessing bpSlices. It looks like this has been done in a few places, but not in others, and there is a TODO in the code to fix this.
{quote}
Thanks.
]