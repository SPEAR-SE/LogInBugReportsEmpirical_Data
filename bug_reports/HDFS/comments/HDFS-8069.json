[Hi [~elserj],

I'm not familiar with the Accumulo span receiver.  Just off of the top of my head:

1. Accumulo uses HDFS underneath.  When processing trace spans using Accumulo, are you creating more trace spans inside HDFS?  This will lead to a kind of infinite recursion.

2. You should be tracing less than 1% of all requests.  We don't really support tracing 100% of all requests on a 300 MB file, or at least not without serious performance degradation.

There are ways to avoid issue #1.  The easiest way is to use htraced, a trace sink built specifically for the purpose of storing spans.  htraced is also developed inside the HTrace project rather than externally.  Issue #2 can be fixed by setting an appropriate sampler such as ProbabilitySampler.

I do think we could potentially make the read pathway less "chatty" but that's somewhat of a separate issue.  No matter how few spans we create on the read pathway, you still will have problems with issue #1 and #2 if you have not configured correctly., I can think of a few ways to solve issue #1:

1. Disable tracing in Hadoop, by setting {{hadoop.htrace.sampler}} to {{NeverSampler}}.  Needless to say, this will allow you to get tracing from Accumlo, which you have currently, but not Hadoop.  So it's not a regression but it won't give you additional functionality.

2. Send the trace spans to a different Accumlo instance than the one you are tracing.  The different Accumlo instance can have tracing turned off (both Accumlo tracing and Hadoop tracing) and so avoid the amplification effect.

3. Just use htraced.  We could add security to htraced if that is a concern.

I wonder if we could simply have Accumlo use a shim API that we could later change over to call HTrace under the covers, once these issues have been worked out.  I'm a little concerned that we may want to change the HTrace API in the future and we might find that Accumlo has done some stuff we weren't expecting with it.  What do you think?, We aren't tracing in the span collector.  We are only tracing one Accumulo operation, but it is a fairly complex operation.  So even if we traced this operation less often, we would still run into this issue.  I'm not sure I understand how the DFSInputStream tracing is supposed to be used.  Is it possible introduce sampling of DFSInputStream read operations within a current trace that has been enabled?  I'm also confused about why it would create a span for a single read operation, which is usually just pulling some bytes from an in-memory buffer (?), rather than only creating spans in the BlockReaders., Thanks for chiming in, [~billie.rinaldi]. I had been chatting with her about what I was seeing.

I tried to break down what I see as the problem to the most trivial usecase, but perhaps I didn't do it well enough the first time. Take a class

{code}
public class Foo implements Writable
{code}

I write some instances of this class to a file in HDFS, and then later read them back out again:

{code}
FSDataInputStream inputstream = filesystem.open(new Path("/my/file"));
for (int i = 0; i < 100; i++) {
  Foo myFoo = new Foo();
  myFoo.readFields(inputstream);
}
{code}

As Billie said, the above is one step in a larger traced operation in Accumulo, but we *do* want to have this information from HDFS (e.g. is the time due to something wrong in Accumulo or HDFS, etc). It just struck me as extremely odd that something as (seemingly) simple as this would cause me such performance issues. Maybe the answer is "don't do that"? I just wanted to bring it up because it came across as very unexpected to me., With regards to your other points:

Comments to solutions on point 1:
# As Billie said, we're not tracing the tracing code :). 
# A non-starter for me. We've had distributed tracing support built into Accumulo for years without issue. To suddenly inform users that they need to spin up a second cluster is a no-go.
# If htraced had support for Accumulo as a backing store, I'd jump for joy. But, running one big-table application at a time is more than enough for me. Security isn't really relevant here -- there's more to Accumulo than just the security aspect. Kind of goes back to point 2: we have this support internally to Accumulo for some time. We really want to see it transparently go down through HDFS for the added insight.

Point 2:
Again, I think Billie got this already: this was caused by the tracing of a single operation. The traced operation in Accumulo read a file off of disk. Performance tanked due to excessive spans from one parent span.

bq. I wonder if we could simply have Accumlo use a shim API that we could later change over to call HTrace under the covers, once these issues have been worked out. I'm a little concerned that we may want to change the HTrace API in the future and we might find that Accumlo has done some stuff we weren't expecting with it. What do you think?

It would certainly be much nicer to get rid of our tracer sink code and push it up into HTrace. Catching API changes early (instead of after a new HTrace version was released and Accumulo tried to use it) is ideal. Perhaps this is something we can start considering. The other side of the coin is that we could (will) be a good consumer that will try to hold you to some semblance of a stable API. Either way, a good discussion we can have over in HTrace rather than here :), bq. Is it possible introduce sampling of DFSInputStream read operations within a current trace that has been enabled?

No. There are some discussions in HADOOP-11758 and HTRACE-69.
, From HTRACE-69:
bq. If DFSInputStream#byteArrayRead is being too chatty, another option is to get rid of that trace span (and the ones on the BlockReader#read methods). We could just trace the functions which refill the buffers inside the BlockReader objects. That is the main operation that is time-consuming, so it might be more appropriate to do that anyway.

I think this might be the case.  Creating spans for byte array reads of one byte or more effectively makes us unable to trace client operations if they happen to use DFSInputStream, which we are using to read walogs.  Operations involving Accumulo's RFiles seem to be in better shape since we are reading blocks from them., bq. Josh wrote: As Billie said, we're not tracing the tracing code .

Thanks for confirming this.  Just to double-check, can you confirm that you have {{hadoop.htrace.sampler}} set to nothing (the default).

bq. Josh wrote: \[a second cluster is\] A non-starter for me. We've had distributed tracing support built into Accumulo for years without issue. To suddenly inform users that they need to spin up a second cluster is a no-go.

Understood.  I think that the configuration you outlined, where {{hadoop.htrace.sampler}} is set to NeverSampler (or left unset) and all sampling happens at the level of Accumulo, should work.  We just need to fix the issues that we have currently.

bq. Billie wrote: I think this might be the case \[that HDFS tracing is too chatty\]. Creating spans for byte array reads of one byte or more effectively makes us unable to trace client operations if they happen to use DFSInputStream, which we are using to read walogs. Operations involving Accumulo's RFiles seem to be in better shape since we are reading blocks from them.

I am going to open an issue in HDFS to only trace the cases where we actually fill the buffer of the HDFS BlockReader.  I think that it's a reasonable tradeoff to make, given that filling the HDFS BlockReader buffer tends to be the main thing that delays readers from HDFS.  Just reading a byte from the in-memory buffer that already exists very seldom causes any delay, if ever.

bq. Billie wrote: We are only tracing one Accumulo operation, but it is a fairly complex operation. So even if we traced this operation less often, we would still run into this issue

If the Accumlo operation is big enough, it may be necessary to split it into multiple HTrace spans.  For example, I think tracing an entire compaction would be too big.  We may have to experiment with this somewhat., bq. Thanks for confirming this. Just to double-check, can you confirm that you have hadoop.htrace.sampler set to nothing (the default).

Sorry I took so long: Yes, I explicitly set {{hadoop.htrace.sampler}} to NeverSampler and re-ran the test with the same end result.

bq. I am going to open an issue in HDFS to only trace the cases where we actually fill the buffer of the HDFS BlockReader. I think that it's a reasonable tradeoff to make, given that filling the HDFS BlockReader buffer tends to be the main thing that delays readers from HDFS. Just reading a byte from the in-memory buffer that already exists very seldom causes any delay, if ever.

Agreed. Thanks for doing this.

bq. If the Accumlo operation is big enough, it may be necessary to split it into multiple HTrace spans. For example, I think tracing an entire compaction would be too big. We may have to experiment with this somewhat.

Agreed on experimentation. Personally, I'd love to be able to know "is a compaction taking long because I'm waiting on HDFS?", "is there an inefficiency in how we read/write the bytes in Accumulo?". I think a happy-medium just needs to be found.

Thanks again for your time with this.]