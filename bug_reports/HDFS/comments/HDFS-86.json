[This is a good one! This might be one of the reasons why we sometimes see under-replicated blocks in a cluster. When you restart the namenode, does this problem get rectified automatically?, Yes, it seems that the under-replicated blocks are gone after restarting the namenode., This patch makes sure that blocks are removed from namespace only after they are removed from a datanode.

It adds a new data structure to FSNamesystem, pendingDeleteSets, which keeps track of all the blocks that are deleted from datanodes but have not been removed from the namespace yet.

Functionally it makes 4 changes:
1. InvalideateBlock does not remove a block from its namespace.
2. When process a heartbeat, if the namenode instrcutes the datanode to remove blocks, all these blocks are moved to pendingDeleteSets.
3. When ReplicationMonitor, the background computation thread, wakes up to work, it removes blocks in pendingDeleteSets from the namespace if there is any.
4. This patch exposed a bug in the ChecksumException handling. Currently the code calls seekToNewSource to select a different replica. But it turned out that a following seek/read still tried to select a replica. Sometimes it happens to be the problematic replica. So this patch makes sure that a seek/read following seekToNewSource does not select a new source.
, I will look at the code more closely, but the approach sounds pretty good. Like we discussed, this approach still cannot solve the race condition entirely. In this approach, we ensure that the namenode has sent out the delete-block request before attempting to allocate the same block on the same datanode, but these requests could still get *processed* on  the datanode out-of-order. This fix reduces the race-window to a minimum.

The other side-effect is that a client than opens the file might try a bad block replica for a longer time (because the block does not get deleted from the blocksmap for a longer time) but this irritant should be minor at best and can be ignored., It's unclear whether this problem actually exists., i have a cluster of two nodes. Say a block with 2 replicas, and one of them get corrupted.
The corrupted block is reported to NN, but it is never deleted or replicated, even after NN restarts.
Not sure this is a bug or just a policy.
I am playing the append-trunk, This jira is too old. It should be closed.

Now HDFS has a different policy with corrupt replicas. A corrupt replica does not get deleted until a good replica gets replicated. 

The problem you have is caused by the 2-node cluster. Because it does not an extra node to place the good replica, the corrupt one never gets deleted. If you add one more node to the cluster, the problem will go away. ]