[Hi [~wheat9] and [~jingzhao],

Thanks for the discussion in HDFS-6776. 

I described the two proposed solutions for addressing HDFS-7036 in the jira description. I think adding the msg-parsing hack in webhdfs seems to be the right approach so we don't have to put the similar hack in different applications. For example, even "hadoop fs -lsr webhdfs://<insecureCluster>" is broken for the same reason.

The argument against this approach was, the webhdfs has a large audience, thus could be an overkill. Would you please provide some more detail here? 

Assuming that we don't put the hack in webhdfs, would you please share some thoughts about the right solution?

Thanks a lot.
 


, Hi [~wheat9] and [~jingzhao],

I agree with [~tucu00] that this is a quite important thing to do to help user:
{quote}
IMO it will be a pita for users with multiple clusters the fact that we are not parsing the exception message. Please lets have a follow up JIRA for it.
{quote}

Not sure whether you have time to comment soon. To establish a base for further discussion, I uploaded the similar approach (rev 001 here) we proposed earlier in HDFS-6776. I'd really appreciate if you guys can comment and we can converge to a solution.

BTW, since the IOException("Failed to get the token .." ) is now removed by HDFS-6776, I didn't introduce a string constant for that. The msg parsed by rev 001 is the sole occurence.

Thanks. 
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12668367/HDFS-7036.001.patch
  against trunk revision 78b0483.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8006//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8006//console

This message is automatically generated., Hi [~wheat9] and [~jingzhao], 

It has been quite a while since I created this jira as a follow-up of HDFS-6776, as we agreed in the discussion there. Would you please comment here?

Thanks a lot.
, My position has not changed since HDFS-6776. Just to recap:

# This use case is so specific to distcp that it should be fixed distcp instead of the underlying filesystem implementation.
# This is a hack for compatibility. There are many more users of {{WebHdfsFileSystem}} compared to distcp. It is more reasonable to contain the changes at higher layers (i.e. distcp) to avoid surprises to other applications.

I understand that hacking {{WebHdfsFileSystem}} is an easy enough fix, I also understand that a hack might be needed either here or there, but parsing the exception message in {{WebHdfsFileSystem}} does not seem the right solution here.

If it turns out that this type of hack is unavoidable, I suggest doing it in distcp.

, HI [~wheat9], Thanks for your comments. 

Would you mind also comment on how we are going to deal with the similar failure of issuing
{code}
hadoop fs -ls <insecureCluster>
{code}
from secure cluster side? There is the possibility that other applications too. Given it's a hack, wonder if you would suggest to have the same hack to different places?

Would you please give some *concrete* example about the potential damage it would cause if we have the hack in webhdfs? I think that would help the discussion most here.

To me, it's an easy and clean solution to have this hack in webhdfs, so all applications are taken care of by the solution; and it's going to be easy to take out this hack in the future when it's the time.

On the other hand, adding the hack in distcp and other applications, the solution in each application is going to be more complicated than the webhdfs one, and it has the potential to introduce more instability to the software than the potential damage I can see with the webhdfs solution. So the *concrete* example I hoped you could give can help the discussion here.

Thanks a lot.
, bq. ...adding the hack in distcp and other applications...

Can you point out what the other applications are that need to access both secure and insecure cluster at the same time? To my best knowledge, distcp is the only use case. The title of this jira suggests it is focusing on distcp.

In contrast, YARN, MR and other applications that are built on top of them use {{WebHdfsFileSystem}} extensively.

My message has been consistent since HDFS-6776 -- the hack should be contained which result minimal damage in the codebase. I cannot +1 for the approach on hacking {{WebHdfsFileSystem}} just for this issue.

If you still don't get it, it might be helpful to go through the distcp code first., Hi [~wheat9],

One example I described several times and in my previous comment is issuing {{hadoop fs -ls <insecureCluster>}} from secure cluster side. It's broken for the same reason. Do you suggest to hack the fsshell too or leave it as broken as is?  In addition, imagine user could write their own applications to access data in both secure and insecure cluster.

Would you please provide some *concrete" example of potential damage?

Thanks.








, Hi [~wheat9],

Hope my answers yesterday addressed your questions. In case not, here is another attempt:
{quote}
Can you point out what the other applications are that need to access both secure and insecure cluster at the same time? To my best knowledge, distcp is the only use case. The title of this jira suggests it is focusing on distcp.
{quote}
Even though the goal of HDFS-6776 and this jira is to fix distcp, the real issue is that *webhdfs is broken* when accessing insecure cluster from secure cluster side, for the same reason. Distcp is just one use case. The fsshell example I gave in the jira description is another. If user has two clusters (secure and insecure), there is chance that user has the need to write applications that access data in a similar fashion.  Do you not agree that we should fix all these cases?

{quote}
My message has been consistent since HDFS-6776 â€“ the hack should be contained which result minimal damage in the codebase. I cannot +1 for the approach on hacking WebHdfsFileSystem just for this issue.
{quote}
Yes, I can see you said this in many comments. But would you please explain with *real* example about the damage of hacking in webhdfs? This is what I did not get from your earlier comments, and I have been asking for.

{quote}
If you still don't get it, it might be helpful to go through the distcp code first.
{quote}
Given that there is already  message parsing in webhdfs code (see HDFS-7026), and there is no complaint about it, there seems to be no real damage, except it's a bit hacky. 

Given the simplicity of this solution I posted that fixed all the above mentioned cases, and no real damage, what I really don't get is, why go with the more complex solution (the complexity of fixing distcp, plus fsshell, any application that user might write with same need)?

After all, it's just a hack that we try use to achieve better user experience, and  we will take out when it's the time. If we hack all over the places, to take the hack out would be costly too. 

Even if it's just for distcp, I think simplicity should be favored if there is no real damage.

Thanks.
 , Another point to emphasize, the *root cause* of the issue we try to solve in HDFS-6776 and HDFS-7036 is that *webhdfs is broken*. IMO, to fix the root cause, the solution would be in webhdfs, and fixing elsewhere would be like the widely heard old Chinese saying "zhi4 biao1 bu2 zhi4 ben3" (solve the symptom but not the root problem).

In summary, I favor fixing webhdfs for the following reasons:
## fixing root cause
## simplicity
## no real damage

Thanks.
, My understanding is that the main point of disagreement here is about whether there is real damage. I have been asking for *concrete* example of damage, which I don't see in any prior comments. I'd appreciate it if it's provided. Thanks.


, Hi [~wheat9],

Although I still think the right solution is to fix webhdfs because the root cause is that webhdfs is broken,  since we really had an disagreement here, I looked into how to fix in distcp you proposed.  Below is what I found. I would appreciate if you can tell if my analysis below makes sense to you.

As a refresher, below is the interesting part of the stack trace (reported in HDFS-6776) relevant to the discussion here.

{code}
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run(WebHdfsFileSystem.java:440)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.listStatus(WebHdfsFileSystem.java:1018)
	at org.apache.hadoop.fs.Globber.listStatus(Globber.java:69)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:217)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1623)
	at org.apache.hadoop.tools.GlobbedCopyListing.doBuildListing(GlobbedCopyListing.java:77)
	at org.apache.hadoop.tools.CopyListing.buildListing(CopyListing.java:81)
	at org.apache.hadoop.tools.DistCp.createInputFileListing(DistCp.java:342)
	at org.apache.hadoop.tools.DistCp.execute(DistCp.java:154)
{code}

To fix in fistcp, we should look into the boundary portion between distcp and file system in the above stack trace:

{code
	at org.apache.hadoop.tools.GlobbedCopyListing.doBuildListing(GlobbedCopyListing.java:77)
	at org.apache.hadoop.tools.CopyListing.buildListing(CopyListing.java:81)
	at org.apache.hadoop.tools.DistCp.createInputFileListing(DistCp.java:342)
{code}

The boundary code in {{GlobbedCopyListing.doBuildListing}} is below 
{code}
  for (Path p : options.getSourcePaths()) {
      FileSystem fs = p.getFileSystem(getConf());
      FileStatus[] inputs = fs.globStatus(p); <================this is where the exception is thrown

      if(inputs != null && inputs.length > 0) {
        for (FileStatus onePath: inputs) {
          globbedPaths.add(onePath.getPath());
        }
      } else {
        throw new InvalidInputException(p + " doesn't exist");        
      }
    }
{code}

I assume your proposed solution is to catch the exception here and do a hacked file listing using the {{p}} in the above code. However, this hack won't work for *wildcard* case:, e,g,:

{code}
 hadoop  distcp webhdfs://<insecureCluster>/path/* webhdfs://<secureCluster>
{code}

We won't be able to find what the wildcard will expand to, which is supposed to be got by {{FileStatus[] inputs = fs.globStatus(p);}}. Unless we hack into the file system.

Did I misunderstand your approach? I'd appreciate if you could comment. Thanks in advance.

BTW, You asked what applications other than distcp could apply in your last comment, I answered you with the "hadoop fs -lsr" example. Would you please confirm whether it addressed your  question? Do you think we should fix it too? Thanks.

, bq. I answered you with the "hadoop fs -lsr" example. Would you please confirm whether it addressed your question? Do you think we should fix it too? Thanks.

Fundamentally both distcp and ls are at the application layers. If you want to run ls in secure clusters to be able to list insecure clusters, then yes, it should be fixed. The problem is that whether it should be the default behavior, which is up to debate. I can see the values of both sides of the arguments. Practically it has not been a concern as {{ls}} only involves one cluster, and it is fairly easy to work around by overriding the configuration at the command line., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12668367/HDFS-7036.001.patch
  against trunk revision 972f1f1.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.tracing.TestTracing
                  org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotReplication
                  org.apache.hadoop.hdfs.server.namenode.TestFsck

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/10036//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/10036//console

This message is automatically generated.]