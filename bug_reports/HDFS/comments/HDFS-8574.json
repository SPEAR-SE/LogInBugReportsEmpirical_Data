[I don't know if it's necessary. If threshold is exceeded, the number of RPCs equals to the number of storages. The number of storages is not quite many, don't you think? What if threshold is exceeded per storage? The approach increases RPCs actually., Hi walter, thanks for that info. You are right, the number of RPCs is equal to number of volumes.
But in my scenario, there is one volume which contains files way more than {{dfs.blockreport.split.threshold}} (may be 10 times)

So the previous loop has created one report with all the blocklist on that volume
{code}
    for(Map.Entry<DatanodeStorage, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {
      BlockListAsLongs blockList = kvPair.getValue();
      reports[i++] = new StorageBlockReport(kvPair.getKey(), blockList); 
      totalBlockCount += blockList.getNumberOfBlocks();
    }
{code}
So next, when it tries to send this block report to NN, it receives 
{code}
java.lang.IllegalStateException: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:369)
        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:347)
        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder.getBlockListAsLongs(BlockListAsLongs.java:325)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.blockReport(DatanodeProtocolClientSideTranslatorPB.java:190)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:473)
{code}

So may be we can redesign so that multiple block reports can be sent per volume.? what do you suggest.?, Updated the issue based on your comment, Hi [~ajithshetty], 

Thanks for reporting this. 

bq. there is one volume which contains files way more than dfs.blockreport.split.threshold (may be 10 times)
The default value of {{dfs.blockreport.split.threshold}} is 1 Million. Even if you have a 10TB drive, 10 Million blocks per drive gives a mean block size of 1MB. HDFS is not designed to deal well with large numbers of such tiny blocks. Would you mind sharing some metrics about your target use case?, Hi [~arpitagarwal]

Thanks for the input. Yes you are right, HDFS was not designed for tiny blocks. My scenario was like, i wanted to test the NN limits so i inserted 10 million files with size ~10KB(10KB because i had smaller disk). My DN had one {{data.dir}} directory, when i faced this exception. But when i increased the {{data.dir}} to 5, the issue was resolved. Later i checked and came across this piece of code where the block report was sent per volume of DN. My question is when we check for overflow, based on number of blocks, then why we split based on report, as in a single report, there might be still overflow for given limit {{dfs.blockreport.split.threshold}}

Please correct me if i am wrong, Hi [~ajithshetty], I don't expect this limit to be hit for a single drive in a real deployment soon given the NN scalability limits and typical disk sizes. Splitting based on storage directories was easier to implement and will handles the block report size issue.

If you just want to test NN namespace/block space limits you can create multiple storage directories on a single volume. The free space calculations will be off but that won't matter for your test. If you feel strongly about splitting reports for a single storage we can keep this Jira around.

Also the message is likely due to hitting the default protobuf message size limit of 64MB which is hit around 9-10M blocks., Can we think about making the protobuf size configurable ? is it feasible ? , bq. Can we think about making the protobuf size configurable ? is it feasible ?
nijel, it is configurable via {{ipc.maximum.data.length}}., In that case, i think there is no need of any change.
User can configure this accordingly, since the scenario is to verify the limits.
Thanks, Closing this issue as per comments, [~arpitagarwal]/[~walter.k.su]/[~ajithshetty]/[~nijel]

Sorry for coming late.Had seen similar issue(where user might not aware ),I am thinking ,shutting down the Datanode is better when this issue occurs,as of now sliently skips {{blockreports}}.

any thoughts..?, Hi [~brahmareddy], it's non-trivial to handle it correctly. Shutting down the DataNode will trigger re-replication and could have a cascading effect.

I still don't expect this situation to be encountered in real clusters soon. [~jingzhao] planned some related work to split block reports per volume via HDFS-9011 (thanks to [~nijel] for commenting on HDFS-9011 which reminded me of that effort).]