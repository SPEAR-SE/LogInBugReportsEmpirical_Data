[This is a pretty corner case, and I stumbled on it accidentally.

When fs.default.name is set to "localhost.localdomain/127.0.0.1:55068", dfs client bails out with this exception. This is definitely a very improper value, but the behaviour sure can be better.

$ hadoop dfs -ls
Exception in thread "main" java.lang.StackOverflowError
        at java.net.URI$Parser.checkChars(URI.java:2980)
        at java.net.URI$Parser.parseHierarchical(URI.java:3066)
        at java.net.URI$Parser.parse(URI.java:3024)
        at java.net.URI.<init>(URI.java:578)
        at java.net.URI.create(URI.java:840)
        at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:126)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:118)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:202)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:118)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:202)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:202)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:118)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:202)
, This is the same as HADOOP-5901]