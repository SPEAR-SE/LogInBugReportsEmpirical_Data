[Colin mentioned on HDFS-6287 that we may also need to address the include of malloc.h on FreeBSD., This code in the stopwatch structure gets the rusage and stores it into {{struct rusage rusage;}} but it is never used. 
{code}
if (getrusage(RUSAGE_THREAD, &watch->rusage) < 0) {
        int err = errno;
        fprintf(stderr, "getrusage failed: error %d (%s)\n",
            err, strerror(err));
        goto error;
    }
{code}

Removing the block as to get REHL4 compiling again., I've actually never heard of anyone deploying Hadoop on RHEL4 (when it was released in 2005, Hadoop didn't exist).  However, I did manage to track down a RHEL4 virtual machine and try to compile.  I noticed that a bunch of stuff didn't work, not only {{vecsum}}.  For example, you get this error when building YARN:

{code}
     [exec] libcontainer.a(container-executor.c.o)(.text+0x593): In function `mkdirs':
     [exec] /home/cmccabe/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c:378: undefined reference to `mkdirat'
     [exec] libcontainer.a(container-executor.c.o)(.text+0x5b4):/home/cmccabe/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c:387: undefined reference to `openat'
{code}

It's going to be tough to "fix" this since RHEL4 just doesn't have big pieces of necessary functionality, like cgroups.

Maybe this is a dumb question on my part, but I wonder if compiling without {{Pnative}} is a possibility for you?  We dropped support for JDK5, which was released just a few months after RHEL4.  Is it realistic to compile trunk on a 10 year-old OS?  What do you guys think?

I definitely think we should fix the malloc.h thing, though, to compile on modern BSD systems., Correction: hadoop existed in the later part of 2005 according to Wikipedia.  But the first release of RHEL4 is still older :), Independently of the compatibility questions, let me review the patch.

* remove {{struct rusage}} if you are not going to use it
* please remove the include malloc.h line

+1 once those are addressed, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12645350/HDFS-6421.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6928//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6928//console

This message is automatically generated., Thanks [~cmccabe] for taking a reviewing the patch.
Attaching the new patch addressing your comments., Correction: Thanks [~cmccabe] for reviewing the patch. :-) , +1 pending jenkins, bq.  Is it realistic to compile trunk on a 10 year-old OS? What do you guys think?

I'm going to say yes: https://access.redhat.com/site/support/policy/updates/errata/

(Red Hat's policy is fairly standard vs. the rest of the industry, at least in my experiences.  So while clearly other vendors will have different policies, this one is a good representative sample, IMHO.)

Java should 'compile' with not too many issues, so we're really talking about the C code.  The last time I looked at the code, it wasn't that bad to rewrite the non-portable portions to be much more portable+feature/structure detection.  If this was done, then I'd expect it to work on 10-year-old OSes without too much effort.  The vast majority of calls we need to make aren't particularly new...

(FWIW, the Hadoop systems at Y! in early 2007 ran on RHEL4.  We started migrating to RHEL5 later that year in limited quantities.  Those servers were some of the first ones in the company to move to RHEL5 on the server-side.), bq. (Red Hat's policy is fairly standard vs. the rest of the industry, at least in my experiences. So while clearly other vendors will have different policies, this one is a good representative sample, IMHO.)

Let's not compare apples and oranges.  When Red Hat says they "support" 10 year old operating systems, it doesn't mean they compile the trunk versions of projects on them.  It means they backport a carefully selected set of security and robustness patches.

bq. Java should 'compile' with not too many issues, so we're really talking about the C code.

I think the issues in Java are pretty similar to the issues in C.  We can choose to support old versions of stuff, or not.  The Java equivalent of supporting a 10-year-old OS would be supporting JDK5, which we did not choose to do in Hadoop trunk.  If we choose not to support JDK5 or RHEL4, we are telling admins that use those products that they need to upgrade to use the latest version of Hadoop, which might be annoying for some.  But we gain the ability to use additional features that were not present in RHEL4 or JDK5, and write simpler and less buggy code because there are fewer special cases.

As it turns out, we don't support JDK5.  So it seems clear that people feel that 10 years is long enough (and some people even think we should drop support for JDK6 in branch-2, although I am not one of them.)  All I am asking is that we apply the same standards to the C code that we apply to the Java code.

bq. The last time I looked at the code, it wasn't that bad to rewrite the non-portable portions to be much more portable+feature/structure detection. If this was done, then I'd expect it to work on 10-year-old OSes without too much effort. The vast majority of calls we need to make aren't particularly new...

Alan, the C code in YARN depends extensively on {{cgroups}}.  This is a Linux feature which is not present before RHEL6.
You can read the manual here: https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/ch01.html  This isn't just a system call that got a new argument or something, it's an entire subsystem which is simply missing in the older OS, and which YARN uses to isolate processes., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12645560/HDFS-6421.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6939//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6939//console

This message is automatically generated., +1.  Thanks, Mit., SUCCESS: Integrated in Hadoop-trunk-Commit #5607 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5607/])
HDFS-6421. Fix vecsum.c compile on BSD and some other systems (Mit Desai via Colin Patrick McCabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1596324)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/vecsum.c
, FAILURE: Integrated in Hadoop-Yarn-trunk #563 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/563/])
HDFS-6421. Fix vecsum.c compile on BSD and some other systems (Mit Desai via Colin Patrick McCabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1596324)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/vecsum.c
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1781 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1781/])
HDFS-6421. Fix vecsum.c compile on BSD and some other systems (Mit Desai via Colin Patrick McCabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1596324)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/vecsum.c
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1755 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1755/])
HDFS-6421. Fix vecsum.c compile on BSD and some other systems (Mit Desai via Colin Patrick McCabe) (cmccabe: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1596324)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test/vecsum.c
]