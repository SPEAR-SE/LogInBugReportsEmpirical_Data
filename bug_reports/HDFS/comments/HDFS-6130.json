[but If I upgrade from 2.x to the trunk, that's ok for me., Add some addtional informations:
1)  I upgraded from 0.20.2-cdh3u1 to trunk with HA disabled  -  successful.
2)  stop HDFS, and enable HA
3) start journal nodes and run:  hdfs namenode -initializeSharedEdits - NPE
, This looks like not related with RU, [~jingzhao], [~wheat9] , I think this was caused by protobuf seralized fsimage. can you take a look?, I try another way, but still failed.
I upgraded from 0.20.2-cdh3u1 to 1.2.1 firstly - successful, then upgraded from 1.2.1 to trunk, also NPE

, again, I looked through the code, but still not sure the root cause.

If I upgrade from 0.20.2-cdh3u1 to 2.2.0(HA disabled), it's successful. and then from 2.2.0 to trunk(HA enabled), which is also successful., I believe that this is a duplicate of HDFS-5988., Fengdong, do you have HDFS-5988 in your build?  If not, could you try again with the latest version of trunk?

> This looks like not related with RU, ...

Let's revise the summary., [~szetszwo], I've included the HDFS-5988,  I tested trunk from Revision: 1579559, [~andrew.wang] , can you also take a look? , From what I gather, between Apache releases the upgrade works, right? If so, cdh release related issues perhaps are best taken up in a forum related to that?, [~sureshms] maybe you are right, I have three test clusters, only this one is cdh release.  others are all Apache release.

but cdh old release can upgrade to Apache 2.2.0 successfully. so I don't think this was caused by cdh release.
, I am not sure I resolved the root casue, but It works now., cacel patch, It loss data sometimes., bq. So I don't think this was caused by cdh release
I suspect this is due to some layout version mismatch issues. Given apache releases do not have this issue, I suggest closing this as invalid. If you want to explore fixing this further, I suggest looking at layout version issue, possibly related to inode id feature introduction and later features. 

Certainly turning of ha and upgrading is one option. The current patch is not a choice as it will ignore many paths on encountering an error., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12635948/HDFS-6130.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6462//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6462//console

This message is automatically generated., [~sureshms] , Thanks for your comments.

bq.. Given apache releases do not have this issue
Apache release also has this issue. Apache 1.0.4 upgrade to the trunk, you can reproduce this issue., Add some reproduce steps:
upgrade from Apache 1.x to the latest trunk as follows to reproduce this issue:

1) running a normal HDFS v1.x, then stop HDFS.
2) switch to the new deploy(trunk), without HA, run command: 'start-dfs.sh -upgrade', which should be successful.
3) stop the HDFS, enable QJM HA in the configuration, scp NAME-NODE-dir to the SNN
4) then start journal nodes, run command: 'hdfs namenode -initializeSharedEdits', - throws NPE here., update:

I miss a step between step 2 and step3. Add as step2.1, otherwise, all upgrades succeed.
step2.1:  
{code}
hdfs dfs -put test.data /
{code}

So, after upgrade from Apache1.x to the trunk, we MUST writer HDFS before ha enabled in the next step.
I don't find any unit tests cover this scenrio.
, > Apache release also has this issue. Apache 1.0.4 upgrade to the trunk, you can reproduce this issue.

Hi Fengdong, I just have tried it but cannot reproduce the NPE.  There were a log of changes since Apache 1.0.4.  I was using 1.3.0 in my test.  Could you also try it?, > I believe that this is a duplicate of HDFS-5988.

Hi [~wheat9], the stack trace posted here is indeed different from the one posted in HDFS-6021 (a dup of HDFS-5988).  So it seems that this is a different issue.  In this bug, FSImageFormatPBINode somehow passes a null inode to FSDirectory.  Could you take a look?

- Stack trace posted here 

{noformat}
14/03/20 15:06:42 FATAL namenode.NameNode: Exception in namenode join
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:120)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)
	...
{noformat}

- Stack trace posted in HDFS-6021 (a dup of HDFS-5988)

{noformat}
2014-02-26 17:03:11,755 FATAL [main] namenode.NameNode (NameNode.java:main(1351)) - Exception in namenode join
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:169)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:225)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:802)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:792)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:624)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:593)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:331)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:251)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:882)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:641)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:435)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:491)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:647)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:632)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1280)
	...
{noformat}
, It would be very helpful if the corresponding fsimage is available., Thanks [~szetszwo]!

[~wheat9], do you want only fsimage or both image and edit log? I'll reproduce today using 1.3.0 and the latest trunk, then I'll keep the corresponding fsimage and edit logs., Can you create a checkpoint so that upgrading from the checkpointed fsimage will triggered the bug?, OK, no problem, I can using  rollingUpgrade -prepare to create check point., hi [~szetszwo], where you get 1.3.0? , please ignore my create check point method, that's wrong., [~wheat9], 

fsimage was uploaded.
please read my following steps carefully before fix the bug.

1)There is no HA enabled during these steps.
2)all test files are all less than one block size

a. start hadoop-1.0.4 hdfs
b. put  one files on the hdfs
c. stop hdfs.
d. start dfs with upgrade option to the lastest trunk
e. put more than ten files on the hdfs
f. stop hdfs
g. start hdfs  (NPE here)

NOTE. if put a few files(such as one file) at step e, there is no NPE at step g.

, > ... where you get 1.3.0? 

It is the current [branch-1|http://svn.apache.org/viewvc/hadoop/common/branches/branch-1/]., I tried the new steps with current branch-1.  Cannot reproduce the NPE., I tried it with 1.0.4 (http://archive.apache.org/dist/hadoop/core/hadoop-1.0.4/).  Still cannot reproduce the NPE., This is similar to HDFS-5988. The attached fsimage has image version -32, where the local name optimization (version -38) was unavailable at that time.

HDFS-5988 has fixed the code path for fsimage that contains the local name optimization, that is, fsimage that is newer than the version -38, but not for the earlier fsimage. Due to the exact reason of HDFS-5988, the upgraded fsimage will be corrupted.

One can create a file inside a directory in the old cluster, and then upgrade to trunk to reproduce this bug.

I'll post a patch shortly.
, Raising the priority to blocker since it leads to data loss during upgrades., I've manually tested the patch against the attached fsimage. It resolves the NPE., +1 pending Jenkins., Thanks for the patch, I will test it.
, [~wheat9] I've tested it. It does works. thanks.

+1 for the patch., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12636772/HDFS-6130.000.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6506//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6506//console

This message is automatically generated., The test failure is irrelevant. I'll commit it shortly., SUCCESS: Integrated in Hadoop-trunk-Commit #5406 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5406/])
HDFS-6130. NPE when upgrading namenode from fsimages older than -32. Contributed by Haohui Mai. (wheat9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581713)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java
, I've committed the patch to trunk, branch-2 and branch-2.4. Thanks for [~jingzhao] for the review, and [~azuryy] for reporting the bug., And, Thanks  [~szetszwo] ., SUCCESS: Integrated in Hadoop-Yarn-trunk #521 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/521/])
HDFS-6130. NPE when upgrading namenode from fsimages older than -32. Contributed by Haohui Mai. (wheat9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581713)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1713 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1713/])
HDFS-6130. NPE when upgrading namenode from fsimages older than -32. Contributed by Haohui Mai. (wheat9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581713)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1738 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1738/])
HDFS-6130. NPE when upgrading namenode from fsimages older than -32. Contributed by Haohui Mai. (wheat9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1581713)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java
]