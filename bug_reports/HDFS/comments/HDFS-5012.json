[Here is the test output., Test output may not provide enough clue to find the cause.

If there is no objection, I plan to mark this dup of HDFS-3220, I think this is not a dupe of HDFS-3220. That jira is for changing the log to make debugging easier, right? Lets use this to understand/debug/fix the real issue., Should I close this JIRA ?, We had the same problem in our Hadoop cluster (not a test cluster, but a live cluster with real data, HDFS version: 2.0.0-cdh4.2.0):
{noformat}
Failed to obtain replica info for block (=BP-655596758-10.10.34.1-1341996058045:blk_2570851709037266390_1527175689) from datanode (=10.10.34.35:50010)


java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() >= recoveryId = 1527175689, block=blk_2570851709037266390_1527175689, replica=FinalizedReplica, blk_2570851709037266390_1527175689, FINALIZED
  getNumBytes()     = 48360562
  getBytesOnDisk()  = 48360562
  getVisibleLength()= 48360562
  getVolume()       = /var/lib/hdfs2/data/current
  getBlockFile()    = /var/lib/hdfs2/data/current/BP-655596758-10.10.34.1-1341996058045/current/finalized/subdir9/blk_2570851709037266390
  unlinked          =false
    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1451)
    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1411)
    at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:1920)
    at org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolServerSideTranslatorPB.initReplicaRecovery(InterDatanodeProtocolServerSideTranslatorPB.java:55)
    at org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(InterDatanodeProtocolProtos.java:2198)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1695)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1691)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1689)

    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
    at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)
    at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
    at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:1933)
    at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2000)
    at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:214)
    at org.apache.hadoop.hdfs.server.datanode.DataNode$2.run(DataNode.java:1905)
    at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() >= recoveryId = 1527175689, block=blk_2570851709037266390_1527175689, replica=FinalizedReplica, blk_2570851709037266390_1527175689, FINALIZED
  getNumBytes()     = 48360562
  getBytesOnDisk()  = 48360562
  getVisibleLength()= 48360562
  getVolume()       = /var/lib/hdfs2/data/current
  getBlockFile()    = /var/lib/hdfs2/data/current/BP-655596758-10.10.34.1-1341996058045/current/finalized/subdir9/blk_2570851709037266390
  unlinked          =false
    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1451)
    at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1411)
    at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:1920)
    at org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolServerSideTranslatorPB.initReplicaRecovery(InterDatanodeProtocolServerSideTranslatorPB.java:55)
    at org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(InterDatanodeProtocolProtos.java:2198)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1695)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1691)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1689)

    at org.apache.hadoop.ipc.Client.call(Client.java:1225)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
    at $Proxy11.initReplicaRecovery(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolTranslatorPB.initReplicaRecovery(InterDatanodeProtocolTranslatorPB.java:83)
    at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:1931)
    ... 4 more
{noformat}, Some additional information to the above stack trace: The error happened after a machine running a DataNode crashed and rebooted unexpectedly. The warning message was logged by that machine after the reboot when the DataNode process started. 

On three other DataNodes (those holding replicas of the block in question) the following error was logged:

{noformat}
PriviledgedActionException as:hdfs (auth:SIMPLE) cause:java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() >= recoveryId = 1527175689, block=blk_2570851709037266390_1527175689, replica=FinalizedReplica, blk_2570851709037266390_1527175689, FINALIZED
  getNumBytes()     = 48360562
  getBytesOnDisk()  = 48360562
  getVisibleLength()= 48360562
  getVolume()       = /var/lib/hdfs5/data/current
  getBlockFile()    = /var/lib/hdfs5/data/current/BP-655596758-10.10.34.1-1341996058045/current/finalized/subdir38/subdir48/blk_2570851709037266390
  unlinked          =false
{noformat}, Planning to resolve this since there has been no repro., FWIW we've seen it in real cluster, and it is a dup of HDFS-2970.
This is possible if the recovery command is issued multiple times and the first one arrives after the recovery completes., Mark it as dup of HDFS-2970. Todd in that jira had a detailed explanation how this scary message can be emitted.]