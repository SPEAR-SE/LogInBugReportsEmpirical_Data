[One possible solution I thought about is, whenever we need to delete a fileOrDirX, do

*  check permission recursively, 
*  if it's permitted to delete fileOrDirX, 
** rename it to a unique name fileOrDirX_to_be_deleted that client won't be using, 
** delete fileOrDirX_to_be_deleted. 

This will cause some confusion in the edit log though. Also if fileOrDirX is not permitted to be deleted, some sub dir / file in it may be deleted, so this operation need to be done at allowed dir/file in a recursively fashion, which may not be clean.




, Seems to be same as HDFS-7414, can you duplicate to HDFS-7414? Where I did not clearly mention the scenario..
Please let me know your opinion.. , How is {{isFileDeleted()}} check defeated? The check walks up the tree following the parent reference, not symbolically using path name. Creation of another directory (i.e. different INode) with the same name should not affect the check. , Hi [~brahmareddy] and [~kihwal],

Thanks a lot for your comments!

Currently {{isFileDeleted()}} does the following:
{code}
 if (tmpParent == null ||
          tmpParent.searchChildren(tmpChild.getLocalNameBytes()) < 0) {
        return true;
      }
{code}
which is to check whether a child name exists in parent directory. 
That's the part I was referring to that gets defeated.

I hope my understanding is correct. I described a possible solution as the first comment, would you please share some insight?

Thanks.
, {{isFileDeletec()}} is always called with the fsn lock held, so no modification is done while in the method and {{tmpParent}} is obtained by calling {{file.getParent()}}. So {{tmpParent}} cannot be a newly created directory inode, unless something is automatically setting the file inode's parent to the new directory inode.   If {{isFileDeletec()}} is called with a wrong file inode, then it is possible to hit this condition.  That means both the parent dir and the file were recreated and NN got confused.  Does this case also involve {{commitBlockSynchronization()}}?

, HI Kihwal,

Thanks a lot for your further comments. I did the analysis based on the edit log. I assumed {{commitBlockSynchronization()}} is involved due to the delayed block removal. Basically the same code path as examined by HDFS-6825. I will take a look at other path too.

Assuming {{commitBlockSynchronization}} is involved (. The {{iNodeFile}} is got by the following code:
{code}
    BlockCollection blockCollection = storedBlock.getBlockCollection();
    INodeFile iFile = ((INode)blockCollection).asFile();
{code}
Do you mean that we could get a wrong iFile here?

BTW, your comment rang a bell to me: when we delete a dir, what's the reason that {{tmpParent}} won't get a null at the {{dirX}} when trying to get the parent of {{dirX}} (if this happened)?
{code}
   while (true) {
      if (tmpParent == null ||
          tmpParent.searchChildren(tmpChild.getLocalNameBytes()) < 0) {
        return true;
      }
      if (tmpParent.isRoot()) {
        break;
      }
      tmpChild = tmpParent;
      tmpParent = tmpParent.getParent();
    }
{code}

Thanks.
, bq. Do you mean that we could get a wrong iFile here?
Since the block collection of a block won't magically get updated to a new inode file, I don't see how it can be a wrong inode file. So it may not be due to delayed block removal.

bq.  what's the reason that tmpParent won't get a null at the dirX when trying to get the parent of dirX (if this happened)?
If snapshot is not involved, the parent will be set to null during delete while in the fsn write lock. Lack of memory barrier can cause stale values to be used in multi-processor and multi-threaded env, but I am not sure whether that is the cause here.

If {{commitBlockSynchronization()}} was involved, was it initiated by client (e.g. revoerLease() or create/append() )?, Thank you so much Kihwal!

What happened was, the user manually delete the dir by issuing {{Hadoop fs –rm –r –skipTrash}} command. So it seems still related to delayed block removal. It appears that snapshot is involved but I will confirm.


 , I reproduced the issue by modifying the testcase I added for HDFS-6825, that is, do a mkdir of the same dir right after deleting it recursively.

, HI Kihwal and other folks who are watching,

I described a possible solution as the first comment of this jira. Now I am  thinking about a possibly cleaner one: if we have a dir/file creation time, then we can compare the creation time to determine that a dir is newer than the file.  I searched and found HADOOP-1377, which initially introduced creation time but dropped later per the discussion there. Due to the nature of delayed block removal, the scenario described in this jira is a valid case to handle. It seems having creation time would make the detection of deleted file much easier.

Well, when we copy file, if we use -p option to preserve attributes, including creation time, then the creation time of a file under a dir may be older than the dir's creation time. So this might not be foolproof.  If files/dirs under a dir have newer creation time than the parent, then it would work. Even if it works, existing clusters don't have creation time, it won't be a backward compatible solution. On the other hand, the first possible solution will be backward compatible.

Just throw the thoughts here, if any one has insight, I'd appreciate if could share.

Thanks.




 
, [~yzhangal] Can you post the test case for reference? , Hi Kihwal, thanks for your follow-up and the good suggestion. I just posted the test case change for reproduction as reproduceHDFS-7707.patch.
, Hi Kihwal,

Inspired by your comment 
https://issues.apache.org/jira/browse/HDFS-7707?focusedCommentId=14299106&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14299106

I think I have a better solution now. That is, instead of checking the name string, check the inode id. Comparing the inode id of the deleted file/dir against a newly created inode id will mismatch, thus the detecting that the file/dir was deleted.

Thanks.


, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696055/HDFS-7707.001.patch
  against trunk revision 8acc5e9.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9405//console

This message is automatically generated., Hi Kihwal,

I submitted patch rev 001 per the solution described in my last comment. Would you please help taking a look? thanks a lot!

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696056/HDFS-7707.001.patch
  against trunk revision 8cb4731.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestCommitBlockSynchronization

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9406//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9406//console

This message is automatically generated., I found that the test failure of {{TestCommitBlockSynchronization}} is caused by incorrect behaviour of {{parent.addChild(file) }} on a mocked {{INodeDirectory}} instance {{parent}}. That is, after {{parent.addChild(file) }}, {{parent}} doesn't have the child {{file}}. I wonder if anyone knows why.

I did a workaround by creating a new {{INodeDirectory}} instance, and uploaded patch rev 002.

Thanks.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696100/HDFS-7707.002.patch
  against trunk revision 8cb4731.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1198 javac compiler warnings (more than the trunk's current 1189 warnings).

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 5 warning messages.
        See https://builds.apache.org/job/PreCommit-HDFS-Build/9410//artifact/patchprocess/diffJavadocWarnings.txt for details.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9410//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9410//artifact/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9410//console

This message is automatically generated., Hi [~brahmareddy],

Sorry for a late clarification here about HDFS-7414, to address your earlier comment:

https://issues.apache.org/jira/browse/HDFS-7707?focusedCommentId=14298588&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14298588

My comments in HDFS-7414 earlier mentioned both HDFS-6527 and HDFS-6825. The code you pasted there indicates that the release you are running has HDFS-6527 but not HDFS-6825. 

HDFS-7707 is to remedy HDFS-6825 fix, for the special the case that a new dir is created with the same name as the previously deleted dir. It's possible HDFS-6825 alone can solve your issue (you can try now to see if that's the case since HDFS-6825 fix is already committed), or you need to wait for HDFS-7707 fix and combine with HDFS-6825 fix. 

Thanks.
, If snapshot is not involved, the parent of the file inode will be null, so the existing check should work. Your test case also reproduces the corruption only when snapshot is used. So it looks like your approach is correct. When walking up the tree for deletion check, it should look up the child with the same inode id. Since look up is only possible symbolically, the id needs to be compared if there is a hit.  I will spend a bit more time on this.

, The patch looks good except the unnecessary imports in FSNamesystem.java., Many thanks [~kihwal], for your review and comments! I just uploaded rev 003 to remove the unnecessary import. 

I also ran the failed test TestFailureToReadEdits locally and it was successful.
, As for {{TestFailureToReadEdits}}, the test is flawed. Since the port for qjm is hard-coded, it sometimes does not work. HDFS-6054 is supposed to fix it., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12696209/HDFS-7707.003.patch
  against trunk revision 80705e0.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9416//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9416//console

This message is automatically generated., +1., I've committed this to branch-2 and trunk. Thanks for reporting and fixing the issue, [~yzhangal]., SUCCESS: Integrated in Hadoop-trunk-Commit #6992 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6992/])
HDFS-7707. Edit log corruption due to delayed block removal again. Contributed by Yongjun Zhang (kihwal: rev 843806d03ab1a24f191782f42eb817505228eb9f)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
, Hi Kihwal,

Thank you so much for the quick help!

About HDFS-6054, I took a look, will post some comment there soon.

, Hi Kihwal,

Thanks a lot for your help with this jira and pointing me to HDFS-6054! I submitted a small patch to HDFS-6054, if time allows, could you help taking a look? Thank you so much!
 
Sorry I attached that patch to this jira by mistake, just removed it.
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #94 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/94/])
HDFS-7707. Edit log corruption due to delayed block removal again. Contributed by Yongjun Zhang (kihwal: rev 843806d03ab1a24f191782f42eb817505228eb9f)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #828 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/828/])
HDFS-7707. Edit log corruption due to delayed block removal again. Contributed by Yongjun Zhang (kihwal: rev 843806d03ab1a24f191782f42eb817505228eb9f)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2026 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2026/])
HDFS-7707. Edit log corruption due to delayed block removal again. Contributed by Yongjun Zhang (kihwal: rev 843806d03ab1a24f191782f42eb817505228eb9f)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #91 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/91/])
HDFS-7707. Edit log corruption due to delayed block removal again. Contributed by Yongjun Zhang (kihwal: rev 843806d03ab1a24f191782f42eb817505228eb9f)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #95 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/95/])
HDFS-7707. Edit log corruption due to delayed block removal again. Contributed by Yongjun Zhang (kihwal: rev 843806d03ab1a24f191782f42eb817505228eb9f)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2045 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2045/])
HDFS-7707. Edit log corruption due to delayed block removal again. Contributed by Yongjun Zhang (kihwal: rev 843806d03ab1a24f191782f42eb817505228eb9f)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, [~sjlee0] backported this to 2.6.1. I just pushed the commit to 2.6.1 after running compilation and TestCommitBlockSynchronization, TestDeleteRace which changed in the patch.
]