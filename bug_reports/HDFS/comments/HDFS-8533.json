[I've seen this issue as well, I'm running with a replication factor of 3 and I
get alerts because the metrics in JMX report missing blocks but fsck doesn't
see any. Here's some relevant output from {{fsck}} and {{JMX}} (redacted for
privacy reasons):

{code:title=hdfs fsck /}
(...)
................................................................Status: HEALTHY
 Minimally replicated blocks:   XXXX (99.99999 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       XXXX (0.57755035 %)
 Mis-replicated blocks:         XXXX (0.1345275 %)
 Default replication factor:    3
 Average block replication:     2.9942245
 Corrupt blocks:                0
 Missing replicas:              XXXX (0.19251679 %)
 Number of data-nodes:          XXXX
 Number of racks:               X
(...)
{code}


{code:title=JMX for Hadoop:service=NameNode,name=FSNamesystem}
(...)
"name": "Hadoop:service=NameNode,name=FSNamesystem",
"CorruptBlocks": 9,
"MissingBlocks": 9,
(...)
{code}

{code:title=JMX for Hadoop:service=NameNode,name=NameNodeInfo}
(...)
"name": "Hadoop:service=NameNode,name=NameNodeInfo"
"NumberOfMissingBlocks": 9,
(...)
{code}


h2. TL;DR;

h3. FSCK
- {{fsck}}: goes over all of the blocks, one by one and:
    - counts blocks as {{corrupt}} by querying {{LocatedBlock}}'s {{isCorrupt()}}
    - count blocks as {{missing}} if {{totalReplicasPerBlock == 0 && !isCorrupt}}

- {{fsck}} reports HEALTHY if there are no missing or corrupt blocks ({{L1078}})


h3. JMX

h4. CorruptBlocks
The internal metrics use {{CorruptReplicasMap}} which is ultimately updated via
{{FSNamesystem}}'s {{reportBadBlocks()}}. This means that the client is
reporting some bad block locations.


Thus, the mismatch is between what triggers the client to call
{{reportBadBlocks()}} and {{LocatedBlock}}'s {{isCorrupt()}} for those blocks.


Looking at who calls {{FSNamesystem}}'s {{reportBadBlocks()}}:

{code:title=NameNodeRpcServer.java}
 895   /**
 896    * The client has detected an error on the specified located blocks.
 897    * and is reporting them to the server.  For now, the namenode will.
 898    * mark the block as corrupt.  In the future we might.
 899    * check the blocks are actually corrupt..
 900    */
 901   @Override // ClientProtocol, DatanodeProtocol
 902   public void reportBadBlocks(LocatedBlock[] blocks) throws IOException {
 903     checkNNStartup();
 904     namesystem.reportBadBlocks(blocks);
 905   }
{code}

So, unlike {{fsck}}, there is no check in there to filter blocks that are
reported as being bad by the datanodes but that are actually not corrupt if you
check {{LocatedBlock}}'s {{isCorrupt()}} after they've been received.


h4. MissingBlocks
The outcome here is the same as {{CorruptBlocks}}, because the update to
{{neededReconstruction}} ({{LowRedundancyBlocks}})also happens within
{{markBlockAsCorrupt}}, by checking {{isPopulatingReplQueues()}}.

The internal metrics use this.neededReconstruction.getCorruptBnlockSize();

This is pretty broken if we compare it to the approach in {{fsck}} because we
are counting corrupt blocks as missing while in {{fsck}}, only those with no
replicas are accounted.

This also helps understand why the {{CorruptBlocks}} always matches
{{MissingBlocks}} in JMX.


h3. Conclusion

What I'd do is:

- filter the bad blocks on reception, within {{NameNodeRpcServer.java}}'s
  {{reportBadBlocks}} and only mark as bad those that pass {{LocatedBlock}}'s
  {{isCorrupt()}}, like {{fsck}} already does.

- Wishlist: keep track of the number of mis-reported blocks in the cell and/or
  which datanodes mis-report blocks as being bad, since this will probably help
  find out bugs and/or bad datanodes / hardware, etc.

- make sure {{MissingBlocks}} are accounted in a similar fashion between
  {{BlockManager}} (JMX) and {{NamenodeFsck}} ({{fsck}}). For the record, I
  think {{fsck}} does The Right Thing (TM): count as {{missing}} only the
  blocks that are completely unavailable *and* that are *not* corrupt.


Finally, it'll be interesting to find out what triggers the Datanode to report
bad blocks that are actually not bad. I'll dig deeper and see if there's
something broken somewhere in my cluster :D, Any update on this? I see the same behavior., I think, I'm concerned too on 2.7.3 using HDP 2.6
Ambari reports a missing block (get from JMX) that I can't find with fsck., Hi
As fsck is ok, is there a way to identify the file or block raised as failed by JMX for trying to fix it manually ?, I've been trying to reproduce this bug following the description in this jira, without success.

Here's my test code for future reference:
{code:java}
/** check if nn.getCorruptFiles() returns a file that has corrupted blocks */
  @Test (timeout=300000)
  public void testListCorruptFilesCorruptedBlock2() throws Exception {
    MiniDFSCluster cluster = null;
    Random random = new Random();

    try {
      Configuration conf = new HdfsConfiguration();
      conf.setInt(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_INTERVAL_KEY, 1); // datanode scans directories
      conf.setInt(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY, 3 * 1000); // datanode sends block reports
      // Set short retry timeouts so this test runs faster
      conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 10);
      // start 2 DNs
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
      FileSystem fs = cluster.getFileSystem();

      // create two files with one block each
      DFSTestUtil util = new DFSTestUtil.Builder().
          setName("testCorruptFilesCorruptedBlock").setNumFiles(2).
          setMaxLevels(1).setMaxSize(512).build();
      util.createFiles(fs, "/srcdat10");

      // Now deliberately corrupt one block
      String bpid = cluster.getNamesystem().getBlockPoolId();
      File storageDir = cluster.getInstanceStorageDir(0, 1);
      File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);
      assertTrue("data directory does not exist", data_dir.exists());
      List<File> metaFiles = MiniDFSCluster.getAllBlockFiles(data_dir);
      assertTrue("Data directory does not contain any blocks or there was an "
          + "IO error", metaFiles != null && !metaFiles.isEmpty());
      File metaFile = metaFiles.get(0);
      RandomAccessFile file = new RandomAccessFile(metaFile, "rw");
      FileChannel channel = file.getChannel();
      long position = channel.size() - 2;
      int length = 2;
      byte[] buffer = new byte[length];
      random.nextBytes(buffer);
      channel.write(ByteBuffer.wrap(buffer), position);
      file.close();
      LOG.info("Deliberately corrupting file " + metaFile.getName() +
          " at offset " + position + " length " + length);


      // read all files to trigger detection of corrupted replica
      try {
        util.checkFiles(fs, "/srcdat10");
      } catch (BlockMissingException e) {
        System.out.println("Received BlockMissingException as expected.");
      } catch (IOException e) {
        assertTrue("Corrupted replicas not handled properly. Expecting BlockMissingException " +
            " but received IOException " + e, false);
      }

      LOG.info("Restarting Datanode to trigger BlockPoolSliceScanner");
      cluster.restartDataNodes();
      cluster.waitActive();

      cluster.stopDataNode(1);

      // fetch bad file list from namenode. There should be one file.
      final NameNode namenode = cluster.getNameNode();
      while (cluster.getNamesystem().getBlockManager().getMissingBlocksCount() == 0) {
        Thread.sleep(1000);
        LOG.info("Still waiting for missing block");
      }
      assertEquals(1, cluster.getNamesystem().getBlockManager().getMissingBlocksCount());
      Collection<FSNamesystem.CorruptFileBlockInfo> badFiles =
          namenode.getNamesystem().listCorruptFileBlocks("/", null);
      LOG.info("Namenode has bad files. " + badFiles.size());
      assertTrue("Namenode has " + badFiles.size() + " bad files. Expecting 1.",
          badFiles.size() == 1);
      util.cleanup(fs, "/srcdat10");
    } finally {
      if (cluster != null) { cluster.shutdown(); }
    }
  }
{code}]