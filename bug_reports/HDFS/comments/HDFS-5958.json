[[~kovyrin], can you please any logs you may have for this issue?, [~sureshms], here is a piece of my log from the balancer: https://gist.github.com/kovyrin/9077741/raw/a30429b213fc4a5faca40f96c54f01d52c60706e/gistfile1.txt

Here is a screenshot with all the nodes in the cluster: http://snap.kovyrin.net/Hadoop_NameNode%C2%A0ops01.dal05.swiftype.net_8020-20140218-141308.jpg

name to address map:
{code}
10.84.56.2    work01
10.60.120.8   work02
10.84.56.10   work03
10.84.56.12   logs01
10.80.72.204  backup01
{code}
, The balancing policy assumes that there are enough blocks for moving around.  In your case, it may be impossible to satisfy the percentage threshold requirement for the large the datanode since it remains under utilized even if it has a replica for all the blocks., I understand perfectly well why it is happening. I've reported the issue to make sure it will be fixed and other users wouldn't need to spend hours pulling their hair out trying to figure out what is going on with their balance processes hanging forever, promising to move data around and not doing it. , I think we might need a new balancing policy for such special cases., Why not fix the default ones? Current behavior is clearly is bug, the balancer lies to a user's face by promising to move data around only to *silently* fail to do it and make another promise it could not keep., The balancer should exit with NO_MOVE_PROGRESS if there is no progress in 5 consecutive iterations.  Did it exit in your case?, No, that's the problem. It keeps trying forever., I see.  The NO_MOVE_PROGRESS somehow does not work., And there is another problem here: there could be more than enough work for the balancer (in my case I had a 20% utilized box, a few 80-90% utilized ones and that huge 4% utilized machine, so few servers were seriously overutilized) and still the balancer would keep choosing that one underutilized one. So if it just stops after N iterations and would ignore the need to balance the remaining nodes in the cluster - that would just be wrong., > ...  in my case I had a 20% utilized box, a few 80-90% utilized ones and that huge 4% utilized machine, ...

In this case, Balancer should move blocks from the 80-90% utilized machines to the other machines.  No?, BTW, your case may be similar to HDFS-8826.]