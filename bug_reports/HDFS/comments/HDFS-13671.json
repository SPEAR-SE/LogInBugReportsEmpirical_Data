[Hi [~linyiqun], thanks for reporting the issue.

Do you have some specific numbers to share on the issue? e.g. deleting how many files/blocks, and how long was NN 'hung' on the stacktrace provided?, Hi [~xiaochen], the deletion behavior is invoked by trash mechanism. So this will happened very frequently in our cluster, Every time there are 12~14w files(blocks) being deleted, then we found NN hung around 3~5 minutes. And all the other RPC requests are blocked., I'm not surprised. I fully expected having to revert when we start scale testing 3.x.

The folded tree jira has micro-benchmarks demonstrating a 4x degradation in performance. Micro-benchmarks tend to exaggerate performance differences, so let's see how that measures up in the real world.

Baseline is a large delete from last month on a 2.8 cluster:
 * 29.9M blocks; 87 seconds = 344k blocks/sec

Let's analyze permutations of the reported numbers:
 * 14M blocks; 3 min = 78k blocks/sec = 4X slower
 * 12M blocks; 3 min = 67k blocks/sec = 5X slower
 * 14M blocks; 6 min = 39k blocks/sec = 9X slower

It appears it's _at least as bad_ as predicted by the micro-benchmarks.  The tree traversal, the internal node copies, and the rebalancing aren't cheap., One more (big) reason to revert HDFS-9260?, Looking into this today and probably discuss with [~andrew.wang]. Will get back soon, I'm fine with reverting if we're seeing production issues. I wasn't that involved with HDFS-9260 except to try and answer Daryn's questions about real-world performance.

Given that there seems to be a lot more interest in maintaining the older version, I'm also inclined to revert for maintenance purposes., [~linyiqun] did you check how much time NN was spending in GC when it was hung? That is, it would be good to verify that the problem is indeed with the NN code running some suboptimal operations for long time, and not with the JVM itself that busily collects the heap. Of course, if you have a relatively small heap (up to 3..5GB), GC is unlikely to take much time anyway. But with bigger heaps, it may become a factor to consider., Thanks for the comments, everyone!
[~misha@cloudera.com], we  did the GC check and are sure there is no GC problem when NN hung. And we looked into the NN log, that explicitly indicated that NN was doing the remove block operation. It lasted around 6 minutes(from 15:01~15:07).
{noformat}
2018-06-06 15:00:59,873 INFO [IPC Server handler 163 on 8020] BlockStateChange: BLOCK* addToInvalidates: blk_1593304672_519567210 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010
2018-06-06 15:00:59,875 INFO [IPC Server handler 163 on 8020] BlockStateChange: BLOCK* addToInvalidates: blk_1593304675_519567213 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010
2018-06-06 15:00:59,879 INFO [IPC Server handler 163 on 8020] BlockStateChange: BLOCK* addToInvalidates: blk_1593304678_519567216 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010
2018-06-06 15:00:59,882 INFO [IPC Server handler 163 on 8020] BlockStateChange: BLOCK* addToInvalidates: blk_1593304679_519567217 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010
.....
2018-06-06 15:07:00,004 INFO [IPC Server handler 163 on 8020] BlockStateChange: BLOCK* addToInvalidates: blk_1595774272_522036817 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010
2018-06-06 15:07:00,005 INFO [IPC Server handler 163 on 8020] BlockStateChange: BLOCK* addToInvalidates: blk_1595774270_522036815 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010
2018-06-06 15:07:00,007 INFO [IPC Server handler 163 on 8020] BlockStateChange: BLOCK* addToInvalidates: blk_1595774256_522036801 xx.xx.xx.xx:50010 xx.xx.xx.xx:50010 xx.xx.xx.xx:500
{noformat}, The microbenchmarks given in [^HDFS Block and Replica Management 20151013.pdf], it also mentions that inserting/removing will takes 4x longer time.
{quote}A concern with the patch is the increased cost of inserting and removing nodes from the DataNodeStorageInfo on the NN. A small microbenchmark using JMH shows that removing and re-adding 32k random entries from a DataNodeStorageInfo that contains 64k entries roughly takes 4x longer
{quote}
There are some settings supported for controlling the compaction behavior of the tree. But this seems not a cheap way, it will hold FSN read/write block when do the scan and compaction in periodic runnable., Thanks all for the comments.


I reviewed HDFS-9260's benchmark and did some minidfscluster with 20k files runs with and without HDFS-9260, which looks like:
||version||create files one-by-one||delete altogether||delete ops/sec||
|with|(189088+183722+182669) / 3 = 185159|(88 + 87 + 89) / 3 = 88|227k|
|without|(197381+194467+183217) / 3 = 191688|(131+104+101) / 3 = 112|178k|

Considering with a real production, the removal will be more scattered resulting in more fragmentation / rebalancing, costing both cpu cycles and gc, I think the 4x slower microbenchmark number isn't very exaggerated. 

[~linyiqun], do you happen to know what 's the deletion rate in your cluster before HDFS-9260? (I doubt if it's at 344k blocks/sec :) )

Given that there seems to be more interest and existing work on the old implementation, reverting is fine by me. The reverting needs to be done compatibly though, and it seems to be compatible as long as the new field in {{DatanodeProtocol.proto}} is not removed., [~linyiqun] thanks for reporting the issue. It seems you've tried to attach a file (HDFS Block and Replica Management 20151013.pdf) but it doesn't uploaded. Would you please share this file again?, [~jojochuang], the link of FoldedTreeSet design doc:https://issues.apache.org/jira/secure/attachment/12767102/HDFS%20Block%20and%20Replica%20Management%2020151013.pdf
{quote}Yiqun Lin, do you happen to know what 's the deletion rate in your cluster before HDFS-9260? (I doubt if it's at 344k blocks/sec )
{quote}
[~xiaochen], I haven't tested the case without the patch of  HDFS-9260. I can have a test if I have some free time, :)., I think we all know the time complexity to update a large balanced binary tree cannot possibly compete with updating a small linked list (triplets) of n-many elements (replication factor).  It seems like we are in agreement to revert?  I'd do it but I've wrecked my local repo too many times to trust myself. :)

 , Reverting seems to be the right answer. It will be non-trivial so we'll need a brave volunteer to work through the conflicts.
, Agree with [~arpitagarwal]'s comment. I have met the same problem in 2.6.0-cdh5.13.1 that also using the FoldedTreeSet structure.
 [~daryn], if you have already done the reverting work for this, feel free to attach your patch, :). I think we have reach the agreement now., I've started working on the revert. So far in BlockManager, I had to deal with the following:
{panel}
HDFS-11681 - retain the change in removeBlocksAssociatedTo()
HDFS-10301 - retain the change in BlockManager.
HDFS-10694 - retained logging changes. The change regarding sorted/unsorted was removed in processReport() as it is no longer relevant.
HDFS-10858 - retain the change.
{panel}

I will post the revert patch and JIRAs involved in conflicts once everything is done., Hi, [~kihwal], how is the revert work? ]