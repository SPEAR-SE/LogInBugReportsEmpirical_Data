[The logic should probably be something more like:
# chose a random location
# rove the entire list
# skip decommissioned _or_ dead nodes

The former logic was apparently intended to detect dead nodes but that was removed., I guess the DataNode list here is sorted? Thus all the decommissioned DNs are actually in the end of the list. So the logic that throws an exception when the first DN is decommissioned should be correct?

The original code before HDFS-5891 does not do random pick for all the cases. The random pick is only for web UI. Thus I guess we do not need to worry about the scenario where too much traffic is to be directed to the same DN. WebHdfs etc. always tries to use the first DN, and this is consistent with the sorted DN list logic., How about the following to provide some level of load balancing we should get the DNs of the first block, filter out decommissioned ones and choose randomly among the avail ones. 

, But for reading through webhdfs you also want to get the data locality. In that case the client wants the local DN or the closest DN for reading. Currently the sorting logic takes this part into account and the first DN in the list is the best one. That's also why in the original/current implementation only web UI uses random pick., You get data locality only for the first block of the file, for subsequent blocks you are stuck to the DN serving the file via webhdfs., hmm, you're right.
bq.  to provide some level of load balancing we should get the DNs of the first block
Yes, this makes sense to me for multiple block cases., We ran into the similar issue Daryn mentioned. It seems possible to have the first node to be the decommissioned node even though ClientProtocol.getBlockLocations put decommissioned nodes at the end. In JspHelper.java,

{noformat}
    HashMap<DatanodeInfo, NodeRecord> map =
      new HashMap<DatanodeInfo, NodeRecord>();
...
    NodeRecord[] nodes = map.values().toArray(new NodeRecord[map.size()]);
...
{noformat}

Jing, Haohui, so it appears the order in nodes can be different from the original order., Looking at the branch-2 code, it looks to me that it always returns the node that contains the most block.

This is different from the behavior of trunk, which only returns the first operational node. Maybe we can backport this behavior in branch-2?, HDFS-6967 is going to make node selection be rack local instead of node local because jobs using webhdfs can wreak havoc on a cluster.  We are having _terrible problems_., Thanks, folks. Haohui, it appears this is due to HashMap entries' order aren't defined when you call HashMap.values() given dfs.hftp.force.random.node is set to true? So we have several solutions here.

1. Pick the first operational node.
2. Randomly pick a node from DNs for the first block.
3. Fix in HDFS-6967.

For our current webHDFS work load, we don't have to use #3. But if Daryn can provide the patch soon, perhaps let us wait for it., Any updates on this?, Moving bugs out of previously closed releases into the next minor release 2.8.0., There has been no movement on this for a while. We'll need to move it out of scope for 2.8.0 soon. Let me know if you disagree., Not much going on here for a long time, dropping from 2.8.0.

Not putting any target-version either anymore, let's target this depending on when there is patch activity.]