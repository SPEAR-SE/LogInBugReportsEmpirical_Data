[I just run and analysis the test case. I found that other reason caused this test case failure if number of blocks before cluster restart are more than one. It because that the MiniDFSCluster used by this case only have two DataNodes in the test source code. According to the BlockPlacementPolicy, if the total DataNode numbers of the cluster is less than the min replication, the real replication will equal to the smaller one(in this case is the total DataNode numbers).

So if you just modified the min replication to 3 and the total number of DataNode is still 2, then the real replication will still 2.
Modify the test scenario
------------------------
1. Modify the number of DataNode in MiniDFSCluster to 3.
2. Write a file with min replication as 2 and replication factor as 2.
3. Change the min replication to 3 and restart the cluster. 
------------------------
Then the test case will pass as expected.
So I support this jira is not a bug. , Hi [~yanbo], thanks for taking a look at this.
{quote}According to the BlockPlacementPolicy, if the total DataNode numbers of the cluster is less than the min replication, the real replication will equal to the smaller one(in this case is the total DataNode numbers).{quote}

Here, Cluster have 2 DataNodes, and file is written with replication factor as 1. 
Later min replication is changed to 2, same as number of datanodes available and cluster is restarted.

So problem is not with the replication. actual problem is namenode entering to safemode on restart, if more than one blocks are there in the cluster and min replication is increased.
And in safemode, no replication will happen to satisfy the min replication.

Here no need of modifying the test scenario,
instead I would suggest to allow to replicate blocks in case of at least one replica is available. After replication, min replication expectation will be satisfied automatically., Cluster have 2 DataNodes, and file is written with replication factor as 1. 
Later min replication is changed to 2, same as number of datanodes available and cluster is restarted.

If the environment configured like this, I run the test case success.
Pls attach the failure case log if it is convenient., I agree. Existing testcase will pass.

If we add one more file before restart with replication factor as 1, then change min replication and restart then testcase will hang in waiting for the cluster to be up.
, Here is the modified test case
{code}@Test(timeout = 60000)
  public void testReplicationAdjusted() throws IOException {
    // start a cluster 
    Configuration conf = new HdfsConfiguration();
    // Replicate and heartbeat fast to shave a few seconds off test
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY, 1);
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1);

    MiniDFSCluster cluster = null;
    try {
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2)
          .build();
      cluster.waitActive();
      FileSystem fs = cluster.getFileSystem();
  
      // Create a file with replication count 1
      Path p = new Path("/testfile");
      DFSTestUtil.createFile(fs, p, 10, /*repl*/ (short)1, 1);
      DFSTestUtil.waitReplication(fs, p, (short)1);

      Path p2 = new Path("/testfile2");
      DFSTestUtil.createFile(fs, p2, 10, /*repl*/ (short)1, 1);
      DFSTestUtil.waitReplication(fs, p2, (short)1);
  
      // Shut down and restart cluster with new minimum replication of 2
      cluster.shutdown();
      cluster = null;
      
      conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_MIN_KEY, 2);
  
      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2)
        .format(false).build();
      cluster.waitActive();
      fs = cluster.getFileSystem();
      
      // The file should get adjusted to replication 2 when
      // the edit log is replayed.
      DFSTestUtil.waitReplication(fs, p, (short)2);
    } finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }{code}, This is because of improper calculation of the blocksThreshold if I am not wrong, @Raju,
You are right. If that is fixed properly, then this test case will fail without any modification., The hang in the safeMode is because of the following code:

private boolean needEnter() {
      return (threshold != 0 && blockSafe < blockThreshold) ||
        (getNumLiveDataNodes() < datanodeThreshold) ||
        (!nameNodeHasResourcesAvailable());
    }

After the administrator have changed the parameter "dfs.namenode.replication.min" bigger, then all the blocks will not satisfy the "safe block" standard, so the parameter "blockSafe" will be zero always. So the above function will return true and then NameNode will hang in the safeMode.

Because of the replication before the cluster restart is persistent stored in the metadata of each file, I think we can calculate safe block number with the former replication rather than the modified one. Then the cluster will exit safe mode and execute replicate operation to achieve required replication. If this scenario is ok, I can fix it. , {quote}Because of the replication before the cluster restart is persistent stored in the metadata of each file, I think we can calculate safe block number with the former replication rather than the modified one. Then the cluster will exit safe mode and execute replicate operation to achieve required replication. If this scenario is ok, I can fix it.{quote}
+1 for this. 

[~umamaheswararao] what you say..?, Cause seems to be same as HDFS-3772 right? If Yes, then we can close one and mark as duplicate., OK]