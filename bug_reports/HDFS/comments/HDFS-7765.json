[By seeing the logs we are getting ArrayIndexOutOfBoundsException when count reached 4608 (default buffer size 512*9=4608)

{code}java.lang.ArrayIndexOutOfBoundsException: 4608{code}

But in code for each increment we are checking the buffer length with count, if it is equal we will flush it.

{code}
buf[count++] = (byte)b;
    if(count == buf.length) {
      flushBuffer();
    }
{code}
{code}
    count += bytesToCopy;
    if (count == buf.length) {
      // local buffer is full
      flushBuffer();
    }
{code}
Means something happened wrong and count reached till full length but buffer not flushed.


According to me it is possible when {{write1(byte b[], int off, int len)}} and {{write(int b)}} executed parallel.

Sequence is like this
1. In {{write1()}} method line {{count += bytesToCopy}} executed and count reached 4608.
2. Now before {{if (count == buf.length)}} line, {{write()}} method executed.
3. {{buf(count++] = (byte)b;}} will throw ArrayIndexOutOfBoundsException but it will increase the count to 4609.
4. Now line {{if (count == buf.length)}} never true and continue.

If I am wrong please correct me, thanks in advance., Attached initial patch. Please review and give your commen, Seems like this could not happen.

 * In 2.6.0 {{write(byte b[], int off, int len)}} and {{write(int b)}} are both synchronized.
 * In 2.6.0 {{write1(byte b[], int off, int len)}} is only called by {{write(byte b[], int off, int len)}}.  Therefore {{write1(...)}} is effectively synchronized in 2.6.0.

So it does not seem possible that {{write1(...)}} could be executed concurrently w/ {{write(int b)}}.  Does this reasoning seem correct?, The attached patch won't work, because it doesn't keep the invariant that count will always be <= array size. Attaching another patch: 0001-PATCH-HDFS-7765-FSOutputSummer-throwing-ArrayIndexOu.patch

, We've seen this as well under conditions which generate large I/O waits:

{noformat}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 321984
        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:76)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:50)
        at java.io.DataOutputStream.writeInt(DataOutputStream.java:197)
{noformat}

Cloudera 5.8.2 (HDFS 2.6.0), [~kturner]
{code:java}
 public synchronized void write(int b) throws IOException {
     buf[count++] = (byte)b;
     if(count == buf.length) {
       flushBuffer();
{code}
If the flushBuffer() throw IOException in the first time ,the count will larger than buf.length  the second time.

, [~janmejay], I think [~wankunde]'sÂ assessment matches my experience for when this issue happens. Once an IOException happens at max buffer size, this class becomes unusable.

Much like this other apache stream class as reference, flush if we can't write, then write. That way the state is not modified until safe. 
https://github.com/apache/commons-io/blob/master/src/main/java/org/apache/commons/io/output/ByteArrayOutputStream.java#L171
{code}
  public synchronized void write(int b) throws IOException {
    int newcount = count + 1;
    if (newcount > buf.length) {
      flushBuffer();
    }
    buf[count++] = (byte)b;
  }
{code}

I haven't checked the rest of the FSOutputSummer for correctness. That is worth checking., Any update about this issue? We've met the same issue in our env while writing sequence file into Hadoop Cluster.]