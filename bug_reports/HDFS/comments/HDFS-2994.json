[Not entirely sure how I got into the state where all the blocks were COMPLETE but a lease was still outstanding - it seems a rare circumstance..., Able to Reproduce till recoverLease releases the lease because of All Blocks COMPLETE, but not able to Reproduce 
*replaceNode* failure.

Scenario may be like this.

1. Client completed the writing the lastpacket to pipeline and got Ack also.
2. Before DN report the finalized block, Client's first *completeFile* call reached NN and marked Block as COMPLETE, but lease not removed since minReplication not satisfied. Say now Client dead.
3. Now DNs reports blocks and same thing is updated in BlockMap.
4. Now recoverLease is called on same file. As part of this file is finalized and Lease is getting removed because of COMPLETE blocks.
5. Now append is also called on the same file. 

In the Issue case, append is getting failed, because of *replaceNode* failure.
But, when tried to reproduce, append is successfully reopening the stream., Hi,

I am able to reproduce same

 *sceanrio 1 Using debug point* 
  ============================
Write a file /home/a.txt
call Append to /home/a.txt.
put a debugpoint in dfsclient at leaserenewer.put(src, result, this);
when control come to above point just renamefile to /home/rename.txt

Now again try to append to renamed file(/home/rename.txt)..then I am getting same exception

{noformat}
java.io.IOException: FSDirectory.replaceNode: failed to remove /home/rename.txt
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceNode(FSDirectory.java:1119)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.prepareFileForWrite(FSNamesystem.java:1674)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1612)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1823)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:417)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:217)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42592)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:423)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:891)
{noformat}


 *Scenario 2:* 
  ==========

step 1:write a file /home/a.txt(size 2MB)
step 2:call append on /home/a.txt(size 1.5MB)
restart DN while second step inprogess multiple times.

then I am getting same



{noformat}
java.io.IOException: FSDirectory.replaceNode: failed to remove /home/a.txt
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceNode(FSDirectory.java:1119)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.prepareFileForWrite(FSNamesystem.java:1670)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1608)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1819)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:416)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:217)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42592)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:417)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:891)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1661)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1657)
	at java.security.AccessController.doPrivileged(Native Method)


{noformat}

, Both scenario's after append,fail write(by renaming or restating DN) and then client should be closed.. like following..then append again.


{code}
       DistributedFileSystem dfs = initHDFS();

try {   		writeFile(dfs, hdfsFile, out, 1,true);
			out=appendFile(dfs,hdfsFile); 
     }			writeFile(dfs, hdfsFile, out, 1,true);		
		catch (Exception e) {
			// TODO: handle exception

			e.printStackTrace();
		}
		finally {
			
			if (dfs != null) 
			{
				dfs.close();
			}
		}

{code}, When there is lease recovery is in progress along with the append call on the same file then I have seen this problem coming.

Currently FSDirectory.replaceNode() is called from 2 methods
FSNameSystem#finalizeINodeFileUnderConstruction()
FSNameSystem#prepareFileForWrite() 

from this method we call to change the entry Inode entry in NN metadata (INode Structure, from InodeFile->InodeFileUnderConstruction ...)

If we observe the change constructor used in this methods

{code}
public LocatedBlock prepareFileForWrite(String src, INode file,
      String leaseHolder, String clientMachine, DatanodeDescriptor clientNode,
      boolean writeToEditLog)
      throws UnresolvedLinkException, IOException {
    INodeFile node = (INodeFile) file;
    INodeFileUnderConstruction cons = new INodeFileUnderConstruction(
                                    node.getLocalNameBytes(),
                                    node.getReplication(),
                                    node.getModificationTime(),
                                    node.getPreferredBlockSize(),
                                    node.getBlocks(),
                                    node.getPermissionStatus(),
                                    leaseHolder,
                                    clientMachine,
                                    clientNode);
    dir.replaceNode(src, node, cons);
    leaseManager.addLease(cons.getClientName(), src);
    
    LocatedBlock ret = blockManager.convertLastBlockToUnderConstruction(cons);
    if (writeToEditLog) {
      getEditLog().logOpenFile(src, cons);
    }
    return ret;
  }
{code}
INodeFileUnderConstruction constructor fails to capture INode.parent attribute causing the cons to have a null entry instead of parent !!!
Similarly 

{code}
private void finalizeINodeFileUnderConstruction(String src, 
      INodeFileUnderConstruction pendingFile) 
      throws IOException, UnresolvedLinkException {
    assert hasWriteLock();
    leaseManager.removeLease(pendingFile.getClientName(), src);

    // The file is no longer pending.
    // Create permanent INode, update blocks
    INodeFile newFile = pendingFile.convertToInodeFile();
    dir.replaceNode(src, pendingFile, newFile);

    // close file and persist block allocations for this file
    dir.closeFile(src, newFile);

    checkReplicationFactor(newFile);
  }
{code} pendingFile.convertToInodeFile(); also looses the parent attribute causing null entry in parent's location.

Similarly I have modified the

{code}
boolean removeNode() {
    if (parent == null) {
      return false;
    } else {
      parent.removeChild(this);
-     parent=null;
      return true;
    }
  } 
{code}
since in 
{code}
      INode myFile = dir.getFileINode(src);
      recoverLeaseInternal(myFile, src, holder, clientMachine, false);
{code}

in recoverLeaseInternal myFile loose the parent attribute.

A test as been added to verify the same behaviour, in which I am creating 3 clients to with different 
{code}
mapreduce.task.attempt.id
{code}

so that we can have different holder for the clients so lease recovery to get triggered when accessed by other client.
 , I have granted the licence but I didn't get Apache feather symbol next to patch :( don't know QA will apply the patch, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12528213/HDFS-2994_1.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 2 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2488//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2488//console

This message is automatically generated., Hi Todd can u review the patch :), Looks like the problem is still there.
In case of opening for append if softLimit expired recoverLeaseInternal() may finalize file and replace myFile with the closed one.
Then prepareFileForWrite() will try to replace the same file again, which will fail because myFile is an outdated / invalid reference to the old indode.
The right fix is to refresh myFile after recoverLeaseInternal() rather than setting its parent field as proposed in attached patch., Addressing Konstantin's comments by 
returning the new INodeFile via finalizeINodeFileUnderConstruction -> internalReleaseLease -> recoverLeaseInternal 
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12597419/HDFS-2994_2.patch
  against trunk revision .

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4801//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12597518/HDFS-2994_2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDFSUpgradeFromImage
                  org.apache.hadoop.hdfs.TestLeaseRecovery2
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4802//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4802//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12597802/HDFS-2994_3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4815//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4815//console

This message is automatically generated., Liked the approach of the last patch, which updates the inode reference only when needed.
I'd recommend to reuse myFile variable, making it non final.
      myFile = INodeFile.valueOf(dir.getINode(src), src, true);
This should make it easier to port to other versions.
The comment is good, just don't use JavaDoc style. Regular "//" comment would do better., I checked your new test case. It works with patched code and fails with current implementation.
But I see tabs, could you please revert to spaces., Thanks Konstantin. The newest patch addresses the above two comments., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12598074/HDFS-2994_4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:red}-1 eclipse:eclipse{color}.  The patch failed to build with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4825//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/4825//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4825//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12598134/HDFS-2994_4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4830//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4830//console

This message is automatically generated., +1.
I just committed this to trunk, branch-2, and branch-2.1. Thank you Tao.
Leaving it open for porting into branch-2.0.
LMK if this should go into 2.1.0 or 0.23., [~shv] Currently 2.1.0 is mainly focussing on API related fixes only. Thanks for merging it to branch-2.1. It will become available in 2.1.1., SUCCESS: Integrated in Hadoop-trunk-Commit #4275 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4275/])
HDFS-2994. If lease soft limit is recovered successfully the append can fail. Contributed by Tao Luo. (shv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1514500)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend.java
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12598329/HDFS-2994-2.0.6-alpha.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4838//console

This message is automatically generated., SUCCESS: Integrated in Hadoop-Yarn-trunk #303 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/303/])
HDFS-2994. If lease soft limit is recovered successfully the append can fail. Contributed by Tao Luo. (shv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1514500)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1493 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1493/])
HDFS-2994. If lease soft limit is recovered successfully the append can fail. Contributed by Tao Luo. (shv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1514500)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1520 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1520/])
HDFS-2994. If lease soft limit is recovered successfully the append can fail. Contributed by Tao Luo. (shv: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1514500)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend.java
, Happen to find this JIRA already integrated into 2.1.1-beta release, but the status here remains unresolved. May someone update the status? :-), {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12598329/HDFS-2994-2.0.6-alpha.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5835//console

This message is automatically generated., [~carp84], thanks for pointing it out. You are right. This was fixed in 2.1.1-beta. Marking this as resolved.]