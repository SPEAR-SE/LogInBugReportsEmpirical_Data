[Hi, [~jiangyu1211]! Are most of the files/blocks very small?, Hi, [~walter.k.su] , that is true, most of the blocks are very small, but i also found some blocks can be MB  in this situation. That is very dangerous when you make fail over of namenode , which excess block can not delete until  namenode think the datanodes are not stale.  I found this when i investigation this jira https://issues.apache.org/jira/browse/HDFS-6425., Blocks should not be scheduled for replication, while they are UNDER_CONSTRUCTION. Could you check if that is the case. If so it is a bug.
If not then your DNs seem to be quite busy because of too many of small blocks.
- You can try to increase {{dfs.namenode.replication.min}} to 2 (the default is 1). That way NN should wait for 2 replicas reported by DNs before scheduling replication.
- Also as you know HDFS does not "like" small files. Where small means < 128MB. You may try to encourage your users to combine data into larger blobs., Hi,[~shv] ,  the situation is reporting RBW after FINALIZE. It is rare when the cluster is not big enough, but for larger cluster,  it is easy to find. I don't know the impact if we change dfs.namenode.replication.min,  i will investigate it from code and test. , Hi,[~shv] ,  the situation is reporting RBW after FINALIZE. It is rare when the cluster is not big enough, but for larger cluster,  it is easy to find. I don't know the impact if we change dfs.namenode.replication.min,  i will investigate it from code and test. , Hi [~jiangyu1211] did the change of {{dfs.namenode.replication.min}} to 2 solve that for you?, The description gives good logs but not a good reason. It's a race condition when you close the small file too early. After file closed, a race between IBR of 3rd replica and replication scheduler.

You have closed the file before x.x.7.73 node reports the 3rd finalized replica of lastBlock. So the lastBlock is scheduled to replicate. After x.x.7.73 reported the 3rd replica. The lastBlock should be removed from {{needReplications}}. But if the replication command is sent to a new DN(x.x.4.65 node). There is no way to cancel the command from x.x.4.65 node.

The solution is to postpone scheduling replication, and wait x.x.7.73 to report the 3rd replica., Another solution is to increase {{dfs.namenode.replication.interval}} to slow down checker/scheduler., Change of {{dfs.namenode.replication.min}} to 3 (not 2) might solve this scenario.

Changing the min replication can lead to new problems. Like safemode on restart, write failures if not enough datanodes, etc.

So better to increase the replication check interval. Thats enough,I feel., Hi all,

agree with changing {{dfs.namenode.replication.interval}}. But i somehow think this just reduces the window of such problem to happen rather than eliminate it. Because at some point of time it might happen that the replication monitor thread overlaps on close and same issue reoccurs. Please correct me if i am wrong :), I think so too. And [~jiangyu1211], please let me know if you think so too.

Uploaded a patch. Make every block in {{needReplications}} wait at least one round before it can be processed. One round costs {{dfs.namenode.replication.interval}} time ( 3s by default), \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |  15m 51s | Findbugs (version ) appears to be broken on trunk. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 4 new or modified test files. |
| {color:green}+1{color} | javac |   7m 51s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |  10m  9s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 33s | There were no new checkstyle issues. |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 37s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   2m 40s | The patch appears to introduce 4 new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | native |   3m 16s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests |  67m 21s | Tests failed in hadoop-hdfs. |
| | | 110m 17s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-hdfs |
| Failed unit tests | hadoop.hdfs.TestLeaseRecovery |
| Timed out tests | org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12752681/HDFS-8763.01.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 4cbbfa2 |
| whitespace | https://builds.apache.org/job/PreCommit-HDFS-Build/12158/artifact/patchprocess/whitespace.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-HDFS-Build/12158/artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/12158/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/12158/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/12158/console |


This message was automatically generated., Hey guys, it looks to me that this works as expected.
- Slowing down {{needReplications}} does not solve the problem when DNs get even slower.
- Slowing down {{needReplications}} will slow replication in regular case, when it is actually needed.
- It was a good discussion with a few suggestions on how the condition can be remedied. But I don't see anything should be fixed here., So one question I always have is whether we need to let NameNode wait for all the block_received msgs to announce the replica is safe. Looking into the code, now we have
# NameNode knows the DataNodes involved when initially setting up the writing pipeline
# If any DataNode fails during the writing, client bumps the GS and finally reports all the DataNodes included in the new pipeline to NameNode through the {{updatePipeline}} RPC.
# When the client received the ack for the last packet of the block (and before the client tries to close the file on NameNode), the replica has been finalized in all the DataNodes.

Then in this case, when NameNode receives the close request from the client, the NameNode already knows the latest replicas for the block. Currently the {{checkReplication}} call only counts in all the replicas that NN has already received the block_received msg, but based on the above #2 and #3, it may be safe to also count in all the replicas in the {{BlockUnderConstructionFeature#replicas}}?, Hi jiangyu,

Did you apply the patch? What was the effect of the patch? Is the parameter for dfs.namenode.replication.min still set to a higher value like 2 or 3., [~jingzhao] raised a very valid point that when the client closes a file it has already received acks from the datanodes for the last block, which has been finalized at the datanodes. Namenode is also aware of the datanodes in the pipeline, therefore namenode does not really need to wait for {{blockReceived}} messages and can complete the file. I think that would be a very useful optimization. Additional advantage is that datanodes can lazily send blockReceived message for newly finalized blocks and thus reduce load on datanode-namenode traffic. I think we should take this discussion to a separate jira., Created HDFS-8999 to discuss the points raised by [~jingzhao]., We occasionally see the same issue, but not frequent enough to be a problem (unlike other BM repl bugs).  Overall, I'm not sure artificially slowing everything down is the right solution...

At any rate, big thumbs down on linearly scanning an array of arrays when removing a block., A different jira for a different topic makes sense.
Should this one be resolved then?, bq. We occasionally see the same issue, but not frequent enough to be a problem (unlike other BM repl bugs).
Thanks for the information. I think in a heavy-load writing situation, a tiny extra-cost saving is acceptable.

bq. Overall, I'm not sure artificially slowing everything down is the right solution...
You're right. Sorry I mispoke. It's not a solution. I just suggest to slower ReplicationMonitor since in a heavy-load writing situation, IBR is more likely delayed, and num of replication changes quickly. 01 patch didn't mean to do that...

bq. At any rate, big thumbs down on linearly scanning an array of arrays when removing a block.
Upload 02 patch to address that.

bq. A different jira for a different topic makes sense. Should this one be resolved then?
HDFS-8999 is in discussion. I'd like to keep this open to track different issue.

I'm ok to close this now or later. I'll be so glad if you could take a look at 02 patch again., Hi [~fernhtls] I don't think it is good idea to change the parameter for dfs.namenode.replication.min to  2, it is dangerous for big cluster.
Hi [~walter.k.su] I will apply the patch to our test cluster , but there is one big question for test  that only big enough cluster with very heavy load can make these happen, i will try to simulate it asap and thank you for the explaination of the reason. 
I agree with  [~shv] that slowing down needReplications will slow replication in regular case, when it is actually needed. 
I think HDFS-8999 is good way to resolve this problem., Reading HDFS-8999, I think currently, slow down needReplications maybe the solution of this problem., I don't think the timing is the root cause, if correct, an artificial delay would only mask a bug.  I'm wracking my brain now to remember - I was about to fix a bug with the same symptoms early this year but higher prio items came up.  I think this is improper bookkeeping of UC blocks.  I'll see if I can find my notes or partial patch..., I think we can close this as a duplicate of HDFS-1172. See the overview [in this comment|https://issues.apache.org/jira/browse/HDFS-8999?focusedCommentId=14733172&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14733172]]