[Use hostname when generating the URL for the datanode, The namenode and the datanode have different origins therefore the browser will not attach the cookies when making a request to the datanode. Redirecting using the domain name will not help., Browser visits namenode.domainname.com:50070 .  "hadoopauth" cookie is dropped with domain as "domainname.com". User clicks "browse File system" on the webui and browser gets redirected to ADataNode.domainname.com:1006 . Then the browser will send cookies to ADataNode.domainname.com  .   

Without this patch, when user clicks "browse File system" on the webui and gets redirected to datanodeipaddress:1006  and hence no cookies will be sent. 
, I'm curious how you manage to pass the cookie to the datanode. Even with your patch, the cookies should not be passed from the namenode and the datanode.

The browser is not supposed to shared the cookies. An origin is defined by the scheme, host, and port of a URL[1], where two hosts with the same hostname but different ports are considered different origins.

The browsers implement the same-origin policy, where the cookies are isolated in different origins [2].

[1] http://www.w3.org/Security/wiki/Same_Origin_Policy
[2] https://code.google.com/p/browsersec/wiki/Part2#Same-origin_policy, For the redirect, I think we are using DelegationToken which is included in the redirect URL? Thus we do not need to worry about hostname/ip address here I guess., Only Namenode will authenticate a client using HDFS Delegation Token ( which is issued by the Namenode.)
The client still needs to authenticate to the datanode and cannot use the Delegation Token for this purpose.
The configuration related to the cookie domain is hadoop.http.authentication.cookie.domain and more details on the authentication of http consoles is here  : https://hadoop.apache.org/docs/current2/hadoop-project-dist/hadoop-common/HttpAuthentication.html

When browsing the filesystem via original webui, the DelegationToken is used by the DataNode to contact the Namenode .
, The only way to access the data on a secure DN is to present a valid delegation token. The HTTP auth tokens do not contain the DT, presenting the HTTP auth tokens to the DN does not grant you the access, thus it makes no sense to pass them around.

Regardless of what UI you're using, the NN fetches the DT on the behalf of the client, and the client presents this DT to authenticate with DN. This should the only way you can access the data.

If you happen to get the data in your approach, this is a security hole and please file a jira to track it.

Again, I'll encourage you to check out the new web UI. It accesses the data through WebHDFS which is much more robust., ------the client presents this DT to authenticate with DN.
This s not correct. How can DN validate and read DT ? 
The DT is issued by NN. client/agent can  authenticate using DT only with NN.

Both NN/DN use https://hadoop.apache.org/docs/current2/hadoop-project-dist/hadoop-common/HttpAuthentication.html  to authenticate a user's access to their http interfaces.



, In AuthFilter#doFilter, we have the following code:
{code}
public void doFilter(ServletRequest request, ServletResponse response,
      FilterChain filterChain) throws IOException, ServletException {
    final HttpServletRequest httpRequest = toLowerCase((HttpServletRequest)request);
    final String tokenString = httpRequest.getParameter(DelegationParam.NAME);
    if (tokenString != null) {
      //Token is present in the url, therefore token will be used for
      //authentication, bypass kerberos authentication.
      filterChain.doFilter(httpRequest, response);
      return;
    }
    super.doFilter(httpRequest, response, filterChain);
  }
{code}

In DatanodeJspHelper#generateDirectoryStructure, we have 
{code}
    String tokenString = req.getParameter(JspHelper.DELEGATION_PARAMETER_NAME);
    UserGroupInformation ugi = JspHelper.getUGI(req, conf);
    .....
    DFSClient dfs = getDFSClient(ugi, nnAddr, conf);
{code}

So I think here the whole process is:
1. NN generates DT and put the DT into the redirect URL
2. DN receives the redirect request, finds that there is DT in the request, thus the corresponding SPNEGO filter will bypass the auth check
3. DN uses the DT and files a getFileInfo RPC call to NN
4. DN shows the result to web ui, AuthFilter.java is used only for webhdfs. While accessing JSP files, AuthenticationFilter is used and AuthenticationFilter  does not use delegationToken. 

Note that the use of IP address while generating redirectURL was introduced with HDFS-5307.  It used to be fqdn before.

From the HDFS-5307 patch ,

-    String fqdn = canonicalize(chosenNode.getIpAddr());
 -    String tailUrl = "///" + fqdn + ":" + chosenNode.getInfoPort()
 +
 +    String tailUrl = "///" + JspHelper.Url.authority(req.getScheme(), chosenNode), bq. AuthFilter.java is used only for webhdfs. While accessing JSP files, AuthenticationFilter is used and AuthenticationFilter does not use delegationToken.

All meaningful JSP on the datadode server (i.e., tail / browseBlock / browseDirectory) accesses the HDFS. You cannot proceed without a delegation token.

If you are able to access it without a DT, this is a security vulnerability and please file a jira to report it.

bq. Note that the use of IP address while generating redirectURL was introduced with HDFS-5307. It used to be fqdn before.

It calls {{InetSocketAddress#getCanonicalHostName()}} internally. It is broken when the machine have multiple DNS names.

Popping up one level, can you please restate what you are trying to achieve? The old UI is no longer under active development, it may be deprecated or removed at some point. It may be worthwhile to spend the time of migrating to the new UI., DelegationToken is used to access Namenode.

Here is the sequence:
# NN generates DT and put the DT into the redirect URL
# DN receives the redirect request. The AuthenticationFIlter  authenticates using "hadoop.auth" cookie if available.
# JSPs on the datadode server (i.e., tail / browseBlock / browseDirectory) access the NN using DelegationToken obtained as a URL parameter. 

For step 2 to work, the uri should have FQDN and the FQDN should be suffixed with "hadoop.http.authentication.cookie.domain" .
One can verify the above  by reviewing the code and testing it.
 
Usage of IP address (introduced in HDFS-5307) broke the file browsing when security is turned on. 
What's the argument against using  FQDN instead of ip address ?  
A hostname is always ensured during DN registration and the attached patch uses it.





, Had a discussion with Mai, Jing and Suresh. 

The decision was to switch back to using fqdn instead of using ip address or using the hostname passed during DN registration. This will retain the previous behavior. 

As a side note, customers might use custom AuthenticationHandlers  other than Kerberos (SpNego) . These custom AuthenticationHandlers could potentially use cookies to share auth information across servers (namenode and datanodes) . Usage of ip address will prevent sharing information using cookies .  This includes cookies used by custom AuthenticationHandlers or hadoop.auth cookie.

, New patch is attached.
The patch uses canonicalized fqdn instead of ip address or hostname passed in by DN.
The canonicalize method is moved from DatanodeJspHelper.java to JspHelper.java, The attached patch is tested on a 6 node cluster with SSL and security enabled., Thanks Benoy! The patch looks good to me. +1 pending Jenkins., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619135/HDFS-5661.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5744//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5744//console

This message is automatically generated., Let me know if anything is needed from my side on committing this patch . , Thanks for the reminder, Benoy! I will commit the patch shortly., SUCCESS: Integrated in Hadoop-trunk-Commit #4915 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4915/])
HDFS-5661. Browsing FileSystem via web ui, should use datanode's fqdn instead of ip address. Contributed by Benoy Antony. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1552177)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeJspHelper.java
, I've committed this to trunk, branch-2 and branch-2.3., FAILURE: Integrated in Hadoop-Yarn-trunk #426 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/426/])
HDFS-5661. Browsing FileSystem via web ui, should use datanode's fqdn instead of ip address. Contributed by Benoy Antony. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1552177)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeJspHelper.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1643 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1643/])
HDFS-5661. Browsing FileSystem via web ui, should use datanode's fqdn instead of ip address. Contributed by Benoy Antony. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1552177)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeJspHelper.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1617 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1617/])
HDFS-5661. Browsing FileSystem via web ui, should use datanode's fqdn instead of ip address. Contributed by Benoy Antony. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1552177)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeJspHelper.java
]