[Listed some selected test failures below, though they are precommit runs, it indicate some flakiness of the test to investigate.

1. https://builds.apache.org/job/PreCommit-HDFS-Build/7360, https://builds.apache.org/job/PreCommit-HDFS-Build/7346 etc.

{code}
Error Message

Deferred
Stacktrace

java.lang.RuntimeException: Deferred
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.checkException(MultithreadedTestUtil.java:130)
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.waitFor(MultithreadedTestUtil.java:121)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testPipelineRecoveryStress(TestPipelinesFailover.java:456)
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[127.0.0.1:54409], original=[127.0.0.1:54409]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:963)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1029)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1174)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:927)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:478)
{code}

2. https://builds.apache.org/job/PreCommit-HDFS-Build/7350
{code}
Error Message

Deferred
Stacktrace

java.lang.RuntimeException: Deferred
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.checkException(MultithreadedTestUtil.java:130)
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.waitFor(MultithreadedTestUtil.java:121)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testPipelineRecoveryStress(TestPipelinesFailover.java:456)
Caused by: org.apache.hadoop.ipc.RemoteException: File /test-10 could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1474)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2791)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:611)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:455)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2084)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2080)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1626)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2078)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1435)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:518)
{code}
, This issue we are seeing quite regularly nowadays in jenkins ( especially after changing build slave machines). 
And everytime I can see {java.io.IOException: Too many open files}}.

I tried running same test in my windows, It passed.

Do openfiles limit needs to be increased in these new machines?, bq. Do openfiles limit needs to be increased in these new machines?
Yes, I think the limit should be increased. I configured the limit to 512 and reproduced the same error., Attaching the logs., Hi Guys,  Thanks for the info. I wonder if someone can collect the openfile limit from the old machines and new machines, so we have idea about how different they are.  Thanks.

, [~gkesavan], could you please review the comments on this issue?  There is some suspicion that ulimit settings are different between the old Jenkins hosts and the new Jenkins hosts, and this might be causing spurious test failures.  Thanks!, Created  INFRA-8147: check on open file limit on jenkins test slaves.

, Upload a version to dump out dbg message.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12659692/HDFS-6694.001.dbg.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7556//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7556//console

This message is automatically generated., My last test run indicated that the ulimit num of open file is 1024 on the machine {{Slave H5 (Build slave for Hadoop project builds : asf905.gq1.ygridcore.net)}}. However, when I ran the test locally, the num of open files is 4096. 

{code} 
YJD ulimit -a contents: 
time(seconds) unlimited 
file(blocks) unlimited 
data(kbytes) unlimited 
stack(kbytes) 8192 
coredump(blocks) 0 
memory(kbytes) unlimited 
locked memory(kbytes) 64 
process 386178 
nofiles 1024 <========== 
vmemory(kbytes) unlimited 
locks unlimited 
{code} 
, I am seeing this with my patch for HDFS-6790

{code}
Running org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 82.647 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
testPipelineRecoveryStress(org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover)  Time elapsed: 33.705 sec  <<< ERROR!
java.lang.RuntimeException: Deferred
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.checkException(MultithreadedTestUtil.java:130)
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.waitFor(MultithreadedTestUtil.java:121)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testPipelineRecoveryStress(TestPipelinesFailover.java:456)
Caused by: org.apache.hadoop.ipc.RemoteException: File /test-7 could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1486)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2801)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:613)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:462)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:607)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2099)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2095)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1626)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2093)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:372)
	at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1442)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1265)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:521)
{code}, HI [~lmccay], thanks for reporting the same issue here. Please refer to  INFRA-8147 (check on open file limit on jenkins test slaves) for latest status. Thanks.

, I am going to tag this as a blocker for 2.6.0, we need to start driving towards clean Jenkins runs.

[~yzhangal], please reassign to someone who can investigate as I assume you don't have permissions to ssh to the build slaves. Also please feel free to downgrade if you disagree with the priority., So this indicates that the build slaves should be configured with a numfiles limit of 60000. Is that correct?

https://github.com/apache/toolchain/blob/master/roles/common/files/limits.conf

That doesn't match with the 1024 limit that Yongjun noted earlier. [~gkesavan], do you know where the numfiles limits for the build slaves is configured? Thanks.

, Hi [~arpitagarwal], thanks for following up. I totally agree with the priority change you made. Since now the INFRA-8147 is said to be fixed, I just uploaded the debug patch for this jira again to confirm both the number of open file limit and whether the test can pass.

I observed another recent failure, either the INFRA-8147 fix is not in when the test run, or the root cause of HDFS-6694 is different: https://builds.apache.org/job/PreCommit-HDFS-Build/7660//testReport/

Thanks.

, I think it's the same failure.

{code}
java.io.IOException: Too many open files
	at sun.nio.ch.IOUtil.initPipe(Native Method)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:49)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:18)
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.get(SocketIOWithTimeout.java:409)
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:325)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at java.io.DataInputStream.read(DataInputStream.java:132)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:194)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:467)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:766)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:710)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:126)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:72)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:218)
	at java.lang.Thread.run(Thread.java:662)
{code}, [~arpitagarwal] this is what I see on the build slave.

{noformat}
[gkesavan@HW11151 ~]$ ssh asf905.gq1.ygridcore.net
Last login: Mon Aug 18 20:03:08 2014 from 192.175.27.2
$ sudo su - jenkins
jenkins@asf905:~$ ulimit -n
60000
jenkins@asf905:~$
{noformat}

, Thanks for confirming that Giri! Not sure what explains the difference between what's configured and what the test case sees., Thanks [~gkesavan] and [~arpitagarwal], there is a test run ongoing with my debug patch that prints out the ulimits. Let's wait to see what it says.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662524/HDFS-6694.001.dbg.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestMultipleNNDataBlockScanner

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7666//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7666//console

This message is automatically generated., Unfortunately this run succeeded, but it used a different host.

Hi [~gkesavan], woud you please help checking the "uname -n" for the following host:

Slave H4 (Build slave for Hadoop project builds : asf904.gq1.ygridcore.net)

on which I observed the latest failure most recently.

Thanks.
, [~yzhangal]
{noformat}
jenkins@asf904:~$ uname -n
asf904.gq1.ygridcore.net
{noformat}, Thanks [~gkesavan], but my bad, I intended to say "ulimit -n" but typed it wrong, would you please help another round with "ulimit -a" to collect all limit info? thanks again.

, {noformat}
jenkins@asf904:~$ ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 386178
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 60000
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 10240
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
{noformat}, Many thanks [~gkesavan] for your quick help!

The number of open files is indeed 60000, it's quite puzzling. I guess the number of open files is different when the failed test was run. 

Hi [~arpitagarwal], I suggest that we commit my dbg patch here that prints out the limit at the beginning of the test, so whenever it failed in the future, we can check the log to see the setting, what do you think? thanks.

 
, I'm seeing this failure today, uploaded the test version to see if there is any luck that the job will be scheduled to the same host.
, bq. Hi Arpit Agarwal, I suggest that we commit my dbg patch here that prints out the limit at the beginning of the test, so whenever it failed in the future, we can check the log to see the setting, what do you think? thanks.
Sounds good. Could you please remove YJD from the output and replace it with 'HDFS-6694 debug data' or something similar? I'll commit it., Thanks [~arpitagarwal], just did the change and uploaded new rev. Let's wait for the test to finish and please help commit. Best regards.
, +1 pending Jenkins., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12663882/HDFS-6694.001.dbg.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestFileCorruption

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7742//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7742//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12663883/HDFS-6694.002.dbg.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.security.TestRefreshUserMappings

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7743//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7743//console

This message is automatically generated., Caught one:

Slave H9 (Build slave for Hadoop project builds : asf909.gq1.ygridcore.net)

 https://builds.apache.org/job/PreCommit-HDFS-Build/7742//testReport/
{code}
Standard Output

YJD ulimit -a contents:
time(seconds)        unlimited
file(blocks)         unlimited
data(kbytes)         unlimited
stack(kbytes)        8192
coredump(blocks)     0
memory(kbytes)       unlimited
locked memory(kbytes) 64
process              386177
nofiles              1024 <====
vmemory(kbytes)      unlimited
locks                unlimited
{code}
, Even we can catch one from time to time, it's still worth to commit the debug patch in case if happens again. Thanks.

, Yes good catch, looks like the configured ulimit doesn't match what's on the slave.

[~gkesavan], do you think rebooting the slaves would fix this? Thanks., Another run failed same way, happen to be on the slave H9:

https://builds.apache.org/job/PreCommit-HDFS-Build/7746//testReport/
, Hi Arpit, thanks for your earlier review, would you please help committing it? Thanks., The repo is not open for commits., Thanks for the info [~arpitagarwal].
, Committed to trunk. I am downgrading the priority for now. Thanks [~yzhangal]., Thanks [~arpitagarwal]. The recent failure I observed all happened at H9 slave. We can close this jira once it's resolved there. Then if it happens again later on, we have a good place to look at.
, HI [~gkesavan] and [~ipv6guru],

Slave {{H9 (Build slave for Hadoop project builds : asf909.gq1.ygridcore.net)}} still has the problem, it's num open files is 1024. Would you please help to  look into? Arpit suggested rebooting the host might solve the problem, which sounds good to me. 

Thanks a lot.
, I created HDFS-7070 for another testcase with "Too many open files symptom", which might or might not be the same reason as HDFS-6694 here.

The fact that TestPipelinesFailover.testPipelineRecoveryStress fails so often makes this test simply ignored, and it has the chance of missing real problems.

Hi [~gkesavan] and [~ipv6guru], I wonder if any of you could follow-up with HDFS-6694 and INFRA-8147? thanks a lot.



, the one I got is: 

java.lang.RuntimeException: Deferred
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.checkException(MultithreadedTestUtil.java:130)
	at org.apache.hadoop.test.MultithreadedTestUtil$TestContext.waitFor(MultithreadedTestUtil.java:121)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testPipelineRecoveryStress(TestPipelinesFailover.java:485)
Caused by: java.lang.AssertionError: expected:<100> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hdfs.AppendTestUtil.check(AppendTestUtil.java:123)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover$PipelineTestThread.doAnAction(TestPipelinesFailover.java:522)
	at org.apache.hadoop.test.MultithreadedTestUtil$RepeatingTestThread.doWork(MultithreadedTestUtil.java:222)
	at org.apache.hadoop.test.MultithreadedTestUtil$TestingThread.run(MultithreadedTestUtil.java:189)


Results :

Tests in error: 
  TestPipelinesFailover.testPipelineRecoveryStress:485 » Runtime Deferred, HI [~airbots],

Thanks for reporting the issue you ran into. Would you please look into your log to see if there are "Too many open files" kind of messages?
, surefire error file that I got when using maven to do the test., BTW, I did not got the "Too many file open" issue., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676587/org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover-output.txt
  against trunk revision d71d40a.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8496//console

This message is automatically generated., Removing Fix-version. Please use Target-version for the intended release and let committers set the fix-version at commit time., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676587/org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover-output.txt
  against trunk revision 7660da9.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/10256//console

This message is automatically generated., What is the status of the temporary debugging patch that was committed to trunk?  Is it still needed, or can it be reverted?  HDFS-9438 reports that the {{ifconfig}} call isn't compatible with Solaris, and it doesn't exist on Windows, so the test is failing on those platforms., Hi [~cnauroth], we might consider keeping the change but with a platform switch, and only run it on linux.  If agreed, I can create a jira for that. Thanks.
, See HDFS-9438 for how to get it to work on both Linux & Solaris., [~yzhangal], if you'd still like to keep the debug code in place for now, then I suggest that you pick up HDFS-9438 and implement Alan's suggestion.  You could also skip the logic that doesn't work on Windows in scope of that JIRA.  Thanks!, Thanks [~alanburlison] and [~cnauroth]. I think it doesn't hurt to keep the debug code, which we might need one day. Alan, I saw you assigned HDFS-9438 to yourself, if you'd like me to pick it up, please feel free to assign to me. Thanks.
, Done., Thanks [~alanburlison].
, FAILURE: Integrated in Hadoop-trunk-Commit #8889 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8889/])
HDFS-6694. Addendum. Update CHANGES.txt for cherry-picking to 2.8. (yzhang: rev 84d01ad7f43bc498bc2e9d3afe68aed7f4a4d462)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #1454 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/1454/])
HDFS-6694. Addendum. Update CHANGES.txt for cherry-picking to 2.8. (yzhang: rev 84d01ad7f43bc498bc2e9d3afe68aed7f4a4d462)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Yarn-trunk-Java8 #734 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/734/])
HDFS-6694. Addendum. Update CHANGES.txt for cherry-picking to 2.8. (yzhang: rev 84d01ad7f43bc498bc2e9d3afe68aed7f4a4d462)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2665 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2665/])
HDFS-6694. Addendum. Update CHANGES.txt for cherry-picking to 2.8. (yzhang: rev 84d01ad7f43bc498bc2e9d3afe68aed7f4a4d462)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, ABORTED: Integrated in Hadoop-Hdfs-trunk-Java8 #643 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/643/])
HDFS-6694. Addendum. Update CHANGES.txt for cherry-picking to 2.8. (yzhang: rev 84d01ad7f43bc498bc2e9d3afe68aed7f4a4d462)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk-Java8 #723 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/723/])
HDFS-6694. Addendum. Update CHANGES.txt for cherry-picking to 2.8. (yzhang: rev 84d01ad7f43bc498bc2e9d3afe68aed7f4a4d462)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2580 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2580/])
HDFS-6694. Addendum. Update CHANGES.txt for cherry-picking to 2.8. (yzhang: rev 84d01ad7f43bc498bc2e9d3afe68aed7f4a4d462)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
]