[The layered file system idea was first proposed in HDFS-5851 by [~sanjay.radia]. More details to follow., I am guessing that anything that is written to in-memory storage via memfs would also be read back in-memory via memfs. The centralized cache management is not the only in-memory read path, right?, bq. I am guessing that anything that is written to in-memory storage via memfs would also be read back in-memory via memfs. The centralized cache management is not the only in-memory read path, right?
Correct. Additionally data written via memfs will support SCR from memory., Hi [~aagarwal],

I'm not sure I see the advantage of having a separate file system, rather than simply putting this into HDFS.  Can you clarify how users would interact with this system?  How does this relate to DDM?, bq. I'm not sure I see the advantage of having a separate file system, rather than simply putting this into HDFS. bq. Can you clarify how users would interact with this system?
Colin, our goal is making it easier for applications to use memory support in HDFS specifically and Hadoop Compatible File Systems in general.
# Allow using memory features without calling HDFS-specific APIs. This also isolates applications from evolving APIs. Applications currently use shims and reflection tricks to work with different versions of HDFS. 
# Once applications start using memfs someone could write a memfs layer over another HCFS e.g. Amazon S3. 

memfs itself will not cache any data when used with hdfs. wrt interaction, applications can choose to use {{memfs://}} paths instead of {{hdfs://}} paths for data targeted to memory.

bq. How does this relate to DDM?
There is no immediate plan to introduce a discardable namespace., bq. Allow using memory features without calling HDFS-specific APIs. This also isolates applications from evolving APIs. Applications currently use shims and reflection tricks to work with different versions of HDFS.

HDFS-4949 didn't require applications to call any HDFS-specific APIs.  The administrator simply set a list of files and directories to be cached.  When applications read those files or directories, they were retrieved from the cache.

We could do something similar here by specifying that we wanted opportunistic caching on a certain directory subtree.  For example we could set a 2Q eviction policy on a certain directory subtree and have the NameNode manage that.  [~andrew.wang] and I discussed doing that for HDFS-4949, but we simply didn't have time.

bq. Once applications start using memfs someone could write a memfs layer over another HCFS e.g. Amazon S3.

That does raise the question of why this belongs in HDFS, though.  If we just want a generic FS caching layer in Hadoop, we could do that in hadoop-common., bq. The administrator simply set a list of files and directories to be cached. When applications read those files or directories, they were retrieved from the cache.
It's impractical to involve the administrator every time a new file is to be cached. We've heard this requirement makes caching difficult to use. There are a couple of other things can help with usability e.g. de-duplication of cache directives, predictability of cache locality.

bq. If we just want a generic FS caching layer in Hadoop, we could do that in hadoop-common.
That was my intention. I'll move the jira to common., bq. It's impractical to involve the administrator every time a new file is to be cached.

Read caching can be done by normal users, not just admins. We also have directory-level cache directives which kick in automatically without any explicit user involvement

If some of the other enhancements you mention could be built into HDFS, that'd also be preferable (de-dupe, predictability (?)).

Anecdotal, but I've heard a lot of users say that changing the scheme is not an option for them. If your concern is ease of use, focusing on improvements to what we already have in HDFS might be more bang for the buck. We have the LAZY_PERSIST storage policy and directory-level cache directives which seem like a start. Colin also mentioned opportunistic cache directives, which would be a really nice enhancement., Consider the following use case:   one wants to run a few jobs and cache the input and the intermediate output just for the duration of these jobs. Today the user has to pin such data by changing the dir-file attributes, and when the jobs are finished he has to reset the attributes. It is easier to say "jobxxx input = memfs://.../input tmp=memfs://.../tmpdir  output=". Here setting the scheme is not inconvenient since it is part of parameters to a program.  Further this works with any existing application - Hive, Pig etc since the hint to cache is in the scheme of the pathname. Our existing policies and dir level setting work when things  are semi-permanent (ie this dir has dimension tables and please cache them - all jobs will benefit). In addition we could add or already have programmatic APIs to indicate that a file being read or written needs to be cached. But this requires change to the application code.   Once we get fully automated memory caching working we will not need  our existing storage policies nor layers like memfs since the system will just take care of it all - but it will take us some time to get there. 

I think both approaches have their own strengths and are complementary. Note  spark-tachyon uses a layered file system and the approach is viewed as a simple way to control which files get cached on a per-job basis.

Further one can also cache specific Hive  tables in hive meta store by giving a path name that has the memfs-scheme. Here the memfs-pathname or setting the dirs attribute are roughly equal from a ease-of-usage perspective.

An additional point about memfs for non-hdfs systems: the Memfs *abstraction* allows caching S3 data in a very similar fashion. Of course one will have to build a full caching implementation of memfs for S3 because the memfs proposed in this Jira is very very thin layer over HDFS because ALL the caching mechanism is already in HDFS. So I expect several implementation of the memfs interface for HCFS file systems.]