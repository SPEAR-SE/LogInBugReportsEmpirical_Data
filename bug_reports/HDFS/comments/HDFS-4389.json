[The problem was discovered while debugging random failures in {{TestPersistBlocks}}. The {{TestRestartDfsWithFlush}} does the following:
# open a stream
# write 5 blocks
# flush
# wait for at least 1 block to be finalized, record size
# bounce the NN
# ensure file is at least as big as before bounce
# write 5 more blocks <- race condition blows up here
# close stream
# ensure all data is there

The problem occurs when {{DFSOutputStream.DataStreamer}} needs to call {{DFSClient#addBlock}} while the NN is down.  It receives a {{ConnectException}} from the IPC layer, which isn't handled so it stores it away and shuts down the stream.  The additional write to the stream after the NN restart throws the stored connect exception.

The end result is streams cannot survive a NN restart or network interruption that lasts longer than the time it takes to write a block.  The issue is probably general to all client methods., Hey Daryn. I vaguely remember this being a conscious decision at some point, but maybe I made that up. Two thoughts that might be relevant:

1) TestPersistBlocks seems to have started failing much more often recently, but I don't have evidence for this. Any chance something else might have caused a regression here?

2) In the old code, which retried over the restart, wouldn't it end up just hitting a SafeModeException and then failing at that point, when the NN was restarted? Given that the NN usually takes 30+seconds to leave safemode after starting, any retrying clients would probably hit that and fail anyway, no?, I think old retry policy used to be longer than 30s, so a restart of a small-mid cluster may not have been noticed

The short-term workaround is to change the retry policy to be something else & to block for longer. , > The short-term workaround is to change the retry policy to be something else & to block for longer. 

Yes, we may enable dfs.client.retry.policy.enabled.  Then, the connection retry policy will retry for ~11 minutes.  (Or also change the default dfs.client.retry.policy.spec if necessary.), # the retry policy Nicholas talks of will catch and handle SafeModeExceptions; tested for Pig, HBase and Hive.
# There's a Groovy Swing UI to see what's going on in the cluster: NN probes + blocking/non-blocking FS operations up on the "should go into contrib" HA-monitor code  https://github.com/hortonworks/HA-Monitor
# The JT needs to be told to keep an eye on HDFS status (inc safe mode) and not overreact to failing tasks during an outage (timeouts & failures don't trigger Job failure or TT blacklisting
# The full list of options is up at http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.2.0/bk_hdp1-system-admin-guide/content/ch_ha-redhat-deploy.html

Even HDFS 2 and its active/passive failover, telling the layers above to be resilient to failures, either from extended retry policies (default) or app specific probes & policies (as the JT does) is good because it gives you resilience to may of the other outages you can encounter (network problems, failure of rack containing both NNs, whale falling out of sky (yes, it happens). There's generally no reason not to flip the switch. 
 , Downgrading this to Major since it's been inactive for years. I think almost all deployments are HA these days too, so perhaps less important.]