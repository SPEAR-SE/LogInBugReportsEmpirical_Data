[I think I know enough to make this change and do some unit testing, but I need a little java guidance (on building everything).

Mainly, I need help on compiling the hadoopthriftapi.jar file from the gen-java files.

I actually really need this for my own uses.

My first take outlined below starts with just to converting the read/write methods to use binary (vs adding new methods). This way I don't have to worry about making sure the correct read/write methods are called in the initial version.

I re-generated the thrift java files with a new thrift interface the reads/writes in binary.

- note that binary data is converted to UTF-8 on write as well as read, so if you just update the thrift client to write binary, the server will add unicode escape characters before it's even saved to hdfs.

The code in:

hadoop-0.20.2/src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/HadoopThriftServer.java

is straightforward as well.

However! this implements the interface defined in:

org.apache.hadoop.thriftfs.api.ThriftHadoopFileSystem.Iface

And even though I update the source in:

hadoop-0.20.2/src/contrib/thriftfs/gen-java

I get an error about overriding the read/write methods incorrectly, so it appears to be pulling the definition of the

org.apache.hadoop.thriftfs.api.ThriftHadoopFileSystem.Iface

from hadoopthriftapi.jar (which makes sense).

However, I don't know how to rebuild hadoopthriftapi.jar.

I'll attach the thrift file and the HadoopThriftServer.java file a little later, but I just wanted to get this comment up - maybe someone can give me simple instructions on how to build hadoopthriftapi.jar from the gen-java files?
, Hadoop thrift IDL file with binary read/write and some fixes to generate c# code correctly.

The charp part was just renaming a variable called "out" (reserved keyword) and renaming a field called pathname (which got capitalized to Pathname in accessor, and then conflicted with the class name)., Modified HadoopThriftServer with binary read/write., Completely hackalicious solution:

Be forewarned, if you know how to rebuild: 

hadoopthriftapi.jar 

From the gen-java files generated by the thrift IDL file above, you're WAY better off, and please let me know.
Otherwise go to:

/hadoop-0.20.2/src/contrib/thriftfs$

Open the file:

/hadoop-0.20.2/src/contrib/thriftfs$ gvim src/java/org/apache/hadoop/thriftfs/HadoopThriftServer.java

import commons-encoder:

import org.apache.commons.codec.binary.Base64;
import org.apache.commons.codec.DecoderException;
import org.apache.commons.codec.EncoderException;

Note that hadoop &  the hadoop thrift api's depend on commons-encoder-1.3 , not 1.4.
This is unforntunate, because 1.3 has a pretty brain-dead interface.

Modify the send and receive functions to treat the string arguments (and return value) as base64 encoded binary:

    /**
     * write to a file
     */
    public boolean write(ThriftHandle tout, String encodedData) throws ThriftIOException {
      try {
        now = now();
        HadoopThriftHandler.LOG.debug("write: " + tout.id);
        FSDataOutputStream out = (FSDataOutputStream)lookup(tout.id);
        Base64 base64 = new Base64();
        byte[] tmp = null;
        tmp = (byte[])base64.decode( (byte[]) encodedData.getBytes("UTF-8") );
            
        out.write(tmp, 0, tmp.length);
        HadoopThriftHandler.LOG.debug("wrote: " + tout.id);
        return true;
      } catch (IOException e) {
        throw new ThriftIOException(e.getMessage());
      }
    }

    /**
     * read from a file
     */
    public String read(ThriftHandle tout, long offset,
                       int length) throws ThriftIOException {
      try {
        now = now();
        HadoopThriftHandler.LOG.debug("read: " + tout.id +
                                     " offset: " + offset +
                                     " length: " + length);
        FSDataInputStream in = (FSDataInputStream)lookup(tout.id);
        if (in.getPos() != offset) {
          in.seek(offset);
        }
        byte[] tmp = new byte[length];
        int numbytes = in.read(offset, tmp, 0, length);
        HadoopThriftHandler.LOG.debug("read done: " + tout.id);
        try
        {
            Base64 base64 = new Base64();
            return new String( (byte[])base64.encode( (Object)tmp ), "UTF-8");
        }
        catch( EncoderException e )
        {
            e.printStackTrace();
            System.exit(0);
            return "";
        }
      } catch (IOException e) {
        throw new ThriftIOException(e.getMessage());
      }
    }

Compile:

/hadoop-0.20.2/src/contrib/thriftfs$ ant

Copy the jar file:

hadoop-0.20.2/build/contrib/thriftfs/hadoop-0.20.2-thriftfs.jar

to your namenode (or wherever you run your hadoop thrift server from), and drop it in:

hadoop-0.20.2/contrib/thriftfs/hadoop-0.20.2-thriftfs.jar

(no build dir).

then start your thrift server as normal:

/hadoop/src/contrib/thriftfs/scripts$ ./start_thrift_server.sh 50050


Now, in all your thrift clients, you have to base64 encode any data before sending it, and decode after receiving.

But you can finally get binary data on hdfs. Albeit at a high price in ugliness & performance (coz I'm assuming your storing large files on hdfs...)

I've only tested this on one 224 Kb file, but I move everything in 8K chunks client side, so it should work on large files (it'll just be horrifically slow).

Again, if anyone figures out how to rebuild: 

hadoopthriftapi.jar 

From the gen-java files, please enlighten!


, oy. didn't format the code, sorry:

{noformat} 
    /**
     * write to a file
     */
    public boolean write(ThriftHandle tout, String encodedData) throws ThriftIOException {
      try {
        now = now();
        HadoopThriftHandler.LOG.debug("write: " + tout.id);
        FSDataOutputStream out = (FSDataOutputStream)lookup(tout.id);
        Base64 base64 = new Base64();
        byte[] tmp = null;
        tmp = (byte[])base64.decode( (byte[]) encodedData.getBytes("UTF-8") );
            
        out.write(tmp, 0, tmp.length);
        HadoopThriftHandler.LOG.debug("wrote: " + tout.id);
        return true;
      } catch (IOException e) {
        throw new ThriftIOException(e.getMessage());
      }
    }

    /**
     * read from a file
     */
    public String read(ThriftHandle tout, long offset,
                       int length) throws ThriftIOException {
      try {
        now = now();
        HadoopThriftHandler.LOG.debug("read: " + tout.id +
                                     " offset: " + offset +
                                     " length: " + length);
        FSDataInputStream in = (FSDataInputStream)lookup(tout.id);
        if (in.getPos() != offset) {
          in.seek(offset);
        }
        byte[] tmp = new byte[length];
        int numbytes = in.read(offset, tmp, 0, length);
        HadoopThriftHandler.LOG.debug("read done: " + tout.id);
        try
        {
            Base64 base64 = new Base64();
            return new String( (byte[])base64.encode( (Object)tmp ), "UTF-8");
        }
        catch( EncoderException e )
        {
            e.printStackTrace();
            System.exit(0);
            return "";
        }
      } catch (IOException e) {
        throw new ThriftIOException(e.getMessage());
      }
    }

{noformat} , thriftfs contrib removed, HadoopThriftServer with changed encoding for reading and writing, Modified version of HadoopThriftServer, Bleh double post and I don't know how to remove it :)
The comment should be that it was enough to change utf-8 into latin1 for me to get the binary data to work, see attached file above]