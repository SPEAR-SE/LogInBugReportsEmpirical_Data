[The getBlockMetaDataInfo() should first terminate all open connections to the block and then return the size of the block. Then it is guaranteed that nobody can change the length of the block between the getBlockMetaDataInfo() and updateBlock() calls. This is documented in Step 5 in https://issues.apache.org/jira/browse/HADOOP-4663?focusedCommentId=12674490&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12674490

will this solve your problem?, In this case, there is no on-going writes. The problem is that data is not flushed to disk when getBlockMetaDataInfo is called. But updateBlock flushes and closes the file. Therefore, there is an inconsistency., After a series of HDFS-265 sub-task patches, getBlockMetaDataInfo(..) and updateBlock(..) were replaced by initReplicaRecovery(..) and updateReplicaUnderRecovery(..), respectively.  The stop writer logic was moved from updateBlock(..) to initReplicaRecovery(..) so that the writer is stopped before getting the replica length.  So this problem disappears in the new codes.

I suggest that we add more validation to check the replica and its file lengths, more specifically, to check
- file length and replica's visible length after stopping a writer, and
- replica's original visible length obtained from initReplicaRecovery(..) and its current visible length in updateReplicaUnderRecovery(..)., h29_20091012.patch: added more validation as described before., h29_20091012b.patch:
- fixed some bugs in TestInterDatanodeProtocol
- also changed TestInterDatanodeProtocol to use junit 4., h29_20091012c.patch:
- replaced ReplicaBeingWritten by FinalizedReplica in the test since the test only checks the logic and does not create files for ReplicaBeingWritten;
- fixed some comments in the test., {noformat}
     [exec] +1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
{noformat}
Also passed all unit tests locally., +1. The patch looks good., Hairong, thank you for reviewing it.

I have committed this to 0.21 and above., Integrated in Hdfs-Patch-h2.grid.sp2.yahoo.net #47 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/47/])
    , Integrated in Hadoop-Hdfs-trunk-Commit #79 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk-Commit/79/])
    , Integrated in Hdfs-Patch-h5.grid.sp2.yahoo.net #78 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/78/])
    , Looks this scenario applicable for 20version also.
patch available for 20 version?, By 0.20, I guess you mean 0.20-append.  Since 0.20-append and 0.21 have different append design and codes, the patch here won't apply without a significant change.  We also have to carefully verify the design difference.  So, how about we create another jira for 0.20-append if a similar problem exist?, > patch available for 20 version?
Forgot to answer you question: I don't have a patch for 0.20-append.]