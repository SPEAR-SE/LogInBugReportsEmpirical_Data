[This is against 0.21 release, This is done against project root & second file changes (imports) are removed, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12483269/patch2.diff
  against trunk revision 1137675.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/804//console

This message is automatically generated., Hi. Can you check if this still exists in trunk and if so prepare a patch against trunk? Thanks., I will try, but I can't be sure that same exceptions or code pathes are used., A trunk patch. Node: Better looking into code it seems that it is not needed to rethrow InterruptedIOException, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12483425/patch.diff
  against trunk revision 1138262.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.hdfs.TestMultiThreadedHflush

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/816//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/816//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/816//console

This message is automatically generated., I've got today one more stack trace that forced "disk check": 

2011-08-30 14:48:39,010 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: checkDiskError: exception: 
java.io.IOException: Broken pipe
        at sun.nio.ch.FileDispatcher.write0(Native Method)
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:29)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
        at sun.nio.ch.IOUtil.write(IOUtil.java:75)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:105)
        at java.io.DataOutputStream.writeShort(DataOutputStream.java:150)
        at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.write(DataTransferProtocol.java:543)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:916)
        at java.lang.Thread.run(Thread.java:619)

It seems that BlockReceiver$PacketResponder.run do not distinguish between disk errors and network errors and runs checkDiskError in any case. And checkDiskError is very time consuming if you have a lot of directories. So, actually there are next problems to be fixed:
 1) checkDiskError should not be called on network errors
 2) checkDiskError should either not lock the whole data dir or be much more lighter.
In any case, why the whole tree (dirs only!) is analyzed? Is it because of possible multiple mount  points possible in there? In any case, checking every single directory looks like overkill for me. In my case, it simply makes node "dead"., OK, today I've got intensive "dead node" storm because of this problem, so my next patch follows. It allows to specify maximum depth to scan in checkDisError, Introduce new parameter to limit checkDiskError depth. Default value of -1 means no limit, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12492453/pathch3.diff
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/1182//console

This message is automatically generated., What do some other folks think about Vitalii's approach? It looks like the code which recursively scans the entire drive is fairly ancient, not sure why we decided to do that.

Another approach would be to do the scanning without holding the volume lock - similar to how we now do block report gathering in branch-1 (HDFS-2379), Folks,

we are hitting this in production with the same kinda of effects mentioned here. We are running cdh3u3. 
Till the time the fix makes it into another update, can anyone suggest any mechanism to work-around this problem.

, After looking at the code in detail i am wondering why do we need to do checkDiskError() for all IOExceptions in PacketResponder if it's JOB is to just send ACKS.

Connection Reset By Peer, Interrupted Channel Exception are all types of IOException and should not causes checkDisk().

What do you folks think about it?


, bq. Another approach would be to do the scanning without holding the volume lock - similar to how we now do block report gathering in branch-1 (HDFS-2379)

I think this approach might be better.  Sometimes, flaky disks experience long I/O pauses, so it seems unwise in general to be holding an important lock while doing this kind of thing., Even we have hit this issue in production.In our case most of checkDiskError() invocations are happening due to network related exceptions in BlockReciever$PacketResponder.run() and DataNode$DataTransfer.run().
Since the current code has a common catch clause for all types of exceptions hence even for network related IO Exceptions chedkDiskError() in executed which lead to check storm thereby slowing the datanode.

One way to fix this could be to check in the catch clause that whether the class of exception belongs to "java.net" package and in those cases skip checking the disk.

Folks,
Please suggest if you think that above mentioned fix is the right approach to go about.
I can than submit the patch for this issue.
, This patch avoids running checkdisk when a socket timeout occurs or any other socket exception is thrown., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12572915/HDFS-2095.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4065//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4065//console

This message is automatically generated., Since my patch only resolves part of this issues i.e invocation of checkDiskError on network related problems.
Hence creating a new jira for the same and submitting this patch there.
New jira: https://issues.apache.org/jira/browse/HDFS-4581, This seems to have been fully duplicated by HDFS-4581]