[let me understand this one. Suppose the datanode is storing its blocks on a Linux ext3 filesystem. are you saying that a stat on the Linux ext3 block file should return a file size that should be the same as reported by InterDatanodeProtocol.getBlockMetaDataInfo().getNumBytes()?

, Hey Dhruba,

That is correct (I guess I should mention, as this is a Java project, not a Unix project, stat is equivalent to File.length...).

This is the use case:
1) Node loses power.
2) On reboot, linux triggers an automatic fsck of hadoop's storage system
3) To clean up some discovered corruption, linux truncates one of Hadoop's blocks
4) Hadoop starts up - reads in the metadata, and assumes the block is OK.

I would like to alter step (4) to be:
4) Hadoop starts up, reads in metadata
5) Hadoop checks to make sure block length recorded in the metadata file is the same as the block length recorded by the ext3 filesystem.

My apologies if this is already done and I am just not understanding things correctly., 
Currently block metadata does not store size of the block. I don't think it should either. But DN can still detect the discrepancy since file lengths of metadata and block sizes don't tally (metadata file length shold be : header + ((block size + 511)/512)*4). 

> This is the use case: [...]
In this case, NN should have detected that that block is smaller than expected. I think it does., This is true for data file, if the length of a data block get truncated 
by ext3 fsck, NN will detect this because the NN knows the length
of the block. 
But we saw a case that once a meta file get truncated, 
and DN boots up, send a block report to NN, NN doesn't detect
that problem. 
This kind of corruption can only be detect by a reader, 
or a data block scanner. 
, Not only was it previously reported Brian, it was done by you! :D

Closing as a dupe, as I found the JIRA.]