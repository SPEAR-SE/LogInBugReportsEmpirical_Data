[Hi [~dmitrybugaychenko].  Thank you for the report.

I noticed that the replication factor is set to 2 in the example that you gave.  Is this the replication factor for all files that experienced this problem?

I haven't seen anything like this.  When we lose a replica from a DataNode going down, we expect the NameNode to schedule a re-replication to another healthy DataNode.  However, replication factor is most often set to 3.  I'm wondering if there might be a bug that only gets triggered with replication factor set to 2., Hi,

We have a cluster with bad disk(controler)s, and see this problem often.
Our replication facor is 3.
Luckely most disks recover after a powercycle.

Regards Jasper

, I see this same behavior in our cluster (Hadoop 2.0.0) CDH4.7.  These are for files with a replica count of 3., We have different RF configured for different files, including 1, 2 and 3. Replicas might get lost for files with any of them. It looks like that there is a workaround that forces namenode to handle recovery properly - after adding this piece to core-site.xml it seems to work:

{code}
  <property>
    <name>net.topology.script.file.name</name>
    <value>/this/enables/BlockManager/shouldCheckForEnoughRacks</value>
  </property>
{code}, We have different RF configured for different files, including 1, 2 and 3. Replicas might get lost for files with any of them. It looks like that there is a workaround that forces namenode to handle recovery properly - after adding this piece to core-site.xml it seems to work:

{code}
  <property>
    <name>net.topology.script.file.name</name>
    <value>/this/enables/BlockManager/shouldCheckForEnoughRacks</value>
  </property>
{code}, We have different RF configured for different files, including 1, 2 and 3. Replicas might get lost for files with any of them. It looks like that there is a workaround that forces namenode to handle recovery properly - after adding this piece to core-site.xml it seems to work:

{code}
  <property>
    <name>net.topology.script.file.name</name>
    <value>/this/enables/BlockManager/shouldCheckForEnoughRacks</value>
  </property>
{code}, We have different RF configured for different files, including 1, 2 and 3. Replicas might get lost for files with any of them. It looks like that there is a workaround that forces namenode to handle recovery properly - after adding this piece to core-site.xml it seems to work:

{code}
  <property>
    <name>net.topology.script.file.name</name>
    <value>/this/enables/BlockManager/shouldCheckForEnoughRacks</value>
  </property>
{code}, Our cluster also have this issue.
After I checked the trunk code, I found that HDFS-7208 already fixed the issue., HDFS-7208 does look like a plausible explanation.  I propose that we resolve this as a duplicate unless there are objections., As per prior comments, I am resolving this as a duplicate of HDFS-7208.]