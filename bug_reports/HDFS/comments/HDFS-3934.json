[This patch resolves the entries in the hosts files to their first IP addresses before de-duplicating everything.  when creating a new {{DatanodeInfo}} for them, it uses {{getCanonicalAddress}}.

This fixes the "invisible node" problem where due to lacking a hostname, the NameNode web UI would show an entry like {{:50010}} in its lists (note missing hostname).

It also fixes the problem where we put two hostnames which refer to the same IP address in a host file, or a hostname and an IP which both turn out to map to the same hostname., * fix test that relies on unresolvable hostname

* fix behavior for hosts files that have lines of the form <entry>:<port> rather than just <entry>, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12577696/HDFS-3934.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4207//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4207//console

This message is automatically generated., * add a test for duplicate hosts entries.

* Add test timeouts for tests in {{TestDecommission}}.

* In {{DatanodeManager#getDatanodeListForReport}}, de-dupe by ip address + port, rather than by ip address alone.  (since multiple DNs can run on the same node with different ports)., when the port is not specified in the hosts file, use a default from the configuration, rather than a compile-time default., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12577885/HDFS-3934.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4211//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4211//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12577899/HDFS-3934.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4212//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4212//console

This message is automatically generated., {code}
+    this.defaultInfoPort = NetUtils.createSocketAddr(
+        conf.get(DFSConfigKeys.DFS_DATANODE_HTTPS_ADDRESS_KEY,
{code}
This doesn't seem right -- the HTTPS port isn't the default unless it's a secure cluster.

----

I'm a little nervous about the additional DNS checks inside getDatanodeListForReport. I think this might end up reverting part of the work done in HDFS-3990. Daryn, can you take a look at this patch?, It's true that this does add additional DNS checks for {{dfs.hosts.exclude}} and {{dfs.hosts}}.

It should be possible to drop the {{FSNamesystem}} lock while these DNS resolutions are going on.

Alternately, we could move the DNS resolutions into {{DatanodeManager#refreshHostsReader}}.  This would mean that administrators would have to re-run "{{dfsadmin -refreshNodes}}" if the DNS mapping changed for a hostname in the include or excludes file.

Finally, we could simply tell people who are concerned about DNS resolution time to use IP addresses in the include and exclude files.  We'd also have to change the code to avoid doing the reverse DNS lookup in this scenario.

I think I like solution #2 the best; what do you think?, Which one's solution #2? This one?

{quote}
Alternately, we could move the DNS resolutions into DatanodeManager#refreshHostsReader. This would mean that administrators would have to re-run "dfsadmin -refreshNodes" if the DNS mapping changed for a hostname in the include or excludes file.
{quote}

I think that's a reasonable thing... basically, when we read the list, we add an exclude entry to our internal data structure both for the IP address and its canonical hostname, so if a DN registers with either one, we'll reject it., here is a new version that is smarter about DNS resolution.  it resolves the DNS only during the refreshNodes operation., Quick notes on the patch:

- Why are you using the visitor pattern instead of just exposing getters from the reader class? Doesn't seem like there's much you're gaining out of it, since it's just a simple iteration over a list. If you're worried about exposing internal state, you could make the fields of the reader class ImmutableMaps -- it looks like they're atomically replaced by refresh() anyway.

- Seems like the refresh code could avoid synchronization until the point where the new maps are swapped in -- otherwise if the disk is slow during refreshNodes, or the DNS is slow, any other operations will end up blocking

- New file is missing interface audience and license

I'll let Daryn or someone else who knows this area of the code a bit better comment on whether the fix is actually semantically correct.
, * use immutablemap

* reduce synchronization, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578129/HDFS-3934.005.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestStartup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4217//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/4217//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4217//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578144/HDFS-3934.006.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDecommission
                  org.apache.hadoop.hdfs.server.namenode.TestStartup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4219//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/4219//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4219//console

This message is automatically generated., * add license header

* use a HashMap rather than ImmutableMap.Builder, to handle duplicates, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578245/HDFS-3934.007.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestStartup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4222//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4222//console

This message is automatically generated., Thinking about this a little more, I think we need to consider an entry in the excludes or includes file which does not contain a port to cover *any* DataNode on that host.  If we don't, there is a risk of breaking existing clients with this change (and that is the breakage observed in {{TestStartup}})., * If the port is missing, treat the include/exclude entry as covering any Datanode on the host.

* grammar fix in comment., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12578837/HDFS-3934.008.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4257//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4257//console

This message is automatically generated., Quick review just by eyeballing the patch:

It'd be nice to continue to use the {{HostsFileReader}} and post-process the result.  Otherwise it's a consistency/maintenance to copy-n-paste any new parsing functionality.

Why does the reader need to instantiate dummy {{DatanodeID}}?  It appears to be for repeatedly making the somewhat fragile assumption that xferAddr is ipAddr+port?  If that relationship changes, we've got a problem...

Patch appears to have dropped supported for the node's registration name.  [~eli] wanted me to maintain that feature in HDFS-3990.  If we need to keep it, doing a lookup and a canonical lookup (can trigger another dns lookup) isn't compatible with supporting the reg name.

Doing a lookup followed by {{getCanonicalName}} is a bad idea.  It does 2 more lookups: hostname -> PTR -> A so it can resolve CNAMES to IP to hostname.  With this change I think it will cause 3 lookups per host.

Question about "// If no transfer port was specified, we take a guess".  Why needed, and what are the ramifications for getting this wrong?  Just a display issue?

It _feels_ like de-dupping the display should be a bit easier to do w/o changing core node registration logic?, bq. It'd be nice to continue to use the HostsFileReader and post-process the result. Otherwise it's a consistency/maintenance to copy-n-paste any new parsing functionality.

OK, I'll use the {{HostsFileReader}} parsing code.

bq. Why does the reader need to instantiate dummy DatanodeID?

You're right.  Re-using {{DatanodeID}} for this purpose doesn't reall ymake sense.  I created a new type called {{HostFileManager#Entry}} to represent host file entries.

bq. It appears to be for repeatedly making the somewhat fragile assumption that xferAddr is ipAddr+port? If that relationship changes, we've got a problem...

Fixed to use getIpAddr() + ":" + getXferPort() in all cases.

bq. Patch appears to have dropped supported for the node's registration name. Eli Collins wanted me to maintain that feature in HDFS-3990. If we need to keep it, doing a lookup and a canonical lookup (can trigger another dns lookup) isn't compatible with supporting the reg name.

Thanks for pointing this out.  I talked to Eli and he explained the distinction between registration names and hostnames to me.  I added back support for "registration names" and added a unit test to ensure this works properly.

bq. Doing a lookup followed by getCanonicalName is a bad idea. It does 2 more lookups: hostname -> PTR -> A so it can resolve CNAMES to IP to hostname. With this change I think it will cause 3 lookups per host.

One key feature of this change is that all the lookups happen when the include and exclude files are read.  *No* lookups happen during {{DatanodeManager#getDatanodeListForReport}}, or any of the other cases where we check the host file entries.

On the advice of Eli, I removed the call to {{getCanonicalName}}.  We can just use the name the user specified in the hosts file; that should be fine.

bq. Question about "// If no transfer port was specified, we take a guess". Why needed, and what are the ramifications for getting this wrong? Just a display issue?

We just don't have the information.  If the datanode is dead, we only know what the entry says in the hosts file(s).  If the entries don't have the port, we have to guess.  I don't see any way around this.  It might be more elegant if the web UI could understand the concept of "port is unknown," but adding that seems out of scope.

In addition to the unit tests, I did some manual testing on this and verified that it got rid of the double-counting of nodes in the web UI for me., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12580790/HDFS-3934.010.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4330//console

This message is automatically generated., rebase on trunk, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12580920/HDFS-3934.011.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 2 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestNNThroughputBenchmark
                  org.apache.hadoop.hdfs.TestDatanodeRegistration

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4335//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/4335//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4335//console

This message is automatically generated., * properly handle DatanodeID objects where {{getIpAddr()}} and/or {{getHostName()}} return {{null}}.  This fixes the two failing tests.

* don't log an error when an include/exclude file is set to the empty string.  (this is perfectly acceptable; it just means that we don't have such a file.)

* log the name of the include/exclude file we failed to read in our error message., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12581007/HDFS-3934.012.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 2 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4336//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/4336//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4336//console

This message is automatically generated., fix javadoc warning, etc, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12581212/HDFS-3934.013.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4345//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4345//console

This message is automatically generated., Test failure looks like https://issues.apache.org/jira/browse/HDFS-4261, unrelated to patch., Patch is bigger than I expected, so I'll try to review soon because critical webhdfs issues are dominating my time.  Poke me next week in case I forget.

I did notice log messages are prefaced with "WATERMELON". :), removed some stray log messages, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12581373/HDFS-3934.014.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4352//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4352//console

This message is automatically generated., Looks like the test failure is https://issues.apache.org/jira/browse/HDFS-3538.  Known flaky test.

By the way, this patch is the same as the previous except for the comment change., The latest patch looks pretty good to me, Colin. A few comments:

# Recommend you add @VisibleForTesting annotation to DataNode#getXferPort.
# Recommend you change this exception message to include the full entry string, not just the port part that couldn't be parsed, and mention explicitly that it was the port component that couldn't be parsed:
{code}
+          throw new IOException("invalid number format when parsing " +
+              portStr, e);
{code}
The fact that it was an invalid number and what the number was will already be contained in the message for the NumberFormatException:
# Recommend you add something to this warning message to make it clear that this is expected if using the DN registration name feature, and to make it clear that this was encountered when reading an include or exclude file:
{code}
+        LOG.warn("unknown host " + prefix, e);
{code}
# In HostFileManager.EntrySet#find(Entry), since right after this we uncondtionally return null, you can condense this code into a single {{if (...)}} condition which first checks that "{{ceil != null}}":
{code}
+        if (ceil == null) {
+          return null;
+        }
+        if (ceil.getValue().getIdentifier().equals(
+              toFind.getIdentifier())) {
+          return ceil.getValue();
+        }
{code}
# In HostFileManager.MutableEntrySet#add(DatanodeID), are we guaranteed that datanodeID.getXferPort() >= 0? Perhaps we should assert that?
# Perhaps make HostFileManager.EntrySet.index protected?
# I see the purpose of delaying the throwing of the errors in HostFileManager#refresh, but you might want to add a comment explaining it, since it's not super obvious. I'd also recommend adding something to the log messages in that method along the lines of "failed to read exclude file, continuing to use previous list of excluded nodes" to make it clear what happens in this case.
# Perhaps I'm missing something, but why have separate classes for EntrySet and MutableEntrySet? The only time we use just the normal EntrySet is for the initial empty sets, so seems like we should just have a single class.
# Seems like you should do an s/DataNode/NameNode/g in this comment:
{code}
+    // These entries will be de-duped by the DataNode, since they refer
+    // to the same IP address + port combo.
{code}
# I don't think this code is doing anything in TestDecommission#testDuplicateHostEntries:
{code}
+    info = client.datanodeReport(DatanodeReportType.DEAD);
{code}
# Seems like you could replace all of this code with just three asserts: two calls to Map#contains(...), and one check that Map#size() == 2:
{code}
+    Iterator<Map.Entry<String, DatanodeInfo>> iter =
+            deadByXferAddr.entrySet().iterator();
+    boolean foundPort1 = false, foundPort2 = false;
+    while (iter.hasNext()) {
+      Map.Entry<String, DatanodeInfo> entry = iter.next();
+      DatanodeInfo dn = entry.getValue();
+      if (dn.getXferPort() == port1) {
+        foundPort1 = true;
+        iter.remove();
+      } else if (dn.getXferPort() == port2) {
+        foundPort2 = true;
+        iter.remove();
+      }
+    }
+    Assert.assertTrue("did not find a dead entry with port " + port1,
+        foundPort1);
+    Assert.assertTrue("did not find a dead entry with port " + port2,
+        foundPort2);
+    Assert.assertTrue(deadByXferAddr.isEmpty());
{code}
# I like that you make a copy of the Configuration object in testIncludeByRegistrationName, and recommend you do the same in testDuplicateHostsEntries, just to minimize the likelihood of inter-test interference.

I'll be +1 once these are addressed.

Daryn (or anyone who's intending to review this) - please do so shortly. I'll be committing this soon after Colin posts a patch addressing these comments unless I hear from someone else., points 1, 2, 3, 4, 5, 6, 7, 9, 10, 12: thanks, will fix.

Point 8: {{MutableEntrySet}} is used internally in {{HostFileManager}}, but I don't want it to escape to {{DatanodeManager}}.  I don't want {{DatanodeManager}] to be able to mutate this set.  I was considering using an {{ImmutableMap}}, but unfortunately, {{ImmutableMap#Builder}} chokes when you try to feed it two identical keys with different values.  There might be a more elegant way of doing this, but for now, this seemed like a nice way to give DatanodeManager a read-only view without doing a lot of copying.

Point 11: I see what you're getting at here, but it's a little trickier than it might seem.  If I used {{Map#containsValue}}, I'd have to create a DatanodeInfo object which compared equal to what I was looking for.  Then I have to start thinking about what fields {{DatanodeInfo#equals}} looks at, which I kind of wanted to avoid.  It would work, but it feels kind of messy (I'd be creating a temporary DatanodeID where a lot of the fields were set to dummy values because {{DatanodeInfo#equals}} doesn't "care" about them, and I don't either.)  So I dunno.  Maybe there is a better way to do, but it's not obvious to me., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12584639/HDFS-3934.015.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDistributedFileSystem
                  org.apache.hadoop.hdfs.TestDatanodeRegistration

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4434//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4434//console

This message is automatically generated., * Fix a place in {{TestDatanodeRegistration}} where we created a mock datanode that returned 0 for its xferPort.  Since we're asserting on this now, we need to return a legal port value here.

* Looks like {{DFS_DATANODE_IPC_ADDRESS_DEFAULT}} has been wrong all along (the colon is missing), but we never noticed because we never tried to parse it as a host:port until now.  So let's fix {{DFS_DATANODE_IPC_ADDRESS_DEFAULT}}., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12584734/HDFS-3934.016.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4440//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4440//console

This message is automatically generated., rebased on trunk, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12585302/HDFS-3934.017.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4455//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4455//console

This message is automatically generated., Makes sense re: points 8 and 11. I agree with both of the other fixes you made in response to the failing tests, too.

+1, the latest patch looks good to me. I suggest you wait a day to see if Daryn has any comments before committing this, though., Thanks, I'll look again this afternoon., thanks, Daryn., Integrated in Hadoop-trunk-Commit #3840 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3840/])
    HDFS-3934. duplicative dfs_hosts entries handled wrong. (cmccabe) (Revision 1489065)

     Result = FAILURE
cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489065
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HostsFileReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeRegistration.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
, I talked to Daryn offline about this and he said he was ok with this going in, though he didn't have time this week to re-review., Integrated in Hadoop-trunk-Commit #3841 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3841/])
    Add needed file for HDFS-3934 (cmccabe) (Revision 1489068)

     Result = SUCCESS
cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489068
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HostFileManager.java
, Integrated in Hadoop-trunk-Commit #3843 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3843/])
    Remove extra code that code erroneously committed in HDFS-3934 (cmccabe) (Revision 1489083)

     Result = SUCCESS
cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489083
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HostFileManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
, Integrated in Hadoop-Yarn-trunk #230 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/230/])
    Remove extra code that code erroneously committed in HDFS-3934 (cmccabe) (Revision 1489083)
Add needed file for HDFS-3934 (cmccabe) (Revision 1489068)
HDFS-3934. duplicative dfs_hosts entries handled wrong. (cmccabe) (Revision 1489065)

     Result = FAILURE
cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489083
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HostFileManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java

cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489068
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HostFileManager.java

cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489065
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HostsFileReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeRegistration.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
, Integrated in Hadoop-Hdfs-trunk #1420 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1420/])
    Remove extra code that code erroneously committed in HDFS-3934 (cmccabe) (Revision 1489083)
Add needed file for HDFS-3934 (cmccabe) (Revision 1489068)
HDFS-3934. duplicative dfs_hosts entries handled wrong. (cmccabe) (Revision 1489065)

     Result = FAILURE
cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489083
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HostFileManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java

cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489068
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HostFileManager.java

cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489065
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HostsFileReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeRegistration.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
, Integrated in Hadoop-Mapreduce-trunk #1446 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1446/])
    Remove extra code that code erroneously committed in HDFS-3934 (cmccabe) (Revision 1489083)
Add needed file for HDFS-3934 (cmccabe) (Revision 1489068)
HDFS-3934. duplicative dfs_hosts entries handled wrong. (cmccabe) (Revision 1489065)

     Result = SUCCESS
cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489083
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HostFileManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java

cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489068
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/HostFileManager.java

cmccabe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1489065
Files : 
* /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/HostsFileReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeRegistration.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
, [~daryn]

I'm going to wait a few days before merging to branch-2, in case you want to take a second look at it., Just a quick reminder, if there are no comments on this we'll commit to branch-2 / branch-2.1 in a day or two., Colin, please mark the Fixed Version(s) field as 3.0.0 on committing a patch to trunk. Please do merge this to branch-2.1-beta., committed to branch-2.1-beta and branch-2]