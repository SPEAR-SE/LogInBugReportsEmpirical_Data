[This is becoming a critical issue.  Wide and/or many jobs overwhelm DNs (due to smaller heap) and sometimes the NN.  Either can OOM or go into full GC.  In the case of DNs, it can GC so much that the NN declares it dead.

Hadoop's jetty version does not support limiting of connections, in fact the code contains "README"s that connections should be limited down in the bowels of private/anonymous classes.  It would require some serious risky hacks to override jetty's behavior.

I'd like to solicit comments on an internal approach we are trying:  rather than redirect clients to a DN with a replica, redirect to a random DN in a rack with a replica.  This distributes the jetty connections, DFSClients, and associated objects over many more nodes with an insignificant impact to throughput - esp. over higher latency links and compared to the time it takes for the client to timeout the connection and retry another node which may also be overloaded., [~daryn], is this because some of the blocks are popular?

Also is there anyway the datanode load (number of xceivers assuming the local DFSClient access for webhdfs is counted towards load) can be factored into redirecting?, Popular in the sense that wide jobs needed access to ~3 replicas, yes.  Jobs with 2-5k mappers that ran fine with hdfs cause significant problems with webhdfs.  We saw up to 20k jetty requests queued up while the DN was in the throes of GC.  Presumably the client is timing out and retrying the connection again, which further exacerbates the memory problem as the jetty queue is full of heavy objects for dead connections.

We could take the existing DN load into account, such that we pick the lightest xceiver loaded DN before picking a random rack local.  However the risk is that an onslaught of tasks will be distributed to the DNs with the replica before the DN can report their spikes in load.  This is already a minor issue for hdfs.  In the past we've considered using a "predictive" load where the NN artificially increases the DN's load stats as it gives out locations.  In the end, we chose to keep it simple with random rack-local., This is another case where it's a real shame that we're stuck on Jetty 6.  Starting in Jetty 7, it's possible to limit the queue that backs the {{QueuedThreadPool}}.  Here is the relevant constructor in current Jetty 9:

http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/util/thread/QueuedThreadPool.html#QueuedThreadPool(int, int, int, java.util.concurrent.BlockingQueue)

[~daryn], I know this has always been a touchy subject, but do you think this problem warrants reopening discussion of a Jetty upgrade?  I believe that would set us up for a relatively simple fix in {{HttpServer2#initializeWebServer}}, perhaps with some new config properties to control queue size., [~cnauroth], I agree jetty should be upgraded but I'd rather not delay this important fix while the issue is hashed out.  My understanding is it's not a trivial upgrade and the change in dependencies may be disruptive to downstream projects.

I don't think only limiting the number of jetty connections is good enough, although it would help.  DFSClients hop between nodes to retrieve blocks and the streamers are lightweight compared to jetty connection objects.  A webhdfs remote read creates a DFSClient on a node containing a replica of the first block it will read.  This creates hotspots on a few nodes with the first block, which is esp. bad for wide jobs that will access multi-block files, ex. a map side join on a big file.

Much as yarn doesn't guarantee node-local, why should webhdfs?, I was assuming that the impact of the Jetty queue largely dominates over the impact of any other objects, based on my reading of the Jetty 6 {{QueuedThreadPool}} code.  Of course, you know best what you're seeing in practice from {{jmap}} output (or whatever).  Overall, your proposal sounds good to me., Ok, I'll clean up the internal patch and post a patch soon.  The heap dump showed a lot of pending jetty connection objects and their associated objects and most importantly pre-allocated buffers.  Limiting the number of connections is fine for preventing this bloat, but it can prevent webhdfs access to files that can otherwise be accessed via a lightweight streamer from another node on the rack., The DN webhdfs server has been re-implemeted using netty.]