[Could you elaborate in the description on the nature of the NPE?  A sample stacktrace would be immensely helpful here., Also was client 2.7 or 3.2 ? and same for server., The problem happens when I was testing the router. The client and router were 3.2 and name node was 2.7. The problem is that 2.7 NN would return null for keyProviderUri, which would cause NPE when the router is calling setKeyProviderUri(), which happens during the router is preparing the result to send back to the client, here is the error trace on the router side. It is a backward compatiability issue in a sense that the client side talking to the namenode should handle the case when the keyProviderUri is returned as null. 

org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException

 at org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder.setKeyProviderUri(HdfsProtos.java:37035)

 at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.convert(PBHelperClient.java:2159)

 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getServerDefaults(ClientNamenodeProtocolServerSideTranslatorPB.java:444)

 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)

 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)

 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)

 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:869)

 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:815)

 at java.security.AccessController.doPrivileged(Native Method)

 at javax.security.auth.Subject.doAs(Subject.java:422)

 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)

 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2675)

 

 

 , | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 11s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 26m 10s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 38s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m  9s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 40s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 29s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 14s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 35s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 23s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 59m 15s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |
| JIRA Issue | HADOOP-15336 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12916665/HADOOP-15336.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 9d86f08b017f 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / cdee0a4 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14404/testReport/ |
| Max. process+thread count | 333 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs-client U: hadoop-hdfs-project/hadoop-hdfs-client |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14404/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks [~zhengxg3] for the stack trace.
The stack trace shows that the router(acting as client) got an NPE from namenode.
I am not familiar with RouterRpcServer so I apologize in advance if I sound confused.
But why does router decodes the protobuf response and then encodes again ?
Why can't router just relays the response from namenode without decoding and re-encoding ?

IIUC now I have to think about backwards compatibility w.r.t to dfs client, namenode *and router* even if majority of people don't use router feature ?
IMO this shouldn't be flagged as backwards incompatible., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 10s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 21s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 25m 48s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 29m 20s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  3m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 19m 55s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 26s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: . {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  5m 12s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 21s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 25m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 26m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 26m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  3m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 19m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} shellcheck {color} | {color:green}  0m  0s{color} | {color:green} There were no new shellcheck issues. {color} |
| {color:green}+1{color} | {color:green} shelldocs {color} | {color:green}  0m 10s{color} | {color:green} There were no new shelldocs issues. {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 15 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 10m 21s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m  0s{color} | {color:blue} Skipped patched modules with no Java source: . {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  5m 19s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 14m  1s{color} | {color:red} root in the patch failed. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 39s{color} | {color:red} The patch generated 1 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}205m 36s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.conf.TestCommonConfigurationFields |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |
| JIRA Issue | HADOOP-15336 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12916655/HADOOP-15336.000.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  shellcheck  shelldocs  |
| uname | Linux 14ff2df4e23a 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / cdee0a4 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| shellcheck | v0.4.6 |
| findbugs | v3.1.0-RC1 |
| whitespace | https://builds.apache.org/job/PreCommit-HADOOP-Build/14403/artifact/out/whitespace-eol.txt |
| unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/14403/artifact/out/patch-unit-root.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/14403/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HADOOP-Build/14403/artifact/out/patch-asflicense-problems.txt |
| Max. process+thread count | 1482 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs-client . U: . |
| Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/14403/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Thanks [~shahrs87] for your comments.
The reason for the Router to decode the request is because it needs to know the path the request is going to.
For example, if you ask the router to create a file in /user/user1, the Router needs to get the request, check the mount table to get the destination subcluster (e.g., /user is in /data in subcluster0), and then send a new request with the right file destination to the Namenode (e.g., /data/user1 in subcluster0).

Regarding the backwards compatibility issue, I don't think this is introducing one but fixing one.
In any case, this is a general issue and you can repro by having a client and a server (no need for a Router).
Note that the reverse convert does actually check fs.hasKeyProviderUri().
Actually I would switch the null check by hasKeyProviderUri()., Thanks [~elgoiri] for chiming in.
bq. The reason for the Router to decode the request is because it needs to know the path the request is going to.
In the above stack trace, router is decoding response from namenode _not request_. So it doesn't need to decode.

bq. Regarding the backwards compatibility issue, I don't think this is introducing one but fixing one.
The {{backwards incompatible}} flag on this jira means {{HADOOP-14104}} introduced this incompatibility and that is not correct. I wasn't clear earlier.

bq. In any case, this is a general issue and you can repro by having a client and a server (no need for a Router).
You can't reproduce this issue if there are only 2 components.
Assuming client (dfs client) is 3.2 and server(namenode) is 2.7.
2.7 Namenode doesn't support return keyProviderUri in {{FsServerDefaults}} and there is {{hasKeyProviderUri}} check on client side so it won't throw NPE.
Assuming client (dfs client) is 2.7 and server(namenode) is 3.2.
Namenode supports returning {{keyProviderUri}} but client doesn't understand {{keyProviderUri}} and since {{keyProviderUri}} is an optional field in protobuf, client will decode the fields only it understands. 

, {quote}
In the above stack trace, router is decoding response from namenode not request. So it doesn't need to decode.
{quote}
My example applies in the opposite direction.
When we get /data/user1 from the NN, we need to translate it back into /user/user1.
Note that this is the simple example, we have a bunch of operations that we also need to aggregate across all subclusters.

{quote}
The backwards incompatible flag on this jira means HADOOP-14104 introduced this incompatibility and that is not correct. I wasn't clear earlier.
{quote}
Not sure who added the incompatibility but there is one here. In any case, we should remove the label and for sure not mark it as such.

{quote}
You can't reproduce this issue if there are only 2 components.
{quote}
Correct, you need three components.

In any case, I think this is worth committing.
, bq. In any case, I think this is worth committing.
Agree.

bq. Actually I would switch the null check by hasKeyProviderUri().
I don't think {{FSserverDefaults}} has {{hasKeyProviderUri}} method.
bq. Note that the reverse convert does actually check fs.hasKeyProviderUri().
Reverse is an object FSserverDefaultsProto, thats why it has {{hasKeyProviderUri}} method.
We can add {{hasKeyProviderUri}} method in {{FSserverDefaults}} and do null check in that method., {quote}
Reverse is an object FSserverDefaultsProto, thats why it has hasKeyProviderUri method.
We can add hasKeyProviderUri method in FSserverDefaults and do null check in that method.
{quote}

True, not PB here.
I'm OK with leaving the null check as it is right now; no need to add the new method.

Given the discussion and  [^HADOOP-15336.001.patch]: +1
I'm removing the compatibility flag too., BTW, not sure why Yetus didn't run  [^HADOOP-15336.001.patch].
It works locally but [~zhengxg3], do you mind reuploading?, Why is this Jira in the HADOOP bucket when it's a hdfs change?

This issue illustrates what I think it's going to be a huge issue for the router:  Handling optional PB fields when re-encoding the (decoded) response back into a PB., [~elgoiri] Sure, will reupload, [~daryn] You are right, will move to HDFS. , {quote}
Handling optional PB fields when re-encoding the (decoded) response back into a PB.
{quote}
Correct. In our current setup, all the components have the same version.
Is there a good way to handle this other than having test coverage for this (which I'm not even sure how) and catching when new optional fields are added?, I guess now  [^HADOOP-15336.001.patch]  should be renamed to HDFS-13371.00.patch.
Hopefully Yetus will run the proper unit tests., {quote}Is there a good way to handle this other than having test coverage for this (which I'm not even sure how) and catching when new optional fields are added?
{quote}
Reflection could be leveraged to generate protobuf records for testing. See TestPBImplRecords for an example. Theoretically any field marked optional in the protobuf record could be missing in the test record, although I suspect in practice many of the fields aren't really optional. So the hard part would be knowing which ones are truly optional despite what the protobuf field metadata says. So it might be a manual process to add a test for a new, optional field that was added. However if there was already a test framework in place and very simple to add the new field to the test (i.e.: by adding an annotation to a method/field or single line to a test) then it would be more likely to be done., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:red}-1{color} | {color:red} test4tests {color} | {color:red}  0m  0s{color} | {color:red} The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 27m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 21s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 19s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 26s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 33s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 17s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 26s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 23s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 24s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 23s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 60m 51s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8620d2b |
| JIRA Issue | HDFS-13371 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12916910/HDFS-13371.000.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 2c3e03cc19d0 3.13.0-143-generic #192-Ubuntu SMP Tue Feb 27 10:45:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 2c6cfad |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/23730/testReport/ |
| Max. process+thread count | 331 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs-client U: hadoop-hdfs-project/hadoop-hdfs-client |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/23730/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

]