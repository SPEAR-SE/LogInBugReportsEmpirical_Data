[I believe the issue may be with any place we check:
{code}
    // Ignore replicas already scheduled to be removed from the DN
    if(invalidateBlocks.contains(dn.getStorageID(), block)) {
{code}
since it is ignoring the fact that, after the replication monitor thread has run, the block is no longer in {{BlockManager.invalidateBlocks}}, but instead in that DatanodeDescriptor's {{invalidateBlocks}} list.

Maybe someone can remind me why we even have two separate invalidateBlocks structures in the first place? (one global map keyed by StorageID and another per-datanode list), Hi Todd,

I think corrupt replicas are invalidated only if the Number of good replicas more than or equal to replication. But you told it is invalidated immediately.
{code}// Add this replica to corruptReplicas Map
    corruptReplicas.addToCorruptReplicasMap(storedBlock, node, reason);
    if (countNodes(storedBlock).liveReplicas() >= inode.getReplication()) {
      // the block is over-replicated so invalidate the replicas immediately
      invalidateBlock(storedBlock, node);
    } else if (namesystem.isPopulatingReplQueues()) {
      // add the block to neededReplication
      updateNeededReplications(storedBlock, -1, 0);
    }{code}

If number of datanodes equal to replication, out of which one replica is marked corrupt, then that replica will never be deleted and replication also wont happen.
Same Issue in HDFS-2932.]