[{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12560036/HDFS-4292.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3625//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3625//console

This message is automatically generated., Nice catch Binglin! +1. Todd do you want to take a look?, I'm a bit hazy on this code, having not looked at it for a little while. So, I had to write out some notes for myself, which I'll include here for anyone else who wants to follow along.

The explanation of these variables is that {{firstChunkOffset}} represents the byte offset of what the DN will start sending, and {{startOffset}} is the byte offset that the client has requested. Then {{checksum.getBytesPerChecksum()}} is the 'checksum chunk size', typically 512 bytes. So, for example, if the client wanted to read starting at byte 1026, we would have:
- {{firstChunkOffset == 1024}} (since the DN always aligns backwards to the beginning of a chunk)
- {{startOffset == 1026}}
- {{checksum.getBytesPerChecksum() == 512}}

The new logic says that if {{firstChunkOffset <= (startOffset - checksum.getBytesPerChecksum())}} it is an error -- in other words, that the {{firstChunkOffset}} should be no more than a single chunk backward from the requested start offset. That seems right to me.

So, +1. I'll commit this momentarily, Committed to branch-2 and trunk. Thanks, Binglin. Good catch., Integrated in Hadoop-trunk-Commit #3105 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3105/])
    HDFS-4292. Sanity check not correct in RemoteBlockReader2.newBlockReader. Contributed by Binglin Chang. (Revision 1419675)

     Result = SUCCESS
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1419675
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java
, Thanks @Luke and @Todd for the review!, Integrated in Hadoop-Yarn-trunk #62 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/62/])
    HDFS-4292. Sanity check not correct in RemoteBlockReader2.newBlockReader. Contributed by Binglin Chang. (Revision 1419675)

     Result = FAILURE
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1419675
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java
, Integrated in Hadoop-Hdfs-trunk #1251 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1251/])
    HDFS-4292. Sanity check not correct in RemoteBlockReader2.newBlockReader. Contributed by Binglin Chang. (Revision 1419675)

     Result = FAILURE
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1419675
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java
, Integrated in Hadoop-Mapreduce-trunk #1282 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1282/])
    HDFS-4292. Sanity check not correct in RemoteBlockReader2.newBlockReader. Contributed by Binglin Chang. (Revision 1419675)

     Result = SUCCESS
todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1419675
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/RemoteBlockReader2.java
]