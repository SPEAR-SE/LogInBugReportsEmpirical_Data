[Thanks for reporting this.
bq. my java client use muti-thread to put a same file to a same hdfs uri
I'm a little confused. How you did this or what's your code like? Would you elaborate this a little bit? It may help to understand why it happened. Thanks., If this is not yet confirmed a bug or a feature request, please send email to [mailto:user@hadoop.apache.org]. People there are willing to help you with your problems., bq. my java client use muti-thread to put a same file to a same hdfs uri
Unless each thread creates a separate instance of UserGroupInformation for its file system, they will all look like one writer to the namenode, causing all sorts of problems., Thank you for your reply. 

my client code :
【main class】
public class HadoopLoader {
  public HadoopLoader() {
  }

  public static void main(String[] args) {
    HadoopLoader hadoopLoader = new HadoopLoader();

    //上传数据
    hadoopLoader.upload();
  }

  private void upload() {
    new UploadProcess().upload();
  }
====================================================================================================
public class UploadProcess {
  private ExecutorService executorService;
  private Map<String, Boolean> processingFileMap = new ConcurrentHashMap<String, Boolean>();

  public void upload() {
    executorService = Executors.newFixedThreadPool(HadoopLoader.CONFIG_PROPERTIES.getHandleNum());

    for (int i = 0; i < 1000; i++) {
      processLoad("/home/ztb/testdata/43.bmp", "hdfs://10.43.156.157:9000/ztbtest");
    }
  }

  private void processLoad(String filePathName, String hdfsFilePathName) {
    LoadThread loadThread = new LoadThread(filePathName, hdfsFilePathName);
    executorService.execute(loadThread);
  }

}

===================================================================================================
public class LoadThread implements Runnable {
  private static final org.apache.commons.logging.Log LOG = LogFactory.getLog(LoadThread.class);

  String filePathName;         //数据文件完整名称（路径名+文件名）
  String hdfsFilePathName;     //数据文件完整名称（路径名+文件名）

  public LoadThread(String filePathName, String hdfsFilePathName) {
    this.filePathName = filePathName;
    this.hdfsFilePathName = hdfsFilePathName;
  }

  public void writeToHdfs(String filePathName, String hdfsFilePathName) throws IOException {
    LOG.info("Start to upload " + filePathName + " to " + hdfsFilePathName);
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(URI.create(hdfsFilePathName), conf);
    InputStream in = null;
    OutputStream out = null;
    Path hdfsFilePath;
    try {
      in = new BufferedInputStream(new FileInputStream(filePathName));
      hdfsFilePath = new Path(hdfsFilePathName);
      out = fs.create(hdfsFilePath);
      IOUtils.copyBytes(in, out, conf);
    } finally {
      if (in != null) {
        in.close();
      }
      if (out != null) {
        out.close();
      }
    }
    LOG.info("Finish uploading " + filePathName + " to " + hdfsFilePathName);
  }

  @Override
  public void run() {
    try {
      writeToHdfs(filePathName, hdfsFilePathName);
    } catch (IOException e) {
      LOG.error(e.getMessage(), e);
    }
  }

}




i get java_pid8820.hprof when i set -XX:+HeapDumpOnOutOfMemoryError
, I do not use "FileSystem get(final URI uri, final Configuration conf, String user)" to get the file system.

By the way, this is my another question, why when i use "FileSystem get(final URI uri, final Configuration conf, String user)" with the same user string , the file system i got are different(DFSClient
  is a new instance for each time), the FS cache is useless ? should i must close the fs everytime if i use "FileSystem get(final URI uri, final Configuration conf, String user)" to get a fs even with the same user?



Thanks., Looking forward to your reply, I am very grateful for your help, Looking at your attached codes, you're crazily trying to use *10000* threads to write to the same HDFS file, which is surely not to work. What's behavior and output would you expect? As Kihwal said, this can cause all sorts of problems. I thought you need to be clear about what you want to achieve, then ask your questions in the user mailing list about how to do it, as MIngliang suggested., Looks like you want to implement a files loading tool that uploads files to HDFS cluster, if so, you may take a look at the work in HDFS-8968, where a benchmark tool does the similar things to benchmark write throughput, using multiple concurrent writers in threads, but surely writing to different HDFS files., Yes i got you, this is a abnormal test scenario, but whatever it should not cause the client dumped with OOM, right? I just want to know why this scenario cause the client OOM, maybe some underlying streams of hdfs have not bean closed?  

Thank you very much., Why you reopened this issue? Please note JIRA is not a place to answer your questions. As suggested above, please move to user/dev mailing list. Please close it., bq. whatever it should not cause the client dumped with OOM
You opened 10000 threads to upload file, and then ask why it OOMed? This sounds really interesting. You please do yourself a favor investigating it and might not expect others to have the time for this., Sorry i did not make it clear. 

When the upload uri is different everytime with 10000 threads, everything is ok.
 

Test codes like this:
  
    for (int i = 0; i < 10000; i++) {
      //this case is ok
      String uri = "hdfs://10.43.156.157:9000/ztbtest/" + i;

      //this case OOMed 
      //String uri= "hdfs://10.43.156.157:9000/ztbtest";

      processLoad("/home/ztb/testdata/43.bmp", uri);
    }


  , I knew what you did, as I said above, you were using so many threads writing to the same HDFS file, which is not supported yet, if it's ever going to be supported. Every time there is only ONE writer to own the file lease and can write to the file. You stacktrace told it clearly: LeaseExpiredException, No lease. You just made the NN and your client crazy, causing all sorts of problems, which I don't think you would need the time to investigate.]