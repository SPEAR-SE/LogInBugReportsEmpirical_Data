[Patch for 0.19 attached, The patch seems to be applicable both for trunk and for 0.19 branch. , -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12394193/4681.patch
  against trunk revision 719431.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 Eclipse classpath. The patch retains Eclipse classpath integrity.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3621/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3621/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3621/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch/3621/console

This message is automatically generated., The current patch causes about 2x (st x=dfs.client.max.block.acquire.failures) the number retries for an unrecoverable block. It really retries at N\*x, where N is the initial value of {{retries}} in DFSClint.DFSInputStream::read. To be fair, it looks like the current code exercises the same retry logic, but without resetting {{failures}}, the first exhaustion of sources causes it to bail out. It's not clear what the semantics are supposed to be in this case, but it's worth noting that this patch would change them.

bq. This seems to be also related to HADOOP-3185, but I'm not sure I really understand necessity of keeping track of failed block accesses in the DFS client.
IIRC, the intent of HADOOP-3185 was to avoid filling the deadNodes list with good nodes hosting earler, bad blocks. It also improves on the quick fix in HADOOP-1911.

I haven't been able to find a path where applying the patch reintroduces an infinite loop., Hi Igor, Thanks for the patch. 

Chris, Do we still need this patch? Can you comment on whether this problem still exists in trunk? And if so, can  Igor supply a unit test too? , The patch did resolve the issue with long-lived streams introduced by HADOOP-1911 without undoing the fix; this almost certainly remains in trunk. I'd expect a unit test for this would be difficult to write...

IIRC, it wasn't committed because the change to the retry semantics may not have been appropriate. Someone more familiar with DFSClient should make that call., +1 on patch being applied to TRUNK as well as to 0.20, and 0.19 branches.  DFSClient currently in its treatment of errors is as crude in effect as California's three-strikes rule; any three errors encountered on a file no matter on which of its N blocks and the stream is ruined (See HADOOP-5903 description for more detail though HADOOP-3185 nails the issue way back)., This bug exists and I don't think the current patch is the right one (yet). We probably don't need this variable at all (see below).

Looking at how 'failures' variable is used, it is pretty limited. My guess is that it was there right from the beginning and a lot of DFSClient has changed around it since then.

I would say we need description of what it means : i.e. when it should be incremented and why there should be a limit. That will also answer when it should be reset. 

As I see it now : it is incremented only  when connect to a datanode fails. That implies, it should be reset when such connect succeeds (in chooseDataNode()). 

But this is still not enough since it does not allow DFSClient to try all the replicas available (what if number of replicas is larger than 3?). May be we should try each datanode once (or twice)...That implies we probably don't need this variable at all. Just some local counter in 'chooseDataNode()' would do.
, bq. May be we should try each datanode once (or twice)...That implies we probably don't need this variable at all. Just some local counter in 'chooseDataNode()' would do.
Wouldn't this revert- and reintroduce- HADOOP-1911?

bq. I would say we need description of what it means : i.e. when it should be incremented and why there should be a limit. That will also answer when it should be reset.
+1, > Wouldn't this revert- and reintroduce- HADOOP-1911?

I see. I just looked at HADOOP-1911 and I don't think it fixed the real problem. The loop is because of combination of reset of dead nodes in chooseDataNode() and 'while (s != null)' loop in blockSeekTo().  Note that actual failure occurs while trying to create BlockReader().. not in chooseDataNode(). The problem is that chooseDataNode() can not decide if the datanode is ok or not.. just an address for DN is not enough. we also need connect(), 'success reply' and at least a few bytes read from the datanode. So HADOOP-1911 fixed the infinite loop, but not for right reasons.

We could define successful datanode conenction as 'being able to read non zero bytes that we need'. A failure count keeps growing until there is a 'successful connection', and should be reset after that. (Some what similar approach to HADOOP-3831).

I think this time we should have a explicitly stated policy of when a hard failure occurs (and may be when we refetch the block data etc).

, .bq We could define successful datanode conenction as 'being able to read non zero bytes that we need'. A failure count keeps growing until there is a 'successful connection', and should be reset after that. (Some what similar approach to HADOOP-3831).

This seems reasonable to me., bq. I just looked at HADOOP-1911 and I don't think it fixed the real problem.

No question; it was literally a last-minute fix for 0.17.0, intended as a partial fix until HADOOP-3185 could be given priority. To be fair, the resolution explicitly renounces any claim to a fix for the real issue...

bq. We could define successful datanode conenction as 'being able to read non zero bytes that we need'. A failure count keeps growing until there is a 'successful connection', and should be reset after that. (Some what similar approach to HADOOP-3831).

Isn't that what this patch effects (+/- the 2x dfs.client.max.block.acquire.failures semantics, which are a little odd)? I completely agree that the client code should be reworked to make the retry mechanism more legible, but- particularly if the idea is to push a fix into 0.19 and later- wouldn't it make sense to use this tweak or a variant of it? Clearly it's tricky code, so it seems reasonable to add something like HADOOP-3831 to trunk rather than back-porting it to earlier branches., > Isn't that what this patch effects (+/- the 2x dfs.client.max.block.acquire.failures semantics, which are a little odd)?

sort of, I think. It is an improvement, but only on the same lines as the current structure (no explicit policy, hard to see what happens and when).  For e.g. it looks like 'a failure' is defined as not being able to connect/fetch from all the replicas. Also success is defined as just being able to get 'ok' from datanode. Which implies it might again go into infinite loop in some conditions (say all replicas are corrupt, thus only one is remaining, and it fails checksum at one particular position).

The final fix could be as simiple as moving resetting 'failures' to readBuffer().  But I would prefer to add things like 'failures' to be incremented for every failure with a replica and limit the count to something like min(#replicas, large value like 10).

Finally, I am mainly proposing an explicit policy in a code comment. 0.21 is fine.. big users like HBASE can have it backported., .bq For e.g. it looks like 'a failure' is defined as not being able to connect/fetch from all the replicas. Also success is defined as just being able to get 'ok' from datanode.

I'd say this a substantial improvement over whats currently in place.  Will suggest that hbase users apply current patch at least for now.

I agree that it'd be better if failure were 'smarter' counting the likes of bad checksums in replica, etc., and that the 'failure' policy be explicitly stated., I just tried this patch after getting a lot of bad blocks reported under heavy load from HBase.

After applying this, I can now get through all my load tests without a problem.  The datanode is heavily loaded and HBase takes a while to perform compactions (~1min worst case) but it manages to get through it whereas without the patch it crapped out and I wasn't able to recover easily.

I'm running an otherwise clean hadoop 0.20.0, +1 this patch is a _must have_ for anyone running hbase. 

The lack of it in hadoop trunk is forcing us to ship a non-standard hadoop jar just for a 2 line fix.

Please commit already!, Raghu> Finally, I am mainly proposing an explicit policy in a code comment. [...]
Stack> I agree that [...] etc., and that the 'failure' policy be explicitly stated.

Is something blocking from adding a description of what 'failures' is meant for?
Otherwise, IMHO it is just a magic fix that could lead to similar problems in future.. b/c it is hard for developers to keep and review contracts that they don't know.. that is the reason why this bug first appeared.

Not a -1 from me.
, Linking HDFS-656. This JIRA is for discussion about what the failure/retry semantics are now and what they really should be., lgor's patch looks good to me but it got stale., h127_20091016.patch: based on lgor's patch,
- removed DFSInputStream.failures and added a local variable in DFSInputStream.chooseDataNode(..) since "failures" is local property;
- changed DFSClient.bestNode(..) to static and removed "throws IOException"; and
- simplified DFSClient.DNAddrPair., here is a fixed version for hdfs-branch-0.21.  , Ryan, your attachment doesn't look like a patch.  Its a git-made patch.  Does hudson know how to apply theses?

Nicholas, your patch looks more interesting moving the failure counter local.  The other changes seem fine.  Any reason for making the bestNode static?   You've changed what bestNode returns.  It now returns null rather than throw an IOE with "no live nodes contain current block".  Us over in hbase are not going to miss that message.

I'm testing your patch now..., > ... Any reason for making the bestNode static? You've changed what bestNode returns. It now returns null rather than throw an IOE with "no live nodes contain current block". Us over in hbase are not going to miss that message.

It is because bestNode(..) is a simple utility method.  It does not use any DFSClient fields/methods.

In the original codes, bestNode(..) is invoked only in chooseDataNode(..), which basically use a try-catch to deal with the IOException thrown by bestNode(..).
{code}
//original codes in chooseDataNode(..)
        try {
          DatanodeInfo chosenNode = bestNode(nodes, deadNodes);
          ...
        } catch (IOException ie) {
          ...
          LOG.info("Could not obtain block " + block.getBlock()
              + " from any node: " + ie
              + ". Will get new block locations from namenode and retry...");
          ...
        }
{code}
As shown above, the IOException ie is used only for a log message.  I think "No live nodes contain current block" is a kind of redundant in the log message.  Do you think that  "Could not obtain block " + block.getBlock() + " from any node" is clear enough?  No?  We may change the log message if necessary., Thanks for explanation.  I think message is fine.  Let me do some local testing.  I'll be back., This patch definitely improves the condition on trunk to prevent a long-running-dfs-reader from baling out prematurely. Is there a requirement to patch this on hadoop 0.20 as well?, +1 to committing a fix on branch-20. We've included the original patch in our distro for some customers and seen no issues running at reasonable scale - it's now part of our standard distro and no complaints there either., Just to say that HBase ships with a patched hadoop that includes the old 4681.patch since our 0.20.0 release.  I only know of how it improved things.  I've not heard of detrimental effects., Will send an email to the mailing list for committing this to 0.20., Since this may be back ported to 0.20, I reverted the code re-factoring in the previous patch to minimize the chance of having silly mistakes.

h127_20091019.patch:
- declared failures as a local variable; and
- added final to maxBlockAcquireFailures., +1 on this patch.

Some notes:

+ This patch does not have the issue that hadoop-1911 "fixed" though it moves the failure counter back to being a local variable.  There is no danger that it "...revert[s]- and reintroduce[s]- HADOOP-1911?" because the patch removes the catching of exceptions inside chooseDataNode while loop.
+ I am of the opinion that this patch should not be gated on the necessary work HDFS-656 "Clarify error handling and retry semantics for DFS read path" (nor HDFS-378 "DFSClient should track failures by block rather than globally").  This patch should actually help some as it narrows scope of the failures variable.  But maybe we can do even more as part of this patch (See below).

If a new patch is cut, the following might be considered:

+ There is no documentation of "dfs.client.max.block.acquire.failures" nor of what local counter 'failures' means.  Above its suggested that we at least doc. what a failure is ("For e.g. it looks like 'a failure' is defined as not being able to connect/fetch from all the replicas. Also success is defined as just being able to get 'ok' from datanode.")
+ By default, "dfs.client.max.block.acquire.failures" is 3.  If more than 3 replicas, we could fail though good replicas out on cluster.  Should DFSClient set this.maxBlockAcquireFailures default to max of dfs.replication/dfs.client.max.block.acquire.failures?
+ I'd put these two LOG.info messages together rather than have two LOG.info lines:

{code}
          if (nodes == null || nodes.length == 0) {
            LOG.info("No node available for block: " + blockInfo);
          }
          LOG.info("Could not obtain block " + block.getBlock()
              + " from any node. Will get new block locations from namenode and retry...");
{code}, bq. By default, "dfs.client.max.block.acquire.failures" is 3. If more than 3 replicas, we could fail though good replicas out on cluster. Should DFSClient set this.maxBlockAcquireFailures default to max of dfs.replication/dfs.client.max.block.acquire.failures?

Another example of bad naming - this variable actually refers to the number of times the read path will go back to the namenode for a new set of locations. It's to deal with the case when some locations have been cached for some amount of time, during which the blocks have been moved to new locations., To be clear, above +1 and comments were for h127_20091016.patch.

Could h127_20091019.patch have the issue hadoop-1911 "fixed"?, h127_20091019.patch failed on TestBlockMissingException.  It may also have problem on HADOOP-1911.  Need more works., h127_20091019b.patch: synced Igor's patch with trunk.

Since this is likely to be committed to 0.20, it is better to use Igor's patch which was already tested extensively.

Stack, your comments totally make sense to me but I don't want to introduce big change on 0.20.  Let's do the improvement on separated issues like HDFS-656, HDFS-378, etc., bq. Stack, your comments totally make sense to me but I don't want to introduce big change on 0.20. Let's do the improvement on separated issues like HDFS-656, HDFS-378, etc.

+1 - agree we should keep the branch-20 change minimal., +1 on Igor's "4681.patch" to 0.20 branch (Good stuff)., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422638/h127_20091019b.patch
  against trunk revision 826905.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/41/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/41/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/41/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/41/console

This message is automatically generated., The failure of TestFileAppend2 is not related to this.

No new tests added since the there is an existing test, TestBlockMissingException, for this and the changes are very simple.

I will wait for a few days for [the vote|http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/200910.mbox/%3C176629.46810.qm@web56207.mail.re3.yahoo.com%3E].  See whether this could be committed to 0.20., I have committed this to 0.20 and above.  Thanks, Igor!

(The assignee of this issue should be Igor.  I merely synced the patch with trunk.), Integrated in Hadoop-Hdfs-trunk-Commit #79 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk-Commit/79/])
    , Integrated in Hdfs-Patch-h2.grid.sp2.yahoo.net #54 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/54/])
    , Integrated in Hadoop-Hdfs-trunk #120 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk/120/])
    , Integrated in Hdfs-Patch-h5.grid.sp2.yahoo.net #78 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/78/])
    , Unfortunately, the 0.20 patch causes TestFsck failing; see [Suresh's comment|https://issues.apache.org/jira/browse/HDFS-784?focusedCommentId=12798232&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12798232]., Checked 0.21 and 0.22, the problem does not exist.  I am going to revert the patch from 0.20., I have reverted the patch committed to 0.20., @Tsz Wo: does the issue happen if the patch is applied against 0.20.1?  HBase ships with this patch; is it safer to not have it?, @Zlatin: yes, I just have checked 0.20.1 release with the patch (4681.patch).  TestFsck failed.

As mentioned by Suresh, the patch causes an infinite loop on DFSClient when reading a block with all the replicas corrupted.  

I don't know much about HBase.  So I cannot answer your HBase question.  Please check with the HBase mailing lists., Integrated in Hadoop-Hdfs-trunk-Commit #169 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk-Commit/169/])
    Move  from 0.20 to 0.21 in CHANGES.txt.
, Integrated in Hdfs-Patch-h2.grid.sp2.yahoo.net #96 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/96/])
    Move  from 0.20 to 0.21 in CHANGES.txt.
, For those interested in this problem, I made some notes towards a proper fix in HDFS-378, Integrated in Hadoop-Hdfs-trunk #202 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk/202/])
    , Integrated in Hdfs-Patch-h5.grid.sp2.yahoo.net #196 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/196/])
    , In case anyone finds this useful, here's an additional test for TestCrcCorruption that times out on branch-20 when HDFS-127 has not been reverted. (I'm using this test to work on a new version of HDFS-127 for branch-20 that doesn't cause infinite loops, for inclusion in our distro and to help out the HBase guys), Here's a redone patch against branch-20. I've taken the approach of resetting the failure counter at the start of the user-facing read and pread calls in DFSInputStream. The logic here is that the failure counter should limit the number of internal retries before throwing an exception back to the client. As long as the client is making some progress, we don't care about the total number of failures over the course of the stream, and it should be reset for each operation.

I've also included two new unit tests. The first, in TestCrcCorruption, guards against the error we saw with the original patch. It reliably reproduces the infinite loop with the broken patch that was originally on branch-20. The second new unit test, in TestDFSClientRetries, verifies the new behavior, namely that a given DFSInputStream can continue to be used even when the _total_ number of failures exceeds maxBlockAcquires, so long as the number of retries on any given read() operation does not.

To accomplish the second test, I pulled in the mockito dependency via ivy. The ability to inject bad block locations into the client made the test a lot more straightforward, and I don't see any downsides to pulling it into branch-20.
, Reopening this issue with a new candidate patch for branch-20, which I believe addresses the issues we saw in the first version (see rationale above), and tests it much more thoroughly., I forgot to update the eclipse classpath with the previous patch. This one's the same, except with that fixed. Everything else on test-patch was +1.

I ran all the unit tests and everything that usually passes on branch-20 passed :), When do we expect the next release on hadoop-0.20 ? , Nicholas, since you did the revert can you check to see if this new patch is ok for 0.20.2?, Sure, I will check it., I ran unit tests on the patch.  The following tests failed
{noformat}
    [junit] Running org.apache.hadoop.io.TestUTF8
    [junit] Tests Run: 3, Failures: 1, Errors: 0, Time elapsed: 0.161 sec

    [junit] Running org.apache.hadoop.hdfsproxy.TestHdfsProxy
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 4.219 sec
{noformat}
BTW, since the new patch is quite different from the patch already committed, I suggest to fix the problem in a new issue so that the new patch could be committed to 0.20, 0.21 and trunk.  Otherwise, I am not sure how to commit the new patch., Hi Nicholas,

TestUTF8 is known to be flaky. See HADOOP-6522

I also see TestHdfsProxy fail in strange ways on a pretty regular basis. I doubt it's related since it doesn't stress any failure scenarios.

I actually did open a new patch for this issue on trunk - HDFS-927 which is linked here. Sorry for the confusion., > I actually did open a new patch for this issue on trunk - HDFS-927 which is linked here. Sorry for the confusion.

Great!  Let's close this and work on HDFS-927., This should be pulled into the branch-0.20-append branch.]