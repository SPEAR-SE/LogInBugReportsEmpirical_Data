[We have two options to fix:
# Change append to support blocks with size greater than the preferred block size. Specifically, we can update the logic of {{BlockManager#convertLastBlockToUnderConstruction}}: if the last block size is greater than or equal to the preferred size, the client should write to a new block. Since the same function is also used by truncate, the new logic should also support the scenario that data is truncated from such block.
# Or we can add a new restriction to concat: the source file's preferred block size cannot be greater than the target. This fix is simpler and keeps the semantic that the preferred block size is the upper limit of a file's blocks., Upload a patch implementing the above #2., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12705144/HDFS-7943.000.patch
  against trunk revision d884670.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDFSClientRetries

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9940//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9940//console

This message is automatically generated., The test failures should be unrelated and they all passed in my local run., - In verifyQuotaForUCBlock,
-* let's add a precondition check to make sure delta >= 0 (for append).
-* It should check if (!fsd.getFSNamesystem().isImageLoaded() || fsd.shouldSkipQuotaChecks()); see FSDirConcatOp.verifyQuota and FSDirRenameOp.verifyQuotaForRename.

- We should also verity quota by storage types; see FSDirConcatOp.computeQuotaDeltas.

- For truncate, the quota should be updated differently as following:
-* Copy-on-truncate for snapshot: need quota check for creating the new block.  Quota usage count is increased.
-* Non-copy-on-truncate OR Copy-on-truncate for upgrade but not snapshot: Quota usage count is decreased.  No quota check is needed.

Do you want to fix truncate in a different JIRA?, Oops, my previous comment was for HDFS-7587.  Please ignore it here., Is it possible to change append so that it always appends to a new block when the last block is great than or equal to the preferred block size?, Yes, that's fix option #1. Then the code change on {{convertLastBlockToUnderConstruction}} also affects truncate. It's a little bit more complicated than option #2. I will upload a patch doing #1 later., I have an initial patch fixing the issue following #1. Then I find that more places have the assumption that the block's size upper limit is the preferred size. E.g., when conversion between UC block and complete block happens, we always follow this assumption to update quota usage. Also, when computing storage usage of a UC file, we use the preferred size as the size of the last UC block. These places require fixes as well.

Currently I prefer #2 because of its simplicity. Also I think it is good to have the preferred block size as the upper limit. This upper limit is also kind of respected by the client while writing data.

What do you think, [~szetszwo]?, 001 patch tries to fix the append. But still need to check/fix the quota usage code., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12705443/HDFS-7943.001.patch
  against trunk revision c239b6d.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDFSClientRetries

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9965//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9965//console

This message is automatically generated., > Currently I prefer #2 because of its simplicity. Also I think it is good to have the preferred block size as the upper limit. This upper limit is also kind of respected by the client while writing data.

Sure, let's do it for now.  We may change convertLastBlockToUnderConstruction in a separated JIRA.

+1 on HDFS-7943.000.patch
, Thanks again for the review, Nicholas! I've committed this to trunk, branch-2 and branch-2.7., FAILURE: Integrated in Hadoop-trunk-Commit #7367 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7367/])
HDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #137 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/137/])
HDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #871 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/871/])
HDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2069 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2069/])
HDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #128 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/128/])
HDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #137 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/137/])
HDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2087 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2087/])
HDFS-7943. Append cannot handle the last block with length greater than the preferred block size. Contributed by Jing Zhao. (jing9: rev bee5a6a64a1c037308fa4d52249be39c82791590)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
]