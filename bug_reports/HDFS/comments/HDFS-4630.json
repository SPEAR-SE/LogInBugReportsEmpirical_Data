[Jiras are for reporting bugs and not a forum for asking questions. Please discuss this in user mailing list on how to size the namenode and datanode processes. , this is not question, Its an issue Datanode going OOM due to incapability of holding all the pointers to all the blocks., bq. this is not question, Its an issue Datanode going OOM due to incapability of holding all the pointers to all the blocks.
The datanode JVM needs to be sized correctly for holding the blocks. Datanodes consume memory proportional to the number of block replicas on a datanode. Hence I said, please get information on how to size the process.
, Closing again per Suresh's comment, as this is by design and you're merely required to raise your heap to accommodate more files (and thereby, blocks). Please also see HDFS-4465 and HDFS-4461 on optimizations of this., I'd say "WONTFIX" over invalid; the OOM is a result of storing all state in memory for bounded time operations against files, including block retrieval. That's a design decision. Now, if you want to put EhCache in behind the scenes, assess its performance with many small files, and its behaviour on big production clusters, that's a project I'm sure we'd all be curious about -feel free to have a go!]