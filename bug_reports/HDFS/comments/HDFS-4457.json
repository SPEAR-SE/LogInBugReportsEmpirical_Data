[If it's not a java client, then the service shouldn't matter.  It's the java token selectors that rely on the service, hence why a java client sets the service since it knows exactly what address/port it connected to.  A non-java client will need to manage the mapping of tokens itself.  Or am I missing something?, Daryn, the use case scenario is a non-java client (Hue) getting the token and passing that to a java client (Pig via Hue shell) for its use. The token file is picked up by Pig and Pig connections to HDFS fail when it tries to use it., canceling patch, put test method in the wrong place and exiting testDelegationTokenWebHdfsApi() is failing as well., ok, now I think I got it right, using the right test class and  setting setTokenServiceUseIp(false) for the testcase minicluster., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12567401/HDFS_4457.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.security.TestDelegationToken

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3918//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3918//console

This message is automatically generated., Ok, I see what you're trying to do, but there are problems:
# The client should use the service as set by the server, esp. if it's going through a proxy.  The client thinks it contacted the proxy host for the token, but the token as returned by the server will have the "real" host, so later the client won't find the token when it uses the proxy.
# You can't assume how the token should be keyed in the Credentials.  The key is a private implementation detail.  Guessing wrong will cause the job to fail to submit when MR tries to get the token again.

-1 to the approach because I've invested a significant effort in ensuring that 0.20.5+ clients all set the service in the token.  I paved the way for a backwards-compatible change to token selection.  I want to better support HA, CNAMEs, multiple A-names, and multi-interface hosts.  The server will set the service to an opaque value and newer clients will use that for token selection, but older clients will continue to set the service and work the same as they ever did.  We can also get rid of all the use_ip stuff.  This patch would ruin that feature...

I'd suggest using something like fetchdt that uses the standard token acquisition apis that will populate the credentials for you.  There is a -tokenCacheFile in {{GenericOptionsParser}} that will allow the binary credentials to be passed into a command.  I've got a little patch somewhere that reads the file into the UGI instead of just setting a mapred conf value.

If fetchdt, or something like it, isn't a reasonable approach, let's chat offline to educate me on Hue., Daryn,

Regarding "The client should use the service as set by the server,", this is not true today, DFSClient.getDelegationToken() & WebHdfsFileSystem.getDelegationToken() sets the service HOST:PORT when it receives the token using the HOST:PORT used by the client. The token fetched by a client should have the HOST:PORT the client used to access the service (HDFS/WEBHDFS), if the service is being offered with multihoming, the service cannot know/assume what HOST are visible to the client, unless the it has a hint from the client (this is what this patch is done for WebHDFS).

Regarding "You can't assume...", I don't think there is any assumption here, the client is getting a blob and passes that blob to another client, that is the whole purpose of delegation tokens, no? This is using the built in mechanism in MapReduce to populate the token cache with the contents of the binary representation of the token. This ends up being used to submit the job, thus in the credentials of the alternate client.

What this patch does, is to enable non java clients that request a delegation token from WebHDFS to have a delegation token ready to be handed to another client. 

This patch does not affect in any maner java clients that request a delegation token as they continue injecting the service when they receive the delegation token. 

Oozie does not have this problem because Oozie uses the java client and gets the service injection for free. Hue in the other hand, because not being Java, cannot do this.

Regarding fetchdt, it does not seem to fetch tokens of WebHDFS.

Finally, in the scenario the approach I'm proposing does not cut it, we still have problem with WebHDFS using setting the IP wildcard of the NN RPC configuration.

Thoughts?
, {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12567421/HDFS_4457.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3921//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3921//console

This message is automatically generated., My apologies, I left the word "not" out of "The client should *not* use the service as set by the server".  The change I alluded to is predicated on that statement being true.  I'll comment further after I tamp out a prod issue., Thanks, that clarifies things a bit. 

Following up, with this patch the clients of the WebHFDS service are 'sending' (as part of the HTTP request info) the hostname to WebHDFS and WebHDFS is setting it on its behalf. This is critical for client environments that do not have (and cannot have) a hadoop client setup (which would allow them to run fetchdt; assuming we fix up fetchdt to serve WebHDFS tokens, which are not served today).

Hope this clarifies., Daryn, does Tucu's explanation address your concerns? I think Tucu's latest comment makes sense - you're right that the client should be setting the token service, and in this case the client is effectively doing just that since the server is using the host/port as sent by the client when creating the DT.

The patch looks good to me, but I don't want to commit it if you have more pending comments. Please let me know., Ignoring the issue of the client relying on the server to set the service - which is what I don't approve of since it's a big step backwards - you still have the problem if a proxy is between the client and the webhdfs server.  The token will contain the hostname that the proxy used to contact the server, not the hostname the client used to contact the proxy.  The proxy, or even some form of NAT may be redirecting the port.  The server doesn't know this, only the client knows what port it thinks it contacted.

The remote server also doesn't have the ability to know if the client has use_ip enabled or disabled.  

Basically, only the client that requested the token knows the exact host:port authority it used to request the token.  When it attempts to re-contact that service, it needs to match the service with the authority.

My second concern is that you must be assuming the key to store the token in the credentials.  It currently happens to be the token's service, but it's a private implementation detail.  If the key format changes, and the passed-along token is added to the credentials with the old format, then job submission will attempt to reacquire the token and fail.  Fetchdt solves this by allowing you to acquire tokens and opaquely pass them along in binary form.

What error are you encountering with fetchdt?  It's working for me on a production cluster:
{noformat}
$ hdfs fetchdt -fs webhdfs://host /tmp/tokens
Fetched token for host:50070 into file:/tmp/tokens
{noformat}, Daryn,

Regarding your concern about having a proxy in the middle, I don't see that as a problem, when you have an HTTP proxy in between, the client still targets the real server hostname, it is the HTTP stack in the client the one that redirects the request to the proxy with the real server hostname used by the client. Then in the server side (webhdfsNN) from the HTTP request you can infer the exact name of the server used by the client (proxy or not).

Regarding NAT in the middle doing port redirection, that should not be an issue either as the host:port information used by the client is transmitted in the HTTP 'Host' header which contains both host:port used by the client when opening the connection (http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html, section '14.23 Host') and this is required in HTTP/1.1.

Regarding your second concern, Is that really a problem, tokens are transient and I would not expect the to be valid across system updates.

On the *fetchdt*, got it. Still, that requires spawning a JVM to get the token.

, True, NAT probably isn't a problem.  However, most proxies rewrite the Host: header (sometimes preserve the client's in X-Forwarded-Host: ) or ignore it due to security issues.  The token comes back with a different authority in the service than the client actually used.  This is just an example of where the server doesn't know how the client originally tried to contact it.

For the creds key issue, if you insert a token into the creds with the wrong key, the fs will try to fetch a token again during job submission.  The creds key typically matches the service, but doesn't have to, ex. HA tokens contain the logical uri's authority.  If/when webhdfs implements HA, this will be a problem.  This is why passing credentials obtained via fetchdt is the only current safe way to ensure tokens are keyed correctly.

All said, my big issue is I specifically ensured in 205+ that clients do not use the server's service and set the service themselves.  This will allow a backwards compatible change of the server setting the service to a unique identifier that new clients can use for better token selection.  If you regress this work by making a client rely on the server setting the service to the client's notion of the authority, then you will break all hope for a backwards compatible change..., Daryn, this patch is not regressing your work of clients setting the service in the token. All that still happens and this patch does not change/prevent any of that. What this patch does is to make WebHDFS:

* Not to set the service in the outgoing tokens to the NN RPC bind host address (0.0.0.0 in the case of multihoming)
* Set the service in the outgoing tokens to the best guest WebHDFS can make based on the HTTP request. Still clients can set the service to whatever they want.

Also, I don't see this a security issue as clients typically set this value, and they can put any host:port before handing the tokens to somebody else.
, As an update, Aaron & Alejandro & I had an offline conversation to educate each other on the requirements and issues with this change.  The next step is discovering how Hue is converting the {{Token}} into a {{Credentials}} for Pig., This one got forgotten in the freezer for a while. 

After following up with the Hue folks, I agree with Daryn's point that the server should not set the service. Hue will most likely use fetchdt. So closing this JIRA, opened HDFS-4571]