[Sathish, thanks for reporting this. This does seem like a bug. We limit the length of how much can be read from the DFSOutputStream based on the length recorded in the snapshot. I am surprised that it is not working in the cat command. A fix along similar lines is what is needed. Let me know if you need any help., Below blockEnd is getting calculated based on block size. I think in snapshotcases, FIleLength may be different when it was snapshotted. But if we use blockSize , it may treat blocksize to read but snapshotted fileSize may be less than this?
{code}
 // update current position
    if (updatePosition) {
      pos = offset;
      blockEnd = blk.getStartOffset() + blk.getBlockSize() - 1;
      currentLocatedBlock = blk;
    }
{code}


Where as direct read API uses fileLength

{code}
long filelen = getFileLength();
    if ((position < 0) || (position >= filelen)) {
      return -1;
    }
    int realLen = length;
    if ((position + length) > filelen) {
      realLen = (int)(filelen - position);
    }
{code}

Do you think this is the cause for this unexpected bytes read?,  // update current position
    if (updatePosition) {
      pos = offset;
      blockEnd = blk.getStartOffset() + blk.getBlockSize() - 1;
      currentLocatedBlock = blk;
    }
Yes,the above check is the cause for reading unexpexcted bytes,when reading from snapshot file,and one more thing is the cat command uses the readstrategy method to read the bytes from a file,in this it taking block end as length to read the content in the file,so even if we snapshot file also,it is not taking lenght as indication to read content in snapshot files,
 private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {
    dfsClient.checkOpen();
    if (closed) {
      throw new IOException("Stream closed");
    }
    Map<ExtendedBlock,Set<DatanodeInfo>> corruptedBlockMap 
      = new HashMap<ExtendedBlock, Set<DatanodeInfo>>();
    failures = 0;
    if (pos < getFileLength()) {
      int retries = 2;
      while (retries > 0) {
        try {
          // currentNode can be left as null if previous read had a checksum
          // error on the same block. See HDFS-3067
          if (pos > blockEnd || currentNode == null) {
            currentNode = blockSeekTo(pos);
          }
          int realLen = (int) Math.min(len, (blockEnd - pos + 1L));
          int result = readBuffer(strategy, off, realLen, corruptedBlockMap);
here it is taking blockend as indication to read the content in that file,so we can change that blockend for that snapshot files to filelength as like this
   // update current position
    if (updatePosition) {
      pos = offset;
    blockEnd = Math.min((locatedBlocks.getFileLength() - 1), (blk.getStartOffset()
        + blk.getBlockSize() - 1));
      currentLocatedBlock = blk;
    }
    return blk;
  }, for this i am attching a fix and testcase in this pacth,please review it, Thanks [~sathish.gurram] for the fix!

So I agree this looks more like an issue in DFSInputStream#readWithStrategy. Before we only tested DFSInputStream#read(long position, byte[] buffer, int offset, int length) and it works fine. However in read(byte[], int, int) and read(ByteBuffer) the file length has not been taken into account.

For the current patch, instead of changing the value of blockEnd, can we modify the readWithStrategy method? I.e., in
{code}
          // currentNode can be left as null if previous read had a checksum
          // error on the same block. See HDFS-3067
          if (pos > blockEnd || currentNode == null) {
            currentNode = blockSeekTo(pos);
          }
          int realLen = (int) Math.min(len, (blockEnd - pos + 1L));
          int result = readBuffer(strategy, off, realLen, corruptedBlockMap);
{code}
we can add an extra check:
{code}
          int realLen = (int) Math.min(len, (blockEnd - pos + 1L));
          if (locatedBlocks.isLastBlockComplete()) {
            realLen = (int) Math.min(realLen, locatedBlocks.getFileLength());
          }
{code}, Besides, some comments for the new unit test part:
# We may want to increase the timeout value from 6s to maybe 60s. 6s is too short and it can even cause timeout in my laptop.
# Instead of calling read(long, byte[], int, int), we should call read(byte[], int, int) to reproduce the bug. Otherwise the new unit test can pass even without the fix (since for "-cat" the unit test does not check result).
# Need to clean the imports section.
# We can call DFSTestUtil#createFile and DFSTestUtil#appendFile to make the test code shorter and cleaner., Thanks Jing Zhao for reviewing the patch,
i changed the patch according to your comments,can you please check it , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12608475/HDFS-5343-002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5204//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5204//console

This message is automatically generated., Thanks Sathish for the updated patch.
I have few comments on the patch.
1) Coding format missing from the following lines:
   DFSTestUtil.createFile(hdfs,file1, BLOCKSIZE,REPLICATION,SEED); 

   ...........
   assertEquals(fileStatus.getLen(), BLOCKSIZE*2);
   ..........
   assertEquals(fileStatus.getLen(),BLOCKSIZE);
   ............
   assertEquals(bytesRead,BLOCKSIZE);
   ...........

2)  Also you have to assert the return code and expected length for cat command.
{code}
try {
+        ToolRunner.run(conf, shell, new String[] { "-cat",
+            "/TestSnapshotFileLength/sub1/.snapshot/snapshot1/file1" });
+    } catch (Exception e) {
+        e.printStackTrace();
+    }
{code}
I think you need not catch exception here right?

3) I would like to see javadoc for test
4) Also for got to say don't delete your older patches from JIRA, Thanks UMA for having a look on the patch
I has updated the patch as per your comments ,please review it, Thanks for updating the patch Sathish.

Here you need to reset the out back with System.out
So, you take backup of System.out first and then your code. In finally you just reset with System.out again.
{code}
ByteArrayOutputStream bao = new ByteArrayOutputStream();
+    System.setOut(new PrintStream(bao));
+    System.setErr(new PrintStream(bao));
..........
{code}

Above code should be something like this.

{code}
PrintStream psBackup = System.out;
ByteArrayOutputStream bao = new ByteArrayOutputStream();
+    System.setOut(new PrintStream(bao));
+    System.setErr(new PrintStream(bao));

try{
..........
} finally {
      System.setOut(psBackup);
 }
{code}
Otherwise all SOP from next test onwards may go into the above stream set.

Also please keep one empty line gap between testcases.
{code}
    fis.close();
   }
+  /**
+   * Adding as part of jira HDFS-5343
{code}
, Thanks UMA for reviewing the patch.
I updated the patch as per your comments.please review the patch, Thanks for the update.
Taking from my previous comment:
{code}
try {
+        ToolRunner.run(conf, shell, new String[] { "-cat",
+            "/TestSnapshotFileLength/sub1/.snapshot/snapshot1/file1" });
+    } catch (Exception e) {
+        e.printStackTrace();
+    }
{code}
I think you need not catch exception here right?

Could you please answer above question?, yes correct ,no need to catch the exception here,in the previous patch itself i updated that comment
but unfortunately it came in latest patch.
I will update my patch., please review the patch, {code}
ToolRunner.run(conf, shell, new String[] { "-cat",
+        "/TestSnapshotFileLength/sub1/.snapshot/snapshot1/file1" });
+    assertEquals(bao.size(), BLOCKSIZE);
+    System.setOut(psBackup);
{code}

here System.setOut must be in try finally. Keep ToolRunner execution in try and keep System.setOut in finally. Previous comment said about unnecessary catching of exception., I updated the patch as per your comments,please review the patch, +1, Patch looks good.   Pending jenkins report., Seems like jenkins url not accessible. Any one knows, whether jenkins down?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12625029/HDFS-5343-0006.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated -14 warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5939//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5939//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5939//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12625029/HDFS-5343-0006.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5942//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5942//console

This message is automatically generated., Thanks a lot, Sathish for the patch. I have just committed this to trunk and branch-2, SUCCESS: Integrated in Hadoop-trunk-Commit #5037 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5037/])
HDFS-5343. When cat command is issued on snapshot files, getting unexpected result. Contributed by Sathish. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561325)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotFileLength.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #462 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/462/])
HDFS-5343. When cat command is issued on snapshot files, getting unexpected result. Contributed by Sathish. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561325)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotFileLength.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1679 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1679/])
HDFS-5343. When cat command is issued on snapshot files, getting unexpected result. Contributed by Sathish. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561325)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotFileLength.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1654 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1654/])
HDFS-5343. When cat command is issued on snapshot files, getting unexpected result. Contributed by Sathish. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1561325)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotFileLength.java
]