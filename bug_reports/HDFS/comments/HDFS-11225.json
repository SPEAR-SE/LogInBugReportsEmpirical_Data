[It also exists in release version 2.7.1 that I have ever seen this issue. , Thanks [~linyiqun] for confirming this issue. Yes I think this is by design an issue since snapshot was introduced in 2.4, and we've observed the same incidence on multiple version of CDH. Have you dig into it and want to contribute a patch?, Thanks [~jojochuang] and [~linyiqun].

I have a further idea about how to do asynchronous delete, first of all, we need to disconnect the item to be deleted from the namespace, and prepare a deletion task to be done asynchronously. When doing the deletion, instead of recursively delete the files and collect blocks to delete as a whole, we can use a stack to pushing stuff to delete, then do pop and delete when an entry has no child. This way, the delete can be interrupted easily: after being interrupted, resume the work from where the work is interrupted at the stack. 

Does that make sense to you guys?


, Had some discussion with Manoj, who has a different approach (but better!) to improve this issue. So assign this issue to Manoj. Thanks!, Sorry for the delay response, [~jojochuang].
{quote}
Have you dig into it and want to contribute a patch?
{quote}
No, we haven't looked into this. It's a bad user experience that every time we did deleteSnapshot then the active namenode crashed then failover happened. So I am very looking forward to seeing the patch here, :)., HI [~manojg],

Would you please take a look at 

https://issues.apache.org/jira/browse/HDFS-11225?focusedCommentId=15736111&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15736111

and also post your thoughts of the different solution you shared with Wei-Chiu?

Thanks.
, Looks like a nice feature, but could be too late for 2.8. Move it to 2.9., Actually, because INodeDirectory.cleanSubtree is invoked in a few other places, (e.g. FSDirectory.unprotectedDelete, FSDirectory.unprotectedRenameTo), delete and rename operations may also suffer from the same bug., It appears to me that storage used by a snapshot is counted against a users storage quota.
So any asynchronous delete procedure will have to keep user quota in perspective.
Could it be possible to mark the blocks, held by a deleted snapshot, for deletion adjust the user quota and have HDFS rebalance process delete the block during the next rebalance run.
, I ran bunch of tests with various directory-files layouts and at least in few of the setups i am able recreate the problem of Snapshot delete taking 45+ seconds with FSN and FSD totally locked down for the entire durations. 

In a directory layout like below, snapdir is snapshottable and has 1000 snapshots and a total of 4 million files under it. Later when deleting one of the earlier snapshot, the deletion operation took 46 seconds on average across several runs. 
{noformat}
/root/
     + snapdir/
                + dir-1/
                          + 1 million files 
                + dir-2/
                          + 1 million files
                + dir-3/
                          + 1 million files 
                + dir-4/
                          + 1 million files
{noformat}

{{DirectoryWithSnapshotFeature#DirectoryDiff#getChildrenList()}} looks like the culprit. Especially the getChildrenList() is getting invoked on every {{get()}} which doesn't sound right to me. Also, when there are max children like 1Million per directory, and with several snapshots, my jstack profiling shows way too much time is being spent in {{combinePosterior}}. Will dig more and add more details.

{noformat}
   private ReadOnlyList<INode> getChildrenList(final INodeDirectory currentDir) {
      return new ReadOnlyList<INode>() {
        private List<INode> children = null;

        private List<INode> initChildren() {
          if (children == null) {
            final ChildrenDiff combined = new ChildrenDiff();
            for (DirectoryDiff d = DirectoryDiff.this; d != null; 
                d = d.getPosterior()) {
              combined.combinePosterior(d.diff, null);
            }
            children = combined.apply2Current(ReadOnlyList.Util.asList(
                currentDir.getChildrenList(Snapshot.CURRENT_STATE_ID)));
          }
          return children;
        }

        @Override
        public Iterator<INode> iterator() {
          return initChildren().iterator();  <===
        }

        @Override
        public INode get(int i) {
          return initChildren().get(i);  <===
        }
{noformat}, *Problem:*
* Unlike {{INodeDirectory}}, {{DirectoryWithSnapshotFeature}} doesn't have its children maintained in a plain list. Instead, there is {{DirectoryDiffList}} which is a list of {{DirectoryDiff}}.
* On every new snapshot, {{DirectoryWithSnapshotFeature}} updates its diffList with a new entry and all the subsequent file creation/deletions will be recorded in the last taken snapshot.
* So, the snap diff list which {{DirectoryWithSnapshotFeature}} maintains is more of Delta/Diff of file creation and deletions since the last snapshot. This is deliberate design so as top keep the order of snapshot creation to a constant.
* Snapshot deletion operation needs to visit all children files for the snapshot to reclaim blocks, and {{DirectoryWithSnapshotFeature#DirectoryDiff#getChildrenList()}} is invoked to get the list.
* To get the children list for any snapshot {{Sx}} under a directory,  all the snapshot diff records after {{Sx}} are combined one by one sequentially, and reversed from the latest current children list of the directory.
* So, listing children under a Snapshot {{Sx}} directory is the order of (#Snapshots after Sx * #FileDiffs in each of those snapshots). With thousands of snapshots and with 100s of thousands of files, this listing operations can easily consume 10s of seconds.
* Above all, all these operations are done by a single threads and one directory at a time, in a recursive fashion. In my testing, I have seen snapshot deletion taking 45+ seconds with a fairly unloaded NN., Hi Manoj, is this still targeted at beta1? Long time since the last update., During the snapshot deletion of an older snapshot , maximum time is consumed while constructing the children list for every directory under the snapshottable root directory which requires adding up all the diffs for subsequent snapshots for each and every directory and then reverse applying to the current filesystem tree. The document attached here outlines the idea where the summation of sunsequent diifs for a certain no of snapshots will be maintained in a skip list for each directory once no of snapshots exceed a certain threshold for a snapshottable directory. This will speed up the construction of children list for a snapshot. Please have a look at the proposal for more details., [~manojg] and [~jingzhao]/others, please have a look at the proposal., [~shashikant],  Thanks for the proposal. Will take a look and get back to you., [~shashikant], please feel free to own this bug and followup with your proposal. I am on to other things and not able to spend time on this. My apologies., [~manojg]/[~shashikant], any progress here? We plan to start merge vote of 3.1.0 on Feb 18, please let me know if you plan to finish this by Feb 18 or we need to move it out., Removed 3.1.0 from target version. There's no way this'll get in soon.]