[When a null {{DatanodeDescriptor}} is returned, we just skip the block invalidation work on this DN and print a warning message., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12674213/HDFS-7225-v1.patch
  against trunk revision cb81bac.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing
                  org.apache.hadoop.hdfs.TestHDFSFileSystemContract

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestBlockReaderLocal

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8391//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8391//artifact/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8391//console

This message is automatically generated., Couple quick notes:

* Nit: "descriptor" is misspelled
* Need unit test
* I also wonder why we have this inconsistency in the first place in InvalidateBlocks. Isn't a better fix to properly update InvalidateBlocks when the set of DNs changes? I believe this is how UnderReplicatedBlocks works, and I think generally we expect the internal state of the active NN to be consistent., [~andrew.wang] Thanks for reviewing. 

bq. I also wonder why we have this inconsistency in the first place in InvalidateBlocks. Isn't a better fix to properly update InvalidateBlocks when the set of DNs changes? I believe this is how UnderReplicatedBlocks works, and I think generally we expect the internal state of the active NN to be consistent.

The root cause of this inconsistency is similar to that of HDFS-6289, i.e., a DN restarts with a reformatted (or new) volume. Thus a different {{datanodeUuid}} is registered for an existing transfer address, replacing the existing entry in {{datanodeMap}}. Therefore when a block invalidation request is scheduled, the DN lookup (using an old {{datanodeUuid}}) could return null. 

Here's what I think this situation should be handled:
# When a DN lookup ({{datanodeManager.getDatanode(dn)}}) returns null in {{invalidateWorkForOneNode}}, we know this DN has been registered with a newer {{datanodeUuid}}. Skip the block invalidation work on this DN for this time.
#* We should keep the block invalidation work under the old {{datanodeUuid}} for a certain amount of time so they still have a chance to be executed if the old volume is attached and registered again in the future
# Count the number of times that a DN has been skipped in block invalidation work. If it's above a certain threshold, clear all entries in {{InvalidateBlocks}} under this DN. 
#* In the very rare case where a volume comes back after a long time, we should just wipe out all blocks on it ([~andrew.wang] do you know if HDFS is already doing it?)
# To reduce the chance of going into this situation in the first place, we should improve the ordering of executing invalidation and replication tasks (HDFS-7211), Nice examination here Zhe. One high-level question though, could we simplify the above by cleaning InvalidateBlocks immediately upon seeing the new datanodeUuid? If the old volume is brought back, the old blocks will be in the block report and the NN will re-populate InvalidateBlocks as needed when it processes the report., [~andrew.wang] Thanks for the suggestion. I think that's a good idea. It assumes that the NN will make the same decision to invalidate those blocks when the volume is back. I think it's a valid assumption. I'll implement that option., This patch removes invalidation work under a DN if the lookup returns null. The added unit test verifies that the invalidation work has been removed instead of skipped., Why not do this cleanup immediately when we remove the DN from the datanodeManager map? removeDatanode is a likely location, and we already do some cleanup there., Sure we should cleanup when a DN is removed. {{wipeDatanode}} looks right place to me, being the only method calling {{datanodeMap.remove()}}. I think we should also keep the cleanup at lookup time in case elements are removed from {{datanodeMap}} unexpectedly. 

Thinking a little more about our previous assumption below:

bq. If the old volume is brought back, the old blocks will be in the block report and the NN will re-populate InvalidateBlocks as needed when it processes the report.

How about file deletions? With our current approach would there be orphan blocks when the volume comes back?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12676149/HDFS-7225-v2.patch
  against trunk revision 4e134a0.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancer
                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
                  org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8472//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8472//console

This message is automatically generated., AFAICT, NN won't try to delete orphan blocks. I verified with the following test:

{code}
  public void testOrphanBlocks() throws IOException {
    DataNode dn = cluster.getDataNodes().get(0);
    DatanodeRegistration dnReg = dn.getDNRegistrationForBP(bpid);
    StorageBlockReport reports[] =
        new StorageBlockReport[cluster.getStoragesPerDatanode()];

    ArrayList<Block> blocks = new ArrayList<Block>();

    for (int i = 0; i < 10; i++) {
      blocks.add(new Block());
    }
    for (int i = 0; i < cluster.getStoragesPerDatanode(); ++i) {
      BlockListAsLongs bll = new BlockListAsLongs(blocks, null);
      FsVolumeSpi v = dn.getFSDataset().getVolumes().get(i);
      DatanodeStorage dns = new DatanodeStorage(v.getStorageID());
      reports[i] = new StorageBlockReport(dns, bll.getBlockListAsLongs());
    }
    cluster.getNameNodeRpc().blockReport(dnReg, bpid, reports);
    LOG.debug("Scheduling to delete " +
        cluster.getNameNode().getNamesystem().getBlockManager().
            getPendingDeletionBlocksCount() + " blocks");
  }
{code}

I wonder if it's the intended behavior for the NN to keep orphan blocks, or we should add the logic to delete them. [~andrew.wang] Do you have a clue?, Currently if a reported block belongs to no file, the block will be finally marked as invalid (not for the first block report though) and will be finally deleted., Thanks [~jingzhao] for the advice. If that's the case we should indeed remove the block invalidation tasks once a new storage UUID has been discovered. I'll submit an updated patch., Stale block invalidation tasks are cleaned in 2 places:
# A failed lookup
# When a datanode is wiped from the map in {{DatanodeManager}}, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12678312/HDFS-7225-v3.patch
  against trunk revision 5e3f428.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8603//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8603//console

This message is automatically generated., The failed test {{TestNameEditsConfigs}} is unrelated and passes on my local machine., Fix looks good, I just have some unit test comments:

* We could break out a @Before method to do the common test setup
* The test is for the uncommon case where we encounter bad work, rather than the standard case where it's cleared out proactively. It'd be good to cover the standard case too., We could also test the concern about readding a DN also readding the right invalidation work, DataNodeTestUtils#triggerBlockReport might be useful., I went ahead and refactored the test, and also added a test for wiping InvalidateBlocks on a node re-registration., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12682043/HDFS-7225.004.patch
  against trunk revision 2fce6d6.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS
                  org.apache.hadoop.hdfs.server.namenode.TestFSImage
                  org.apache.hadoop.hdfs.server.namenode.TestCheckpoint
                  org.apache.hadoop.hdfs.server.namenode.TestDeleteRace
                  org.apache.hadoop.hdfs.TestDFSMkdirs
                  org.apache.hadoop.hdfs.TestEncryptionZonesWithHA
                  org.apache.hadoop.hdfs.TestLargeBlock
                  org.apache.hadoop.hdfs.TestEncryptionZones
                  org.apache.hadoop.hdfs.TestReservedRawPaths

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8770//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8770//console

This message is automatically generated., [~andrew.wang] Thanks again for helping out! The refactored test code looks good to me. Two minor points (I made the changes in this new patch, let me know if you agree):
# DFSTestUtil.createFile(dfs, path, dfs.getDefaultBlockSize(), (short) 3, 0xED0ED0); --> changed 3 to NUM_OF_DATANODES to ensure the following assert works with a different NUM_OF_DATANODES
# Changed and moved the comment " Shutdown the DNs and create a file, which populates InvalidateBlocks", to match the code.

I ran the failed tests and they all passed locally. , Latest patch looks good to me as well, +1 pending Jenkins.

Good work, Zhe and Andrew., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12682287/HDFS-7225.005.patch
  against trunk revision ef38fb9.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancer

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8775//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8775//console

This message is automatically generated., {{TestBalancer}} is not related, and passes locally., Thanks for the final +1 ATM! I've committed this to trunk and branch-2. Thanks to Zhe for working on this, and also to Jing for his advice earlier., FAILURE: Integrated in Hadoop-trunk-Commit #6573 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/6573/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (wang: rev 406c09ad1150c4971c2b7675fcb0263d40517fbf)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #10 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/10/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (wang: rev 406c09ad1150c4971c2b7675fcb0263d40517fbf)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #748 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/748/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (wang: rev 406c09ad1150c4971c2b7675fcb0263d40517fbf)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #10 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/10/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (wang: rev 406c09ad1150c4971c2b7675fcb0263d40517fbf)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1938 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1938/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (wang: rev 406c09ad1150c4971c2b7675fcb0263d40517fbf)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1962 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1962/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (wang: rev 406c09ad1150c4971c2b7675fcb0263d40517fbf)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk-Java8 #10 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/10/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (wang: rev 406c09ad1150c4971c2b7675fcb0263d40517fbf)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
, Cherry-picked to 2.6.1, FAILURE: Integrated in Hadoop-trunk-Commit #8302 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8302/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (vinayakumarb: rev 08bd4edf4092901273da0d73a5cc760fdc11052b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #287 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/287/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (vinayakumarb: rev 08bd4edf4092901273da0d73a5cc760fdc11052b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #1017 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/1017/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (vinayakumarb: rev 08bd4edf4092901273da0d73a5cc760fdc11052b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #284 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/284/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (vinayakumarb: rev 08bd4edf4092901273da0d73a5cc760fdc11052b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2233 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2233/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (vinayakumarb: rev 08bd4edf4092901273da0d73a5cc760fdc11052b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2214 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2214/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (vinayakumarb: rev 08bd4edf4092901273da0d73a5cc760fdc11052b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #276 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/276/])
HDFS-7225. Remove stale block invalidation work when DN re-registers with different UUID. (Zhe Zhang and Andrew Wang) (vinayakumarb: rev 08bd4edf4092901273da0d73a5cc760fdc11052b)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, This wasn't originally in 2.6.1, must have been committed to 2.6, which was already 2.6.2. I just committed this to 2.6.1 taking [~sjlee0] cherry-pick, which must have come from branch-2.6.

Ran compilation and TestComputeInvalidateWork before the push.]