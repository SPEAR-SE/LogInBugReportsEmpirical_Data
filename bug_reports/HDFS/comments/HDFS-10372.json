[[~kihwal], [~jojochuang]: can you please review., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 10s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 6m 42s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 41s {color} | {color:green} trunk passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 40s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 21s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 50s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 13s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 53s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 2s {color} | {color:green} trunk passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 47s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 47s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 38s {color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 38s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 39s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 39s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 17s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 48s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 11s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 10s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 3s {color} | {color:green} the patch passed with JDK v1.8.0_91 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 42s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 57m 31s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_91. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 54m 49s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 23s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 137m 19s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_91 Failed junit tests | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
| JDK v1.7.0_95 Failed junit tests | hadoop.hdfs.server.namenode.TestEditLog |
|   | hadoop.hdfs.TestHFlush |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:cf2ee45 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12802583/HDFS-10372.patch |
| JIRA Issue | HDFS-10372 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 33b258198e8f 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 8d48266 |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_91 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15379/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_91.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/15379/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/15379/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_91.txt https://builds.apache.org/job/PreCommit-HDFS-Build/15379/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/15379/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/15379/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

, I don't think this patch have caused all the failing tests since the patch only changes one line in the test case., +1 I will commit it shortly, Thanks for the contribution [~shahrs87] and [~kihwal]. I linked this to HDFS-10260.
bq. I thought that LocatedBlock#getLocations() returns an array of DatanodeInfo but now I realized that it returns an array of DatandeStorageInfo (which is subclass of DatanodeInfo).
Sorry I don't understand this statement. {{DatanodeID}} appears to be a superclass of {{DatanodeInfo}}, and has the {{getXferAddr}} method. {{DatanodeStorageInfo}} is not a subclass of of the above. Fix makes sense to me though (verifying the xferAddr).

I guess you meant to say {{DatanodeID#toString}} (which calls {{getXferAddr()}}) overrides {{DatanodeInfo#toString}} (which doesn't)?, s/DatanodeStorageInfo/DatanodeInfoWithStorage
The names are so confusing.
DatanodeInfoWithStorage is a sub class of DatanodeInfo., bq. I linked this to HDFS-10260.
Thanks !, Thanks for the review., I see... thanks for the explanation!, I'm +1 too on this, though I think it would be better to create and write another file after one of the volume is removed in order to make it sure that the datanode is still available.

I put the error logs on my environment as a reference.

DataNode in mini cluster was started with 2 volumes (data1 and data2).

{noformat}
2016-05-08 10:00:39,003 [DataNode: [[[DISK]file:/home/iwasakims/srcs/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/home/iwasakims/srcs/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:37720] INFO  common.Storage (DataStorage.java:createStorageID(158)) - Generated new storageID DS-040e0757-ea7f-4465-80e3-9f8c00abeb83 for directory /home/iwasakims/srcs/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
...(snip)
2016-05-08 10:00:39,109 [DataNode: [[[DISK]file:/home/iwasakims/srcs/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/home/iwasakims/srcs/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:37720] INFO  common.Storage (DataStorage.java:createStorageID(158)) - Generated new storageID DS-8f82ba58-61ae-4cb1-b019-0c387d25b5d2 for directory /home/iwasakims/srcs/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
{noformat}

The volume of data1 was removed since the test broke it.

{noformat}
2016-05-08 10:00:42,321 [IPC Server handler 4 on 37720] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateHeartbeatState(453)) - Number of failed storages changes from 0 to 1
2016-05-08 10:00:42,321 [IPC Server handler 4 on 37720] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateFailedStorage(539)) - [DISK]DS-040e0757-ea7f-4465-80e3-9f8c00abeb83:NORMAL:127.0.0.1:59604 failed.
2016-05-08 10:00:42,321 [IPC Server handler 4 on 37720] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:pruneStorageMap(525)) - Removed storage [DISK]DS-040e0757-ea7f-4465-80e3-9f8c00abeb83:FAILED:127.0.0.1:59604 from DataNode 127.0.0.1:59604
{noformat}

The test expected that the message in exception on {{out.close()}} contains the name of failed volume (to which the replica was written) but it contained only info about live volume (data2).

{noformat}
testCleanShutdownOfVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl)  Time elapsed: 8.468 sec  <<< FAILURE!
java.lang.AssertionError: Expected to find 'DatanodeInfoWithStorage[127.0.0.1:59604,DS-040e0757-ea7f-4465-80e3-9f8c00abeb83,DISK]' but got unexpected exception:java.io.IOException: All datanodes [DatanodeInfoWithStorage[127.0.0.1:59604,DS-8f82ba58-61ae-4cb1-b019-0c387d25b5d2,DISK]] are bad. Aborting...
	at org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1395)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1338)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1325)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1122)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:549)
{noformat}
, [~shahrs87], it's up to you to decide whether to improve it as [~iwasakims] suggested., bq. The test expected that the message in exception on out.close() contains the name of failed volume (to which the replica was written) but it contained only info about live volume (data2).
When the client asked for locations for first block, namenode selected a datanode with any random storage info within that datanode.
Refer to {{DataStreamer.locateFollowingBlock(DatanodeInfo[] excludedNodes)}} method for more details.
When the client started writing to datanode, the datanode selects a volume accordingto  RoundRobinVolumeChoosingPolicy policy and it can select a storage which is different than what namenode has stored in its triplets.
When the datanode sends an IBR (with RECIEVING_BLOCK), the namenode will change the storage info in its triplets with the storage info which datanode reported.
But the change in storage info is not propogated back to client.
So the client still has stale storage info.
When the client tried to close the file, the datanode threw an exception (since the volume has gone bad) but since the client has stale storage info, it  saved the exception with the old storage info.
This is the reason why the test was flaky in the first place.
In my machine, the test finishes within 2 seconds. So the datanode didn't send any IBR and the storage info was not changed in namenode. 
But in the  jenkins build machines, the test ran for more than 8 seconds which gave datanode ample of time  to send an IBR.
[~iwasakims]: I hope this answers your question.

, bq. it's up to you to decide whether to improve it as Masatake Iwasaki suggested.
I don't think this is required.
I edited the test on my machine to create one file after one of the volume gone bad. It is able to create the file and I can see the block on the datanode's good volume. But I don't see any value adding it to patch.
[~iwasakims]: Let me know if you still want me to create a new file. I will edit my patch.
, [~iwasakims] has already +1'ed it, meaning the suggested change is not strictly necessary.
I am committing this as is., I've committed this to trunk through branch-2.7. Thanks for fixing this, [~shahrs87]. Thanks for valuable reviews, [~xiaochen] and [~iwasakims]., Thanks [~kihwal] for reviews and committing. 
Thanks [~xiaochen] and [~iwasakims] for reviews., FAILURE: Integrated in Hadoop-trunk-Commit #9737 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/9737/])
HDFS-10372. Fix for failing TestFsDatasetImpl#testCleanShutdownOfVolume. (kihwal: rev b9e5a32fa14b727b44118ec7f43fb95de05a7c2c)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
, Closing the JIRA as part of 2.7.3 release.]