[[~manojg] want to pick this one up?, [~andrew.wang], sure, will work this on this., Looking at this, it's not pretty. FileSystem returns an FSDataOutputStream, which implements Syncable. Its implementation either does a real hflush, or just calls flush. See:

{code:title=FSDataOutputStream}
  @Override  // Syncable
  public void hflush() throws IOException {
    if (wrappedStream instanceof Syncable) {
      ((Syncable)wrappedStream).hflush();
    } else {
      wrappedStream.flush();
    }
  }
{code}

I don't understand how users can figure out if they're getting a real hflush. FSDataOutputStream implements Syncable, so you can't query with {{instanceof}}. There's currently no public way of querying the wrapped stream either. I think it was a mistake to add {{Syncable}} to FSDataOutputStream, we should have forced users to check with {{instanceof}} and cast it.

I don't like changing DFSStripedOutputStream#hflush to simply call flush, since then HDFS users who turn on EC will silently stop getting real hflush/hsync. The current behavior of throwing an exception is safer.

[~stevel@apache.org], any thoughts on this? I notice that output streams aren't covered by the FileSystem spec. This also relates to discussions about querying which features are supported by a FS., [~andrew.wang], [~stevel@apache.org],
IMHO, instead of assuming all Syncable implementations supporting hflush()/hsync(), we can have the interface additionally expose isSyncable() or similar which can be queried to find whether the stream supports hflush()/hsync() or not. And, maybe we should have uniform implementation for hsync/hflush that either fall back or throw exception., Good idea on isSyncable. I'm still peeved that checking {{instanceof Syncable}} doesn't work, but a runtime check is okay too.

There are compatibility implications for changing existing implementations to newly throw a runtime exception, so I think the right answer is to have them all fallback to flush or a no-op.

Since we're using Java 8, I think we can finally compatibly add methods to an existing interface! Exciting.

https://dzone.com/articles/interface-default-methods-java, I've slowly started on output streams, HADOOP-13327, [doc here|https://github.com/steveloughran/hadoop/blob/s3/HADOOP-13327-outputstream-spec/hadoop-common-project/hadoop-common/src/site/markdown/filesystem/outputstream.md]

No tests, etc.

In that draft I've said, "The purpose of Syncable interface is to provide guarantees that data is written to a filesystem for both visibility and durability."

I can see the benefit of runtime checks. However, I don't want a whole new set of methods for features, as they become a problem too: probe for interface, cast, query, maintain.  Like you say, at least with Java 8 you can add new methods: I was about to say "no can do" for a new method there.

Runtime probes allow for dynamic behaviour based on FS semantics, things we need for stuff like "case-sensitive-fs", "create-consistency", etc: things which are a function of the specific FS instance, rather than just the {{org.apache.hadoop.FileSystem}} subclasses.

I'd advocate what we've been discussing in HADOOP-9565, some single method {{boolean hasFeature(String)}}, where a feature can be probed for. Implement that and have one probe "syncable" to make: base classes can implement, subclasses can override that specific string. Add that method as a new Interface and we can adopt it across all our streams.



, bq. "The current behavior of throwing an exception is safer."

... but changes precedent?

Semantics in general are messy here around sync, et al. It is a reflection of he torturous journey taken by sync/flush/hflush/hsync in HDFS.

The blessed [~stevel@apache.org] tried writing a spec for DFS and got far on the read-side. Helps. Write-side is to do. I like Steves' comment that rather than "....probe for interface, cast, query, maintain.." at each point at which we encounter a feature, rather, there'd be an upfront query that could be run before engaging w/ the fs implementation (though how does this work if tiering changes the underlying storage on us at runtime?).

Meantime, having DFSStripedOutputStream throw an exception breaking all that run on top (with no means of querying whether support or not) seems disruptive., Hi Steve, Stack, thanks for replying!

bq. I'd advocate what we've been discussing in HADOOP-9565, some single method boolean hasFeature(String), where a feature can be probed for. Implement that and have one probe "syncable" to make: base classes can implement, subclasses can override that specific string. Add that method as a new Interface and we can adopt it across all our streams.
bq.  at each point at which we encounter a feature, rather, there'd be an upfront query that could be run before engaging w/ the fs implementation 

In this specific case though, HDFS can return either a normal DFSOutputStream, which can sync, or a DFSStripedOutputStream, which can't sync. So, we need a feature probe that's at the stream level rather than the FS level.

I thought more about {{instanceof}} checks for an interface and decided that it's inferior, since we'd potentially end up with an exponential explosion in subclasses. +1 for a probe method.

bq. (though how does this work if tiering changes the underlying storage on us at runtime?).

That's one issue. Something similar happens with federation, we don't know what the child FileSystems are until runtime since it's specified in configuration. And there's the current issue where features are mutually exclusive (e.g. EC and Syncable). We often don't know until runtime, and until we open the stream.

bq. Meantime, having DFSStripedOutputStream throw an exception breaking all that run on top (with no means of querying whether support or not) seems disruptive.

It's disruptive, but I thought it better than losing data? Bit moot though, since we intend to fix it properly here :)

BTW, HDFS-11643 might be interesting for HBase as well. We're discussing adding a boolean parameter to {{create}} to always create a replicated file., Stack-who-must-be-listened-to has some good insights. Like your point about asking for a feature not of an FS, but a path under an FS.

Less of an issue for things like streams tho'. 

What about having some interface, 

{code}
public interface StreamCapabilities {
  boolean hasCapability(String capability);
}
{code}

Define a standard set of strings for the current capabilities; add more over time as needed, and you can see if flush/hflush etc are implemented on specific instance of a stream.

 We cannot do a simple "implements interface" check as that doesn't allow for subclasses to declare that they don't impement an interface â€”not unless we define some new interfaces like {{NotSyncable}} fo them to implement, and checks to become {{if (stream instanceof Syncable && ! stream instanceof NotSyncable}}. Which is just wrong. Especially if a subclass of the notsyncable stream goes on to implement syncability.

, Bumping this to a blocker for visibility. Important for API transparency as Stack noted earlier., Any progress on this one? It's a blocker for alpha3 since it breaks YARN and HBase on EC as it is.

If this is involved, we have the quick-fix option of just making EC hflush/hsync no-ops without the new capabilities API, but it doesn't sound too complicated based on Steve's description., Attaching the quick-fix v01 patch while I work on the fix as suggested by Steve. [~andrew.wang], can you please take a look ?, Downgraded the priority from blocker to major as the other bug HDFS-11718 is taking care of the quick fix., Based on suggestion from [~stevel@apache.org] and Stack, used a new interface to query for stream capabilities. [~andrew.wang], can you please take a look ?, Core design of a new interface looks good

I don't want an enum though, I'd like a string. Why? Let's us and implementation classes add new methods without renegotiating the source tree. For example, on S3a, I could add the new attributes "s3a:delayed-commit", blobstore:put-on-close ( the latter being something which other blobstores could also do), etc. Some convention that for store-specific options, we use a prefix, so as to stop incompatibilities.

for outstreams, we could have the flush attributes, put-on-close, consistent-metadata (which of course, HDFS output streams lack), 


One minor detail: your tests don't need to catch raised exceptions, just throw them up and let JUnit deal with it., I'm going to link this to HDFS-11170: add a builder interface for creating files and HADOOP-14365, make that builder interface ready for use, because i could imagine actually having the builder aware of an option existing before its created. That is, maybe, you could probe for an FS having an option before you build the output stream.

I'm not sure though; it may put too much intelligence into the builder, when the output stream is the one which really knows what it has. If you want to query FS capabilities, e.g HBase wants to make sure hsync works, it could create a stream, check the capabilities, fail if unsupported., Thanks for working on this Manoj, looks good overall, few little q's:

{code}
        if (wrappedStream instanceof StreamCapabilities &&
            wrappedStream instanceof Syncable) {
{code}

Does this need to check Syncable?

Seems like there will be a fair amount of code duplication for implementing hasCapabilities in each stream. How about a helper method that takes a stream-specific string->boolean map, or something like that?, Thanks [~stevel@apache.org], [~andrew.wang] for the review. Attached v03 patch with following comments addressed. please take a look.

1. (StreamCapability) I don't want an enum though, I'd like a string.
Done. I had it as an enum so that client applications can get to easily see list of available capabilities and also for easy documentation purposes. But, string based capability checking model also has good benefits like you detailed. I combined the enum and string approach to get the best of both. Let me please know if you think otherwise.
2. your tests don't need to catch raised exceptions, just throw them up and let JUnit deal with it.
Done.
3. Does this need to check Syncable?
Should not be needed. Removed.
4, helper method that takes a stream-specific string->boolean map, or something like that
As of now only one output stream supports Syncable capabilities. Will add some helper routine once more streams start supporting various capabilties.
, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  1m 11s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 2 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 35s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 33s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 18m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 59s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 34s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 45s{color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 41s{color} | {color:red} hadoop-common-project/hadoop-common in trunk has 19 extant Findbugs warnings. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  1m 50s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs-client in trunk has 2 extant Findbugs warnings. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  2m  9s{color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk has 10 extant Findbugs warnings. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 41s{color} | {color:red} hadoop-tools/hadoop-azure in trunk has 1 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 38s{color} | {color:green} trunk passed {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 14m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 14m 37s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  3m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 23s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 22s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  7m 41s{color} | {color:green} hadoop-common in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 24s{color} | {color:green} hadoop-hdfs-client in the patch passed. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 90m 39s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 32s{color} | {color:green} hadoop-azure in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 36s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}187m 43s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.balancer.TestBalancer |
|   | hadoop.hdfs.server.namenode.TestNameNodeMXBean |
|   | hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:14b5c93 |
| JIRA Issue | HDFS-11644 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12866356/HDFS-11644.03.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux c7c1c08f016c 3.13.0-108-generic #155-Ubuntu SMP Wed Jan 11 16:58:52 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 81092b1 |
| Default Java | 1.8.0_121 |
| findbugs | v3.1.0-RC1 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/19311/artifact/patchprocess/branch-findbugs-hadoop-common-project_hadoop-common-warnings.html |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/19311/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-client-warnings.html |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/19311/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs-warnings.html |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/19311/artifact/patchprocess/branch-findbugs-hadoop-tools_hadoop-azure-warnings.html |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/19311/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/19311/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-azure U: . |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/19311/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, LGTM, will wait until tomorrow so Steve can review too., Committed to trunk, thanks for working on this Manoj! Mind adding a short release note as well for this new functionality?, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11701 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11701/])
HDFS-11644. Support for querying outputstream capabilities. Contributed (wang: rev 54fd0e44b76c4b982dcfb47932b6159851f14136)
* (edit) hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SyncableDataOutputStream.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java
* (add) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StreamCapabilities.java
* (edit) hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSOutputStream.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStream.java
* (edit) hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStream.java
, yeah, looks good to me.

I'm going to try and submit my PoC for something related to this for HADOOP-9565; again string based, though I'm thinking of explicitly returning {{Option<Boolean>}} to allow the call to return yes, no and "don't know". The current proto just reuses all the XML /contract/*/xml files used in testing the FS contracts so that we can reuse that list of FS behaviours, Talked with Manoj offline, we think this would be good to get into branch-2 as well.

Manoj, do you mind posting a branch-2 patch? Thanks., Thanks for the review and commit help [~andrew.wang], [~stevel@apache.org]. Attaching branch-2 patch. Please take a look., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 16m  6s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 38s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m  0s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m  2s{color} | {color:green} branch-2 passed with JDK v1.8.0_131 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  7m  9s{color} | {color:green} branch-2 passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 26s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 51s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m 35s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  5m 55s{color} | {color:green} branch-2 passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m 52s{color} | {color:green} branch-2 passed with JDK v1.8.0_131 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 28s{color} | {color:green} branch-2 passed with JDK v1.7.0_121 {color} |
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  2m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  5m 48s{color} | {color:green} the patch passed with JDK v1.8.0_131 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  5m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 38s{color} | {color:green} the patch passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  6m 38s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  2m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  6m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m  7s{color} | {color:green} the patch passed with JDK v1.8.0_131 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  2m 27s{color} | {color:green} the patch passed with JDK v1.7.0_121 {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  8m 19s{color} | {color:green} hadoop-common in the patch passed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 15s{color} | {color:green} hadoop-hdfs-client in the patch passed with JDK v1.7.0_121. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 55m 48s{color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m 35s{color} | {color:green} hadoop-azure in the patch passed with JDK v1.7.0_121. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 23s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}227m  4s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_131 Failed junit tests | hadoop.hdfs.TestBlockStoragePolicy |
|   | hadoop.hdfs.server.namenode.TestNamenodeCapacityReport |
| JDK v1.8.0_131 Timed out junit tests | org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |
| JDK v1.7.0_121 Failed junit tests | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithUpgradeDomain |
| JDK v1.7.0_121 Timed out junit tests | org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:8515d35 |
| JIRA Issue | HDFS-11644 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12867184/HDFS-11644-branch-2.01.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 54e38d16b7e8 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | branch-2 / 3403ed3 |
| Default Java | 1.7.0_121 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_131 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_121 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/19368/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_121.txt |
| JDK v1.7.0_121  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/19368/testReport/ |
| modules | C: hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-client hadoop-hdfs-project/hadoop-hdfs hadoop-tools/hadoop-azure U: . |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/19368/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Committed to branch-2, thanks Manoj!, We need docs on using this feature. I'd like to file a blocker on HBase 2.0 to make use of it where it is present, specifically so that we can avoid deploying the HBase WAL on filesystems that don't support hflush/hsync (such as EC). The lack of ability to reliably flush/sync doesn't appear to be called out in the Erasure Coding docs as a concern for those who might deploy it and I'd rather HBase bring the risk to operators' attention by failing fast when it finds the configuration.

But I can't find any guidance on using this feature (or that it exists) in the 3.0.0-alpha4 release notes nor in the filesystem docs.  I also can't tell from the javadocs on StreamCapabilities what I'm even supposed to query for to check on the fundamentals that this jira is addressing (hflush / hsync)., [~busbey]

I filed HDFS-12251 for the filesystem docs.   

In the meantime, I have a question for hbase. As {{StreamCapabilities}} is bind to an {{OutputStream}}, Hbase needs to firstly open a file for write (i.e., getting the {{output stream}}, before it can query the capabilities. Would this satisfy the needs from hbase side?, {quote}
In the meantime, I have a question for hbase. As StreamCapabilities is bind to an OutputStream, Hbase needs to firstly open a file for write (i.e., getting the output stream, before it can query the capabilities. Would this satisfy the needs from hbase side?
{quote}

Yeah that's fine I think. I'll come complain if implementing use of it makes me change my mind. ;), * it may be dynamic based on the stream opening args
* and it may also depend where in the FS it is opened. Examples: viewfs, azure FS blob vs non-blob streams. If an API call is added to the FS, it'd have to take a path
]