[To add a bit of detail:

You can pretty easily recreate this issue by setting up a 3-node cluster where each DN has two drives, one large and one very small (say 2GB). Running RandomWriter will then actually fail due to lost pipeline errors. What happens is that, when the small volume has around 100M left, it will still allow multiple in-flight blocks to write to it. When each of these blocks hits the end of the volume, they'll fail. Due to HADOOP-5796, the pipeline retry sometimes is incorrect and actually includes the same datanode - at that point it will try to *continue* the half-written block, and of course fail again since the disk is out of space.

This bug also accounts for confusing behavior with regards to dfs.datanode.du.reserved - if you reserve 1GB, for example, you can sometimes end up "overshooting" that reservation and end up with only 600M free. The amount of overshoot depends on the average number of concurrent writers to a given disk. With client applications like HBase that write commit logs slowl over time, or slow MR jobs on clusters with lots of slots, this can be reasonably high. A large block size exacerbates the issue.

The solution is to use the already-existing list of in-progress writes to track how much space has been "promised" (the writers already pass their block size in the request for a volume). The "promise" value should be subtracted from the available space in determining the round robin policy. This ends up being conservative, since it double-accounts a block that has written most of its data but not yet committed yet. In my opinion conservative is OK - I'd rather have it save a bit more space than absolutely necessary rather than fail the writes. To be absolutely correct, we'd have to track the amount of space used by each of the in-progress writers, and subtract that from the "promise" during the accounting - probably not too difficult but a slightly more involved change., I agree, Being conservative is better than failing the write!, Please also look at MAPREDUCE-1296, which is  also a  similar sort of issue., This has likely been fixed., From a cursory look I don't think it is fixed. Would need to take a closer look to be sure.]