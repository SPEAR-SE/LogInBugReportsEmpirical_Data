[Although this is a valid concern, the problem won't happen in practice since
{code}
Integer.MAX_VALUE milliseconds > 596 hours.  // (2^31-1)/3600,000 = 596.52...
{code}
blockReportInterval won't be set to such large value., * implement nextLong correctly, Hi Nicholas,

I agree that 596 hours is rather a long interval to set blockReportInterval to.  However, given that we accept a long, it seems simplest just to honor the user's input.  Alternately, we could throw an exception and let the user know that his input is being ignored.  What do you think?, I think the patch works only for some special cases.  It is incorrect for the following cases:
# Low bits values should depend on high bits; try
{code}
max = (1L << 32) + (1L << 16)
{code}
# High bits should range from 0 to highMax INCLUSIVELY in some cases; try
{code}
max = (1L << 32) - 1
{code}
# Two 31-bit int's cannot handle a 63-bit long; try
{code}
max = Long.MAX_VALUE
{code}

> However, given that we accept a long, ...

Correct me if I am wrong: I think we have not specified the type (int or long) or the max value for this conf property in the documentation.  The code somehow reads it as a long.

> Alternately, we could throw an exception and let the user know that his input is being ignored.

It is better to throw an exception if blockReportInterval > Integer.MAX_VALUE (or some other large values.), bq. Low bits values should depend on high bits; try max = (1L << 32) + (1L << 16)

(1L << 32) + (1L << 16) = 4295032832.
I guess I can sort of see what you're getting at here.  The (1L << 16) part will be completely ignored with this current code.

bq. High bits should range from 0 to highMax INCLUSIVELY in some cases; try (1L << 32) - 1

(1L << 32) - 1 = 0xffffffff
This is MAX_INT.  So you'll take the first branch of the 'if' statement and get a number between [0 and 0xffffffff).
So, it works fine here.

bq. Two 31-bit int's cannot handle a 63-bit long; try Long.MAX_VALUE

Now this criticism is fair.  It could be fixed by just using nextInt() rather than nextInt(Int.MAX_INT) to get the lower half of the return value.

I think I can fix this code to work in the 64-bit case.  The big insight of this code was that a uniformly randomly distributed 63-bit number can be viewed as a linear decomposition of several smaller uniformly distributed random numbers.

I'm tempted to try fixing this.  However, it may be more practical to limit ourselves to 31-bit values..., > This is MAX_INT. ...

Max int is
{code}
(1L << 31) - 1 = 0x7FFFFFFF
{code}
In Java, int is signed.  Please try to understand the problem before fixing it.  Thanks., Colin, I don't want to waste your time.  I will -1 on adding something like nextRandomLong(..) since it is not needed in this case.  There is no benefit since blockReportInterval is always small enough.

BTW, nextRandomLong(..) won't be needed for other problems in general.  I believe there is a good reason that Java has nextInt but not nextLong in their library., similar issue with casting between long and int, Hi Nicholas,

I appreciate you taking the time to look at this (minor) JIRA and the bugs you found in my code.

I feel like there are a lot of issues similar to this lurking in the code where we cast between long and int.  It is simply not a good practice to perform unchecked typecasts.  The lack of a nextLong() method tends to produce this kind of code.  For that reason alone, it would be a good addition to Hadoop-- don't you agree?

Check out HDFS-2541, where we also needed a nextRandomLong, didn't have it, and ended up doing an (incorrect) typecast.  The problem is more widespread than you think.  Does that make sense at all?  The alternative is to change a lot of our long configuration values to int, or introduce artificial maximum values for them., We also do not need nextRandomLong in HDFS-2541.  It is a bug in the calculation.  Having nextRandomLong is one way to fix it but we could fix the calculation.

If you could show one case that the code won't work without nextRandomLong, then we should add it.

BTW, adding nextRandomLong does not fix the casting problem., * fix bugs in RandomUtils.nextRandomLong

* add unit test for RandomUtils.nextRandomLong

* fix a few places where we're compromising type safety by casting between long and int, due to the previous lack of a nextRandomLong function., * fix bug where low bits might not be randomized by the previous code, -1 on the patch:
- all use of nextRandomLong are unnecessary;
- there are bugs in nextRandomLong for both HDFS-3456.002.patch and HDFS-3456.003.patch.
, I have a feeling that people will keep running into these kind of bugs where they typecast between long and int, and screw it up.  That was the motivation for creating nextRandomLong.

Regarding version 3 of the patch:  It's true that nextRandomDouble() only gives 53 bits of randomness, but the JavaDoc for my helper function clearly states that the resolution is limited by floating point rounding errors.  I'm assuming that was the "bug" you referred to, since you didn't specify.  Of course, it's clear that people might not realize this limitation when they call the function, so this is a legitimate cause for concern.

I don't have time to make this quite as perfect as I would like.  So I will stick to fixing the immediate problem, and not make nextRandomLong.  If we see any more bugs like this, then I guess we can link them back to here and evaluate whether this decision was the right one., .003.patch uses nextDouble, which does not provide enough entropy -- there are only 53 bits in the mantissa of a double.  So if I ask for 60-bit random values, it will return a 60-bit value with 7 zero bits at the bottom.  (Or something like that -- I didn't test the specific failure mode.)

I am not aware of any easy way to implement nextRandomLong given only a 31-bit nextInt.  So, I would suggest to address the potential overflow in scheduleBlockReport -- either use nextDouble directly, or limit the maximum range to Integer.MAX_VALUE., Here is a simple patch that just fixes the immediate problem.  It does this without introducing arbitrary limits or breaking the backwards compatibility of HDFS configurations.

Check it out., Thanks, Andy.

I came up with this patch, which just uses nextDouble directly, without any attempt to be generic.

2**53 milliseconds is thousands of years.  I'm willing to guess that if the block report delay is set that high, we have other problems besides a slightly granular random number generator. :), > 2**53 milliseconds is thousands of years. ...

- Java double (IEEE 754 64-bit) has only 52 precision (not 53, one bit is for the sign).  So it should be 2**52.

- This is exactly the point that nextRandomLong is unnecessary.  I personally am very interested in the implementation of nextRandomLong or, more generally, this kind of computation problem.  However, I don't want to add a unless method to Hadoop and I believe such method would lead to more bugs than preventing bugs since people will use it unnecessarily instead of fixing the calculation.

The new patch looks much better but it is even better to simply check delay limit and then throw exception.  As you mentioned, setting delay to such large value does not make sense.

For the so called "backwards compatibility", do you mean that when there was a bug in the previous version, we should support the same bug in order to keep the same behavior?, Well, I was going by http://docs.oracle.com/javase/1.4.2/docs/api/java/util/Random.html#nextDouble%28%29, which includes this text:

bq. The general contract of nextDouble is that one double value, chosen (approximately) uniformly from the range 0.0d (inclusive) to 1.0d (exclusive), is pseudorandomly generated and returned. All 2^53 possible float values of the form m x 2^53 , where m is a positive integer less than 2^53, are produced with (approximately) equal probability.

I haven't done the analysis myself so I don't know how they came to this number.  If you think you've found a bug in Oracle's docs, you should definitely let them know.

We have a lot of other time periods that are given as longs.  I just don't like mixing and matching types, because the potential for mistakes is too high.  At the same time, it's silly to make something a long and then throw an exception if it's greater than 31 bits.  Just my 2 cents., {quote}2**53 milliseconds is thousands of years. I'm willing to guess that if the block report delay is set that high, we have other problems besides a slightly granular random number generator.{quote}

Precisely, *this* code does not care about the exact random value used.  That's why it's fine to use nextDouble or whatever, *here*.  But we should not add a helper function that appears to provide a generic interface, but fails to implement that interface fully.

.004.patch LGTM.  I'm also fine with the suggestion to throw an exception if the delay is greater than Integer.MAX_VALUE.  We don't need to preserve bug-for-bug compatibility with clearly unreasonable configurations -- a config that sets the reportInterval to many days can get different behavior and we don't care.  (So long as the different behavior is not too surprising.)

One tiny nit -- please fix the spacing in
{code}
-      - ( dnConf.blockReportInterval - DFSUtil.getRandom().nextInt((int)(delay)));
{code}

there should not be a space between "(" and "dnConf".  You had fixed this in .003 but it fell out of .004. :)

Thanks for driving this issue,
-andy, > Well, I was going by http://docs.oracle.com/javase/1.4.2/docs/api/java/util/Random.html#nextDouble%28%29, which includes this text:

Colin, you are right that there is a bug in the 1.4.2 javadoc you read but it was already fixed in Java 6 (http://docs.oracle.com/javase/6/docs/api/java/util/Random.html#nextDouble%28%29).  They removed "All 2^53 possible float values ...", > I haven't done the analysis myself so I don't know how they came to this number. ...

That's why I hope you could first understand the problem before fixing it., Patch v4 looks reasonable to me. It does make sense to provide a long value for the block report interval, for example if you have a lot of trust in the _incremental_ block reports and thus have no need for periodic _full_ block reports. However, I'd also be fine just clamping the "jitter" at something like 1 day. The point of the randomization is just to jitter the startup block reports, and I don't think you really need to jitter over a multi-day period., Nicholas, have you had an opportunity to look at version 4 of the patch?  It's a 1-line patch.

You would have to set dfs.blockreport.intervalMsec to more than 100 million years before the limited resolution of nextRandomDouble would become apparent in this context, and even then I do not believe it would cause any function problem here., > ... and I don't think you really need to jitter over a multi-day period.

Keeping block report interval as long is fine.  However, generating random long (or double) is unnecessary since int already covers 596 hours.  So I consider the v4 patch as zero value to the current code (but it is not as harmful as adding nextRandomLong.)

It would be much helpful on checking max: if the delay or conf value > Integer.MAX_VALUE, throw an exception since it does indicate misconfiguration or other bugs., Nicholas, if you think it's appropriate to limit the block report interval to ~24 days (596 hours), can you file a separate JIRA for that?, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12530285/HDFS-3456.004.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2562//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2562//console

This message is automatically generated., h3456_20120601.patch: check max for dfs.blockreport.intervalMsec and dfs.blockreport.initialDelay., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12530588/h3456_20120601.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2570//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2570//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12530588/h3456_20120601.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2587//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2587//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12530588/h3456_20120601.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2588//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2588//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12530588/h3456_20120601.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2594//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2594//console

This message is automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12530588/h3456_20120601.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10521/console |


This message was automatically generated.]