[From the stack trace
{code}
org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.40.8.10:50010]
{code}
our server at 10.40.8.10 appears to be down or unreachable., I wish the answer were that simple, here's some cluster status taken at the time of the issue which indicates all nodes were running healthy. Also, the operation ultimately did not fail.

3-node Apache Hadoop 2.5.2 cluster running on Ubuntu 14.04 

dfshealth overview:
Security is off.
Safemode is off.

8 files and directories, 9 blocks = 17 total filesystem object(s).

Heap Memory used 45.78 MB of 90.5 MB Heap Memory. Max Heap Memory is 889 MB.

Non Heap Memory used 36.3 MB of 70.44 MB Commited Non Heap Memory. Max Non Heap Memory is 130 MB.
Configured Capacity:	118.02 GB
DFS Used:	2.77 GB
Non DFS Used:	12.19 GB
DFS Remaining:	103.06 GB
DFS Used%:	2.35%
DFS Remaining%:	87.32%
Block Pool Used:	2.77 GB
Block Pool Used%:	2.35%
DataNodes usages% (Min/Median/Max/stdDev): 	2.35% / 2.35% / 2.35% / 0.00%
Live Nodes	3 (Decommissioned: 0)
Dead Nodes	0 (Decommissioned: 0)
Decommissioning Nodes	0
Number of Under-Replicated Blocks	0
Number of Blocks Pending Deletion	0

Datanode Information
In operation
Node	Last contact	Admin State	Capacity	Used	Non DFS Used	Remaining	Blocks	Block pool used	Failed Volumes	Version
hadoop252-3 (x.x.x.10:50010)	1	In Service	39.34 GB	944.85 MB	3.63 GB	34.79 GB	9	944.85 MB (2.35%)	0	2.5.2
hadoop252-1 (x.x.x.8:50010)	0	In Service	39.34 GB	944.85 MB	4.94 GB	33.48 GB	9	944.85 MB (2.35%)	0	2.5.2
hadoop252-2 (x.x.x.9:50010)	1	In Service	39.34 GB	944.85 MB	3.63 GB	34.79 GB	9	944.85 MB (2.35%)	0	2.5.2, it ultimately worked as after timing out, the DFS client tried a different host.

what may be happening is that the datanodes are reporting in as healthy, but the address they publish for clients to get that data isn't accessible. Wrong hostname or firewalls being the common causes; network & routing problems another

try a telnet to the hostname & port listed, from the machine that isn't able to connect, and see what happens, The hadoop user can ssh paswordless into and from all 3 nodes. 
Also, port 50010 is reachable on all nodes from all nodes using hostnames and ip address...
hadoop@hadoop252-3:~$ nc 10.40.8.10 50010 < /dev/null; echo $?
0
hadoop@hadoop252-3:~$ nc 10.40.8.9 50010 < /dev/null; echo $?
0
hadoop@hadoop252-3:~$ nc 10.40.8.8 50010 < /dev/null; echo $?
0
hadoop@hadoop252-3:~$ nc hadoop252-1 50010 < /dev/null; echo $?
0
hadoop@hadoop252-3:~$ nc hadoop252-2 50010 < /dev/null; echo $?
0
hadoop@hadoop252-3:~$ nc hadoop252-3 50010 < /dev/null; echo $?
0

Any more ideas what this could be?, nothing obvious springs to mind. what happens if you kill that first DN?]