[-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531030/HDFS-3510-b1.001.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2598//console

This message is automatically generated., * fill with all 0xff, so that we don't have to handle OP_INVALID specially.

Also, technically it is undefined what byte pattern ByteBuffer.allocateDirect fills the buffer with, although it is 0 in practice in Oracle implementations.  In JDK7 they specified that it zero-fills., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531044/HDFS-3510-b1.002.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2599//console

This message is automatically generated., - writeFully: should probably take a WritableByteChannel, not a FileChannel
- tiledFill: I dont think this is general-purpose enough to merit inclusion in {{IOUtils}}.

It seems like some of these changes are not present in trunk, so we should fix them in trunk and then backport. Maybe we can just use this JIRA to bring branch-1 up to equality with current trunk, and address any further improvements separately?, bq. writeFully: should probably take a WritableByteChannel, not a FileChannel

I did consider that, but unfortunately the method I need is FileChannel(ByteBuffer, long), which isn't present in the superclass.  (The superclass only has WritableByteChannel(ByteBuffer), which updates the current offset, something we need to avoid doing in this code.)

bq. tiledFill: I dont think this is general-purpose enough to merit inclusion in IOUtils.

Yeah, I guess... it seemed so clear to me, but then writing the JavaDoc I started to have doubts about how generic it really was :)

bq. Seems like some of these changes are not present in trunk, so we should fix them in trunk and then backport.

Yeah, that makes sense.  I'll create a JIRA for implementing writeFully and do that in trunk.  It's general-purpose enough to be worth its own JIRA.

I think trunk might have the "We only try to do preallocation when the current position is less than 4096 bytes from the end of the file" problem, so maybe that is also worth putting into a separate JIRA.

Hopefully I can get those done and backported fast..., Here's a patch for trunk., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531446/HDFS-3510.001.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestEditLogFileOutputStream

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2622//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2622//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531734/HDFS-3510.003.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2640//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2640//console

This message is automatically generated., * fix javadoc, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531849/HDFS-3510.004.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2647//console

This message is automatically generated., rebase, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531856/HDFS-3510.004.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2648//console

This message is automatically generated., rebase again, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12531883/HDFS-3510.006.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestDecommission

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2649//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2649//console

This message is automatically generated., HDFS-3531 is checked in so the utility methods are now available., * use IOUtils#writeFully now that it's available

* write a megabyte at a time to minimize seeks, bq. The idea is that if we're going to encounter an out-of-disk-space condition, we don't want it to happen in the middle of writing valid data. Instead, we want it to happen in the middle of writing padding bytes.

Adding related jira, HADOOP-2330, that adds preallocation functionality. The description in this jira characterizes it differently from the original intent of preallocation.

Given this, could you please update the description in the jira. (BTW it would be good to keep description short, since it is sent in every email. The convention is to add more detailed description in the subsequent comment).

Given the different intent of pre-allocation, is this jira still valid?, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12532249/HDFS-3510.007.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestEditLogFileOutputStream

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2658//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2658//console

This message is automatically generated., * remove unused constant from test
* always add padding to the very end of the file, not the current offset
* reset position of the padding ByteBuffer to 0 before each call to writeFully., update description to reflect the fact that we're still adding padding 1MB at a time., fix description markup, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12532441/HDFS-3510.008.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer
                  org.apache.hadoop.hdfs.server.namenode.TestEditLog
                  org.apache.hadoop.hdfs.server.namenode.TestNameNodeRecovery
                  org.apache.hadoop.hdfs.server.namenode.TestFSEditLogLoader

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2665//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2665//console

This message is automatically generated., bq. Given the different intent of pre-allocation, is this jira still valid?
I am assuming given the updates to description, the answer is yes.

bq. The idea is that if we're going to encounter an out-of-disk-space condition, we don't want it to happen in the middle of writing valid data.
Can you explain why this is important. Also can you explain how you solve the problem? It would be good add a brief description of the solution you are proposing. That way there is no need for looking at the code to understand the proposal.


, Yep, this JIRA is still valid.  The problem is that we don't always pre-allocate enough space on the disk.  There is a check that looks like this:

bq. if (position + 4096 >= fc.size()) { ... do preallocation ...

As you can see, this will not work correctly if the next batch of writes to the edit log is greater than 4096 bytes.

For this reason, we continue to get bug reports and mailing list posts about how running out of disk space leads to a corrupt edit log.
For an example of a public one, check out https://groups.google.com/a/cloudera.org/group/scm-users/browse_thread/thread/3ec955a120daf241?hl=en#
(This mailing list post pertains to CDH3 / branch-1, but the code in question has the same problem.)

If you don't have time to look at the code, you can also check out the ASCII art in the description of this JIRA.  As always, thanks for taking the time to look at this., * when closing an edit log, remember to remove any preallocated padding at the end.  (the same as it currently does in trunk.), bq. Can you explain why this is important. Also can you explain how you solve the problem? It would be good add a brief description of the solution you are proposing. That way there is no need for looking at the code to understand the proposal.

Let me give a high level overview.  As you know, we don't want corrupt edit logs.  They are bad.  One way that an edit log can become corrupt is by ending with an edit log operation that is cut off in the middle.  This is what happens when you are writing to the edit log, and you get "cut off" in the middle by a disk full condition.

When it's done properly, preallocation solves this problem by making sure that we have enough space before starting the write.  In this case, we maintain the invariant that all of the current batch of edits are written out, or none of them are, even when the disk starts getting close to full.

Preallocation was introduced by Dhruba Borthakur in HADOOP-2330 in 2008.  At that time, the motivation was better edit log performance, rather than preventing partial edit log operations at the end of the file.  So it is perhaps understandable that the original scheme doesn't guarantee that preallocation will always happen before the write.  However, it has become clear over time that when preallocation isn't done, corrupt edit logs are all too common when the disk runs out of space.

If you're curious why we fill with 0xff rather than 0x00, check out HDFS-1846.

Hope that helps.

cheers,
Colin, +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12532488/HDFS-3510.009.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 1 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2670//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2670//console

This message is automatically generated., I guess I should also post a short description of how this solves the problem I talked about above.  Anyway, the patch solves the problem in the simplest possible way: by checking to make sure we have enough pre-allocated space *before* writing a batch of edits to the edit log.  If there isn't, we add 1MB of padding to the end of the edit log.

It's better to check before the write than after, because we can't know exactly how long the next batch of writes will be.  This also ensures that all writes are preallocated, even the first one., Thanks for adding description of the solution.

bq. Preallocation was introduced by Dhruba Borthakur in HADOOP-2330 in 2008.
I know this quite well and hence was pointing out that the description of this jira needs to be updated. BTW please address my earlier comment about updating the description of the jira. Instead of saying preallocation is flawed, you may want to state that it can be improved to avoid partial writes during disk becoming full. Please also update the current jira title "Fix FSEditLog pre-allocation" to "Improve FSEditLog pre-allocation".

Some comments on the patch:
# Please retain the pre-allocation method in EditLogFileOutputStream. This functionality does not belong in EditsDoubleBuffer.
# {{fc.position(fc.position() - 1); // skip back the end-of-file marker}} - is no longer required?
# Please add the debug logs back - it is useful for debugging.

Under heavy load/long disk i/o times, you could still see a large number of edits batched and synced together. Some of the editlog operations such as Close operation can be quite large. In that case, the preallocated 1 MB still may not be sufficient, though you have reduced most of the partial write cases.
, bq. 1. Please retain the pre-allocation method in EditLogFileOutputStream. This functionality does not belong in EditsDoubleBuffer.

OK

bq. fc.position(fc.position() - 1); // skip back the end-of-file marker - is no longer required?

It's not needed.  I believe this is a leftover from the days when we pre-allocated with 0x00 rather than 0xff.  Because the end-of-file marker is 0xff, in those days we needed to explicitly add the 0xff before the padding.  This should no longer be necessary after HDFS-1846.  You can see in FSEditLogOp#Reader#decodeOp that "EOF at an opcode boundary is expected." as the comment says.  So there is no need to add a terminator after the final opcode.

bq. Please add the debug logs back - it is useful for debugging.

OK

bq. the preallocated 1 MB still may not be sufficient...

I'll add code to handle the bufReady > 1MB case, just to be sure., bq. Because the end-of-file marker is 0xff, in those days we needed to explicitly add the 0xff before the padding.
Sorry, I do not follow your comments.

In the current code after every flush the current buffer has OP_INVALID at the end, added in EditLogFileOutputStream#setReadyToFlush. This is flushed to a file. The file now has:

{noformat}
--- journal records ---|OP_INVALID|
                                   ^
                                   |position
{noformat}

The position needs to be set back by one to continue writing to the file (irrespective of for preallocation or new editlog entries). Otherwise you end up getting: |Journal records|OP_INVALID|Journal records|OP_INVALID|

May be I have missed some recent change that avoids this?
, bq. In the current code after every flush the current buffer has OP_INVALID at the end, added in EditLogFileOutputStream#setReadyToFlush. This is flushed to a file.

Right.  However, this patch changes EditLogFileOutputStream#setReadyToFlush so that it doesn't do that any more.

What I was trying to explain above (sorry if it was unclear) was that this is no longer necessary.

As you know, an OP_INVALID operation is a single byte whose value is 0xff.

Back before HDFS-1846, we would have:
{code}
--- journal records --- 0xff 0x00 0x00 0x00 0x00 ... end-of-file
{code}
You can see that the single 0xff byte at the end is necessary because otherwise we'll see zeros and try to interpret them as an OP_ADD (opcode 0).

However, after HDFS-1846 went in, we now have:
{code}
--- journal records --- 0xff 0xff 0xff 0xff 0xff ... end-of-file
{code}

There isn't any need to explicitly add another byte of 0xff because *all* of the padding bytes are 0xff.  We already added all the OP_INVALIDs we needed to during pre-allocation; there's no need to add another., Earlier Suresh commented that we should support more than 1MB of preallocation at a time.  This patch makes that possible.

* EditLogOutputStream: rename PREALLOCATION_LENGTH to MIN_PREALLOCATION_LENGTH, to reflect the fact that we can now pre-allocate more than 1MB at a time.

* FSEditLogOp#Reader#verifyTerminator: allow more than 1.5MB of padding at the end of the edit log.  In order to do this, the verifyTerminator function disables the StreamLimiter, and uses the InputStream#mark interface.

* EditLogOutputStream: in contrast to the previous patch, move the preallocation logic into EditLogOutputStream#preallocate, where it originally was.  Handle pre-allocating more than 1MB at a time.  Keep the LOG.debug
message on every preallocation.

* There are many new unit tests, bq. Right. However, this patch changes EditLogFileOutputStream#setReadyToFlush so that it doesn't do that any more.
Sorry missed that.

BTW, with the current patch the following is possible (no OP_INVALID following a journal record):
{noformat}
--- journal records --- end-of-file
{noformat}

I have not had a chance to look at how rest of the code deals with that condition., bq. BTW, with the current patch the following is possible (no OP_INVALID following a journal record):
bq. {code}
--- journal records --- end-of-file
{code}
bq. I have not had a chance to look at how rest of the code deals with that condition.

It works.  The relevant code is in decodeOp:

{code}
byte opCodeByte;
try {
  opCodeByte = in.readByte();
} catch (EOFException eof) {
  // EOF at an opcode boundary is expected.
  return null;
}
{code}

As always, thanks for your reviews.  It's good to be thorough when dealing with this stuff. -C, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12532932/HDFS-3510.010.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 javadoc.  The javadoc tool appears to have generated 13 warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2681//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2681//console

This message is automatically generated., The "13 new javadoc warnings messages" are coming from RAID, it seems, not this patch

{code}
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/Decoder. java:180: warning - @param argument "parityFile" is not a parameter name.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         DistRaidNode.java:58: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         DistRaidNode.java:71: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         DistRaidNode.java:58: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         DistRaidNode.java:58: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         DistRaidNode.java:71: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         DistRaidNode.java:71: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/Encoder. java:340: warning - @param argument "srcFile" is not a parameter name.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         RaidConfigurationException.java:24: warning - Tag @link: reference not found: CronNode
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         RaidConfigurationException.java:24: warning - Tag @link: reference not found: CronNode
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         DistRaidNode.java:58: warning - @inheritDocs is an unknown tag.
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         RaidConfigurationException.java:24: warning - Tag @link: reference not found: CronNode
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs-raid/src/main/java/org/apache/hadoop/raid/         DistRaidNode.java:71: warning - @inheritDocs is an unknown tag.
{code}, Could the logic be simplified- you could just write the MIN_PREALLOCATION_LENGTH buffer multiple times:
{noformat}
if (need <= 0) {
  return;
}
while(need > 0) {
  fill.position(0);
  IOUtils.writeFully(fc, fill, size);
  need -= fillCapacity;
  size += fillCapacity;
} 
{noformat}
, Finally took some time to review the tests as well. Here are some comments:
# TestNamenodeRecovery
#* Since you have just modified the tests, could you please add some javadoc to EditLogTestSetup class. Also please add javadoc to newly added class EltsTestOpcodesAfterPadding.
#* you may want to move adding delete operation to a method, to avoid code duplication.
#* very minor - if padding length is zero, you may want to just return from #padEditLog()
# TestEditlogFileOutputStream
#* Can you please add some javadoc to the class.
#* testRawWrites() - you can move the code that performs setReadyToFlusy(), elos.flushAndSync() and length check to a method to avoid repeating the same code
#* You do not need try finally. @Before method deleted the file.

, bq. Could the logic be simplified- you could just write the MIN_PREALLOCATION_LENGTH buffer multiple times?

Yeah, that seems reasonable., bq. TestEditLogFileOutputStream: You do not need try finally. @Before method deleted the file.

The @Before method only runs once, before all the junit tests in TestEditLogFileOutputStream are executed.  The reason why it's there is that EditLogFileOutputStream always appends to the end of a file; it never truncates.  So if there is garbage in the test directory before the test begins, it's very important to remove it.  But the tests also have to clean up after themselves, since the @Before method only runs once, not between individual junit tests.  It makes more sense to do this in a try... finally block so that the failure of one test does not cause all of them to fail.

I agree with the rest of the comments.  I'll add some more doxygenh and address the other stuff, bq. @Before method only runs once
It runs before every test - http://junit.sourceforge.net/javadoc/org/junit/Before.html. So deletion of TEST_EDITS in finally clauses/finally clause just for that may be unnecessary.

bq. I'll add some more doxygen
We use javadoc conventions. Not sure how compatible doxygen is with javadoc., Hmm... you're right.  I must have been thinking of @BeforeClass rather than @Before.  I suppose the try... catch clauses are not necessary then.

With regard to JavaDocs vs. doxygen... I meant JavaDoc., * rebase, address comments, +1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12533705/HDFS-3510.011.patch
  against trunk revision .

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified test files.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 eclipse:eclipse.  The patch built with eclipse:eclipse.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/2714//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/2714//console

This message is automatically generated., +1 for the patch. I committed it to trunk. Thank you Colin., Integrated in Hadoop-Hdfs-trunk-Commit #2476 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2476/])
    HDFS-3510.  Editlog pre-allocation is performed prior to writing edits to avoid partial edits case disk out of space. Contributed by Collin McCabe. (Revision 1355189)

     Result = SUCCESS
suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355189
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java
, Integrated in Hadoop-Common-trunk-Commit #2408 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2408/])
    HDFS-3510.  Editlog pre-allocation is performed prior to writing edits to avoid partial edits case disk out of space. Contributed by Collin McCabe. (Revision 1355189)

     Result = SUCCESS
suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355189
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java
, Integrated in Hadoop-Mapreduce-trunk-Commit #2425 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2425/])
    HDFS-3510.  Editlog pre-allocation is performed prior to writing edits to avoid partial edits case disk out of space. Contributed by Collin McCabe. (Revision 1355189)

     Result = FAILURE
suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355189
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java
, Integrated in Hadoop-Hdfs-trunk #1091 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1091/])
    HDFS-3510.  Editlog pre-allocation is performed prior to writing edits to avoid partial edits case disk out of space. Contributed by Collin McCabe. (Revision 1355189)

     Result = FAILURE
suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355189
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java
, Integrated in Hadoop-Mapreduce-trunk #1124 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1124/])
    HDFS-3510.  Editlog pre-allocation is performed prior to writing edits to avoid partial edits case disk out of space. Contributed by Collin McCabe. (Revision 1355189)

     Result = FAILURE
suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1355189
Files : 
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditsDoubleBuffer.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogFileOutputStream.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java
, Colin, making this change for 1.x release seems like a good idea. If you have some time, can you please attach a patch for branch-1?, Yeah, the branch-1 version is in the pipeline..., Colin, let me know if you are free to work on branch-1 version of this patch. If you are busy, I can see if someone else can pick it up., Ah-- we actually did do a branch-1 version of this, in HDFS-3596.
Sorry... I forgot to add a comment here after finishing with that., This seems to be missing from branch-2, though?, Here's a patch which applies against branch-2, bq. Here's a patch which applies against branch-2

Looks good to me, very similar to the original branch-3 patch.  Obviously be sure to run at least: TestCheckpoint,TestEditLog,TestNameNodeRecovery,TestEditLogLoading,TestNameNodeMXBean,TestSaveNamespace,TestSecurityTokenEditLog,TestStorageDirectoryFailure,TestStorageRestore,TestFileJournalManager,TestEditLogsDuringFailover,TestEditLogTailer,TestEditLogFileOutputStream, Cool. I ran all of those tests and they passed. Will commit to branch-2 momentarily.]