[My opinion: 

The user api of hdfsRead is not good. We should support the following use case.

{noformat}
    // read from the file
    tSize curSize = bufferSize;
    for (; curSize == bufferSize;) {
        curSize = hdfsRead(fs, readFile, (void*)buffer, curSize);
    }

{noformat}, Our modification for hdfs.c is: 

{noformat}

tSize hdfsRead(hdfsFS fs, hdfsFile f, void* buffer, tSize length)
{
    // JAVA EQUIVALENT:
    //  byte [] bR = new byte[length];
    //  fis.read(bR);

    //Get the JNIEnv* corresponding to current thread
    JNIEnv* env = getJNIEnv();
    if (env == NULL) {
      errno = EINTERNAL;
      return -1;
    }

    //Parameters
    jobject jInputStream = (jobject)(f ? f->file : NULL);

    jbyteArray jbRarray;
    jint noReadBytes = 0;
    jvalue jVal;
    jthrowable jExc = NULL;

    int hasReadBytes = 0;

    //Sanity check
    if (!f || f->type == UNINITIALIZED) {
        errno = EBADF;
        return -1;
    }

    //Error checking... make sure that this file is 'readable'
    if (f->type != INPUT) {
        fprintf(stderr, "Cannot read from a non-InputStream object!\n");
        errno = EINVAL;
        return -1;
    }

     /////////////////////////////////////////////////////////////////////////////////////////////////
    // > OUR MODIFICATION
    jbRarray = (*env)->NewByteArray(env, length);
    while (hasReadBytes < length) {
        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream, HADOOP_ISTRM,
                         "read", JMETHOD3("[B", "I", "I", "I") , jbRarray, hasReadBytes, length-hasReadBytes) != 0) {
            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
                                       "FSDataInputStream::read");
            noReadBytes = -1;
        }
        else {
            noReadBytes = jVal.i;
            if (noReadBytes >= 0) {
                (*env)->GetByteArrayRegion(env, jbRarray, 0, noReadBytes, buffer+hasReadBytes);
                hasReadBytes += noReadBytes;
            }  else {
                //This is a valid case: there aren't any bytes left to read!
                break;
            }
            errno = 0;
        }

     // > OUR MODIFICATION
    ///////////////////////////////////////////////////////////////////////////////////////

    }

    destroyLocalReference(env, jbRarray);
    return hasReadBytes;
}


{noformat}, Firstly, to resolve this problem, we should make sure whether the current hdfsRead api is good (correctly).

I support the following:

If the returned length of hdfsRead is not equal to buffer length, we could make sure that the file is EOF.

Maybe, we can provide another api: hdfsReadFully().


your suggestions?, The latest modification of hdfsRead api impl:

{noformat}

tSize hdfsRead(hdfsFS fs, hdfsFile f, void* buffer, tSize length)
{
    // JAVA EQUIVALENT:
    //  byte [] bR = new byte[length];
    //  fis.read(bR);

    //Get the JNIEnv* corresponding to current thread
    JNIEnv* env = getJNIEnv();
    if (env == NULL) {
      errno = EINTERNAL;
      return -1;
    }

    //Parameters
    jobject jInputStream = (jobject)(f ? f->file : NULL);

    jbyteArray jbRarray;
    jint noReadBytes = 0;
    jvalue jVal;
    jthrowable jExc = NULL;

    int hasReadBytes = 0;

    //Sanity check
    if (!f || f->type == UNINITIALIZED) {
        errno = EBADF;
        return -1;
    }

    //Error checking... make sure that this file is 'readable'
    if (f->type != INPUT) {
        fprintf(stderr, "Cannot read from a non-InputStream object!\n");
        errno = EINVAL;
        return -1;
    }

     /////////////////////////////////////////////////////////////////////////////////////////////////
    // > OUR MODIFICATION
    int exception = 0;
    jbRarray = (*env)->NewByteArray(env, length);
    while (hasReadBytes < length) {
        if (invokeMethod(env, &jVal, &jExc, INSTANCE, jInputStream, HADOOP_ISTRM,
                         "read", JMETHOD3("[B", "I", "I", "I") , jbRarray, hasReadBytes, length-hasReadBytes) != 0) {
            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
                                       "FSDataInputStream::read");
            exception = 1;
            break;
        }
        else {
            noReadBytes = jVal.i;
            if (noReadBytes >= 0) {
                (*env)->GetByteArrayRegion(env, jbRarray, 0, noReadBytes, buffer+hasReadBytes);
                hasReadBytes += noReadBytes;
            }  else {
                //This is a valid case: there aren't any bytes left to read!
                break;
            }
            errno = 0;
        }


    ///////////////////////////////////////////////////////////////////////////////////////

    }

    destroyLocalReference(env, jbRarray);
    if (exception == 1) return -1;
    return hasReadBytes;
     // > OUR MODIFICATION
}
{noformat}, trunk patch, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440608/hdfs-923.patch
  against trunk revision 929406.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    -1 contrib tests.  The patch failed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/144/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/144/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/144/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/144/console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12440608/hdfs-923.patch
  against trunk revision 1072023.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    -1 patch.  The patch command could not apply the patch.

Console output: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/201//console

This message is automatically generated., If the problem is that hdfs_read is wrong, shouldn't we just fix hdfs_read.c?

If the issue is that you think a readFully method would be a nice thing to have, why not file a JIRA with that description, and then implement (and document) hdfsReadFully?

In any case, we should not change the original hdfsRead method., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12440608/hdfs-923.patch |
| Optional Tests | javac unit |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10538/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12440608/hdfs-923.patch |
| Optional Tests | javac unit |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10552/console |


This message was automatically generated., This issue is not applicable now, closing the issue.. Please feel free to reopen... ]