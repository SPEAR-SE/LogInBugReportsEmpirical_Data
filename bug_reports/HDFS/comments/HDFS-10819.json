[
Here is the log snippet from a failed test which is totally different from the signatures in HDFS-10780

{noformat}

1148 2016-08-25 18:21:19,853 [ReplicationMonitor] WARN  net.NetworkTopology (NetworkTopology.java:chooseRandom(816)) - Failed to find datanode (scope="" excludedScope="/default-rack").
1149 2016-08-25 18:21:19,853 [ReplicationMonitor] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(402)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
1150 2016-08-25 18:21:19,854 [ReplicationMonitor] WARN  net.NetworkTopology (NetworkTopology.java:chooseRandom(816)) - Failed to find datanode (scope="" excludedScope="/default-rack").
1151 2016-08-25 18:21:19,854 [ReplicationMonitor] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(402)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
1152 2016-08-25 18:21:19,854 [ReplicationMonitor] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK],      policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
1153 2016-08-25 18:21:19,854 [ReplicationMonitor] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(402)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
1154 2016-08-25 18:21:19,854 [ReplicationMonitor] DEBUG BlockStateChange (BlockManager.java:computeReconstructionWorkForBlocks(1680)) - BLOCK* neededReconstruction = 1 pendingReconstruction = 0
1155 2016-08-25 18:21:20,071 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:63869] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid a8be1f44-ba6d-4a90-a380-ccab72e69aeb) service to localhost/127.0.0.1:63869
1156 2016-08-25 18:21:20,684 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/]]  heartbeating to localhost/127.0.0.1:63869] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 2 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid ce51d074-8231-4ca6-8dff-21de2281910b) service to localhost/127.0.0.1:63869
1157 2016-08-25 18:21:20,684 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/]]  heartbeating to localhost/127.0.0.1:63869] DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 2 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid 31f5b7d6-6d2e-4410-8ca8-27bfe5b768fc) service to localhost/127.0.0.1:63869
1158 2016-08-25 18:21:20,688 [IPC Server handler 0 on 63869] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7015)) - allowed=true       ugi=manoj (auth:SIMPLE) ip=/127.0.0.1   cmd=getfileinfo src=/test       dst=null        perm=null       proto=rpc
1159 2016-08-25 18:21:20,690 [IPC Server handler 9 on 63869] DEBUG blockmanagement.BlockManager (BlockManager.java:createLocatedBlocks(1109)) - blocks = [blk_1073741825_1002]
1160 2016-08-25 18:21:20,690 [IPC Server handler 9 on 63869] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7015)) - allowed=true       ugi=manoj (auth:SIMPLE) ip=/127.0.0.1   cmd=open        src=/test       dst=null        perm=null       proto=rpc
1161 Block 0 of file /test has replication factor 2 (desired 3); locations 127.0.0.1:63870 127.0.0.1:63874
1162 2016-08-25 18:21:21,072 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:63869] DEB     UG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid a8be1f44-ba6d-4a90-a380-ccab72e69aeb) service to localhost/127.0.0.1:63869
1163 2016-08-25 18:21:21,690 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/]]  heartbeating to localhost/127.0.0.1:63869] DEB     UG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 2 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid ce51d074-8231-4ca6-8dff-21de2281910b) service to localhost/127.0.0.1:63869
1164 2016-08-25 18:21:21,690 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/]]  heartbeating to localhost/127.0.0.1:63869] DEB     UG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 2 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid 31f5b7d6-6d2e-4410-8ca8-27bfe5b768fc) service to localhost/127.0.0.1:63869


1165 2016-08-25 18:21:21,696 [IPC Server handler 3 on 63869] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7015)) - allowed=true       ugi=manoj (auth:SIMPLE) ip=/127.0.0.1   cmd=getfileinfo src=/test       dst=null        perm=null       proto=rpc
1166 2016-08-25 18:21:21,698 [IPC Server handler 2 on 63869] DEBUG blockmanagement.BlockManager (BlockManager.java:createLocatedBlocks(1109)) - blocks = [blk_1073741825_1002]
1167 2016-08-25 18:21:21,698 [IPC Server handler 2 on 63869] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7015)) - allowed=true       ugi=manoj (auth:SIMPLE) ip=/127.0.0.1   cmd=open        src=/test       dst=null        perm=null       proto=rpc
1168 Block 0 of file /test has replication factor 2 (desired 3); locations 127.0.0.1:63874 127.0.0.1:63870


1169 2016-08-25 18:21:22,078 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:63869] DEB     UG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 1 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid a8be1f44-ba6d-4a90-a380-ccab72e69aeb) service to localhost/127.0.0.1:63869
1170 2016-08-25 18:21:22,691 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/]]  heartbeating to localhost/127.0.0.1:63869] DEB     UG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 2 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid 31f5b7d6-6d2e-4410-8ca8-27bfe5b768fc) service to localhost/127.0.0.1:63869
1171 2016-08-25 18:21:22,691 [DataNode: [[[DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/, [DISK]file:/Users/manoj/work/ups-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/]]  heartbeating to localhost/127.0.0.1:63869] DEB     UG datanode.DataNode (BPServiceActor.java:sendHeartBeat(500)) - Sending heartbeat with 2 storage reports from service actor: Block pool BP-1210227499-172.16.3.66-1472174461128 (Datanode Uuid ce51d074-8231-4ca6-8dff-21de2281910b) service to localhost/127.0.0.1:63869
1172 2016-08-25 18:21:22,703 [IPC Server handler 4 on 63869] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7015)) - allowed=true       ugi=manoj (auth:SIMPLE) ip=/127.0.0.1   cmd=getfileinfo src=/test       dst=null        perm=null       proto=rpc
1173 2016-08-25 18:21:22,704 [IPC Server handler 5 on 63869] DEBUG blockmanagement.BlockManager (BlockManager.java:createLocatedBlocks(1109)) - blocks = [blk_1073741825_1002]
1174 2016-08-25 18:21:22,704 [IPC Server handler 5 on 63869] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7015)) - allowed=true       ugi=manoj (auth:SIMPLE) ip=/127.0.0.1   cmd=open        src=/test       dst=null        perm=null       proto=rpc
1175 Block 0 of file /test has replication factor 2 (desired 3); locations 127.0.0.1:63870 127.0.0.1:63874
{noformat}, *Problem:*
— BlockManager reports incorrect replica count for a file block even after successful replication to all replicas, 
— TestDataNodeHotSwapVolumes fails with “TimeoutException: Timed out waiting for /test to reach 3 replicas” error

*Analysis:*
- Client wrote data to DN1 as part of the initial write pipeline DN1 -> Dn2 -> DN3 
— DN1 persisted (say in storage volume *S1*) the block BLK_xyz_001, mirrored the block to downstreams and was waiting for the ack back.
- Later, one of the storage volumes in DN1 (say S2) was removed. Client detects pipeline issue, triggers pipeline recovery and gets the new write pipeline as DN2 —> DN3 
— On a successful {{FSNameSystem::updatePipeline}} request from Client, NameNode bumps up the Generation Stamp (from 001 to 002) of the UnderConstruction (that is, the last) block of the file.
- Client writes the new allocated Block BLK_xyz_002 to the new write pipeline nodes. (DN2 and DN3) 
- Client closed the file stream. NameNode ran the LowRedundancy checker for all the blocks in the file. Detected the block BLK_xyz having a replication factor of 2 Vs the expected 3.
- NameNode asked DN2 to replicate BLK_xyz_002 to DN1. Say DN1 persisted BLK_xyz_002 onto storage volume *S1* again.
- Now DN1 sends IBR to NameNode with the RECEIVED_BLOCK info about BLK_xyz_002 on *S1*

- BlockManager processed the incremental block report from DN1, was trying to store (metadata) the block BLK_xyz_002 for DN1 on storage *S1*
- But, DN1 S1 already had BLK_xyz_001 and was marked corrupted later as part of pipeline update. The check at line 2878 thus failed.
- So, when a storage had a corrupt block and later when the same storage reported a good block, BlockManager fails to update block --> datanode mapping and prune neededReconstruction list. Refer: {{BlockManager::addStoredBlock}}

{noformat}

  2871   void addStoredBlockUnderConstruction(StatefulBlockInfo ucBlock,    
  2872       DatanodeStorageInfo storageInfo) throws IOException {    
  2873     BlockInfo block = ucBlock.storedBlock;
  2874     block.getUnderConstructionFeature().addReplicaIfNotPresent(    
  2875         storageInfo, ucBlock.reportedBlock, ucBlock.reportedState);
  2876 
  2877     if (ucBlock.reportedState == ReplicaState.FINALIZED &&
  2878         (block.findStorageInfo(storageInfo) < 0)) {    
  2879       addStoredBlock(block, ucBlock.reportedBlock, storageInfo, null, true);
  2880     }   
  2881   }   

{noformat}

- Replication Monitor which runs continuously tried to reconstruct the block on DN1, but {{BlockPlacementPolicyDefault}} failed to find the choose the same target

{noformat}
1148 2016-08-25 18:21:19,853 [ReplicationMonitor] WARN  net.NetworkTopology (NetworkTopology.java:chooseRandom(816)) - Failed to find datanode (scope="" excludedScope="/default-rack").
1149 2016-08-25 18:21:19,853 [ReplicationMonitor] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(402)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
1150 2016-08-25 18:21:19,854 [ReplicationMonitor] WARN  net.NetworkTopology (NetworkTopology.java:chooseRandom(816)) - Failed to find datanode (scope="" excludedScope="/default-rack").
1151 2016-08-25 18:21:19,854 [ReplicationMonitor] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(402)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
1152 2016-08-25 18:21:19,854 [ReplicationMonitor] WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(161)) - Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK],      policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
1153 2016-08-25 18:21:19,854 [ReplicationMonitor] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(402)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
1154 2016-08-25 18:21:19,854 [ReplicationMonitor] DEBUG BlockStateChange (BlockManager.java:computeReconstructionWorkForBlocks(1680)) - BLOCK* neededReconstruction = 1 pendingReconstruction = 0
{noformat}


*Fix:*

- {{BlockManager::addStoredBlockUnderConstruction}} should not check for block --> datanode storage mapping for invoking BlockManager::addStoredBlock
- {{BlockManager::addStoredBlock}} already handles Block addition/replacement/already_exists cases. And, more importantly it also prunes the {{LowRedundancyBlocks}} list

Attached patch has the fix. Also, updated the unit test TestDataNodeHotSwapVolumes#testRemoveVolumeBeingWrittenForDatanode to expose race conditions which helped to recreate the above problem frequently. With the proposed fix, BlockManager handles the case properly and the test passes.


, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 47s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 72m 46s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 18s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 92m  6s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Issue | HDFS-10819 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12826475/HDFS-10819.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 89adb670a904 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 01721dd |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/16597/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/16597/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/16597/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Unit test failure is not related to the patch. Have seen same failures earlier, without the patch.
java.lang.AssertionError: expected:<0> but was:<9580424252>
	at org.junit.Assert.fail(Assert.java:88)
, Thanks for working on this Manoj. Great investigation here.

IIUC this is going to be a problem mostly for small clusters, right? We need to have a collision between two genstamps of the same block.

Would this also be addressed by having the NN first invalidate the corrupt replica before replicating the correct one? I'm wondering if the safer fix is to wait for this invalidation by excluding nodes with corrupt replicas when doing block placement.

Also curious, would invalidation eventually fix this case, or is it truly stuck? That seems like another bug we should address., [~andrew.wang],

Thanks for reviewing the patch.

{quote}We need to have a collision between two genstamps of the same block.{quote}
More importantly, if the same storage volume in DN happens to hold a block and its various genstamps, then without fix NN will not "store" the blocks with recent/higher genstamps. 

{quote}Would this also be addressed by having the NN first invalidate the corrupt replica before replicating the correct one{quote}
{{BlockManager#markBlockAsCorrupt}} already tries to invalidate the corrupt blocks. But block invalidations are postponed if any of the replica are stale  and might not be invalidated for some time and will delay the block reaching to replication factor.

{quote}Also curious, would invalidation eventually fix this case, or is it truly stuck?
{code}
    // add block to the datanode
    AddBlockResult result = storageInfo.addBlock(storedBlock, reportedBlock);

    if (result == AddBlockResult.ADDED) {
    .. ..
    } else if (result == AddBlockResult.REPLACED) {
    .. .. 
    } else {
      // if the same block is added again and the replica was corrupt
      // previously because of a wrong gen stamp, remove it from the
      // corrupt block list.
      corruptReplicas.removeFromCorruptReplicasMap(block, node,
          Reason.GENSTAMP_MISMATCH);
      curReplicaDelta = 0;
      blockLog.debug("BLOCK* addStoredBlock: Redundant addStoredBlock request"
              + " received for {} on node {} size {}", storedBlock, node,
          storedBlock.getNumBytes());
    }
{code}

As you see above, there is code already in {{BlockManager#addStoredBlock}} to handle the case we are interested in -- Block with latest GS on the same storage volume. Except, the caller {{BlockManager#addStoredBlockUnderConstruction}} is mistakenly skipping the block and not allowing the other module to handle the case properly. Haven't explored the invalidation path fully and not sure if it solve the problem for testRemoveVolumeBeingWrittenForDatanode. Please let me know I need to explore this path.

, [~andrew.wang],

{quote} Also curious, would invalidation eventually fix this case, or is it truly stuck? {quote}
* I find it totally stuck in this test case as we have only 3 DNs and the expected Replication factor is also 3. 
* Block invalidation was not going through and the replication factor failed to catch up. The reason why Block Invalidation at DataNode didn't go through was because the disk which held the block is already closed as it was removed.

{noformat}
 730 2016-10-04 15:52:30,709 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:invalidate(1990)) - Volume /Users/manoj/work/cdh-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current is closed, ignore the deletion task for block ReplicaBeingWritten, blk_1073741825_1001, RBW
 731   getNumBytes()     = 512
 732   getBytesOnDisk()  = 512
 733   getVisibleLength()= 512
 734   getVolume()       = /Users/manoj/work/cdh-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current
 735   getBlockFile()    = /Users/manoj/work/cdh-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-473099417-172.16.3.66-1475621545787/current/rbw/blk_1073741825
 736   bytesAcked=512
 737   bytesOnDisk=512
{noformat}

The core fix here is letting {{BlockManager#addStoredBlockUnderConstruction}} invoke {{addStoredBlock}} for all Finalized blocks and let addStoredBlocks decide on (which is already happening) follow up actions of invalidations removal of corrupt replicas. 

[~andrew.wang], [~eddyxu], would like to hear your further thoughts on this.

]