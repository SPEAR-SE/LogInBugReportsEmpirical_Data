[patch for trunk, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12576518/HDFS-4660.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.fs.TestFcHdfsSymlink

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4179//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4179//console

This message is automatically generated., [~sureshms] plz help to review this issue, Hi Peng. Can you please see if you can add a unit test for this?, The scenario for this issue is simple: client send a packet, part of which needs to write and part needs to skip. When amount of data to skip reaches trunk size, receiver doesn't skip checksum and has it duplicated. 

But for unit test, I found receivePacket() got many dependences and there's no test for it before. So I think it's not easy to add unit tests for it.

Any good ideas, Todd?, Can you create a functional test which does something like this?
- Create a pipeline and write a number of bytes which isn't an exact multiple of the checksum chunk size (eg 800 bytes).
- Call hflush to ensure that all DNs have the full length
- Restart the second DN in the pipeline, to trigger adding DN4
- Write a bit more and close the file.
- Verify that all replicas have identical checksum files., #Call hflush to ensure that all DNs have the full length

I think if this process happened, bug will not be triggered.

After client called hflush() without all DNs acked, DN1 may had more bytes than other DNs. 
So if DN2 died and new added DN4 located at 2nd position of the pipeline(controlled by NM's pipeline sort algorithm), it will recover RBW from DN1.
After recovery, DN4 may had more bytes than DN3. 
And client will continue sending from smallest offset that it received acks. 
So this will cause DN4 to "receive a packet, part of which needs to write and part needs to skip. When amount of data to skip reaches trunk size, receiver doesn't skip checksum and has it duplicated". 

Creating a test case from high-level may be not easy, because we need to control DNs file position after hflush, and also DN4's location in recovered pipeline., bq. After recovery, DN4 may had more bytes than DN3. 

In HDFS-3875, such behaviors cause checksum errors in unacked portion of data to be uncaught and later detected when NN tries to asynchronously replicate the block. At that point, no one has a valid copy and the use experiences a data loss.  The latest proposed solution in HDFS-3875 is to truncate the block on recoverRbw(). The reasoning is that the unacked portion is not checksum verified and thus cannot be trusted.  Will this also address this issue?, If recoverRbw() truncates the block to acked length, then in this issue's scenario, after pipeline recovery: len(DN1) = len(DN4) <= len(DN3). 
And then client will continue from acked offset (len(DN1)), so DN3 may falls into trap that DN4 had before. 

IMHO, boundary check is still needed to solve duplicated checksum problem.

 , \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12576518/HDFS-4660.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10537/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12576518/HDFS-4660.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10562/console |


This message was automatically generated., Actually this is a serious data corruption issue. It is easily reproduced when timeout is set shorter and data is written and flushed frequently. If a sufficient load is put on, timeout can occur and a pipeline recovery is triggered. If a new node is added, the partial block copy can make the ACKed size on the new node bigger than others. Although less likely, the same thing can happen without involving a new node. It can also happen in partial chunk cases, which the existing patch does not handle.

I have a patch that was stress tested and internally reviewed. I am in the process of adding a unit test., Canceling the existing patch and assigning it to me., We saw this kind of corruption happening when the copied partial block data does not end at a packet boundary. The un-acked packets are resent from the client and if the end of the on-disk data is not aligned, corruption happens.  This is very difficult to reproduce in unit test without being too invasive.

However, data corruption can be reproduced in a 10-node cluster. Here is how we reproduced it and verified the patch (Credit goes to [~nroberts]) :

- Modify teragen to hflush() every 10000 records
- Change datanode WRITE_TIMEOUT_EXTENSION from 5000 ms to 1ms - allows socket write timeout config to have full control over the write timeout
- Config dfs.datanode.socket.write.timeout to 2000ms
- Config dfs.client.block.write.replace-datanode-on-failure.policy to ALWAYS so that write pipelines are always immediately reconstructed when a failure occurs
- Run teragen with 100 maps, each outputting 10000000000
- Success criteria is no "Checksum verification failed" in any datanode logs. This is from added checksum verification in recoverRbw(). A patch will be provided in HDFS-8395. 
- The write timeout is so aggressive that the teragen job will probably fail due to multiple, repeated failures eventually causing task attempts to fail, this is expected.

, \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 35s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 29s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 39s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:red}-1{color} | checkstyle |   2m 14s | The applied patch generated  4 new checkstyle issues (total was 62, now 63). |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 6  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 33s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 35s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   3m  2s | The patch does not introduce any new Findbugs (version 2.0.3) warnings. |
| {color:green}+1{color} | native |   3m 13s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests | 168m 19s | Tests failed in hadoop-hdfs. |
| | | 211m  5s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.tools.TestHdfsConfigFields |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12732698/HDFS-4660.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 281d47a |
| checkstyle |  https://builds.apache.org/job/PreCommit-HDFS-Build/10961/artifact/patchprocess/diffcheckstylehadoop-hdfs.txt |
| whitespace | https://builds.apache.org/job/PreCommit-HDFS-Build/10961/artifact/patchprocess/whitespace.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/10961/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/10961/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf902.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10961/console |


This message was automatically generated., \\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | pre-patch |  14m 38s | Pre-patch trunk compilation is healthy. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:red}-1{color} | tests included |   0m  0s | The patch doesn't appear to include any new or modified tests.  Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
| {color:green}+1{color} | javac |   7m 37s | There were no new javac warning messages. |
| {color:green}+1{color} | javadoc |   9m 37s | There were no new javadoc warning messages. |
| {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 36s | There were no new checkstyle issues. |
| {color:red}-1{color} | whitespace |   0m  0s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. |
| {color:green}+1{color} | install |   1m 39s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:green}+1{color} | findbugs |   3m  4s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | native |   3m 16s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests | 163m 24s | Tests failed in hadoop-hdfs. |
| | | 204m 50s | |
\\
\\
|| Reason || Tests ||
| Failed unit tests | hadoop.hdfs.TestAppendSnapshotTruncate |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12733630/HDFS-4660.v2.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / 0790275 |
| whitespace | https://builds.apache.org/job/PreCommit-HDFS-Build/11038/artifact/patchprocess/whitespace.txt |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/11038/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/11038/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf900.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11038/console |


This message was automatically generated., [~kihwal], I see you marked this as a blocker for 2.7.1.

Assuming you can get hold of someone's review bandwidth to get this done soonish, we are good. Otherwise, also given this is a long standing issue, I recommend we track this instead for 2.7.2. What do you think?, In short description: Packet's offsetInBlock points back more than bytesPerChecksum(512) than onDisk length, checksum will have duplicates, but not data.

Patch looks very nice with the detailed description.
+1 LGTM.

{quote}// Determine how many checksums need to be skipped up to the last
// boundary. The checksum after the boundary was already counted
// above. Only count the number of checksums skipped up to the
// boundary here.{quote}
Actual fix required was this. i.e. Number of checksum bytes to skip.

But patch enhanced the readability along with fix.

For the calculation purpose, I used the numbers given in description. It seems to me that, problem will be solved from this.

I would love to see, if someone else also confirms the calculation, before going ahead for commit., Thanks for the review, [~vinayrpet]. After stress testing using the setup mentioned above, we have deployed the fix to the production cluster that generated checksum errors frequently. We have not seen any corruption so far.  We are confident that it fixes the issue., [~vinodkv] If the commit does not happen today, I will move it to 2.7.2., +1 on the patch. I have reviewed the patch previously and it is currently running in production at scale. 

The stress test we ran against this in https://issues.apache.org/jira/browse/HDFS-4660?focusedCommentId=14542862&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14542862 heavily exercised this path. 
, Thanks, Nathan.  With Vinay's binding +1 and Nathan's review, I wil commit this., Thanks for reports and reviews. I've committed this to trunk, branch-2 and branch-2.7., FAILURE: Integrated in Hadoop-trunk-Commit #8028 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/8028/])
HDFS-4660. Block corruption can happen during pipeline recovery. Contributed by Kihwal Lee. (kihwal: rev c74517c46bf00af408ed866b6577623cdec02de1)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, bq. After stress testing using the setup mentioned above, we have deployed the fix to the production cluster that generated checksum errors frequently. We have not seen any corruption so far. We are confident that it fixes the issue.
Thanks for the info and contribution [~kihwal]., FAILURE: Integrated in Hadoop-Yarn-trunk #961 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/961/])
HDFS-4660. Block corruption can happen during pipeline recovery. Contributed by Kihwal Lee. (kihwal: rev c74517c46bf00af408ed866b6577623cdec02de1)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #231 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/231/])
HDFS-4660. Block corruption can happen during pipeline recovery. Contributed by Kihwal Lee. (kihwal: rev c74517c46bf00af408ed866b6577623cdec02de1)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #220 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/220/])
HDFS-4660. Block corruption can happen during pipeline recovery. Contributed by Kihwal Lee. (kihwal: rev c74517c46bf00af408ed866b6577623cdec02de1)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #229 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/229/])
HDFS-4660. Block corruption can happen during pipeline recovery. Contributed by Kihwal Lee. (kihwal: rev c74517c46bf00af408ed866b6577623cdec02de1)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2177 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2177/])
HDFS-4660. Block corruption can happen during pipeline recovery. Contributed by Kihwal Lee. (kihwal: rev c74517c46bf00af408ed866b6577623cdec02de1)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #2159 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2159/])
HDFS-4660. Block corruption can happen during pipeline recovery. Contributed by Kihwal Lee. (kihwal: rev c74517c46bf00af408ed866b6577623cdec02de1)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, Hi [~kihwal], shall we backport this patch to 2.6.x branch?, Attaching a 2.6 version of the patch., I have commit the 2.6 patch to branch-2.6. Thanks [~kihwal] for updating the patch., Hello [~kihwal] we are seeing a similar bug on a CDH5.5 cluster, which has this fix (HDFS-4660), so it may be a different bug. Would you please take a look at HDFS-10587? We've analyzed the log and reconstructed the sequence of events, and we are in the process of creating a unit test.

Thanks!, Hi [~nroberts],

Thanks for your earlier work here. Would you please explain how you did the first step

"Modify teragen to hflush() every 10000 records"

in

https://issues.apache.org/jira/browse/HDFS-4660?focusedCommentId=14542862&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14542862

Thanks much.

, Hi [~yzhangal]. Had to go back to an old git stash, but I'll attach a sample patch to TeraOutputFormat., Thank you very much [~nroberts]!, SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #10363 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/10363/])
HDFS-10652. Add a unit test for HDFS-4660. Contributed by Vinayakumar (yzhang: rev c25817159af17753b398956cfe6ff14984801b01)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNodeFaultInjector.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientProtocolForPipelineRecovery.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
]