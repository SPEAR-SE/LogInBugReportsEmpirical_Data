[
Analysis
------------
1. startLogSegment success in JN1 and failed in JN2,JN3
{noformat}
	2016-09-21 21:34:13,807 FATAL org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: flush failed for required journal (JournalAndStream(mgr=QJM to [192.167.100.47:8485, 192.167.100.49:8485, 192.167.100.51:8485], stream=QuorumOutputStream starting at txid 195222))
	org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 1 successful responses:
	192.167.100.49:8485: null [success]
	2 exceptions thrown:
	192.167.100.51:8485: Call From oraclec1h3-iscsi/192.167.100.49 to oraclec1h2-iscsi:8485 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	192.167.100.47:8485: Call From oraclec1h3-iscsi/192.167.100.49 to oraclec1h1-iscsi:8485 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
		at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81)
		at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:223)
		at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:142)
		at org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream.flushAndSync(QuorumOutputStream.java:107)
		at org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:113)
		at org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:107)
		at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$8.apply(JournalSet.java:533)
		at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:393)
		at org.apache.hadoop.hdfs.server.namenode.JournalSet.access$100(JournalSet.java:57)
		at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream.flush(JournalSet.java:529)
		at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:648)
		at org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1268)
		at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:323)
		at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1123)
		at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1869)
		at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
		at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)
		at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
		at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1735)
		at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1634)
		at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
		at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)
		at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
		at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:973)
		at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2143)
		at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2139)
		at java.security.AccessController.doPrivileged(Native Method)
		at javax.security.auth.Subject.doAs(Subject.java:422)
		at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1710)
		at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2137)
	2016-09-21 21:34:13,807 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Aborting QuorumOutputStream starting at txid 195222
{noformat}
2. Next "recoverUnclosedJournal" streams is not finalizing in JN1, because JN1's response is coming as last. So its not considered.
{noformat}	
	2016-09-21 21:40:00,339 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 195029
	2016-09-21 21:40:00,380 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
	192.167.100.47:8485: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } lastWriterEpoch: 15 lastCommittedTxId: 195221
	192.167.100.51:8485: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } lastWriterEpoch: 15 lastCommittedTxId: 195221
	2016-09-21 21:40:00,381 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 192.167.100.47:8485=segmentState {
	  startTxId: 195029
	  endTxId: 195221
	  isInProgress: false
	}
	lastWriterEpoch: 15
	lastCommittedTxId: 195221
{noformat}
3. While catching up, 195222 was not considered as that was not finalized in JN1.

4. While trying to create the next logSegment, NameNode is finding the same stream available in JN1, so creation failed
{noformat}
	2016-09-21 21:40:00,463 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Error encountered requiring NN shutdown. Shutting down immediately.
	java.lang.IllegalStateException: Cannot start writing at txid 195222 when there is a stream available for read: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1d5067d4
		at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:320)
		at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1123)
		at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1869)
		at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
		at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)
{noformat}, One possible solution could be as below:

1. From all available JNs we can come to know that what is the last readable txn available in each JN.
2. We can select the *minimum lastReadable txn*, which is matching in min quorum JNs, as max Txn to consider as valid.
3. If there is any mismatch between lastReadableTxn in available JNs and there is no quorum matching, then read can be failed as stale txns should not be considered.

[~andrew.wang], [~kihwal], [~umamaheswararao], [~szetszwo], please provide your opinions on this, as this seems to be critical to decide valid txns.
, Below are some more logs related to recoverUnFinalizedSegments during failover.

1. Active NameNode logs
{noformat}
2016-09-21 21:40:00,291 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2016-09-21 21:40:00,301 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Starting recovery process for unclosed journal segments...
2016-09-21 21:40:00,339 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Successfully started new epoch 16
2016-09-21 21:40:00,339 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Beginning recovery of unclosed segment starting at txid 195029
2016-09-21 21:40:00,380 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Recovery prepare phase complete. Responses:
192.167.100.47:8485: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } lastWriterEpoch: 15 lastCommittedTxId: 195221
192.167.100.51:8485: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } lastWriterEpoch: 15 lastCommittedTxId: 195221
2016-09-21 21:40:00,381 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Using longest log: 192.167.100.47:8485=segmentState {
  startTxId: 195029
  endTxId: 195221
  isInProgress: false
}
lastWriterEpoch: 15
lastCommittedTxId: 195221
.
.
.
2016-09-21 21:40:00,424 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Catching up to latest edits from old active before taking over writer role in edits logs
.
.
.
2016-09-21 21:40:00,435 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Will take over writing edit logs at txnid 195222
2016-09-21 21:40:00,463 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Error encountered requiring NN shutdown. Shutting down immediately.
java.lang.IllegalStateException: Cannot start writing at txid 195222 when there is a stream available for read: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1d5067d4
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:320)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1123)
{noformat}

2. From JN with stale txn: 192.167.100.49:8485
{noformat}
2016-09-21 21:40:00,424 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from 15 to 16 for client /192.167.100.49
2016-09-21 21:40:00,428 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage FileJournalManager(root=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster)
2016-09-21 21:40:00,433 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_inprogress_0000000000000195222,first=0000000000000195222,last=0000000000000195222,inProgress=true,hasCorruptHeader=false)
2016-09-21 21:40:00,460 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo(195029): EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_0000000000000195029-0000000000000195221,first=0000000000000195029,last=0000000000000195221,inProgress=false,hasCorruptHeader=false) -> startTxId: 195029 endTxId: 195221 isInProgress: false
2016-09-21 21:40:00,461 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Prepared recovery for segment 195029: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } lastWriterEpoch: 15 lastCommittedTxId: 195221
2016-09-21 21:40:00,470 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo(195029): EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_0000000000000195029-0000000000000195221,first=0000000000000195029,last=0000000000000195221,inProgress=false,hasCorruptHeader=false) -> startTxId: 195029 endTxId: 195221 isInProgress: false
2016-09-21 21:40:00,470 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Skipping download of log startTxId: 195029 endTxId: 195221 isInProgress: false: already have up-to-date logs
2016-09-21 21:40:00,472 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment 195029: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } acceptedInEpoch: 16
{noformat}

3. JN 192.167.100.47:8485
{noformat}
2016-09-21 21:40:15,874 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from 15 to 16 for client /192.167.100.49
2016-09-21 21:40:15,877 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage FileJournalManager(root=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster)
2016-09-21 21:40:15,881 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_0000000000000195029-0000000000000195221,first=0000000000000195029,last=0000000000000195221,inProgress=false,hasCorruptHeader=false)
2016-09-21 21:40:15,908 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo(195029): EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_0000000000000195029-0000000000000195221,first=0000000000000195029,last=0000000000000195221,inProgress=false,hasCorruptHeader=false) -> startTxId: 195029 endTxId: 195221 isInProgress: false
2016-09-21 21:40:15,909 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Prepared recovery for segment 195029: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } lastWriterEpoch: 15 lastCommittedTxId: 195221
2016-09-21 21:40:15,939 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo(195029): EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_0000000000000195029-0000000000000195221,first=0000000000000195029,last=0000000000000195221,inProgress=false,hasCorruptHeader=false) -> startTxId: 195029 endTxId: 195221 isInProgress: false
2016-09-21 21:40:15,939 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Skipping download of log startTxId: 195029 endTxId: 195221 isInProgress: false: already have up-to-date logs
2016-09-21 21:40:15,941 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment 195029: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } acceptedInEpoch: 16
{noformat}

4. JN 192.167.100.51:8485
{noformat}
2016-09-21 21:40:33,523 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Updating lastPromisedEpoch from 15 to 16 for client /192.167.100.49
2016-09-21 21:40:33,526 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Scanning storage FileJournalManager(root=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster)
2016-09-21 21:40:33,530 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log is EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_0000000000000195029-0000000000000195221,first=0000000000000195029,last=0000000000000195221,inProgress=false,hasCorruptHeader=false)
2016-09-21 21:40:33,547 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo(195029): EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_0000000000000195029-0000000000000195221,first=0000000000000195029,last=0000000000000195221,inProgress=false,hasCorruptHeader=false) -> startTxId: 195029 endTxId: 195221 isInProgress: false
2016-09-21 21:40:33,548 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Prepared recovery for segment 195029: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } lastWriterEpoch: 15 lastCommittedTxId: 195221
2016-09-21 21:40:33,581 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: getSegmentInfo(195029): EditLogFile(file=/opt/carbondata/HACluster/install/hadoop/journalnode/journalDir/hacluster/current/edits_0000000000000195029-0000000000000195221,first=0000000000000195029,last=0000000000000195221,inProgress=false,hasCorruptHeader=false) -> startTxId: 195029 endTxId: 195221 isInProgress: false
2016-09-21 21:40:33,582 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Skipping download of log startTxId: 195029 endTxId: 195221 isInProgress: false: already have up-to-date logs
2016-09-21 21:40:33,583 INFO org.apache.hadoop.hdfs.qjournal.server.Journal: Accepted recovery for segment 195029: segmentState { startTxId: 195029 endTxId: 195221 isInProgress: false } acceptedInEpoch: 16
{noformat}

From above logs its clear that, JN with highest txn(stale) was not participated in recovery. But during fseditLog.openForWrite(), selectInputStreams() it responded, so starrLogSegment() failed.

*So Another possible solution could be:*
1. During recovery, discard all segments which are more than requested segment to recover.]