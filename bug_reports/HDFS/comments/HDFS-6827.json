[Submit a patch to fix this bug.
BTW: The {{healthCheckLock}} is used to distinguish the graceful failover and the above scenario. , {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12660080/HDFS-6827.1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7569//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7569//console

This message is automatically generated., Hi [~arpitagarwal], can you help review this patch?, This issue looks a little ugly.  NameNodes stuck in standby mode?  This production?  What did it look like?

bq. BTW: The healthCheckLock is used to distinguish the graceful failover and the above scenario.

I don't know this code well.  How is the above done?

The code in checkServiceStatus seems 'fragile' looking for a explicity transition. Is there a more explicit check that can be done to learn if 'service is restarted'?

In the below test, is it possible the state gets moved to standby before the service healthy check runs?  Would the test get stuck in this case?

+      // fake svc0 is restarted and change to standby, but the health monitor
+      // doesn't realize that
+      svc0.proxy.transitionToStandby(new StateChangeRequestInfo(
+          RequestSource.REQUEST_BY_ZKFC));
+
+      cluster.waitForHealthState(0, State.SERVICE_HEALTHY);
+      cluster.waitForHealthState(1, State.SERVICE_HEALTHY);

Thanks., Thanks [~stack].
bq. This issue looks a little ugly. NameNodes stuck in standby mode? This production? What did it look like?
Yes, both NameNodes stuck in standby mode, and the hbase cluster over it coundn't read/write any more.
We can reproduce the issue in the following way:
1. Change the sleep time of {{Client#handleConnectionFailure()}} longer
{code}
     try {
        Thread.sleep(action.delayMillis); // default is 1s, can change to 10s or longer
      } catch (InterruptedException e) {
        throw (IOException)new InterruptedIOException("Interrupted: action="
            + action + ", retry policy=" + connectionRetryPolicy).initCause(e);
      }
{code}
2. Restart the active NameNode quickly, and ensure that the NameNode starts successfully before ZKFC retrying connect., bq. BTW: The healthCheckLock is used to distinguish the graceful failover and the above scenario. 
bq. I don't know this code well. How is the above done?
bq. The code in checkServiceStatus seems 'fragile' looking for a explicity transition. Is there a more explicit check that can be done to learn if 'service is restarted'?
The solution for this issue is to let the ZKFC learn 'service is restarted'. 

One straightforward way is to add a field in the {{MonitorHealthResponseProto}} to identify that the service is restarted, for example the pid of the NN process, or a generated UUID will satisfy our requirement. Another way is to let the ZKFC learn 'service is restarted' by comparing the service's current state and last state. We choose the later one, in this way we can fix the problem inside ZKFC, don't influence other services. 

As we know that ZKFC supports gracefully failover from the command line tool, and during graceful failover, the ZKFC may encounter a scenario like this: the last state of the service Active, the current is Standby, and the service is healthy. This scenario is just the same as the buggy scenario described above, we must distinguish these two scenarios. So we  add the {{healthCheckLock}} to 'fragile' the health checking when doing graceful failover.

Hope I expressed myself clearly:), bq. In the below test, is it possible the state gets moved to standby before the service healthy check runs? Would the test get stuck in this case?
It's not possible, the state is only changed during health checking or graceful failover, here's no graceful failover, so it's only changed by the health monitor. If the health monitor doesn't run, the state won't be moved., Nice script for reproducing the issue.

bq. The solution for this issue is to let the ZKFC learn 'service is restarted'.

Yes. Is there anything more definitive than a check for a particular state transition? (Sorry, don't know this area well).

bq. One straightforward way is to add a field in the MonitorHealthResponseProto to identify that the service is restarted, for example the pid of the NN process, or a generated UUID will satisfy our requirement.

This seems less prone to misinterpretation.

Your NN came up inside a second?

A hacky workaround in meantime would have the NN start sleep first for a second?

Thanks for looking into this [~wuzesheng], Thanks [~stack]:)

bq. Yes. Is there anything more definitive than a check for a particular state transition? (Sorry, don't know this area well).
If we want to fix the bug inside ZKFC, there's no other definitive indicator according to my current knowledge of ZKFC. 

bq. This seems less prone to misinterpretation.
Yes, this is more straightforward and less prone to misinterpretation. But change the {{MonitorHealthResponseProto}} proto may introduce an incompatible changing, if folks think this is acceptable, perhaps we can use this method.

bq. Your NN came up inside a second?
As I described in the issue description, our NN came up inside about 6 seconds. 1 second for {{Client#handleConnectionFailure()}} sleep, the other 5 seconds for some unknown reasons, maybe GC or network problems, we haven't found direct evidences. 

bq. A hacky workaround in meantime would have the NN start sleep first for a second?
Yes,  we can let NN sleep sometime before startup. Indeed we use this method to quick fix the bug in our production cluster temporarily. But for a long term and general solution, we should fix this in the ZKFC side. One more thing, ZKFC is a general automatic HA failover framework, it is used in HDFS, but not only for HDFS, it may be used in other system who needs automatic HA failover. From this perspective, we should fix this inside ZKFC.
, bq. As I described in the issue description, our NN came up inside about 6 seconds. 1 second for Client#handleConnectionFailure() sleep, the other 5 seconds for some unknown reasons, maybe GC or network problems, we haven't found direct evidences.

Sorry, this description is not very accurate.
Our NN came up inside about 6 seconds. And the ZKFC retried connection exactly after NN starting successfully. There are about 6 seconds between ZKFC detected 'Connection reset by peer' and reconnected NN successfully. 1 second for {{Client#handleConnectionFailure()}} sleep is definitely, the other 5 seconds for some unknown reasons, maybe GC or network problems, we haven't found direct evidences.
, You got this issue in 2.4.1 version?
By any chance.. Your issue is HADOOP-10251 ? This was not merged to 2.4.1, Verified that, added test in the patch passes without any code change in trunk.
Verified again without the code change of HADOOP-10251, then it timeouts.
So I feel, HADOOP-10251 fixes the issue., I've looked into HADOOP-10251,  it's quite different from this issue.
The root cause of this issue is that ANN's ZKFC isn't aware that ANN is retarted and doesn't trigger failover., [~vinayrpet], Can you verify according to the reproduce method as I described [here | https://issues.apache.org/jira/browse/HDFS-6827?focusedCommentId=14101725&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14101725]? 
Thanks., bq. The root cause of this issue is that ANN's ZKFC isn't aware that ANN is retarted and doesn't trigger failover.
This is fine. After HADOOP-10251, ZKFC will trigger failover based on the state check from the NameNode.
for ex: Active ZKFC expects its NN to be in ACTIVE state, but after restart of NN its STANDBY, (ZKFC need not know NN is restrarted and NN doesnot automatically comes to ACTIVE). So ZKFC quits the election, and re-election will happen for the new Active., I gone through your patch of HADOOP-10251.
{code}
private synchronized void setLastServiceStatus(HAServiceStatus status) {
    this.lastServiceState = status;
    for (ServiceStateCallback cb : serviceStateCallbacks) {
      cb.reportServiceStatus(lastServiceState);
    }
  }
{code}
The above method is only called by the {{HealthMonitor}}, so the state check will only be performed during the health check. How does your patch handle the following two scenarios(NN1 is Active, NN2 is standby):
# NN1 is restarted, but ZKFC1 isn't aware of it, ZKFC1 thinks NN1 is healthy, last state is Active, current state is Standby
# Do a graceful failover from the command line tool, ZKFC1 thinks NN1 is healthy, last state is Active, current state is Standby, Ping [~vinayrpet].., Hi [~wuzesheng],

Please check the {{ZKFailOverController#verifyChangedServiceState(..)}} this will be called for every health check callback, which will happen every 1 sec by default.
In your case,
1. Service is HEALTHY, even though NN was restarted within this 1 sec interval.
2. But, after restart of NN, first healthcheck callback will identify state change of ANN in {{ZKFailOverController#verifyChangedServiceState(..)}} and ZKFC will quit the election marking {{quitElectionOnBadState}} to true. Now another STANDBY ZKFC will have a chance to become ACTIVE.
3.  Next callback from healthcheck will call {{rechedElectability()}} which will in-turn make ZKFC to join the election back if the service is still HEALTHY. Meanwhile other ZKFC would have won the leader election and became ACTIVE.

So after HADOOP-10251, I feel your problem also will be solved. 
Have you tried same scenario in latest trunk code?, Update: In point #2, quitting the election will happen on second healthcheck callback. two continuous callbacks will be checked before quitting election to make sure that state change differences are not coming due to parellel transition. And on third callback {{rechedElectability(..)}} will be called.
, Thanks [~vinayrpet], I will try the scenario in the latest trunk code soon., [~vinayrpet], I verified your patch of HADOOP-10251 on my cluster, it works as expected. Thanks.
I will resolve this issue as 'duplicated'., Duplicate of HADOOP-10251.]