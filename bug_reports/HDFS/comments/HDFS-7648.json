[[~szetszwo] I'm going through the "block ID-based block layout on datanodes" design and I come across this jira. I'm interested to implement this idea. I feel block report generation would be feasible one. Could you briefly explain about the verification points if you have anything specific in your mind. Thanks!, During block report generation or directory scanning, it traverses the directory for collecting all the replica information.  We should verify whether the actual directory location of a replica has the expected directory path computed using its block ID.  If a mismatch is found, it should also fix it.  On a second thought, DirectoryScanner seems a better place to do the verification since the purpose of the DirectoryScanner is to verify and fix the blocks stored in the local directories.  What do you think?, bq.If a mismatch is found, it should also fix it
Should I need to worry about the race between DatanodeBlockId_Layout_threads(they will do a linking ) in Datastorage and this call path?

bq. DirectoryScanner seems a better place to do the verification
Thanks for the hint. Let me try this as well.
, > Should I need to worry about the race between DatanodeBlockId_Layout_threads(they will do a linking ) in Datastorage and this call path?

Could you show me the line number in DataStorage.java?, Ah, this is upgrade path.

{{DataStorage.java:}}
{code}
line#1036

ExecutorService linkWorkers = Executors.newFixedThreadPool(numLinkWorkers);
.
.
futures.add(linkWorkers.submit(new Callable<Void>() {
{code}, Yes. It won't be a problem then., I agree with Nicholas that we should implement this in the DirectoryScanner.  Probably what we want to do is log a warning about files that are in locations they do not belong in.  I do not think we should to implement this in the layout version upgrade code, since that only gets run once.  The fact that hardlinking is done in parallel in the upgrade code should not be relevant for implementing this., Thanks [~cmccabe] for comments.
bq.Probably what we want to do is log a warning about files that are in locations they do not belong in
Do you meant, DirectoryScanner should identify the blocks which are not in the expected directory path computed using its block ID and just do a log message without fixing it. As per the earlier discussion it need to fix. I was having an idea which is similar to the mechanism used in Datastorage, create hardlinks. Please correct me if I miss anything., bq. Do you meant, DirectoryScanner should identify the blocks which are not in the expected directory path computed using its block ID and just do a log message without fixing it

correct

bq. As per the earlier discussion it need to fix. I was having an idea which is similar to the mechanism used in Datastorage, create hardlinks. Please correct me if I miss anything.

It's not the goal of DirectoryScanner to fix anything.  It should not modify the filesystem., [~cmccabe]
bq. It's not the goal of DirectoryScanner to fix anything. It should not modify the filesystem.
What would be the suggested way to fix these unmatched blocks. Also, if it is not fixed then this warning message will be printed repeatedly during the directory scanning interval.

[~szetszwo] do you have any suggestions about this approach. Thanks!, > It's not the goal of DirectoryScanner to fix anything. ...

The original design of DirectoryScanner is to reconciles the differences between the block information maintained in memory and the actual blocks stored in disks.  So it does fix the in-memory data structure.

> What would be the suggested way to fix these unmatched blocks. Also, if it is not fixed then this warning message will be printed repeatedly during the directory scanning interval.

Yet more questions if the blocks are not fixed: should the block report include those blocks?  How to access those blocks?  How and when to fix those blocks?

It seems fixing the blocks is better.  Of course, we still log an error message for those blocks., bq. The original design of DirectoryScanner is to reconciles the differences between the block information maintained in memory and the actual blocks stored in disks. So it does fix the in-memory data structure.

Fixing the in-memory data structure is different than fixing the on-disk data structure.  I do not think that the DirectoryScanner should modify the files on the disk.  It just introduces too much potential for error and mistakes in the scanner to cause data loss.

bq. Yet more questions if the blocks are not fixed: should the block report include those blocks? How to access those blocks? How and when to fix those blocks?

The only way we could ever get into this state is:
* if someone manually renamed some block files on ext4
* if someone introduced a bug in the datanode code that put blocks in the wrong place.
* if there is serious ext4 filesystem corruption

None of those cases seems like something we should be trying to automatically recover from., Hi [~szetszwo], [~cmccabe], Thanks for the comments. It would be great if we can reach to an agreement about the way to verify and handle the mismatched blocks.

As an initial attempt, I'm attaching a patch where I tried to compare the directory paths and logging WARN message. Later based on the discussion in the jira will do the necessary changes. Please review the approach when you get some time., I think the current patch is fine, except I do not think we should be creating a giant list of all the blocks that are mismatched.  Creating giant lists in the directory scanner could potentially cause an out of memory condition., > None of those cases seems like something we should be trying to automatically recover from.

If we do not recover automatically, we need to answer the questions: should the block report include those blocks? How to access those blocks? How and when to fix those blocks?

> As an initial attempt, I'm attaching a patch where I tried to compare the directory paths and logging WARN message. ...

I am fine even if we do it in two JIRAs., [~szetszwo]Nicholas
bq.I am fine even if we do it in two JIRAs.
+1 for this approach.

bq.except I do not think we should be creating a giant list of all the blocks that are mismatched
Thats good point. How about having a list of fixed size, say 100. While verification if {{mismatchFiles}} list reaches this limit, will print all the blocks and make it empty. ok?. I'd like to avoid too many logs which may noise. , Attach new patch where it will print the log message and clear the list, if the {{mismatchFiles}} reaches 100. Please review. Thanks!, Thank you for sticking with this.

bq. Attach new patch where it will print the log message and clear the list, if the mismatchFiles reaches 100. Please review. Thanks!

Why don't you just log them as you go?  I really see no reason for a list and it adds complexity.

Also, please give different patch files different names, to avoid confusion.

thanks., bq. Why don't you just log them as you go? I really see no reason for a list and it adds complexity

yes,  I agree with you and that makes it simple. I thought of avoiding too many logs which may noise. Any comments?
, If there are a bunch of files in the wrong location, we want that "noise." , [~cmccabe], in case that you missed [my questions|https://issues.apache.org/jira/browse/HDFS-7648?focusedCommentId=14315307&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14315307].  Any thought?, Hi [~szetszwo], since you suggested splitting this JIRA into two, I had assumed that you wanted to have the discussion about "automatic fixing" on the second JIRA.  However if you want to have it now, I'll share my thoughts.

As I stated earlier, I don't think we should do automatic fixing.  We simply don't know *why* the DataNode got into a state where the directory layout is wrong.  This is similar to "what happens if there is no VERSION file?"  We don't try to automatically fix this.  If there is no VERSION file, then it's very likely that there is a serious misconfiguration and/or filesystem bug, and our attempts to fix it would only make things worse.

The same logic applies here.  If there are blocks in the wrong location, why is that happening?  It could be because there is a serious bug in the software.  In that case, deleting the blocks, as you have suggested, would only lead to data loss.  It could be because the sysadmin manually edited a {{VERSION}} file for an old (pre HDFS-6482) datanode directory to look like it was post-HDFS-6482, bypassing the upgrade process.  In this case, deleting *all* the data is still the wrong thing to do... the sysadmin should instead see logs telling him that this configuration is wrong.  Finally, blocks could be in the wrong place because there is a serious disk drive or local FS error.  In this case, deletion will still do no good, because the device is in a seriously unusable state.

I'd also like to note that we've spent quite a lot of time discussing theoretical failures that may or may not ever happen.  Who knows whether we actually will ever find blocks in the wrong place?  You are asking for automatic handling of something that, to our knowledge, has never even happened once.  That seems like putting the cart before the horse., [~cmccabe], I means these questions: should the block report include those blocks? How to access those blocks? How and when to fix those blocks?, bq. should the block report include those blocks

yes.  I do not think we should take any special effort to exclude these blocks.

bq. How to access those blocks

They can't be accessed, because they are in the wrong position

bq. How and when to fix those blocks?

They should be fixed by the sysadmin, based on what the actual problem is (which the software doesn't know)., I see Colin that point that before we understand the problem, our system should not be too smart fixing it.  However, after we know the cause of the problem (say, the admin moved some blocks manually), we need some way to fix those misplaced blocks.  How about adding a conf to enable/disable the auto-fix feature and the default is disabled?, bq. If there are a bunch of files in the wrong location, we want that "noise."
Attached new patch where it logs warning message for each mismatched block. Please review the patch.

bq.Also, please give different patch files different names, to avoid confusion.
Thank you for this comment. I'll take care now onwards. I've named new patch HDFS-*3*.patch as there were two more patches before this.

bq. since you suggested splitting this JIRA into two, I had assumed that you wanted to have the discussion about "automatic fixing" on the second JIRA
Shall I create a new jira and do a detailed brainstorming there., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12698625/HDFS-7648-3.patch
  against trunk revision b0d81e0.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.TestFSImageWithSnapshot
                  org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner
                  org.apache.hadoop.hdfs.server.namenode.TestFileContextXAttr

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestFsck

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9570//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9570//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12699031/HDFS-7648-4.patch
  against trunk revision ab0b958.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9588//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9588//console

This message is automatically generated., [~cmccabe] kindly review the patch. Thanks!
I could see existing test case is covering the newly added log statement, so not included any specific tests., bq. I see Colin that point that before we understand the problem, our system should not be too smart fixing it. However, after we know the cause of the problem (say, the admin moved some blocks manually), we need some way to fix those misplaced blocks. How about adding a conf to enable/disable the auto-fix feature and the default is disabled?

I wouldn't object to a configuration like that, but I also question whether it is needed.  Has this ever actually happened?  And if it did happen, isn't the answer more likely to be "stop editing the VERSION file manually, silly" or "your ext4 filesystem is bad and needs to be completely reformatted" rather than "DN should cleverly fix"?

bq. Colin Patrick McCabe kindly review the patch. Thanks!

We should be logging in the {{compileReport}} function, not a new function.  We can check whether the location is correct around the same place we're checking the file name, etc., Attached new patch where it checks the block-ID layout inside {{compileReport}} function. Please have a look., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12699451/HDFS-7648-5.patch
  against trunk revision 3f56a4c.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistFiles

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9607//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9607//console

This message is automatically generated., > ... Has this ever actually happened? And if it did happen, isn't the answer more likely to be "stop editing the VERSION file manually, silly" or "your ext4 filesystem is bad and needs to be completely reformatted" rather than "DN should cleverly fix"?

If we wait until it happened, how could we help the users?  The may also be other reasons such as there is a bug in the software, the admin manually moved the block files around, etc., Latest patch looks better.  Can we please not have the same code duplicated in two places, though?  Use a function if needed, or just put it further up in the loop.

bq. If we wait until it happened, how could we help the users? The may also be other reasons such as there is a bug in the software, the admin manually moved the block files around, etc.

We can help the users by pointing out that there is a problem with the block layout.

If there is a bug in the software, we cannot automatically fix the bug (I hope you will agree to this much, at least).  And automatically undoing something that the admin did, even if that was a silly thing, is a bad thing to do., > We can help the users by pointing out that there is a problem with the block layout.

The problem is already known to the users.  Pointing out does not help much.

> If there is a bug in the software, we cannot automatically fix the bug ...

Mmm ... We are not fixing the software bug but fixing the blocks/data.  Also, there is a conf.  So it is manually but not automatically.

I wish there is a way to automatically fix a future software bug.  :), There is a long discussion thread about automatically fixing the unmatched blocks and I feel it will be good to continue the discussion in this jira itself.Will try to reach a common agreement, then will implement the logic accordingly. Thanks [~szetszwo] and [~cmccabe] for the inputs so far. Since its decided to log a warn message of unmatched block and I've raised a separate sub-task to fix this part alone. Kindly review HDFS-7819.
, bq. Mmm ... We are not fixing the software bug but fixing the blocks/data. Also, there is a conf. So it is manually but not automatically.

If you don't fix the bug, isn't this just going to re-occur?  You cannot fix a problem if you don't know what it is.  And the software doesn't know why files would be in the wrong position.  Similarly, the software doesn't know why VERSION might be missing, or a meta file might not be there, or a volume might not be accessible., > If you don't fix the bug, isn't this just going to re-occur? ...

If you keep using the buggy software, yes.  However, we could use a (previous) bug free software.  Also, there are few potiental reasons to cause the problem as you mentioned (e.g. someone manually renamed some block files on ext4, there is serious ext4 filesystem corruption), then we could first fix the cause (e.g. stop the manual steps, fix ext4) and then fix the datanode directory layout.

Anyway, you feel strongly that we should not fix the layout.  Please feel free to -1 on HDFS-7819.

> ... Kindly review HDFS-7819.

Will review it.  Thanks., Hi guys. I ran into trouble because I used https://github.com/killerwhile/volume-balancer with Hadoop 2.6.0 and it messed up my datadirs because that software makes invalid assumptions about what directory moves can it do. Now the DN logs are filled with these:

WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: I/O error while finding block BP-680964103-77.234.46.18-1375882473930:blk_5822441067008155275_0 on volume /data/19/cdfs/dn

What can I do to fix this? I don't know what dirs were moved and from where but is there a reasonable way out of this? Such as editing VERSION file to a previous version when DN is down so that it fixes the layout by itself - would that work?

Please note that I've lost the other replica due to a filesystem error so I can't just ignore it. This is literally my only option to recover some missing blocks.

Thanks, Hi [~dwatzke], this question sounds like it should be asked on the hdfs-user list, not on JIRA, since it is a question about recovering from a specific admin mistake rather than a bug with the code., Hi [~rakesh_r], can you rebase the patch on trunk?

{code}
              LOG.warn("Block: " + blockId
	                  + " has to be upgraded to block ID-based layout");
{code}
Perhaps "Block XYZ is in the wrong directory" would be clearer?

+1 once these are addressed., Thanks [~cmccabe] for making this jira active. Actually we have pushed the logging changes as part of the sub-task HDFS-7819. Sorry for the confusion with the attached patch in this jira. Now, the pending case is about the corrective actions [discussed|https://issues.apache.org/jira/browse/HDFS-7648?focusedCommentId=14299128&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14299128] some time earlier in this jira. But the automatic way of fixing is not decided yet., Would it be possible to extend the WARN message to also contain full path of both the wrong and the correct block directory? If it won't get fixed automatically then why not help admins out that way?, Agree that it should print out the actual and the expected directories.

Please also create a helper method but not duplicating the code., Thanks [~dwatzke], [~szetszwo]
bq. Agree that it should print out the actual and the expected directories.
I think we can utilize this parent jira to continue the discussion of corrective actions. I've raised separate sub-task HDFS-10186 to improve logging part. Please take a look at it.

bq. Please also create a helper method but not duplicating the code.
I failed to understand it, could you please provide few more details., > ... I've raised separate sub-task HDFS-10186 to improve logging part. ...

Why it needs a separated sub-task but not just updating the patch here?

> I failed to understand it, could you please provide few more details.

{code}
+            // Check whether the actual directory location of block file
+            // is block ID-based layout
+            File blockDir = DatanodeUtil.idToBlockDir(bpFinalizedDir, blockId);
+            File actualBlockDir = files[i].getParentFile();
+            if (actualBlockDir.compareTo(blockDir) != 0) {
+              LOG.warn("Block: " + blockId
+                  + " has to be upgraded to block ID-based layout");
+            }
             report.add(new ScanInfo(blockId, null, files[i], vol));
           }
           continue;
@@ -646,6 +655,14 @@ public ScanInfoPerBlockPool call() throws Exception {
             break;
           }
         }
+        // Check whether the actual directory location of block file
+        // is block ID-based layout
+        File blockDir = DatanodeUtil.idToBlockDir(bpFinalizedDir, blockId);
+        File actualBlockDir = blockFile.getParentFile();
+        if (actualBlockDir.compareTo(blockDir) != 0) {
+          LOG.warn("Block: " + blockId
+              + " has to be upgraded to block ID-based layout");
+        }
         report.add(new ScanInfo(blockId, blockFile, metaFile, vol));
{code}
The two chunks of code above are duplicated.  Please add a helper method to avoid the duplication.  Thanks., Thanks [~szetszwo] for the reply.
bq. Why it needs a separated sub-task but not just updating the patch here?
bq. The two chunks of code above are duplicated. Please add a helper method to avoid the duplication. Thanks.
The attached patches in this jira is quite old. Please ignore these patches as the propsed changes has been implemented using HDFS-7819. I have raised this sub-task based on the [discussion|https://issues.apache.org/jira/browse/HDFS-7648?focusedCommentId=14332305&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14332305]. Also, please refer [DirectoryScanner.java#L916|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java#L916] to see the code changes. Initially the discussion was to identify the wrong items and automatically fixing it, but couldn't reach to a conclusion. Since we haven't fully implemented the original idea of fixing the wrong blocks I thought of keeping this jira open(one can refer this jira comments to get the background) and fix smaller parts separately through sub-tasks.

Should I delete the attached old patches from this jira to avoid confusions if any?]