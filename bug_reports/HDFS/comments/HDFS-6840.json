[HDFS-6701 gives the option to randomize the returned datanodes but the default is off.  I'm not sure if defaulting to off is a good thing, given the significantly different load behavior and heavy skew to the one datanode.  If that skew is desired then I think it should be opted-in rather than having to opt-out to avoid the skew., Hey Jason, did you bisect this behavior to HDFS-6268? My impression of the old pseudo-sort was that it was deterministic. AFAIK there wasn't a Random doing a shuffle. The idea of not-shuffling by default was to preserve the old behavior, and in case there were any nice page cache effects from directing reads to the same replica., I think the previous behavior was not deterministic due to this change that was removed in the HDFS-6268 patch:

{code}
    // put a random node at position 0 if it is not a local/local-rack node
    if(tempIndex == 0 && localRackNode == -1 && nodes.length != 0) {
      swap(nodes, 0, r.nextInt(nodes.length));
{code}

The list used to be mostly deterministic, but the first node in the list (i.e.: the one clients are likely to be the only one to use) was random.

I have not done the bisect to prove without a doubt it was HDFS-6268, but we've run builds based on something 2.4.1+ and 2.5 and this behavior is brand-new with 2.5.  There weren't a lot of changes in the topology sorting arena besides this one between 2.4.1 and 2.5.0, and the code and JIRA for HDFS-6268 state it's intentionally not randomizing the datanode list between clients.  Besides the bisect approach I probably can try replacing the network topology class with the one from before HDFS-6268 and see if the behavior reverts to what it used to be., bq.  My impression of the old pseudo-sort was that it was deterministic. AFAIK there wasn't a Random doing a shuffle.

I'm familiar with the block location selection and helped debug this issue.  As Jason points out, the sort result used to be the node-local, rack-local, off-rack, decommed.  If the the first location isn't node/rack local, then it picked a random off-rack.  The rest of the list, yes, was deterministic.

bq.  in case there were any nice page cache effects from directing reads to the same replica.

Getting crushed by thousands of off-rack requests ruins any benefit from the page cache.  At a minimum, the network interface will be saturated at which point choosing another random replica off-rack replica is faster.  Let's say the first node is "bad" because of the load or for other reasons.  All off-rack tasks will be slowed until they fail over to the next node, which again will be the another deterministic node, which may cause it to be considered "bad", repeat.

Even on the same rack, I'm not sure there's a benefit to deterministically picking the same node.  There's generally only 2 replicas per rack.  If a large number of rack-local tasks start up then dividing the load between the two replicas probably has better performance.
, Alright, I'm sufficiently convinced :) Is the fix as simple as switching the param to true? Happy to +1 such a change., Commenting out the setSeed call in NetworkTopology on our 2.5-based build fixes the issue, so I suspect changing the param would work as well in 2.6+.  Given how poorly off-rack load balancing is without the randomization, do we even want to keep the parameter added in HDFS-6701 at all?, Simplest way is to set {{dfs.namenode.randomize-block-locations-per-block}} default to {{true}}., I'd like complete removal of the random seed.  Why allow users the option of "shooting themselves in the foot"?  As far as I can tell, the seed was added due a misunderstanding that former behavior was deterministic?  I cannot envision a use case where all off-rack clients bombarding a single node is a good idea., We believe but haven't proven that this deterministic behavior is causing even more problems.  Block replication and invalidation appear to be impacted.  As in changing the replication factor sometimes takes up to an hour to start, and there's a slow but steady increase in blocks pending deletion on clusters running 2.5.  We believe the NN is repeatedly picking the same faulty DN to issue the copy block and invalidate block., Since we are planning to make a 2.5.1 release, should this jira be made a blocker for 2.5.1 ?
, Here's a patch that removes the "random seed" stuff. Fortunately HDFS-6701 hasn't been released yet, so we don't need to do any config key deprecation.

branch-2 is probably going to need a new patch like the others. This might be a good time to try and figure out why we have those extra test files., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664852/hdfs-6840.001.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7807//console

This message is automatically generated., Thanks [~andrew.wang], the patch looks good to me.  Just one thing, we need also remove the configuration from hdfs-default.xml
{code}
<property>
  <name>dfs.namenode.randomize-block-locations-per-block</name>
  <value>false</value>
  <description>When fetching replica locations of a block, the replicas
   are sorted based on network distance. This configuration parameter
   determines whether the replicas at the same network distance are randomly
   shuffled. By default, this is false, such that repeated requests for a block's
   replicas always result in the same order. This potentially improves page cache
   behavior. However, for some network topologies, it is desirable to shuffle this
   order for better load balancing.
  </description>
</property>
{code}, Nice catch Yi, new patch that removes that param from the XML file., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12665081/hdfs-6840.002.patch
  against trunk revision c4c9a78.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7839//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7839//console

This message is automatically generated., Thanks [~andrew.wang], +1 (non-binding), The test failures appear to be unrelated, and they pass for me locally with this patch applied.

Looks good overall, just one nit.  This comment was left in the code and no longer applies:

{code}
    // Seed is normally the block id
    // This means we use the same pseudo-random order for each block, for
    // potentially better page cache usage.
    // Seed is not used if we want to randomize block location for every block
{code}
, In addition to Jason's comment, I'm mildly concerned with the tests assuming and hardcoding the ordering based on seed.  Presumably the jdk could change how the seeding works at anytime which would cause test failures.  Note that a few months ago I saw a jdk bug about how java's randomness isn't very random at all so it's possible the ordering could change in the near future., Sorry for the delay in revving this. New patch removes the stale comment as per Jason's feedback.

Daryn, do you mind if we fix any seed issues in a separate JIRA? I think we depend on this behavior in other places too, so if/when said JDK change does hit, we could address them all at once., Latest patch looks good to me, +1. I agree that we can reasonably move the improvements to the tests to make them deterministic to another JIRA. Andrew, could you please go ahead and file that?, Filed HADOOP-11107 as a follow-on for the random issue, thanks for reviewing ATM., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12669817/hdfs-6840.003.patch
  against trunk revision eb92cc6.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8090//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8090//console

This message is automatically generated., Branch-2 patch attached. Couple conflicts:

* FSNamesystem had the old config key being imported twice, so have to remove it twice
* I removed TestHdfsNetworkTopologyWithNodeGroup. AFAICT, it looks like an old version of TestNetworkTopologyWithNodeGroup. It's Junit3 still, and has fewer tests. I think someone just missed an svn rename or something., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12669878/hdfs-6840.branch-2.patch
  against trunk revision 6434572.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8094//console

This message is automatically generated., +1, branch-2 patch looks good to me.

Thanks, Andrew., Thanks ATM, pushed to branch-2 as well., SUCCESS: Integrated in Hadoop-Yarn-trunk #685 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/685/])
HDFS-6840. Clients are always sent to the same datanode when read is off rack. (wang) (wang: rev 8e73084491c9f317bc8cc3590f93ca67a63687a8)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/net/TestNetworkTopology.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetworkTopologyWithNodeGroup.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1901 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1901/])
HDFS-6840. Clients are always sent to the same datanode when read is off rack. (wang) (wang: rev 8e73084491c9f317bc8cc3590f93ca67a63687a8)
* hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetworkTopologyWithNodeGroup.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/net/TestNetworkTopology.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1876 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1876/])
HDFS-6840. Clients are always sent to the same datanode when read is off rack. (wang) (wang: rev 8e73084491c9f317bc8cc3590f93ca67a63687a8)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/net/TestNetworkTopology.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
* hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestNetworkTopologyWithNodeGroup.java
* hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
]