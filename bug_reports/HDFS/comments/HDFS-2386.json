[fsck command ran and the following stack trace was seen

Exception in thread "main" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure
	at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Alerts.java:174)
	at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Alerts.java:136)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.recvAlert(SSLSocketImpl.java:1774)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:954)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1138)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1165)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1149)
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:434)
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:166)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1172)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:234)
	at org.apache.hadoop.hdfs.tools.DFSck$1.run(DFSck.java:141)
	at org.apache.hadoop.hdfs.tools.DFSck$1.run(DFSck.java:110)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.hdfs.tools.DFSck.run(DFSck.java:110)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.hdfs.tools.DFSck.main(DFSck.java:182), from the namenode logs

2011-09-29 16:03:55,496 WARN org.mortbay.log: EXCEPTION
javax.net.ssl.SSLHandshakeException: Invalid padding
        at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Alerts.java:174)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1699)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:852)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1138)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1165)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1149)
        at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: javax.crypto.BadPaddingException: Padding length invalid: 92
        at com.sun.net.ssl.internal.ssl.CipherBox.removePadding(CipherBox.java:399)
        at com.sun.net.ssl.internal.ssl.CipherBox.decrypt(CipherBox.java:247)
        at com.sun.net.ssl.internal.ssl.InputRecord.decrypt(InputRecord.java:153)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:840), Run a dfs command with the path specified as hftp leads to the following issue

bash-3.2$ /bin/hadoop --config HADOOP_CONF_DIR dfs -ls hftp://NN_HOST:50070/path
11/09/29 16:07:23 INFO fs.FileSystem: Couldn't get a delegation token from https://NN_HOST:50470 using https.
ls: Security enabled but user not authenticated by filter


The namenode logs has the same exception as in the above comment., Seems like fsck not working should be a blocker., Did you remember to set the property:

{quote}
<property>
  <name>dfs.https.enable</name>
  <value>true</value>
</property>
{quote}

It needs to be set in the NameNode's config and *not* the DataNode's config., That property was not present in the namenode however even after adding the property and restarting the namenode same issue still exists., I installed a 6 node cluster and it works for me. This seems to be something cluster/machine specific. I used java version jdk1.6.0_27., Arpit and Jitendra: am I correct in assuming that both of you were testing this on clusters with security enabled? If so, what encryption types did you each use for you keytabs/principals?, Also make sure that your java home has the jce security stuff added to it.

http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html, So far, on three clusters we've tried this on, it worked fine in two.  Only our QA system test cluster experienced the difficulty.  We are continuing to investigate, however, at this time we do not consider it a blocker because it is more likely due to a problem with the cluster setup than with the code.

When we identify the issue we will share it here., And of course if it does turn out to be due to a code problem, will re-raise the "blocker" severity., Matt: That seems reasonable. Thanks for the info.

I also suspect that the description of this JIRA should be changed to "With security enabled, [rest of description]." Do you agree?, Aaron you are correct, description updated., @Aaron
From kerberos debug logs the encryption used between nodes in my cluster seems to be DES_CBC_CRC.
, FWIW, we are actively hitting this issue with the secondary namenode and fsck with the 204.  JDK 1.6.0_29, RHEL 6.1, MIT 1.8.x, AES-256, AES-128, and RC4 enc types are enabled.  JCE is installed.   

We see on the NN side that we throw an invalid_padding error while the 2nd NN and fsck throw the handshake_failure message.  

At this point, I'm leaning towards ripping out the SSL code from the namenode and running at least that portion unsecured., Can you try with DES_CBC_CRC? It is also available in Java 6 by default., bq. At this point, I'm leaning towards ripping out the SSL code from the namenode and running at least that portion unsecured.
Now that we have a SPNEGO filter, we no longer need to use Kerberized SSL. It would be good to remove that entirely.  It was added as a work-around to having no suitable SPNEGO solution and is rather unique to Hadoop (although apparently ActiveMQ took the code as well).  This would save us from having to use the host principal (and instead use the more standard http principal) and simplify a lot of the config., +1 to Jakob's recommendation. In practice, I've found setting up a 2NN to successfully checkpoint to be the most annoying and difficult-to-debug part of configuring secure Hadoop., > Can you try with DES_CBC_CRC?

No, because at that level you might as well send it across the wire unencrypted., >>
we are actively hitting this issue with the secondary namenode and fsck with the 204. JDK 1.6.0_29, RHEL 6.1, MIT 1.8.x, AES-256, AES-128, and RC4 enc types are enabled. JCE is installed.
>>

+1, We are facing this issue as well and get the following exception in NameNode.


11/12/29 18:47:02 WARN mortbay.log: EXCEPTION
javax.net.ssl.SSLHandshakeException: Invalid padding
        at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Alerts.java:174)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1699)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:852)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1138)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1165)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1149)
        at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: javax.crypto.BadPaddingException: Padding length invalid: 238
        at com.sun.net.ssl.internal.ssl.CipherBox.removePadding(CipherBox.java:399)
        at com.sun.net.ssl.internal.ssl.CipherBox.decrypt(CipherBox.java:247)
        at com.sun.net.ssl.internal.ssl.InputRecord.decrypt(InputRecord.java:153)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:840)
        ... 5 more

Pasting the javax.net.debug output from secondary namenode (if this would be of help)

Enabled javax.net.debug=all in secondary namenode and got the following output


Cipher Suite: TLS_KRB5_WITH_3DES_EDE_CBC_SHA
Compression Method: 0
Extension renegotiation_info, renegotiated_connection: <empty>
***
%% Created:  [Session-1, TLS_KRB5_WITH_3DES_EDE_CBC_SHA]
** TLS_KRB5_WITH_3DES_EDE_CBC_SHA
*** ServerHelloDone
*** ClientKeyExchange, Kerberos
...
...
..

*** Finished
verify_data:  { 190, 127, 20, 131, 10, 136, 84, 207, 172, 130, 31, 53 }
***
main, WRITE: TLSv1 Handshake, length = 40
main, READ: TLSv1 Alert, length = 2
main, RECV TLSv1 ALERT:  fatal, handshake_failure
main, called closeSocket()
main, handling exception: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure
11/12/29 18:47:02 ERROR namenode.SecondaryNameNode: checkpoint: Content-Length header is not provided by the namenode when trying to fetch https://NN:50475/getimage?getimage=1
, From testing I've been doing it looks like KSSL won't work without at least one of the DES encryption types enabled (e.g. DES_CBC_CRC). This looks like it's caused by a bug in the JDK. Basically, AES and RC4 don't pad unless they encrypt a message which is not a multiple of a block. However, the JDK is assuming that the PreMasterSecret will be padded and assumes that the last byte in the decrypted secret is the length of the padding. When using AES or RC4, this ends up being a random byte and usually will cause the JDK to end up with an invalid PreMasterSecret. In defense of this, the JDK generates a random secret that then caused the handshake to fail later on. I need to do some more testing with another version of Kerberos, but I plan on filing a JDK bug., We're been running with HDFS-2617 in place of the KSSL support for the past few months rather than deal with these issues., [HDFS-2617] is definitely the right solution for Hadoop. I still plan on filing the JDK bug to make the world a little less broken., Looks like this was already filed and is fixed in JDK 7:

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6946669

I recommend closing this ticket as either "Won't Fix" or "Duplicate" since this issue will be fixed in [HDFS-2617] and I doubt there's anything we can do to make this work on JDK 6., Fixed via HDFS-2617., Duplicate HDFS-2617 addresses this.]