[This bug has been there since 2.1.0-beta. It involves two client threads creating and deleting the same file., Is it possible to avoid having fsdir up-call to the fsn, apparently to clear leases, but rather fsdir removes the inodes from the map and fsn clears the leases?, Instead of moving inode removal inside lock, we could have getAdditionalBlock() to do additional check after acquiring the FSN write lock.  One possibility is to have it acquire FSDirectory read lock and check the parent of the inode. If deleted, it should be null.  Or we could make checkLease() do more than just checking the clientName recorded in the inode., The new patch simply checks the parent of inode against null.  This is done in checkLease(), which is called by getAdditionalBlock() after acquring the FSN writelock.

Also added is a new test case that reproduces the race between delete() and getAdditionalBlock(). Without the change in checkLease(), the test case fails. Its failure means getAdditionalBlock() was successful even after delete(). This causes the problematic edit log sequence., Thanks for the fix, [~kihwal]! 

bq. The new patch simply checks the parent of inode against null
Currently if the inode is in a snapshot then its parent will not be set to null after deletion. In that case can we run into a scenario where a block is added to a deleted file that is in our read-only snapshot? Maybe we also need to check FileWithSnapshotFeature#isCurrentFileDeleted?, Thanks for the comment, [~jingzhao]. We can null out the client name while deleting files. Then lease check is guaranteed to fail.

In {{INodeFile#destroyAndCollectBlocks()}}, we can delete the client name.
{code}
     if (sf != null) {
       sf.clearDiffs();
     }
+
+    // Delete client name if under construction. This destroys a half of
+    // the lease. The other half will be removed later from LeaseManager.
+    FileUnderConstructionFeature uc = getFileUnderConstructionFeature();
+    if (uc != null) {
+      uc.setClientName(null);
+    }
   }
{code}

And in {{FSNamesystem#checkLease()}}, we can have the following check instead of the parent == null check.
{code}
     String clientName = file.getFileUnderConstructionFeature().getClientName();
+    if (clientName == null) {
+      // clientName is removed when the file is deleted.
+      throw new FileNotFoundException(src);
+    }
{code}

This will make lease checks to fail once the "real" file is deleted, whether it is in a snapshot or not.  Do you think it is reasonable?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650347/HDFS-6527.v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

      {color:red}-1 javac{color}.  The applied patch generated 1260 javac compiler warnings (more than the trunk's current 1259 warnings).

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7115//testReport/
Javac warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/7115//artifact/trunk/patchprocess/diffJavacWarnings.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7115//console

This message is automatically generated., The new v3 patch implements what I suggested above. It nulls out the client name field.  Any further client actions against the file will be rejected.  Also fixed the javac warning caused by the use of the deprecated delete() method in the new test case., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650386/HDFS-6527.v3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7118//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7118//console

This message is automatically generated., The v3 may not work when the file is contained in a snapshot. The new unit test can fail if we create a snapshot on root after the file creation:
{code}
      FSDataOutputStream out = null;
      out = fs.create(filePath);
      SnapshotTestHelper.createSnapshot(fs, new Path("/"), "s1");
      Thread deleteThread = new DeleteThread(fs, filePath, true);
{code}

Instead of the changes made in v3 patch, I guess the v2 patch may work with the following change:
{code}
@@ -3018,6 +3036,13 @@ private INodeFile checkLease(String src, String holder, INode inode,
           + (lease != null ? lease.toString()
               : "Holder " + holder + " does not have any open files."));
     }
+    // If parent is not there or we mark the file as deleted in its snapshot
+    // feature, it must have been deleted.
+    if (file.getParent() == null
+        || (file.isWithSnapshot() && file.getFileWithSnapshotFeature()
+            .isCurrentFileDeleted())) {
+      throw new FileNotFoundException(src);
+    }
     String clientName = file.getFileUnderConstructionFeature().getClientName();
     if (holder != null && !clientName.equals(holder)) {
       throw new LeaseExpiredException("Lease mismatch on " + ident +
{code}, Besides, is it possible that we can hide the sleepForTesting part inside a customized BlockPlacementPolicy? The customized BlockPlacementPolicy implementation uses the same policy as BlockPlacementPolicyDefault, but make the thread sleep 1s before returning. In this way we do not need to inject testing code into FSNamesystem., The v4 patch does what you suggested.  Regarding the test code in {{FSNamesystem}}, {{delete()}} also needs a delay. We already have fault injection in various critical parts of the system., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650582/HDFS-6527.v4.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7133//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7133//console

This message is automatically generated., Thanks Kihwal! The v4 patch looks good to me. But I guess the unit test now cannot cover the non-snapshot case since the inode will not be removed from the inodemap if it is still contained in a snapshot.

So based on your v4 patch I added a new unit test to cover both scenarios. Also I use a customized block placement policy and use whitebox to add the deleted inode back to the inodemap so as to remove the dependency of the fault injection code., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12650667/HDFS-6527.v5.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7141//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7141//console

This message is automatically generated., +1 for the test improvement., Thanks for reviewing the test change, [~kihwal]! I will commit this late today if no further comments., Changing the target version from 2.4.1 to 2.5.0 since 2.4.1 is already cut., FAILURE: Integrated in Hadoop-trunk-Commit #5720 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5720/])
HDFS-6527. Edit log corruption due to defered INode removal. Contributed by Kihwal Lee and Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1603340)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
, Thanks for the fix, [~kihwal]! I've committed this to trunk and branch-2., FAILURE: Integrated in Hadoop-Yarn-trunk #587 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/587/])
HDFS-6527. Edit log corruption due to defered INode removal. Contributed by Kihwal Lee and Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1603340)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1778 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1778/])
HDFS-6527. Edit log corruption due to defered INode removal. Contributed by Kihwal Lee and Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1603340)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1805 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1805/])
HDFS-6527. Edit log corruption due to defered INode removal. Contributed by Kihwal Lee and Jing Zhao. (jing9: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1603340)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeleteRace.java
, Changed fix version to 2.4.1 since it got merged in for rc1., When running unit tests in this patch v5, I get the following errors

2014-06-23 13:36:09,516 ERROR hdfs.DFSClient (DFSClient.java:closeAllFilesBeingWritten(873)) - Failed to close file /testDeleteAddBlockRace
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /testDeleteAddBlockRace: File does not exist. Holder DFSClient_NONMAPREDUCE_1652233532_1 does not have any open files.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2941)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2762)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2706)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:585)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:394)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2008)

        at org.apache.hadoop.ipc.Client.call(Client.java:1410)
        at org.apache.hadoop.ipc.Client.call(Client.java:1363)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:188)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
        at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:361)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1443)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1265)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:529)
 , Thanks for the report, [~l201514]! Actually the unit test failure can be reproduced in branch-2.4.1.

I think the patch should not be included in branch-2.4.1. In branch-2.4.1, we have the following code in the beginning of analyzeFileState:
{code}
final INodesInPath iip = dir.getINodesInPath4Write(src);
final INodeFile pendingFile
    = checkLease(src, fileId, clientName, iip.getLastINode());
{code}
I.e., the inode is retrieved by resolving the path. Since we remove the inode from the children list while holding the fsnamesystem lock, the race condition can actually be avoided I guess.

HDFS-6294 changed this part to load the INode from the inode map, while in FSNamesystem#deleteInternal, the inode is removed from the inode map without holding fsnamesystem lock. I guess this is the cause of the issue, and the issue is only in 2.5 and trunk., Reopen the jira and wait for [~kihwal] to comment., [~jingzhao] You are right. Since it reresolves inside the write lock, it will detect the deletion.  I will revert it from 2.4.1., Reverted it from branch-2.4.1 and also updated the release note., Hey folks,

I've been looking into this a bit and have come to the conclusion that we should actually include this fix in 2.4.1. The reason is that though the original {{addBlock}} scenario sort of incidentally can't happen in 2.4.0, I believe that a similar scenario can happen with a race between {{close}} and {{delete}}.

Even though {{close}} doesn't do any sort of dropping of its lock during the duration of its RPC, the entirety of a single {{close}} operation can begin and end successfully between when the {{delete}} edit log op is logged, and when the INode is later removed in the {{delete}} call. See the attached additional test case which demonstrates the issue.

This will result in a similarly invalid edit log op sequence wherein you'll see an {{OP_ADD}}, {{OP_DELETE}}, and then {{OP_CLOSE}}, which can't be successfully replayed by the NN since the {{OP_CLOSE}} will get a {{FileNotFound}}. I've seen this happen on two clusters now.

Kihwal/Jing - if you agree with my analysis, let's reopen this JIRA so this fix can be included in 2.4.1, though without the {{addBlock}} test case, and with only the {{close}} test case., Thanks [~atm]! The analysis makes sense to me. Let's reopen the jira and fix it in 2.4.1.

In the meanwhile, in FSNamesystem#delete, maybe we should move "dir.removeFromInodeMap(removedINodes)" into the fsnamesystem write lock? I guess this will prevent similar issue in the future., bq. In the meanwhile, in FSNamesystem#delete, maybe we should move "dir.removeFromInodeMap(removedINodes)" into the fsnamesystem write lock? I guess this will prevent similar issue in the future.

This currently is done under the write lock, but it's done after having dropped the write lock briefly, so presumably you're proposing to make all of the contents of {{deleteInternal}} happen under a single write lock? That seems reasonable to me, but to be honest I'm not sure of the history of why this deferred inode removal was done in the first place., Yeah, let's merge the fix back to 2.4.1 with unit test changes first. We can revisit the deferred removal in a separate jira., [~atm], do you already have a updated patch for 2.4.1? Otherwise I will try it., Hi Jing, I haven't currently create that patch, no. Please do feel free to go ahead and do that. Thanks a lot., Copied from release vote mailing thread:

{code}
That's fine by me. Like I said, assuming that rc1 does indeed include the fix in HDFS-6527, and not the revert, then rc1 should be functionally correct. What's in branch-2.4.1 doesn't currently match what's in this RC, but if that doesn't bother anyone else then I won't lose any sleep over it.
--
Aaron T. Myers
Software Engineer, Cloudera

> On Jun 27, 2014, at 3:04 PM, "Arun C. Murthy" <acm@hortonworks.com> wrote:
>
> Aaron,
>
> Since the amend was just to the test, I'll keep this RC as-is.
>
> I'll also comment on jira.
>
> thanks,
> Arun
{code}

In this way, I plan not to change branch-2.4.1 and leave it as it is. What do you think [~atm] and [~acmurthy]?, I'd say we should still probably do it, but put it only in branch-2.4, not branch-2.4.1, in case we end up later doing a 2.4.2 release., Thanks for the quick reply, [~atm]. I will merge it to branch-2.4 then., Actually the patch has already been merged into 2.4. I will leave it there as it is now.]