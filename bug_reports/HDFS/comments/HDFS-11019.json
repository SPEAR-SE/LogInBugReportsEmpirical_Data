[[~kshukla] has debugged and worked on this quite a bit. She might have an idea., [~jojochuang] Thank you for reporting this.

In Hadoop 2.6(CDH 5.7.2) The attached test shows the same behavior as mentioned above:
{code}
 INFO  BlockStateChange (CorruptReplicasMap.java:addToCorruptReplicasMap(76)) - BLOCK NameSystem.addToCorruptReplicasMap: blk_12345 added as corrupt on 127.0.0.1:12345 by null because TEST
1. corruptReplicaMap=[127.0.0.1:12345]
2. corruptReplicaMap=null
 INFO  BlockStateChange (CorruptReplicasMap.java:addToCorruptReplicasMap(76)) - BLOCK NameSystem.addToCorruptReplicasMap: blk_12345 added as corrupt on 127.0.0.1:12345 by null because TEST
3. corruptReplicaMap=[127.0.0.1:12345]  //should be null
4. corruptReplicaMap=[127.0.0.1:12345]  //should be null
{code}

This behavior is fixed thru HDFS-9958 and if you run the same test it has the following output .
{code}
1. corruptReplicaMap=[127.0.0.1:63829]
2. corruptReplicaMap=null
3. corruptReplicaMap=null
4. corruptReplicaMap=null
{code}
The code change is in BlockManager#findAndMarkBlockAsCorrupt in 2.7.3 and up releases.
{code}
    if (storage == null) {
      storage = storedBlock.findStorageInfo(node);
    }

    if (storage == null) {
      blockLog.debug("BLOCK* findAndMarkBlockAsCorrupt: {} not found on {}",
          blk, dn);
      return;
    }
{code}

Hope this helps., Thanks [~kshukla] for that information! It does look like HDFS-9958 is in play here. Looks like you fixed two bugs with one patch! Unfortunately we have it backported in CDH5.7.4 but not in CDH5.7.2., I am pretty sure this is a dup of HDFS-9958 . Thanks [~kshukla] for confirming this!]