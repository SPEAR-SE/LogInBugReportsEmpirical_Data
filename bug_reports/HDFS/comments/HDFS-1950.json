[-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480115/HDFS-1950-2.patch
  against trunk revision 1126312.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 3 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/612//console

This message is automatically generated., This patch applies for 0.20 append branch., Cleaned up and updated the test case for trunk to make sure this bug does not exist there. Luckily it passes :) I'd like to commit these tests to trunk anyway - good coverage.

I also verified the failures on the append branch, so will review the code changes there., same patch with shorter test method names, Hi Ramkrishna. Could you please explain in some detail the changes in FSNamesystem? It seems like very messy code -- eg passing a Boolean[] around in order to add "return arguments", etc? Can this be done more cleanly? Having code like that in FSN scares me., cleaned up tests on 0.20-append, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12481633/hdfs-1950-0.20-append-tests.txt
  against trunk revision 1132779.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 5 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/731//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12481631/hdfs-1950-trunk-test.txt
  against trunk revision 1132779.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestHDFSCLI

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/729//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/729//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/729//console

This message is automatically generated., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12481632/hdfs-1950-trunk-test.txt
  against trunk revision 1132779.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 2 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestHDFSCLI

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/730//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/730//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/730//console

This message is automatically generated., In my cluster also, i have seen this problem.
Changing it to blocker., Hi Todd,

I have updated the patch for review!

HDFS-1950.1.patch: I merged your re-factored tests as well. 

Can you please have a look on it?
, -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12490392/HDFS-1950.1.patch
  against trunk revision 1157232.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/1103//console

This message is automatically generated., This patch is for 20Append!, Patch (HDFS-1950.1.patch) ready for review!, Hi Todd,
Can you please take a look on this?

--thanks, Hi,

 Fix is basically for two scenarios.
  
 1) DFSClient side changes is basically to ensure reading the partial block.

 2) Problem here is, DFClient will go for next fetch based on the blocks size.

Consider a corner boundary case (take prefetch size as 10 *blocksize), number of blocks are exactly same as 10.5 or 20.5 ...etc can create problem, because clinet will not even bother for next fetch because initially he will not know the size of that partial block. He will know only that 10blocks size. 

To fix this problem, we have introduce on check in FSNameSystem.

{code}
     ......
     ......
     LocatedBlocks createLocatedBlocks = inode.createLocatedBlocks(results);

     createLocatedBlocksForThePartialBlock(inode, blocks, curPos,
			createLocatedBlocks);

     return createLocatedBlocks;
  }


   private void createLocatedBlocksForThePartialBlock(INodeFile inode,
		Block[] blocks, long curPos, LocatedBlocks createLocatedBlocks) {
	int curBlk;
	if(blocks.length > PRE_FETCH_SIZE &&
			blocks.length % PRE_FETCH_SIZE == 1 && createLocatedBlocks.getFileLength() == curPos)
    {

   .........
   ........
{code}

When blocks are exactly 10.5, then FSNameSystem will populate the 0.5th block id also. So, that client anyway will update the partial block size. Client can take care of reading this boundary partial block.

This patch is basically for review. Here one more problem i wanted to raise is that, prefetch size is configured.
In this patch i put that value as 10.

{code}
 prefetchSize = conf.getLong("dfs.read.prefetch.size", prefetchSize);
 {code}
I am planning to include this property in server namenode as well, because of above reason. or Do yu have any other suggestion.

Once the patch is approved i will prepare it for 20Append and 205 branches with the provided suggestions.


Thanks
Uma

, Would be nice if the config parameter wasn't another string embedded in the src. We have  too many of those. In an ideal world there'd be an Interface "DfsConfigOptions" with every string listed and javadoc'd. Even if it doesn't exist over everything, there's no reason to continue existing bad habits., Thanks Steve for taking a look.

This confuguration not yet included in patch. Once config parameter is finalized for NN side, i will include it in DFSConfigKeys.

@Jitendra, 
  Can you please take a look? testWhenSizeHasPartialBlockwithMultiplesOf10 will give you the answer for change in FSNameSystem.java. But here it required prefetch size in FSNameSystem as well. That is presently client side configuration. Do you have any suggestion for handling this conguration at server side?


Thanks
Uma, moved target version to 1.2.0 upon publishing 1.1.1 RC., Changed Target Version to 1.3.0 upon release of 1.2.0. Please change to 1.2.1 if you intend to submit a fix for branch-1.2., Could we proceed to add the test to branch-2 and trunk (if not already done by Todd) and close this out?, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12490392/HDFS-1950.1.patch
  against trunk revision a16bfff.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/10053//console

This message is automatically generated., Considering this issue has been unresolved for 5+ years, I'm downgrading it from a blocker and removing the branch-1 target version since that release series is EOL.

If this issue really is important, please re-prioritize and re-add appropriate target versions for tracking. Thanks all., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HDFS-1950 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDFS-1950 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12490392/HDFS-1950.1.patch |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17192/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

]