[The stack trace doesn't seem to match with the source http://svn.apache.org/viewvc/hadoop/common/tags/release-0.20.203.0/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java?view=markup, is the hadoop version correct?, Seems the code is in branch-20-append branch - http://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.20-append/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java .  Can somebody confirm whether it is fixed in later branches?, I think this still persists in 1.x -- it looks like it could happen with the following race:

- client gets block locations for an over-replicated block, where the NN has enqueued a command to one DN to delete its extra replica
- the command reaches the DN and it deletes the block
- the client issues primary.getBlockInfo(...) but the block doesn't exist, so it returns null
- client gets NPE

This might also happen if the block's genstamp is changing (perhaps when reading a file under construction where the pipeline has just been recovered?), Thanks Todd for the inputs. Will look into DN logs and update the issue.

Elaborating more on the usecase : The file is getting flushed every second on the producer. There is consumer tailing the file every 5 seconds. Below is the code snippet of the consumer:
{noformat}

{ 
  FSDataInputStream inStream;
  BufferedReader reader;
  long currentOffset = 0;

  while (true) {
    if (reader != null) reader.close();
    if (inStream != null) inStream.close();

    inStream = fs.open(mypath);
    reader = new BufferedReader(new InputStreamReader(in));
    inStream.seek(currentOffset);


    while (true) {
      line = reader.readLine();

      if (line == null) {
        Thread.sleep(5000);
        break;
      } else {
        currentOffset = inStream.getPos();
        .....
      }
    }
  }
}
{noformat}

We are hitting NPE in our production cluster atmost once every one hour, where the file produced is of size 100-150MB per minute. File could be opened properly after retry.

We are also hitting the following Exception more often than NPE (5 times in one hour):
{noformat}
java.io.IOException: Cannot seek after EOF
        at
org.apache.hadoop.hdfs.DFSClient$DFSInputStream.seek(DFSClient.java:2149)
        at
org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:37)
{noformat}

Seeking also works properly after retry. 

The second exception looks like another race. If they are not related, I can raise another issue for the second exception.
, I see that there is a delete block command for one the blocks.
Attaching the logs of Namenode and datanode, I see new generationtimestamp for the block, which got Cannot seek after EOF exception.

Attaching both nn and dn logs for the same., For all the above cases dfs.block.size is 64MB., Todd is right, looks like the DFSClient gets the first node from the list of available locations

 DatanodeInfo primaryNode = last.getLocations()[0];

then it would work on the primaryNode, but there is a chance that the block might have been deleted from the node due to over replication as already mentioned by Todd and also confirmed due to logs. This can happen only when the file is still under construction not for already present files. One way to fix would be to loop through all the nodes to get the correct block. This code seems to be changed in 3.0, Please give your inputs on how to proceed to fix this issue., I am running into the same issue. My current workaround is to retry opening the file, after a retry it usually works. Sometimes i have to retry multiple times (up to 4 was the maximum so far.)

{code:title=Pseudocode.java|borderStyle=solid}
void Open(Path path) {
  DFSDataInputStream in;
  try {
     in = (DFSClient.DFSDataInputStream)mFilesystem.open(path);
  }
  catch (NullPointerException e) {
    // wait for a while
    Thread.sleep(1000);
    // call recursively
    Open(path);
    return;
  }
  // ...
}
{code}

My application is very similar to the one described by Amareshwari; i have a producer and a consumer, both are usually on different datanodes. Every 5 seconds the consumer tries to continue reading the new data.

My question: is this a viable workaround? Do i need the sleep()? Is there a better/faster way to refresh the node list?, A note for everyone else who runs into this issue: the Thread.sleep(1000) is necessary.]