[I attach a patch file which I added the processing about a block deleting., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12660983/HDFS-6833.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7609//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12661147/HDFS-6833.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7613//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7613//console

This message is automatically generated., I attach a patch file which I added a test case., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12661423/HDFS-6833.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7624//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7624//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12661465/HDFS-6833.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7626//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7626//console

This message is automatically generated., I understood that former patch file caused memory leak in the case of directoryScanner = null.
Therefore I attach a patch file which works in directoryScanner = null., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12661656/HDFS-6833.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings
org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA
org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions
org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics
org.apache.hadoop.hdfs.server.datanode.TestFsDatasetCache
org.apache.hadoop.hdfs.TestHDFSServerPorts

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7634//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7634//console

This message is automatically generated., Test org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover succeeded in my environment., HI Shinichi,

Thanks for finding this issue and the patch work.

The reason for "blocks is missing in memory" is that the block is already removed from the memory map, and deletion of the physical block is to be done by FsDatasetAsyncDiskService, which is asynchrous operation. Though the DirecotryScanner is only scheduled to run every 6 hours (by default), the FsDatasetAsyncDiskService's block deletion could be so delayed that DirectoryScanner can see some blocks already removed from memory but still exist on disk. I worked out patch that can possibly helps the slowness of the disk removal, see HDFS-6788. However, I think this jira should help from a different perspective.

Overall the latest patch looks good to me. I have some comments here:

0. suggest to have a version number when you upload new patch.

1. suggest to change {{isDirectoryScanner()}} to {{isDirecotryScannerInited}}.

2, using List<Long> for deletingBlocks may not be efficient since you do search in 
{{public void removeDeletedBlocks(String bpid, List<Long> blockIds)}} which means sequential search. You might consider using HashSet.

3. DirectoryScanner.java. Not related to your change, but I saw it when looking at your change:
{code}
while (m < memReport.length && d < blockpoolReport.length) {
  Block memBlock = memReport[Math.min(m, memReport.length - 1)];
  ScanInfo info = blockpoolReport[Math.min(
      d, blockpoolReport.length - 1)];
{code}

Math.min(m, memReport.length - 1) is guaranteed to be m and 
Math.min(d, blockpoolReport.length - 1) is guaranteed to be d,
the code can be simplified to not call Math.min.

4. DirecotryScanner.java
{code}
while (d < blockpoolReport.length) {
  if (!dataset.isDeletingBlock(bpid, blockpoolReport[d].getBlockId())) {
    statsRecord.missingMemoryBlocks++;
    addDifference(diffRecord, statsRecord, blockpoolReport[d++]);
  } else {
    deletingBlockIds.add(blockpoolReport[d].getBlockId());
    d++;
  }
}
{code}
the "d++" logic can be extracted out to be shared by both branches.

Thanks.
, Hi [~yzhangal],

Thank you for your review and comments.
I attach a renew patch which reflected your comments., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662084/HDFS-6833-6.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints
org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics
org.apache.hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA
org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions
org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA
org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings
org.apache.hadoop.hdfs.TestHDFSServerPorts

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7646//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7646//console

This message is automatically generated., The following tests succeeded in my environment.

{quote}
org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer
{quote}, Hi [~sinchii], thanks for addressing my earlier comments. 

A second thought. WIth the patch you worked out, the block is added to deletingBlock right after it is removed from volumeMap, why we don't remove the block from the deletingBlock right after FsDatasetAsyncDiskService remove it from disk?. Another thing is, more items are kept being added to the deletingBlockId, but never cleaned up.

If we remove the block from the deletingBlock whenever FsDatasetAsyncDiskService removes a block from disk, then there is no need for deletingBlockId data structure, right?

Thanks.


, Hi [~yzhangal],

Thank you for your second comments.

With the former patch, we can handle deletion block information safely.
When we removed deletion block information immediately, I thought that a file information might not match memory information by slow processing (e.g. DirectoryScanner.getDiskReport()).

But, I have a new idea.
I change it not to remove deltingblock information during DirectoryScanner.scan().
It keeps the deleting block information in ReplicaFileDeleteTask during DirectoryScanner.scan(). 
And DirectoryScanner deletes it after scan.
In addition, I add stats about deleting block.

I attach a new patch file about these changes.
, It seems that Jenkins didn't report test result. retry..., HI [~sinchii],

Thanks for the work to address my comments. Originally I thought we could maintain deleteBlock in a way that it always just hold the blocks to be deleted and not yet deleted from disk (add a block to it when deciding to delete the block; and remove the block from it when deleted from disk). After further thought, I think your version 006 is reasonable. Because even though the block is not removed from the deleteBlock right after it's deleted from disk, the change you did in DirectoryScanner can identify the block and remove it from deleteBlock, in asynchronous fashion.

I briefly looked at your version 7, I suspect there may be synchronization issue in the change made there. Version 6 looks fine to me. If you agree, I think we can get a second eye to look at version 6.

Thanks. 
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12662664/HDFS-6833-7-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS
                  org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7680//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7680//console

This message is automatically generated., Hi [~yzhangal],

Thank you for your comment. I think that I push forward ver.6 patch at first. And I think that we should improve ver.7 patch by a different JIRA ticket.
I found ConcurrentModificationException occurring with the ver.6 patch. So, I attach the ver.6 renew patch file., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12663093/HDFS-6833-6-2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
                  org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestDatanodeBlockScanner

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7693//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7693//console

This message is automatically generated., HI [~sinchii],

Good catch of the synchronization issue. I have a few minor comments.

- Did you figure out exactly what contributed to the ConcurrentModificationException? I saw you changed the incorrect remove-while-iterate code from previous revision.
- change {{", deleting blocks:" + deletingBlocks;}} to {{", to-be-deleted blocks: " + deletingBlocks;}}
- similarly, change "deleting" in {{ LOG.info("Block file " + blockpoolReport[d].getBlockFile() + " is deleting");}} to {{is to be deleted}}
- move {{statsRecord.deletingBlocks++;}} to before {{deletingBlockIds.add(info.getBlockId());}} and after {{LOG.info(...);}} in the else block, so to be consistent with the if branch code.

Thanks.
, Hi [~yzhangal],

Thank you for your further comments. And I'm sorry for the late reply. 
I attatch the renew patch file.

bq. ConcurrentModificationException

When I tested the patch of the previous version, I confirmed that the following messages were output. And I changed to iterator-while-remove code.

{code}
2014-08-11 21:28:01,795 ERROR org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Exception during DirectoryScanner execution - will continue next cycle
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)
        at java.util.HashMap$ValueIterator.next(HashMap.java:950)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.removeDeletedBlocks(FsDatasetImpl.java:1968)
        at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.scan(DirectoryScanner.java:498)
        at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:404)
        at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:360)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664131/HDFS-6833-6-3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/7746//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7746//console

This message is automatically generated., HI [~sinchii], Thanks for the new revision and the stack trace. Version HDFS-6833-6-3.patch looks good to me. Hope you can get a committer's review soon.
, BTW, I saw why your fix addressed the exception: http://stackoverflow.com/questions/602636/concurrentmodificationexception-and-a-hashmap
{quote}
, Hi [~yzhangal],

Thank you for your cooperation and your support!
BTW, large number of "Missing Blocks" was caused by this problem this week. This problem occurred by normal decommission and recommission operation.
I recognize that this problem is a serious problem and I change the priority from "Major" to "Critical".
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12664131/HDFS-6833-6-3.patch
  against trunk revision 26d3b7e.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8761//console

This message is automatically generated., Hi [~sinchii], would you rebase the patch for the latest trunk? , Refreshing a patch by Shinichi for trunk., [~ajisakaa] Shinichi is busy now, so I rebased a patch instead of him. , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12681898/HDFS-6833.8.patch
  against trunk revision 26d3b7e.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8762//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8762//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8762//console

This message is automatically generated., Hi  Shinichi, thanks for filing the bug and working on it.  Some suggestion:

There is already ReplicaFileDeleteTask in FsDatasetAsyncDiskService.  How about reusing it?  In more details,
- in deleteAsync(..), add deletionTask to a (new) map;
- add an API FsDatasetAsyncDiskService for looking up deleting tasks and then DirectoryScanner uses it for checking if a replica file is being deleted;
- in deletionTask.run(), remove itself from the deletion task map once the replica file have been deleted/moved., > For example, when we execute recommission or change the number of replication, NameNode may delete the right block as "ExcessReplicate" by this problem. And "Under-Replicated Blocks" and "Missing Blocks" occur.

Missing blocks is a serious problem.  Do you really see this bug causing missing blocks (for replication >= 3)?  I beg it is very very unlikely so that we won't see it happening., Thanks Nicholas for commenting!

Hi [~sinchii], would you please share what replication setting is in your cluster when the problem happened? thanks.
, I talked with Shinichi offline about this. Though he got "Missing Blocks" for the file with dfs.replication=2, it is possible even if replication is >= 3 in the scenario as

- 1. There is asynchronous block deletion by excess replicas.
- 2. to-be-deleted block is reported as live block due to DataBlockScanner's scan.
- 3. NameNode thinks the block is still excess and invokes another block invalidation.
- 4. Scan while asynchronous deletion (same to 1 and 2) happens again on another datanode.
- 5. NameNode invokes another block invalidation.
, Hi all, thank you for more comments!

>[~szetszwo]
>There is already ReplicaFileDeleteTask in FsDatasetAsyncDiskService. How about reusing it? 

Thank you for your advice!
I thought about your plan first, too. 
However, it may cause a problem because synchronization of fsdataset is insufficient in FsDataSetImpl.invalidate() and DirectoryScanner.scan().
Therefore I adopted a current patch (HDFS-6833-6-3.patch) to be able to delete surely.

>[~ozawa]
Thank you for your work! "HDFS-6833-7-2.patch" may cause ConcurrentModificationException, and latest patch is "HDFS-6833-6-3.patch". 
I'm sorry to confuse you.
I attach the latest patch later.

>[~ajisakaa] , [~yzhangal], [~iwasakims]
Thank you for your comment and follow!, I attach a refresh patch file., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12682168/HDFS-6833.9.patch
  against trunk revision 9dd5d67.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.TestEncryptionZonesWithHA
                  org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner
                  org.apache.hadoop.hdfs.TestEncryptionZones
                  org.apache.hadoop.hdfs.TestParallelUnixDomainRead
                  org.apache.hadoop.hdfs.TestReservedRawPaths
                  org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.TestDatanodeBlockScanner
org.apache.hadoop.hdfs.TestLeaseRecovery2

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8772//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8772//console

This message is automatically generated., Thank you for working on this, Shinichi.  Echoing earlier comments, I'm a bit confused about why {{DirectoryScanner}} has the responsibility to call {{FsDatasetSpi#removeDeletedBlocks}}.  This causes us to delete the block ID from the internal data structure tracking still-to-be-deleted-from-disk blocks.  This part of the code is logically disconnected from the code that actually does the delete syscall, so it has no way to guarantee that the delete has really finished.  It seems there would still be a race condition.  If the next scan triggered before the delete completed, then the scanner wouldn't know that the block is still waiting to be deleted.  (Of course, I'd expect this to be extremely rare given the fact that scan periods are usually quite long, 6 hours by default.)  Moving this logic closer to the actual delete in {{ReplicaFileDeleteTask}} would address this.

I'm curious if you can provide any more details about why this is so easy to reproduce in your environment.  There is no doubt there is a bug here, but from what I can tell, it has been there a long time, and I'd expect it to occur only very rarely.  The scan period is so long (again, 6 hours by default) that I can't see how this can happen very often.  Your comments seem to suggest that you can see this happen regularly, and on multiple DataNodes simultaneously, resulting in data loss.  That would require scanners on independent DataNodes landing in a lock-step schedule with each other.  For a typical 3-replica file, this should be very unlikely.  For a 1 or even a 2-replica file, there is already a much higher risk of data loss due to hardware failure despite this bug.  Is there anything specific to your configuration that could make this more likely?  Have you configured the scan period to something much more frequent?  Are you very rapidly decommissioning and recommissioning nodes?, Also related to the last comment, are you seeing anything in your environment that indicates the async deletions take an unusually long time to complete?, bq. I talked with Shinichi offline about this. Though he got "Missing Blocks" for the file with dfs.replication=2, it is possible even if replication is >= 3 in the scenario as
While the probability of this happening is non-zero, the likelihood of such a scenario is very rare. We should certainly consider this for 2.6.1., >  talked with Shinichi offline about this. Though he got "Missing Blocks" for the file with dfs.replication=2, it is possible even if replication is >= 3 . ...

Yes, it is definitely possible.  The probability of losing a block is always non-zero even without this bug.  For example, all the disks storing the replicas can fail at the same time.  The question is how likely would it happen?, Hi Nicholas and Chris,

Thanks for your good comments!

Let me try to describe my understanding of the current flow, the problem of the current flow, the solution Shinichi worked out, the flaw you guys pointed out, and a proposed change here.

The current flow:

# When a block is to be deleted, it's removed from the in-memory record (volumeMap), and the disk removal is scheduled to happen asynchronously to later, to be done by FsDatasetAsyncDiskService.
# When DirectoryScanner is running ({{DirectoryScan#scan}}), it checks the in-memory blocks (retrieved via {{dataset.getFinalizedBlocks(bpid)}}) against the block on disk {{get via {{DirectoryScanner#getDiskReport()}}. If it sees a block is not in memory but on disk, or if the block is in memory but not on disk, it will remember this in {{DirectoryScanner#diffs}}.
# After {{DirectoryScan#scan} is done. The {{DirectoryScanner#diffs}} is processed via calling {{dataset.checkAndUpdate}} in {{DirectoryScanner#reconcile}}. If a block is not in memory but on disk, the block will be re-added to memory and reported back to NN later. 

The problem of the current flow:

* Though DirectoryScanner is only scheduled to run every 6 hours by default, the asynchonous block removal from disk described could be so delayed that DirectoryScanner would see difference between in-memory record and disk, that a block to be deleted is not in memory but on disk, even though the block is to be deleted. In this case, the later block report sent to NN would say this block as a good block, this is the problem.

The solution that Shinichi worked out:

# {{final ReplicaMap deletingBlock;}} is introduced in FsDatasetImpl to remember the blocks to be deleted. These blocks are recorded to this structure by  {{FsDatasetImpl#invalidateBlock}} when called by {{BPOfferService#processCommandFromActive}}. That is, right after a block is removed from in-memory volumeMap due to a invalidate request from NN, it's recorded in {{FsDatasetImpl#deletingBlock}}.
# When  DirectoryScanner is running ({{DirectoryScan#scan}}), when it sees a block is not in memory but on disk, don't jump to conclusion that the block need to be recorded in  {{DirectoryScanner#diffs}}, instead, check against {{FsDatasetImpl#deletingBlock}} first.
# At the end of each DirectoryScanner run, it will remove from the {{FsDatasetImpl#deletingBlock}} the blocks examined in step 2, by calling {{dataset.removeDeletedBlocks(bpid, deletingBlockIds);}}

The flaw that Chris pointed out:

* In step 3 of the solution, removing from {{FsDatasetImpl#deletingBlock}} should happen after the disk block is removed.

This is a good catch here! Though the chance the blocks don't get removed from disk is slim, it is still possible.

Nicholas suggested to create map <block, ReplicaFileDeleteTask>, and manages it FsDatasetAsyncDiskService. Chris suggested to move the step 3 closer to the disk block removal. Both are nice.

A proposed change to address the comments:

Since the volumeMap is managed in FsDatasetImpl, I think it's not too bad to keep the {{FsDatasetImpl#deletingBlock}} as where it is put now (sitting together with the volumMap). 

The key is that we want to be sure that we remove entries from it only after disk removal. There are two approaches:

# Let FsDatasetAsyncDiskService assumulated the list of blocks whose {{ReplicaFileDeleteTask}} is FINISHED to a certain size, then call the FsDataset api to remove them from {{FsDatasetImpl#deletingBlock}}.
# Let  FsDatasetAsyncDiskService call the FsDataset api to remove a block each time a {{ReplicaFileDeleteTask}}  finishes.

Since these operations requires synchronization, it may be better to use approach 1 performance wise. 

To go with approach 1, we need to add a data structure (assume a list) in {{FsDatasetAsyncDiskService}} to remember deleted blocks, and update the list in {{ReplicaFileDeleteTask#deleteFiles}} and {{ReplicaFileDeleteTask#moveFiles}} in a synchronized way as:

* add block deleted to the list
* when the size of the list reaches a certain number, call a similar API like {{dataset.removeDeletedBlocks(bpid, deletingBlockIds);}} in the patch to remove the list entries from  {{FsDatasetImpl#deletingBlock}}, and empty the list

Do you guys think this proposal would addresses your comments? thanks a lot.

I will leave to Shinichi to address the good questions about possible configuration issue.

BTW, the same thought did come to me earlier, but my bad to let it slip away due to a later incorrect thought. See https://issues.apache.org/jira/browse/HDFS-6833?focusedCommentId=14099884&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14099884. I will certainly be more careful in the future.
, Hi,
For your information, I tell you about some information when missing blocks occurred.
(These information is one DataNode's information. The similar situation was occurred in other DataNode.)

(1) The number of the blocks to delete : about 500K blocks
(2) The number of deleting blocks registered with memory again by DirectoryScanner.scan() & FsDatasetImpl.checkandupdate() : about 300K blocks
  (I confirmed this message "Added missing block to memory ..." in DataNode log)

DataNodes sent block report to NameNode, and "about 100 missing blocks" occurred.
(Maybe there were a lot of under-replicated blocks.)

The wrong information is solved in next DirectoryScanner.scan() & block report.
And there are a lot of map-tasks to choose DataNode without a block.
, Yongjun, Suresh, Chris, Shinichi, Masatake, Thanks for taking this JIRA and your comments. Should we make this change into 2.6.0 or 2.6.1? What do you think?, Oops, I overlooked that 2.6.0 is now being released. Let's target this into branch-2 and 2.6.1. Thanks!, Hi [~sinchii].  Are you planning on posting an updated patch or otherwise addressing the last round of feedback?  I'm not sure of the timeline for the 2.6.1 release, but if you want to include this, it would be good to get it in quickly.  Thanks!, Hi [~cnauroth], thank you for your care, and I'm sorry for the late reply.
I revise a patch based on the latest comment of Yongjun.
After the current of my work was settled, and I attach new patch next week.
, Thank you for the update, [~yamashitasni]., Hi all,

I attach a new patch file which reflected the following approach.

{quote}
Let FsDatasetAsyncDiskService assumulated the list of blocks whose ReplicaFileDeleteTask is FINISHED to a certain size, then call the FsDataset api to remove them from FsDatasetImpl#deletingBlock.
{quote}

In this patch, if the size of the deleted block list is 1000, it calls FsDatasetImpl#deletingBlock., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12686021/HDFS-6833-10.patch
  against trunk revision 82707b4.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8973//console

This message is automatically generated., HI [~sinchii],

Thanks for working on the new rev. It doesn't apply now due to the latest trunk changes. 

I looked at rev10 and have the following comments:
* {{deletingBlock = new ReplicaMap(this);}} is currently using FsDataset object ("this") as the mutex, we should replace the "this" with a different one that's dedicated to synchronize access to {{deletingBlock}} to avoid holding FsDataset object.
* There is no need to have {{private boolean scanning}} in {{DirectoryScanner}}
* There is no need for the local {{Set<Long> deletingBlockIds }} in {{scan()}} method, and we don't want to do the following because we do it from the other place after deleting block files:
519	        if (dataset.getNumDeletingBlocks(bpid) > 0) {
520	          dataset.removeDeletedBlocks(bpid, deletingBlockIds, true);
521	        } 
* The log 
{code}
    statsRecord.deletingBlocks++;
    LOG.info("Block file " + blockpoolReport[d].getBlockFile() + " is to be deleted");
{code}
can be moved to the following place:
1570	        ReplicaInfo removing = volumeMap.remove(bpid, invalidBlks[i]);
1571	        if (datanode.isDirectoryScannerInited()) {
1572	          deletingBlock.add(bpid, removing);
1573	        }
And we should not check whether the directory scanner is running in the above code because the directory scanner can start any time.
E.g., change the above 1570-1572 code to something like:
{code}
ReplicaInfo removing = volumeMap.remove(bpid, invalidBlks[i]);
deletingBlock.add(bpid, removing);
statsRecord.deletingBlocks++;
LOG.info("Block file " + blockpoolReport[d].getBlockFile() + " is to be deleted");  ==> need to revise to only print at debug level?
{code}

* The following code need to be removed
{code}
} else {
    LOG.info("Block file " + blockpoolReport[d].getBlockFile() + " is to be deleted");
    statsRecord.deletingBlocks++;
    deletingBlockIds.add(info.getBlockId());
}
{code}
, (sorry please ignore last one, submitted from wrong window)

HI [~sinchii],

Thanks for working on the new rev. It doesn't apply now due to the latest trunk changes, but I looked at rev10 based on earlier commit and have the following comments:

* {{deletingBlock = new ReplicaMap(this);}} is currently using {{this}} (FsDataset object) as the mutex, we should replace the "this" with a different one that's dedicated to synchronize access to {{deletingBlock}} to avoid holding FsDataset object.
* There is no need to have {{private boolean scanning}} and in {{DirectoryScanner}}. And we need to remove all references to it in the changes. We don't care whether scanning is true or not, we just need to make sure blocks deleted from disk are also removed from {{deleteingBlock}}
* There is no need for {{public boolean isDirectoryScannerInited()}} in DataNode.
* Remove the local variable {{Set<Long> deletingBlockIds}} in method scan() , and we don't want to do the following because we do it from the other place after deleting block files:
{code}
519	        if (dataset.getNumDeletingBlocks(bpid) > 0) {
520	          dataset.removeDeletedBlocks(bpid, deletingBlockIds, true);
521	        }
{code} 
* The following code that does the logging
{code}
    statsRecord.deletingBlocks++;
    LOG.info("Block file " + blockpoolReport[d].getBlockFile() + " is to be deleted");
{code}
can be moved to the following place:
{code}
1570	        ReplicaInfo removing = volumeMap.remove(bpid, invalidBlks[i]);
1571	        if (datanode.isDirectoryScannerInited()) {
1572	          deletingBlock.add(bpid, removing);
1573	        }
{code}
And we should not check whether the directory scanner is running in the above code because the directory scanner can start any time.
E.g., change the above 1570-1572 code to something like:
{code}
ReplicaInfo removing = volumeMap.remove(bpid, invalidBlks[i]);
deletingBlock.add(bpid, removing);
statsRecord.deletingBlocks++;
LOG.info("Block file " + blockpoolReport[d].getBlockFile() + " is to be deleted");  ==> need to revise to only print at debug level?
{code}

* The following code need to be removed
{code}
} else {
    LOG.info("Block file " + blockpoolReport[d].getBlockFile() + " is to be deleted");
    statsRecord.deletingBlocks++;
    deletingBlockIds.add(info.getBlockId());
}
{code}
* The 1000 below is a bit high, I'd suggest we start with a bit smaller one, such as 64. We might need to make it a configuration property later.
{code}
 private static final int MAX_DELETED_BLOCKS = 1000;
{code}
 
BTW, are you still able to reproduce the issue and test the fix? If so, we can test it out later after getting committer's review.

, Hi [~yzhangal],

Thank you for your many comments! I had attached a patch file for branch 2.6.
I attach a patch file for trunk that reflected your comments.
I　have a old version of Hadoop cluster. So I will prepare trunk version of Hadoop cluster and comfirm it.

BTW, *private boolean scanning* in DirectoryScanner of previous patch was to prevent false detection in DirectoryScanner#scan.
In other words, the current patch are concerned about the following, I think.

{code}
  void scan() {
    clear();
    Map<String, ScanInfo[]> diskReport = getDiskReport();  (1) DirectoryScanner comfirms block file and meta file

    (2) In FsDatasetAsyncDiskService, block file delete and remove deleteBlock 

    synchronized(dataset) {
      (3) dataset.isDeletingBlock is false and set addDifference() -> Incorrect.
    ...
{code}

I think I might need additional control on DirectoryScanner#getDiskReport or additional lock on DirectoryScanner#scan., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12686286/HDFS-6833-11.patch
  against trunk revision 2e98ad3.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 40 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.cli.TestHDFSCLI
                  org.apache.hadoop.hdfs.TestDecommission

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/8992//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/8992//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/8992//console

This message is automatically generated., Hi [~sinchii],

Thanks for your hard work and patience. I reviewed the last rev, have a few more comments:

* remove unused import of HashSet and Set in DirectoryScanner.java
* about code at line 291-304 in FsDatasetAsyncDiskService.java
**  the logic at  line 291 in FsDatasetAsyncDiskService.java is inverted
**  suggest to replace line 291-295 with
{code}
Set<Long> blockIds =  deletedBlockIds.get(block.getBlockPoolId());
if (blockIds == null) {
  blockIds = new HashSet<Long>();
  deletedBlockIds.put(block.getBlockPoolId(), blockIds);
}
blockIds.add(block.getBlockId());
{code}
** need to create a synchronized method {{synchronized void updateDeletedBlockId}} in FsDatasetAsyncDiskService, and move this whole block of code (line 291-304) to this method

* The functionality of {{FsDatasetImpl#removeDeletedBlocks}} need to be moved to 
ReplicaMap, by adding a similar method in ReplicaMap, access to this method should be protected by the mutex inside ReplicaMap. And then call this new method within {{FsDatasetImpl#removeDeletedBlocks}} as a delegation.

* About your concern of not having {{private boolean scanning}}, I think the relevant code in prior patch (that only removes deleted blocks from deletingBlock when this variable is true) is not correct, because the scan can start any time, and you would likely end up not removing some deleted blocks from deletingBlock. About locking, we need to protect access to {{deletingBlock}}, and {{deletedBlockIds}}, and make them in sync. Would you please elaborate if you have more concerns?

Thanks.


, Hi [~yzhangal]
Thank you for your review and comments. I attach a new patch which reflected the review results.

bq. About locking, we need to protect access to deletingBlock, and deletedBlockIds, and make them in sync. 
I'm agree with your comment., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12687118/HDFS-6833-12.patch
  against trunk revision 25a0440.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.balancer.TestBalancer

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9036//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9036//console

This message is automatically generated., {quote}
 -1 core tests. The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.balancer.TestBalancer
{quote}

This test  was successful in my environment., Hi [~sinchii],

Thanks for your new rev and sorry for late response. It looks good to me except for two minor things that you can take care of after getting committer's review:

*  {{public int getNumDeletingBlocks(String bpid)}} is not used anywhere. Consider removing. It might be possible that we need to have such an util in the future, if so, the method need to be implemented in the ReplicaMap protected with the internal mutex.

*   About {{if (m != null) {}} in {{void removeBlocks(String bpid, Set<Long> blockIds)}}, it's better to check if m is null and return if so right after getting m, instead of doing the check again and again in the loop. Or you can put the loop within {{if (m != null) {...}.}}

HI [~cnauroth] and [~szetszwo], thanks for your earlier review. Wonder if any of you would have time to take a look at the latest? thanks much.



, Hi [~yzhangal],

Thank you for your review! My previous patch file was not sufficient.
I attach a patch file that fixed two things.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12688016/HDFS-6833-13.patch
  against trunk revision 1050d42.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9073//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9073//console

This message is automatically generated., {quote}
 -1 core tests. The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration
{quote}
This result does not relate to my patch.

{quote}
 The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager
{quote}

This test was successful in my environment., [~szetszwo] [~cnauroth] do you mind taking a look?, [~sinchii] Can you add the test code which adds more than 64 blocks and invalidates them in TestFsDatasetImpl#testDeletingBlocks?, [~iwasakims] Thank you for your comment. I attach the patch which I added test code of the addition and invalidation of 64 blocks., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12689678/HDFS-6833-14.patch
  against trunk revision e7257ac.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA

                                      The following test timeouts occurred in hadoop-hdfs-project/hadoop-hdfs:

org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9135//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9135//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9135//console

This message is automatically generated., Findbugs problem and faild test have nothing to do with my patch., [~szetszwo] [~cnauroth] sorry for iterative ping, but could you take a look the latest patch(HDFS-6833-14.patch)?, Sure, will review the patch., Patch looks good in general.  Some comments:
- Do not add removeDeletedBlocks to FsDatasetSpi.  FsDatasetAsyncDiskService is a part of the FsDataset implementation.  Simply pass FsDatasetImpl in the constructor. 

- The ReplicaInfo added to the new deletingBlock ReplicaMap is never used.  How about simply using Map<String, Set<Long>>?

- In FsDatasetAsyncDiskService.updateDeletedBlockId(..), use entrySet() to avoid multiple lookups.

- Check if debug is enabled before calling LOG.debug(..).

- The patch does not apply anymore.  Need to update it.
, [~szetszwo], [~ozawa]
Thank you for your comment and advice. And I'm sorry for late reply.
I attach a new patch file which reflected previous comments.

, Thanks for the update.  Some comments:
- In FsDatasetAsyncDiskService.updateDeletedBlockId(..), you may keep the for-each loop so that the code is shorter.
{code}
      for(Entry<String, Set<Long>> e : deletedBlockIds.entrySet()) {
        String bpid = e.getKey();
        Set<Long> bs = e.getValue();
        fsdatasetImpl.removeDeletedBlocks(bpid, bs);
        bs.clear();
      }
{code}

- In FsDatasetImpl.invalidate/removeDeletedBlocks/isDeletingBlock, the synchronization should be on deletingBlock instead of asyncDiskService.  BTW, it may be a good idea to add a method for addDeletingBlock.

- In FsDatasetImpl.isDeletingBlock, it should return false when s == null.

- You may use a for-each in FsDatasetImpl.removeDeletedBlocks
{code}
        for(Long id : blockIds)
          s.remove(id);
        }
{code}

, Hi [~szetszwo],
Thank you very much for your review. I attach improved patch file., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12702401/HDFS-6833-15.patch
  against trunk revision 29bb689.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9724//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9724//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9724//console

This message is automatically generated., {color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12702428/HDFS-6833-16.patch
  against trunk revision 3560180.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9726//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9726//console

This message is automatically generated., +1 the new patch looks good., Thank you very much for your review!, [~szetszwo] could you commit this?, Will commit this shortly., I have committed this.  Thanks, Shinichi!, FAILURE: Integrated in Hadoop-trunk-Commit #7310 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7310/])
HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita (szetszwo: rev 6dae6d12ec5abb716e1501cd4e18b10ae7809b94)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #131 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/131/])
HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita (szetszwo: rev 6dae6d12ec5abb716e1501cd4e18b10ae7809b94)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
, SUCCESS: Integrated in Hadoop-Yarn-trunk #865 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/865/])
HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita (szetszwo: rev 6dae6d12ec5abb716e1501cd4e18b10ae7809b94)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2081 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2081/])
HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita (szetszwo: rev 6dae6d12ec5abb716e1501cd4e18b10ae7809b94)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2063 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2063/])
HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita (szetszwo: rev 6dae6d12ec5abb716e1501cd4e18b10ae7809b94)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #122 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/122/])
HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita (szetszwo: rev 6dae6d12ec5abb716e1501cd4e18b10ae7809b94)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #131 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/131/])
HDFS-6833.  DirectoryScanner should not register a deleting block with memory of DataNode.  Contributed by Shinichi Yamashita (szetszwo: rev 6dae6d12ec5abb716e1501cd4e18b10ae7809b94)
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
, Thanks [~yamashitasni] for working hard this and thanks [~szetszwo], [~yzhangal], [~cnauroth], [~sureshms], [~iwasakims]  for your review!]