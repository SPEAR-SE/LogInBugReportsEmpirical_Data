[Here is the log for one of the block :

{noformat}
2013-03-06 00:02:21,787 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/mapred/system/job_201303040902_85451/jobToken. blk_6341538266279390032_231558736
2013-03-06 00:02:21,794 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: x.y.z.63:50010 is added to blk_6341538266279390032_231558736 size 226
2013-03-06 00:02:21,794 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: x.y.z.73:50010 is added to blk_6341538266279390032_231558736 size 226
2013-03-06 00:02:21,803 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask x.y.z.63:50010 to replicate blk_6341538266279390032_231558736 to datanode(s) x.y.z.74:50010
2013-03-06 00:02:21,803 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: x.y.z.67:50010 is added to blk_6341538266279390032_231558736 size 226
2013-03-06 00:02:23,784 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: x.y.z.74:50010 is added to blk_6341538266279390032_231558736 size 226
2013-03-06 00:02:23,784 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.chooseExcessReplicates: (192.168.176.74:50010, blk_6341538266279390032_231558736) is added to recentInvalidateSets
2013-03-06 00:02:30,805 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask x.y.z.73:50010 to delete  blk_2922851987421619867_231558569 blk_6341538266279390032_231558736 blk_7283921456384156061_231558575
{noformat}

For almost every that is created in HDFS, we are seeing excess replica being created. Here is some analysis that I have done :

Looking at the following code in FSNameSystem.computeReplicationWorkForBlock :

{noformat}
        numEffectiveReplicas = numReplicas.liveReplicas() +
        pendingReplications.getNumReplicas(block);
        if(numEffectiveReplicas >= requiredReplication) {
          neededReplications.remove(block, priority); // remove from neededReplications
          replIndex--;
          NameNode.stateChangeLog.info("BLOCK* "
              + "Removing block " + block
              + " from neededReplications as it has enough replicas.");
          return false;
        } 
{noformat}

There should not be an excess replica getting created. From the above log, if the replica corresponding to x.y.z.67 has been in pending replicas.
I see that pendingReplications does not get updated during the creation of the file. Correct me if I'm wrong.

I'm not sure if the above analysis is valid. Please correct me if I'm wrong., Can you also include in the log snippet, lines printed for the file /user/mapred/system/job_201303040902_85451/jobToken?

If I were to guess, one of the datanode in the pipeline reports a replica for a block late. The replication monitor is too aggressive and creates an additional replica meanwhile. However this should not happen for every block that is created. Can you please give the replication related configurations from hdfs-site.xml?, Can you also check, whether the DNs registration are getting expired due to some reason and they are rejoing again in the cluster? [should see the logs like 'Adding a new node'] By the time of rejoining the DNs, NN initiating the replication for that blocks?
As Suresh said, we can check any node is very low responsive and NN is replicating that blocks before he reports.

{quote}
There should not be an excess replica getting created. From the above log, if the replica corresponding to x.y.z.67 has been in pending replicas.
I see that pendingReplications does not get updated
{quote}
That check is considering perdingReolications only (i.e numEffectiveReplicas). SO, we should not remove if it is in pending replications. If more than that any thing present in neededReplication again, we are removing I think.

{quote}
I see that pendingReplications does not get updated during the creation of the file. Correct me if I'm wrong.
{quote}
I did not see 1.1.1 code now, but we should be updating neededReplications while finalizing block.no? we will not update it while creating.



, Here is log for /user/mapred/system/job_201303040902_85451/jobToken :

{noformat}
grep '/user/mapred/system/job_201303040902_85451/jobToken' namenode.log
2013-03-06 00:02:21,787 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/mapred/system/job_201303040902_85451/jobToken. blk_6341538266279390032_231558736
2013-03-06 00:02:21,795 INFO org.apache.hadoop.hdfs.StateChange: Removing lease on  file /user/mapred/system/job_201303040902_85451/jobToken from client DFSClient_201543923
2013-03-06 00:02:21,795 INFO org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.completeFile: file /user/mapred/system/job_201303040902_85451/jobToken is closed by DFSClient_201543923
{noformat}

Here is the configuration wrt replication:
{noformat}
      <name>dfs.replication.max</name>
      <value>50</value>

      <name>dfs.replication</name>
      <value>3</value>

      <name>mapred.submit.replication</name>
      <value>3</value>
{noformat}

bq. If I were to guess, one of the datanode in the pipeline reports a replica for a block late. The replication monitor is too aggressive and creates an additional replica meanwhile. However this should not happen for every block that is created.
I also agree this is not happening for every block. But we are seeing this number so huge in our cluster. Also we have many files short lived in our cluster, which cause two consecutive delete requests to a data node, then hitting HDFS-4544. , bq. Can you also check, whether the DNs registration are getting expired due to some reason and they are rejoing again in the cluster? [should see the logs like 'Adding a new node'] 

No. We are not seeing this. 

bq. we can check any node is very low responsive and NN is replicating that blocks before he reports.
we are not seeing there is any particular datanode that is responding slow. 

Looking at my first comment again, the block was allocated at 00:02:21,787. 
First addStoredBlock at 00:02:21,794, second at 00:02:21,794. 
Replication request at 00:02:21,803. And third report at the same time - 00:02:21,803.

This says the replication monitor is too aggressive. Is there any other way to control this?
, Possible issue as you agreed that this is not happening for all blocks. Long back Todd has filed a JIRA for this. I think that is still open. HDFS-1172. 
we can target to review and address in that JIRA? Same issue you are hitting by seeing the logs and your provided info right., Thanks Uma. Looking at Todd's description on HDFS-1172, Yes it seems we are hitting the same. 

bq. we can target to review and address in that JIRA? 
Sure we can address the issue in HDFS-1172, Closing this as duplicate of HDFS-1172 by marking target versions to 1.2.0 as this JIRA expects fix for it., Closed upon release of Hadoop 1.2.0.]