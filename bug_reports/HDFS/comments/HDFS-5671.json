[Would you have a patch [~jamesli]?  If so, would you mind posting?  Could you try it on your cluster to see if it cuts down on instances of CLOSE_WAIT?  Thanks., @stack I have add the patch file,but we haven't test yet. According to our datanode and regionserver's error log,it appears that client side does not close the connection when datanode throw IOException and close the connection.
Here are our error logs,hope will help you:

RegionServer:
2013-12-13 15:48:31,474 INFO org.apache.hadoop.hdfs.DFSClient: Will fetch a new access token and retry, access token was invalid when connecting to /192.168.2.27:1004 : org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error for OP_READ_BLOCK, self=/192.168.2.27:56975, remote=/192.168.2.27:1004, for file /hbase/XXXX/b50bf1b95c9242cdd242dc4e6549bc90/raw/d59819ebe5574c79a5d1cf13a733d2ed, for pool BP-621472495-192.168.2.25-1375176775166 block -882505774551713967_11426277 

Datanode:
2013-12-13 15:48:31,474 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: dn2:1004:DataXceiver error processing READ_BLOCK operation  src: /192.168.2.27:56975 dest: /192.168.2.27:1004
org.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1386914547771, keyId=2020397153, userId=hbase, blockPoolId=BP-621472495-192.168.2.25-1375176775166, blockId=-882505774551713967, access modes=[READ]) is expired.
        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java)
        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java)
        at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java)
        at java.lang.Thread.run(Thread.java)
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619041/5671.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5740//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5740//console

This message is automatically generated., It seems like we should move this section:
{code}
    // Will be getting a new BlockReader.
    if (blockReader != null) {
      blockReader.close();
      blockReader = null;
    }
{code}

to be before the section which assigns a new value to {{blockReader}}, rather than special-casing one kind of error.  Also, it might make sense to use {{IOUtils#cleanup}} here, although I'm not aware of any {{BlockReader}} objects that throw an exception from {{close}}., 5671.patch is not correct,this patch fix the CLOSE_WAIT bug,in our test,after patch,CLOSE_WAIT do not increase., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12619492/5671v1.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5766//console

This message is automatically generated., In DFSInputStream.blockSeekTo()-line 533,invoke getBlockReader() which wil generate a peer use newTcpPeer(dnAddr) -line 1107,when BlockReaderFactory.newBlockReader throw IOException,the peer will not be  closed which will cause a CLOSE_WAIT connection. 
In our test,when datanode get a InvalidToken exception in  DataXceiver.checkAccess(),it will close the connection.At regionserver side, in RemoteBlockReader2.newBlockReader(),checkSuccess() will throw a InvalidBlockTokenException, DFSInputStream.blockSeekTo() will catch the exception, but the connection is NOT closed, it become CLOSE_WAIT., {code}
DFSClient.LOG.info("Close connection,rethrow exception");
{code}
Please change the log message like ( "Exception while getting block reader, closing stale " + peer , exception )  and change the priority to debug level, Patch LGTM caveat the Uma comment (Maybe add the ex.getMessage to the string logged so can be tied to the exception when it comes out higher up the stack?)., change log level and message., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620702/5671v2.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5797//console

This message is automatically generated., diff with svn trunk.
PS: Can anybody tell me why  Hadoop QA test failed with patch error,thanks., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620709/5671v3.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer
                  org.apache.hadoop.hdfs.TestDatanodeConfig

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5798//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5798//console

This message is automatically generated., {code}
 } catch (IOException ex) {
+            DFSClient.LOG.debug("Exception while getting block reader, closing stale " + peer, ex);
+            throw ex;
+    } finally {
+            if (reader == null) {
+                    IOUtils.closeQuietly(peer);
+            }
+    }
{code}

Looks like you are using wrong coding formatter.  Could you please update the patch with correct coding format.  Please note Hadoop code indentation space will have 2 spaces, Fix coding formatter.
Thank you Uma., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620790/5671v4.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5800//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620790/5671v4.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5804//console

This message is automatically generated., I am able to apply this patch locally to trunk. Not sure why its failing in jenkins. Does anyone has idea on this?

, +1 on changes. I have attached a patch with some formatting changes, such as long lines in patch. I will wait for jenkins +1, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12620927/HDFS-5671.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5805//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5805//console

This message is automatically generated., +1

Nice one [~hbjoylee], Thanks a lot, JamesLi for the patch and thanks stack for the review. I will commit this patch shortly., SUCCESS: Integrated in Hadoop-trunk-Commit #4942 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4942/])
HDFS-5671. Fix socket leak in DFSInputStream#getBlockReader. Contributed by JamesLi (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1554553)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
, I have added JamesLi as contributor. and I have just committed this to trunk and branch-2, Thanks stack and  Uma for the review and give me very helpful advices, I fell very happy to see  this patch integrated in Hadoop trunk at the beginning of 2014,it's the best new year gift for me.Thank you all., FAILURE: Integrated in Hadoop-Yarn-trunk #439 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/439/])
HDFS-5671. Fix socket leak in DFSInputStream#getBlockReader. Contributed by JamesLi (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1554553)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk #1631 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1631/])
HDFS-5671. Fix socket leak in DFSInputStream#getBlockReader. Contributed by JamesLi (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1554553)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
, SUCCESS: Integrated in Hadoop-Mapreduce-trunk #1656 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1656/])
HDFS-5671. Fix socket leak in DFSInputStream#getBlockReader. Contributed by JamesLi (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1554553)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
]