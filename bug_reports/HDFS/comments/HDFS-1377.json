[I made a link from HDFS-1487. I found the same problem when testing the issue. I think I can fix this soon and attach a patch. Any comments?, Patch attached and run hudson.

I removed disk checking in FSDirectory.replaceNode(). It is duplicated with BlockManager.commitBlock(). All diskspace counting will be updated at a block level when writing (appending) a file, in addBlock(), commitBlock() and abandonBlock() (HDFS-1487).

I also added a test case in TestQuota. The original tests hiding this issue because all file length is mutiple of 1024 while block size is 512. I create a file which is not an exact multiple of the block size to test this case.

Hadoop developers and committers, thanks for your time to review this patch and give me some suggestions! HDFS quota is very important to manage our clusters which is highly shareable with dozens of users and PBs storage., So what is the original idea of the code to be removed in your patch?, Weichao,

The reason is the same with the issue description from Eli Collins. When finalizing an uncompleted file, BlockManager reduce the pending block size in commitBlock() while FSDirecotry.replaceNode() do the same thing. Diskspace count will be reduced twice and less than the actual value, even more, the value maybe nagetive which is obviously unleagle.

You can reproduce the problem using the steps from Eli Collins. Or make a regression test using my test case.
, Zhong,

Thanks. In fact, i encountered the same problem in our cluster.
And i also test your patch and it looks good.
+1 for this patch., bq. So what is the original idea of the code to be removed in your patch?

If the oldnode and newnode arguments to replaceNode always have the same blocks then the disk updating space code in replaceNode is redundant. This code goes back to the original space quota addition (HADOOP-3938), and it looks like it was redundant as well there (note that the comment near the code even says "Currently oldnode and newnode are assumed to contain the same blocks" so perhaps they were trying to be conservative).

So the question becomes do oldnode and newnode ever have different blocks? On branch 20 I don't think that's the case. The only caller where it seems that it could potentially be the case is loadFilesUnderConstruction, in which case the under construction INode may have an extra block, but then wouldn't that have been accounted for via addStoredBlock already?

I still need to look at the code for trunk more thoroughly, the latest code, particularly append, makes things less obvious. I do think we should remove this particular file-level accounting code in replaceNode, and always make quota adjustments at block granularity., Patch for branch 20 attached.  I think this bug is bad enough to fix on this branch.

Apologies, I thought I had posted this earlier.  
* Adds TestQuotaViolations which covers these particular cases
* Removes the account in replaceNode (which fixes these particular bugs), similar fix to the patch Zhong created 
* Adds a warning in INodeDirectory#computeContent summary to warn against future cases. I've run this patch on a cluster and not seen it warn, but it will catch cases like HDFS-1487.

, Right patch for branch 20 attached this time., 
I am still refreshing my memory of this NN internals...

I think the patch is correct. I too am not sure if this causes problem else where. looks like it fixes more that it might break.

This patch fixes bug the case where replaceNode(oldINode, newINode) is called under normal close().. where though oldINode is a INodeUnderConstruction only by its Java type, but not logically (it is already been removed from the active leases etc). 

What about the other way around where newINode is an INodeUnderConstruction? as described by Eli below : 

bq. So the question becomes do oldnode and newnode ever have different blocks? On branch 20 I don't think that's the case. The only caller where it seems that it could potentially be the case is loadFilesUnderConstruction, in which case the under construction INode may have an extra block, but then wouldn't that have been accounted for via addStoredBlock already?

Not sure how it manifests in real life, but here a file is going from INodeFile to INodeFIleUnderConstruction. So the actual space consumed should be rounded upwards to the block boundery. Add stored block is not called at this time. 

How about calling replaceNode(..., updateDiskspace = false) in finalizeFileUnderconstruction(), since we know that oldNode is not under construction anymore?

btw, do we separate test file for this? TestQuota.java is already supposed to test violations (FWIW, it does test some violations).



, Thanks for taking a look Raghu!

bq. Not sure how it manifests in real life, but here a file is going from INodeFile to INodeFIleUnderConstruction. So the actual space consumed should be rounded upwards to the block boundery. Add stored block is not called at this time.

The sequence of events is:

* A client starts appending to an existing file
* dfsadmin -saveNamespace  (persists the INodeFileUnderConstruction)
* The NN restarts, when loading the image we replace the old INode with the persisted INodeFileUnderConstruction.

Since disk usage is not persisted in the image, is calculated as DNs report blocks on startup, I don't think this path should update disk usage. 

bq. How about calling replaceNode(..., updateDiskspace = false) in finalizeFileUnderconstruction(), since we know that oldNode is not under construction anymore?

I think we can remove the disk updating code there entirely, even on trunk, and add tests to make sure the block-level accounting is working.

bq. btw, do we separate test file for this? TestQuota.java is already supposed to test violations (FWIW, it does test some violations).

I had the put them in a separate class so they could share setup and teardown (vs TestQuota which has setup/teardown code for each test). I'll update the patches to add the tests to TestQuota so they're all in one place., Removing the disk space updating code in replaceNode also looks safe in trunk, though I noticed we're missing coverage for quotas and append. Filed HDFS-1515., Patch for branch 20 attached.  Address Raghu's feedback.    All test pass modulo TestDU which is HADOOP-7045. Think patch is ready to be checked in. 

{noformat}
     [exec] 
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 9 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     -1 Eclipse classpath. The patch causes the Eclipse classpath to differ from the contents of the lib directories.
     [exec] 
{noformat}

Not sure what the eclipse classpath warning is about, this patch just edits some source files., Patch for trunk attached.  Release audit warnings are a separate issue.  No change in test failures.

{noformat}
     [exec] 
     [exec] -1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 9 new or modifi
ed tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messa
ges.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number 
of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs (ver
sion 1.3.9) warnings.
     [exec] 
     [exec]     -1 release audit.  The applied patch generated 103 release audit
 warnings (more than the trunk's current 98 warnings).
     [exec] 
     [exec]     +1 system test framework.  The patch passed system test framework compile.
     [exec] 
{noformat}, Thanks Eli. will review the patch tonight (tomorrow night at the latest)., +1.

Thanks Eli. The warning in contentSummary() is also useful.
, Thanks Raghu!   I've committed this to trunk, and the 22, 21, and 20 branches., Committed also this to Federation Branch.  Thanks, Eli!, Integrated in Hadoop-Hdfs-trunk #643 (See [https://builds.apache.org/hudson/job/Hadoop-Hdfs-trunk/643/])
    , Integrated in Hadoop-Hdfs-22-branch #35 (See [https://builds.apache.org/hudson/job/Hadoop-Hdfs-22-branch/35/])
    , Can someone (Nicholas?) commit the 20 patch in this JIRA to branch-0.20-security?, It is already checked into branch-0.20-security and branch-0.20-security-204.  Updating the fix version to match. Would be great if this were done for the other jiras in the branch., I am sure I am missing something here, but in my "view" or branch-20-security, I do not see the patch. I am looking at:
http://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.20-security, I merged this to 0.20-security.

Eli, it is not yet in branch-0.20-security-204 but thanks for taking a look.

John, could you test 0.20-security branch one more time?, Thanks Nicholas and Eli. The fix is now in branch-20-security. I ran the related tests and it passed., Weird. You're right, this change is not checked into branch-0.20-security but according to the git mirror it is. The following is from before Nicholas' push:
{noformat}
common1 (branch-0.20-security-204)$ git log origin/branch-0.20-security-204 |grep HDFS-1377
    HDFS-1377. Quota bug for partial blocks allows quotas to be violated. Contributed by Eli Collins
common1 (branch-0.20-security-204)$  
{noformat}

Is this because the change was checked into branch-0.20 and the mirror things branch-0.20-security is based on branch-0.20?
, I have merged this to branch-0.20-security-204.]