[Thanks for reporting this Bogdan. I'd like to work on this.

The test program attached actually passes on trunk. I further tested on branch-2.7 and it fails. The reason for this is that we have HDFS-5356 which automatically closes DFS on trunk.

To double check this, if I change the {{FileSystem}} to {{DistributedFileSystem}}, and call {{close}} before shutdown, the test passes on branch-2.7 as well.
{code:java}
		/* start cluster and write something */
//		FileSystem fs = cluster.getFileSystem();
		DistributedFileSystem fs = cluster.getFileSystem();    // <=========== change to DFS
		FSDataOutputStream out = fs.create(path, true);
		out.write(bytes);
		out.hflush();
		/* stop cluster while file is open for writing */
		cluster.shutdown();
		fs.close();    // <=========== move this line up by 1
                ....
{code}

So now comes the original question: what if the client didn't call {{DFS#close}}, and HDFS restarts? Currently the file lease is revoked after the hard limit (1hr). Before that READ fails (e.g. {{cp: Cannot obtain block length for ...}}). This is because {{fs.delete}} or {{fs.create}} will remove the lease, but {{fs.open}} will not.

Since it's the read that fails, I don't think it's a good idea to recover the lease on the read operation. One possible solution is perhaps to detect this and revoke on restart. I need to further investigate and will update here. , OK, thanks.
My first idea was to just signal this case through a special exception, e.g. ReplicaWaitingRecoveryException which is reported to the client app. Then, the client app can choose what to do. In my case, when I get this exception, I would call DFSClient.recoverLease to trigger the lease recovery and open again after this. That's what I do in the write case, before the soft limit expires.
In fact, as a workaround, I do this even for reading now, just now I get a generic exception. Just an idea. Maybe you find something better.]