[A patch for review., Patch looks good.  Do we need a test?, The existing tests sometime fail with this error. So I do not think there is a need for new test. Furthuremore, I have tested this manually too., -1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421358/pkgRmOrder.patch
  against trunk revision 822153.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/59/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/59/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/59/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/59/console

This message is automatically generated., There were two failed tests:  org.apache.hadoop.hdfs.server.namenode.TestBlockUnderConstruction.testBlockCreation  and org.apache.hadoop.hdfs.TestFileAppend2.testComplexAppend. The first one is a known bug. For the second one, the append write failed on "Too many open files".
This seems not related to the change in this jira. I filed HDFS-690 to track the bug., +1 on the patch., I committed this to trunk and branch 0.21.

I checked branch 0.20. It seems to have the same problem. But strangely I have not seen tests failed on this in 0.20 branch. Anyboday has seen this bug in 0.20 and wants to commit the fix to 0.20?

A consequence of this bug is that it introduces false alarms. A good datanode may fail to send an ack to the client on closedSocket exception when receiving a block., I close this jira for now. If anyone wants this fix to be in 0.20, I will do so later., Integrated in Hdfs-Patch-h2.grid.sp2.yahoo.net #47 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/47/])
    , Integrated in Hadoop-Hdfs-trunk-Commit #79 (See [http://hudson.zones.apache.org/hudson/job/Hadoop-Hdfs-trunk-Commit/79/])
    , Integrated in Hdfs-Patch-h5.grid.sp2.yahoo.net #78 (See [http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/78/])
    , Hi Hairong, I am using 20.1 hadoop, I did not find the problem that you are actually mentioning. What I am getting from the code is Dataxceiver thread will close the streams only after the responder thread has been completed. Can you please check and validate my comments, if I am wrong can you explain the problem.]