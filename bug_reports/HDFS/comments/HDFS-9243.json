[De-assign myself because I am not able to work on this right now. But this failure occurred again in today's jenkins build https://builds.apache.org/job/Hadoop-Hdfs-trunk/2453/changes, The reason is ReplicationMonitor have chosen one DataNode as target twice for the same block.

totalNodes=\{DN0, DN1, DN2\}
one block with replFactor=3, liveNodes=\{DN0\}, so block is under replicated.
For some reason, DN2 is not chosen by ReplicationMonitor.
ReplicationMonitor chose DN1 as target, schedule 1st recovery. pendingNum=1.
Later, before DN1 reported, ReplicationMonitor found liveNodes + pendingNum < replFactor, it chose DN1 as target again, schedule 2nd recovery. pendingNum=2.

DN1 can't have 2 identical replicas. So one creplica is recovered at DN1. But liveNodes + pendingNum = replFactor, so it wait until timeout at {{PendingReplicationBlocks}} map.

Some testCase wait liveNodes to reach replFactor, but the testCase cound't wait 5min so it failed. Due to randomness of BlockPlacmentPolicy and relatively large number of DNs, it may not be a problem in production.

I checked [testReport|https://builds.apache.org/job/PreCommit-HDFS-Build/13124/testReport/org.apache.hadoop.hdfs.server.blockmanagement/TestUnderReplicatedBlocks/testSetrepIncWithUnderReplicatedBlocks/] . I saw
{noformat}
2015-10-22 15:10:18,400 [DataXceiver for client  at /127.0.0.1:47728 [Receiving block BP-57081724-67.195.81.153-1445526607531:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:run(270)) - 127.0.0.1:47391:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:47728 dst: /127.0.0.1:47391; org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-57081724-67.195.81.153-1445526607531:blk_1073741825_1001 already exists in state FINALIZED and thus cannot be created.
{noformat}
I've saw this before at HDFS-9275., Thanks for the analysis. Please feel free to assign this jira to yourself. Because the failure appear quite frequent, it should be possible to improve upon it, even though it may not be an issue in production. I am thinking it could be resolved by reducing certain timeout parameters, so that the test case doesn't need to wait long. ]