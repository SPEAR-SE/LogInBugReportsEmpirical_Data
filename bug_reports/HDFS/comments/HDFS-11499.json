[GitHub user lukmajercak opened a pull request:

    https://github.com/apache/hadoop/pull/199

    HDFS-11499 Decommissioning stuck because of failing recovery

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/lukmajercak/hadoop HDFS-11499

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hadoop/pull/199.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #199
    
----
commit 3609b1353e64a24dee4746b8fa23ed7547768d68
Author: Lukas Majercak <lumajerc@microsoft.com>
Date:   2017-03-05T20:04:06Z

    HDFS-11499 add TestDecommission.testDecommissionWithOpenFileAndDatanodeFailing for testing recovery

commit 3f97d89f75d8a20f878da8c438141f9b6adf7da0
Author: Lukas Majercak <lumajerc@microsoft.com>
Date:   2017-03-05T20:05:08Z

    HDFS-11499 count decommissioning replicas when completing last block in BlockManager.commitOrCompleteLastBlock

----
, Hi Lukas,
this issue sounds related to HDFS-11486. The block recovery failure here and the client close failure in HDFS-11486 are both caused by the same buggy check.

Can you update the status of this jira to Patch available to trigger precommit check? Thanks!, The fix seems correct to me and the unit test seems good enough.
I think the other uses of {{hasMinStorage()}} are correct.
It seems this was introduced in HDFS-1172 a couple years ago; [~tlipcon], [~iwasakims], do you mind taking a look?, HI [~lukmajercak]
thanks for the patch. I quickly reviewed it and I think it did the right thing. The test looks good too. In addition, it also fixes the close() failure bug I described in HDFS-11486.

* HDFS now implements a new replica state called maintenance mode. It seems that case is not being considered in the patch. Would it make sense to also fix the same issue with maintenance mode? [~manojg] how do you feel?, Sure [~jojochuang]. I can give you a patch for the same, soon., Hi [~jojochuang], [~manojg]

So shall we count in the replicas on maintenance nodes as well? Can we need to add a test case to cover this/modify the one in the patch?

Thanks, yes [~lukmajercak], we need to additionally count in the ENTERING MAINTENANCE nodes as well. I am adding a new test based on the one given by [~linyiqun] in HDFS-11486 in TestMaintenanceState to cover this case. 

[~lukmajercak]/[~jojochuang], Shall I merge all the fix and test patches along with mine and post a complete patch covering both HDFS-11499 and HDFS-11486 ? Or shall I submit v02 patch for this HDFS-11499 alone with maintenance state included. Your suggestion please ?, [~manojg], I don't really mind, let's go for the second option? Also, + [~mingma], do you want to check this? Seems like it is related to your HDFS-9390., [~lukmajercak], [~jojochuang], [~linyiqun],
Attached v02 patch to address the following. Can you please take a look at the patch.
* {{BlockManager#commitOrCompleteLastBlock()}} to consider entering_maintenance replicase along with decommissioning replicas for usable replicas.
* Added unit test testFileCloseAfterEnteringMaintenance to {{TestMaintenanceState}} based on the test given by Yiqun. Without fix, the test fails at file close., LGTM, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 33s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 58s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 23s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 25s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 11s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 43s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 4 new + 146 unchanged - 0 fixed = 150 total (was 146) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 96m 49s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  1m 58s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}136m 19s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.TestFileAppend3 |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
|   | hadoop.hdfs.TestDecommission |
|   | hadoop.hdfs.server.datanode.TestDirectoryScanner |
|   | hadoop.hdfs.server.datanode.TestDataNodeUUID |
| Timed out junit tests | org.apache.hadoop.hdfs.server.namenode.TestEditLog |
|   | org.apache.hadoop.hdfs.server.namenode.TestQuotaByStorageType |
|   | org.apache.hadoop.hdfs.server.blockmanagement.TestBlockStatsMXBean |
|   | org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure030 |
|   | org.apache.hadoop.hdfs.server.namenode.TestEditLogAutoroll |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HDFS-11499 |
| GITHUB PR | https://github.com/apache/hadoop/pull/199 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 278985d535e8 3.13.0-92-generic #139-Ubuntu SMP Tue Jun 28 20:42:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d9dc444 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/18598/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/18598/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/18598/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/18598/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Looks like the test timed out, [~manojg], do you mind increasing the timeout to 360sec (based on the other tests in TestDecommission) and submitting a patch? Thanks!, [~lukmajercak],
Are you referring to the timeout in TestDecommission#testDecommissionWithOpenFileAndDatanodeFailing() which was part of the patch v01 ? In the patch v02 I added Maintenance State related test. Not sure, if extending the timeout for the failed test is going to solve the problem. Because, the nodes didn't move to DECOMMISSIONED state as the test is expecting .

{noformat}

2017-03-06 23:33:49,462 [Thread-782] INFO  hdfs.AdminStatesBaseTest (AdminStatesBaseTest.java:waitNodeState(342)) - Waiting for node 127.0.0.1:33069 to change state to Decommissioned current state: Decommission In Progress
2017-03-06 23:33:49,462 [Thread-782] INFO  hdfs.AdminStatesBaseTest (AdminStatesBaseTest.java:waitNodeState(342)) - Waiting for node 127.0.0.1:33069 to change state to Decommissioned current state: Decommission In Progress

[test timeout]

2017-03-06 23:33:49,486 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1951)) - Shutting down the Mini HDFS Cluster
{noformat}, [~manojg], yes TestDecommission#testDecommissionWithOpenFileAndDatanodeFailing(). It will wait for all three DNs to be decommissioned right, and the log you showed is just 1 of them. I would suggest increasing it to 360sec yes. It finishes in 30seconds~ on my machine, similar to TestDecommission#testDeadNodeCountAfterNamenodeRestart, which has 360sec timeout., [~lukmajercak],
Attached v03 patch to have the same test timeout as the other tests and also fixed checkstyle issues. Please take a look., Great work, everyone!
Only one nit for the v03 patch:
{code}
+
+    Path openFile = new Path("/testClosingFileInMaintenance.dat");
+    // Lets write 2 blocks of data to the openFile
+    writeFile(getCluster().getFileSystem(), openFile, (short) 3);
+
{code}
The comment seems not accurate, here we write three block replica to openFile , right? [~manojg]. Or this should be replaced by "two more blocks"., [~linyiqun], 

Thanks for the review. I was using the below {{writeFile}} method version, where the last param is the file replication factor and not block count. The method in turn creates a total of 2 blocks for the file with the provided repl factor. Is the comment still wrong ? please let me know.

{code}
  static protected void writeFile(FileSystem fileSys, Path name, int repl)
      throws IOException {
    writeFile(fileSys, name, repl, 2);
  }
{code}, You are right, I misreading for this. +1 for the patch. Pending Jenkins., Thanks for working on this, [~lukmajercak] and [~manojg].

While testing the 03 patch, the added testDecommissionWithOpenFileAndDatanodeFailing intermittently timeouts waiting for decomission. I'm looking into the cause.
, [~iwasakims], indeed it sometimes times out, also looking into the cause. , Looks like the issue is in the configuration, I have been running this test on 2.7.1 with no problems, and just found that trunk is missing some configurations, specifically :
{code:xml}
conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_PENDING_TIMEOUT_SEC_KEY, 4).
{code}

The test times out because of a block being in PendingReconstructionBlocks. , It looks like the {{DFS_NAMENODE_REPLICATION_PENDING_TIMEOUT_SEC_KEY}} setting was removed in HDFS-9392.
I don't see a good reason to remove it; I think we should bring it back.
[~lukmajercak], do you mind adding the conf back?, Unit test broken., Submitted a patch with the configuration added back to {{AdminStatesBaseTest}}., The timeout seems to be relevant since replica recovery was not attempted after first 30 seconds in failed test case.
{noformat}
$ grep 'initReplicaRecovery:' org.apache.hadoop.hdfs.TestDecommission-output.txt.failed
2017-03-07 14:13:35,095 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@7dcda518] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2382)) - initReplicaRecovery: blk_1073741826_1002, recoveryId=1004, replica=FinalizedReplica, blk_1073741826_1002, FINALIZED
2017-03-07 14:13:35,096 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@7dcda518] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2440)) - initReplicaRecovery: changing replica state for blk_1073741826_1002 from FINALIZED to RUR
...snip
2017-03-07 14:14:03,092 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@5c8628b1] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2382)) - initReplicaRecovery: blk_1073741826_1002, recoveryId=1018, replica=ReplicaUnderRecovery, blk_1073741826_1002, RUR
2017-03-07 14:14:03,092 [org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$1@5c8628b1] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:initReplicaRecoveryImpl(2433)) - initReplicaRecovery: update recovery id for blk_1073741826_1002 from 1017 to 1018
{noformat}

{noformat}
$ tail -n2 org.apache.hadoop.hdfs.TestDecommission-output.txt.failed
2017-03-07 14:19:26,875 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2017-03-07 14:19:26,987 [Thread-11] INFO  hdfs.AdminStatesBaseTest (AdminStatesBaseTest.java:waitNodeState(342)) - Waiting for node 127.0.0.1:43314 to change state to Decommissioned current state: Decommission In Progress
{noformat}

DFS_NAMENODE_REPLICATION_PENDING_TIMEOUT_SEC_KEY was replace by DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY while keeping effective default value based on the description of HDFS-10219.
{noformat}
  public static final String  DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY =
      "dfs.namenode.reconstruction.pending.timeout-sec";
  public static final int
      DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_DEFAULT = 300;
{noformat}

Trying to set the timeout to 4 in {{AdminStatesBaseTest#setup}} and seeing effect.
, Setting DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY in TestDecoommission was removed by HDFS-9392. [~mingma], was this intentional change?, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 31s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 14m 18s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 57s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 35s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 4 new + 146 unchanged - 0 fixed = 150 total (was 146) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 38s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 80m 35s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 33s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}108m 48s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.blockmanagement.TestReconstructStripedBlocksWithRackAwareness |
|   | hadoop.hdfs.TestDecommission |
|   | hadoop.hdfs.server.datanode.checker.TestThrottledAsyncChecker |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
| Timed out junit tests | org.apache.hadoop.hdfs.TestLeaseRecovery2 |
|   | org.apache.hadoop.hdfs.server.namenode.TestLargeDirectoryDelete |
|   | org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport |
|   | org.apache.hadoop.hdfs.server.namenode.TestListCorruptFileBlocks |
|   | org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionFunctional |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HDFS-11499 |
| GITHUB PR | https://github.com/apache/hadoop/pull/199 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 81930ca04995 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 28daaf0 |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/18633/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/18633/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/18633/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/18633/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Github user lukmajercak closed the pull request at:

    https://github.com/apache/hadoop/pull/199
, The build results were from the outdated github PR, closed that and resubmitted the patch., bq. I think the other uses of hasMinStorage() are correct. It seems this was introduced in HDFS-1172 a couple years ago

The relevant code had existed before HDFS-1172, but anyway we should fix this., a comment about {{testDecommissionWithOpenFileAndDatanodeFailing}}.

{noformat}
678	    // Kill one of the datanodes of the last block
679	    getCluster().stopDataNode(lastBlockLocations[0].getName());
{noformat}

I think this is misleading and makes test time unnecessary long. If my understanding is correct, the issue is reproduced only if nodes are in decommissioning state while trying to complete the last block.

How about make nodes decommissioning first then invoke lease recovery? like

{noformat}
    // Decommission all nodes of the last block
    ArrayList<String> toDecom = new ArrayList<>();
    for (DatanodeInfo dnDecom : lastBlockLocations) {
      toDecom.add(dnDecom.getXferAddr());
    }
    initExcludeHosts(toDecom);
    refreshNodes(0);

    // Make sure hard lease expires
    getCluster().setLeasePeriod(300L, 300L);
    Thread.sleep(2 * BLOCKREPORT_INTERVAL_MSEC);

    for (DatanodeInfo dnDecom : lastBlockLocations) {
      DatanodeInfo datanode = NameNodeAdapter.getDatanode(
          getCluster().getNamesystem(), dnDecom);
      waitNodeState(datanode, AdminStates.DECOMMISSIONED);
    }
{noformat}

Stopping the datanode causes connection failure to the dead node and retry on replica recovery and just makes it highly probable that nodes are in decommissioning state before the last block is completed.
, also we don't need to set DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY  if the test does not depend on retrying behavior., That looks good [~iwasakims], the test you suggested fails without the change to {{BlockManager.commitOrCompleteLastBlock}} and finishes quicker with the change in place. 
Please see the new patch I've attached, with the {{DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY}} removed as well., | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 23s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 19m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 45s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 36s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 52s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 42s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 33s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 4 new + 146 unchanged - 0 fixed = 150 total (was 146) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 10s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 85m 17s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}116m 28s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HDFS-11499 |
| GITHUB PR | https://github.com/apache/hadoop/pull/199 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux e354c5a9ece8 3.13.0-107-generic #154-Ubuntu SMP Tue Dec 20 09:57:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 5addacb |
| Default Java | 1.8.0_121 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/18643/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/18643/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/18643/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/18643/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, The failing tests seem unrelated to the patch., The unit tests in version 05 of the patch seem cleaner. LGTM., +1 on 05, will commit it shortly., SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #11379 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/11379/])
HDFS-11499. Decommissioning stuck because of failing recovery. (iwasakims: rev 385d2cb777a0272ac20c62336c944fad295d5d12)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMaintenanceState.java
, Committed to branch-2 and trunk. Thanks to all for the contribution., Thanks for digging through the test failures and fix [~lukmajercak] and review, commit help [~iwasakims]., I think we should backport the fix into 2.7 and 2.8. It fixes a bug that makes file close() to fail. Administrators may think it suffer from corruption, because file recovery will also fail.

Here's a branch-2.8 patch., Reopen the issue to submit the branch-2.8 patch, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  7s{color} | {color:red} HDFS-11499 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDFS-11499 |
| GITHUB PR | https://github.com/apache/hadoop/pull/199 |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/18675/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, Branch-2.7 patch, | (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  8s{color} | {color:red} HDFS-11499 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDFS-11499 |
| GITHUB PR | https://github.com/apache/hadoop/pull/199 |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/18676/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

, The precommit build failed because it attempted to run the patch from github PR.
Not sure how to force it to take the patch from here, but the branch-2.8 and branch-2.7 patches are straightforward with minor conflicts., I'm +1 if it's minor conflicts. Precommit won't run against a patch once there's a github PR., Thanks for the review, [~andrew.wang]. Pushed the commit into branch-2.7 and branch-2.8., 2.8.1 became a security release. Moving fix-version to 2.8.2 after the fact.]