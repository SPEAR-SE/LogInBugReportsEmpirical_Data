[the probable cause of the bug is that when an unauthorized user attempts to access a file/dir, the namenode throws an authorizationexception, which is eventually caught by nfs server's rpc handler. The rpc handler simply returns an ACCESS3Response object with empty file attributes and non NFS3_OK status which violates the protocol between nfs server and client, causing the client to hang, Looks like the client attribute cache is massed up by the returned file attributes in the response. Remount can remove the stale handle issue, but we should fix the bug too.
[~zhongyi-altiscale], which platform were you running the test? , [~brandon] The test was run on centos 6.3 and 6.5. We saw the same issue on both., Here is what happened:
1. supported user mounted the export, say at /my-mount-point. Linux NFS client keeps the related file attributes in cache.
2. non-supported user tried to access the export starting with an ACCESS call. As [~zhongyi-altiscale] noticed,  NFS gateway returned an empty file attributes for /my-mount-point as part of the ACCESS response when the request failed. 
3. Linux NFS client replaced the cached file attribute with the empty one. Note, the file handle is all zero now.
4. when the supported user tried to access the mounted directory, Linux NFS client checked the cached file handle of /my-mount-point to see its all zeros and it returns stale-file-handle-error to caller. In this step, the Linux NFS client doesn't send request to NFS server. 

We have no control in what happens in step 4 since it's up to NFS client implementation. Some client implementation might be more tolerate.  For example, I feel MacOS NFS client ignores the file attributes if the call failed (didn't look into its implementation so I could be wrong).

However, we should fix step 3 by not sending the empty attribute when the call fails. This is compatible with NFSv3 protocol and could make some NFS clients happy. 

I will post a patch soon., [~zhongyi-altiscale] and [~ashahab], I've upload a patch for this issue. It worked for me in my tests. Please give it a try to see if it fixes the problem in your environment., Hi [~brandonli], I tried your patch on our 2.2 branch but looks like the problem persists: 
[root@alexie-dt ~]# ls /hdfs
backups  hive  mr-history  system  tmp  user
[root@alexie-dt ~]# su test-nfs 
[test-nfs@alexie-dt root]$ ls /hdfs
ls: cannot open directory /hdfs: Input/output error
[test-nfs@alexie-dt root]$ ls /hdfs
ls: cannot access /hdfs: Stale file handle
[test-nfs@alexie-dt root]$ ls /hdfs
ls: cannot access /hdfs: Stale file handle
[test-nfs@alexie-dt root]$ exit
exit
[root@alexie-dt ~]# ls /hdfs
ls: cannot access /hdfs: Stale file handle
[root@alexie-dt ~]# ls /hdfs
ls: cannot access /hdfs: Stale file handle

I've uploaded my 2.2 patch, can you please take a look and try it in your env? 
Also in your tests, I assume you are seeing 
"ls: cannot open directory /hdfs: Input/output error" and the NFS handle is still responding, right?
I believe "Permission denied" is more appropriate here, what do you think?
, That's interesting. Here is what I see on a CentOS6 VM after applying the patch.
{noformat}
[root@host2 brandon]# mount -t nfs -o vers=3,proto=tcp,nolock,soft,noacl 10.11.3.67:/ tmp
[root@host2 brandon]# ls tmp
forall
[root@host2 brandon]# su brandon
[brandon@host2 ~]$ ls tmp
ls: cannot open directory tmp: Input/output error
[brandon@host2 ~]$ exit
exit
[root@host2 brandon]# ls tmp
forall
[root@host2 brandon]# 
{noformat}

[~zhongyi-altiscale], could you collect and upload the network trace so we get more details?
, tcpdump on altiscale's 2.2 branch + Brandon's fix in ACCESS3Response
tcpdump command used:
tcpdump -i lo -w tcpdump-HDFS-6411-Brandon.out

the commands I used for test:
[root@alexie-dt hdfs]# ls /hdfs
backups  hive  mr-history  system  tmp  user
[root@alexie-dt hdfs]# su test-nfs 
[test-nfs@alexie-dt hdfs]$ ls /hdfs
ls: cannot open directory /hdfs: Input/output error
[test-nfs@alexie-dt hdfs]$ ls /hdfs
ls: cannot access /hdfs: Stale file handle
[test-nfs@alexie-dt hdfs]$ exit
exit
[root@alexie-dt hdfs]# ls /hdfs
ls: cannot access /hdfs: Stale file handle, [~zhongyi-altiscale], I looked the trace you uploaded. According to the trace, the patch was in the build you tested.
Maybe the test didn't take the new jar file?, Thanks Brandon, after updating the hadoop-nfs-2.2.0.jar the problem is now resolved. 
but do you think the error message
"ls: cannot open directory /hdfs: Permission denied" 
is better than the current message
"ls: cannot open directory /hdfs: Input/output error" in this case?
with your fix, we can do this easily by examining the exception type and set the status to NFS3ERR_ACCES, I agree that "Permission denied" is a better message than "Input/output error". I've uploaded a new patch which returns NFS3ERR_PERM. [~zhongyi-altiscale], please let me know if the new patch works as expected in your environment., Hi Brandon, 

I think you missed 
import org.apache.hadoop.ipc.RemoteException;
because the patch doesn’t compile and complains about unable to find symbol at line 494

Thanks!
-Zhongyi


, It compiled for me. This import statement is already in RpcProgramNfs3.java and the patch doesn't change that.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12646622/HDFS-6411.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs-nfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6962//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6962//console

This message is automatically generated., I guess it was added between 2.2 and trunk, sorry for the false alarm
the message is different though:I got 
ls: cannot open directory /hdfs: Operation not permitted
I think "ls: cannot open directory /hdfs: Permission denied" maps to NFS3ERR_ACCES, {quote}I guess it was added between 2.2 and trunk{quote}
sorry, I forgot you are using 2.2.

{quote}ls: cannot open directory /hdfs: Permission denied" maps to NFS3ERR_ACCES{quote}
The "Permission denied" is from the error status NFS3ERR_PERM., maybe it got changed as well between versions but after applying the new patch, I got "Operation not permitted" error
and in my earlier tests using my 2.2 patch, I got "Permission denied" error msg, At protocol level, we don't have much control about how error status is translated into user-readable error message. The two error messages both look ok to me, at lease better than "..i/o error" :-). , below are the definition in Nfs3Status.java

  /**
   * The operation was not allowed because the caller is either not a
   * privileged user (root) or not the owner of the target of the operation.
   */
  public final static int NFS3ERR_PERM = 1;

  /**
   * Permission denied. The caller does not have the correct permission to
   * perform the requested operation. Contrast this with NFS3ERR_PERM, which
   * restricts itself to owner or privileged user permission failures.
   */
  public final static int NFS3ERR_ACCES = 13;

looks like the first one is more of the user privilege error but in our case, 
it's checking the user name against the proxy user/group list, 
we don't have the standard privilege checking on the namenode, 
which leads me to think NFS3ERR_ACCES is a better choice here, 

but like you said, this is more of a cosmetic thing, thanks!, NFS3ERR_ACCES was originally used and the shell gives error message "i/o error" which is misleading.

The caller application may not respect the exact meaning of NFS error status. Looks like NFS3ERR_PERM could trigger a better error message to end user. 

, 

actually the original error code was NFS3ERR_IO, which maps to “i/o error”
NSF3ERR_PERM maps to "Operation not permitted”
and NFS3ERR_ACCES maps to “Permission denied”
I guess in terms of language the two messages sound like synonyms  to each other, 
I was just wondering if there is any implicit meanings in the UNIX context between the two. If there is no such a thing then either is OK



, In general, the messages do have sort of specific meanings from an operations perspective:

"Permission denied" = "I might be able to chown or chmod this object and then get access."

"Operation not permitted" = "There is a system setting or control in the way such that even chown or chmod isn't going to fix this access problem."

If you reference the relevant RFCs (http://tools.ietf.org/html/rfc1813, specifically) , you'll note that it says that "NFS3ERR_PERM, which restricts itself to owner or privileged user permission failures."

, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12646622/HDFS-6411.002.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs-nfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6968//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6968//console

This message is automatically generated., Thank you, [~aw] and [~zhongyi-altiscale]. 
The error massage printed by shell is always "permission denied" in my tests with either NFS3ERR_ACCES or NFS3ERR_PERM.   

Regardless, I agree that NFS3ERR_ACCES is a better error status than NFS3ERR_PERM in this case. I've updated a new patch for the update. 

, [~brandonli], can you please also add an "else" clause in the getattr function, like you did in access, just in case the unwrapped exception happens to be something other than AuthorizationException? 
there is another related issue with fsstat where NFS3ERR_IO could also be replaced with NFS3_ACCESS, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12646945/HDFS-6411.003.patch
  against trunk revision .

    {color:red}-1 patch{color}.  Trunk compilation may be broken.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6984//console

This message is automatically generated., Uploaded the patch to address Zhongyi's comments. Thanks!, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12646945/HDFS-6411.003.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs-nfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6982//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6982//console

This message is automatically generated., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12646956/HDFS-6411.004.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-nfs hadoop-hdfs-project/hadoop-hdfs-nfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/6985//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/6985//console

This message is automatically generated., [~zhongyi-altiscale], how does the new patch look?, [~brandonli], it looks good, but I haven't got a chance to test it out since my VM is broken today, will run the test cases once my VM is back to normal, will let you know by then, thanks!, Hi [~brandonli], I've tested it on my VM and looks like the problem is fixed

However I did see something interesting
[alti-test-02@alexie-dt root]$ mkdir /hdfs/tmp/dir
mkdir: cannot create directory `/hdfs/tmp/dir': Permission denied
[alti-test-02@alexie-dt root]$ rmdir /hdfs/tmp
rmdir: failed to remove `/hdfs/tmp': Permission denied
[alti-test-02@alexie-dt root]$ rmdir /hdfs/
rmdir: failed to remove `/hdfs/': Permission denied
[alti-test-02@alexie-dt root]$ ls /hdfs
ls: cannot access /hdfs: Stale file handle

but once I log out of alti-test-02 user back to root, the NFS handle is still working
[root@alexie-dt ~]# ls /hdfs
backups  hive  mr-history  system  tmp  user

and I retried these steps, the problem goes away (i.e. I didn't see the Stale file handle again), 
so unless there is some consistent repro steps and if the handle is hang again, 
I won't worry about that, The current patch looks good to me. +1

One issue of the current code is that we may also want to catch the AccessControlException from the HDFS calls, and return NFS3ERR_PERM instead of NFS3ERR_IO for it. But I guess we can do it in a separate jira.

Another possible future improvement is that we can have a single class/method for the common exception handling process, instead of repeating the same exception handling process in different NFS methods., Thank you, [~jingzhao] for the review. I've filed HDFS-6451 to track the improvement you described., Hi [~jingzhao], it's definitely good to have a single exception handler instead of replicating the same code everywhere, but since each server procedure (ACCESS, GETATTR, FSSTAT, etc) might have their private data that needs to be written out, the child NFS3Response class still need to overload the writeHeaderAndResponse anyways
for AccessControlException, do you mean we need to catch it together with AuthorizationException in RpcProgramNfs3.java? 
or do you mean we need to examine the whole codebase looking for every function that could potentially throw AccessControlException, 
and make sure the error code is set correctly in the catch clause?, Thank you, guys. I've committed the patch.
Let's move further discussions for the code optimization to HDFS-6451., SUCCESS: Integrated in Hadoop-trunk-Commit #5613 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/5613/])
HDFS-6411. nfs-hdfs-gateway mount raises I/O error and hangs when a unauthorized user attempts to access it. Contributed by Brandon Li (brandonli: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1597895)
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/ACCESS3Response.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk #566 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/566/])
HDFS-6411. nfs-hdfs-gateway mount raises I/O error and hangs when a unauthorized user attempts to access it. Contributed by Brandon Li (brandonli: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1597895)
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/ACCESS3Response.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1757 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1757/])
HDFS-6411. nfs-hdfs-gateway mount raises I/O error and hangs when a unauthorized user attempts to access it. Contributed by Brandon Li (brandonli: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1597895)
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/ACCESS3Response.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1784 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1784/])
HDFS-6411. nfs-hdfs-gateway mount raises I/O error and hangs when a unauthorized user attempts to access it. Contributed by Brandon Li (brandonli: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1597895)
* /hadoop/common/trunk/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/nfs3/response/ACCESS3Response.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
]