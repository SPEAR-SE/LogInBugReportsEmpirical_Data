[Here is the grepped logs with the block:

11:35:02,926 INFO  hdfs.StateChange (FSNamesystem.java:addStoredBlock(3912)) - BLOCK* NameSystem.addStoredBlock: blockMap updated: xx.xx.xx.55:50086 is added to blk_1332906029734_1719 size 30
11:37:00,818 INFO  hdfs.StateChange (FSNamesystem.java:processReport(3773)) - BLOCK* NameSystem.processReport: block blk_1332906029734_1490 on xx.xx.xx.55:50086 size 30 does not belong to any file.
11:37:00,818 INFO  hdfs.StateChange (FSNamesystem.java:addToInvalidates(2002)) - BLOCK* NameSystem.addToInvalidates: blk_1332906029734 to xx.xx.xx.55:50086
11:37:02,777 INFO  hdfs.StateChange (FSNamesystem.java:invalidateWorkForOneNode(3459)) - BLOCK* ask xx.xx.xx.55:50086 to delete  blk_1332906029758_1514 blk_1332906029734_1490 blk_1332906029745_1501 blk_1332906029703_1459 blk_1332906029746_1502 blk_1332906029704_1460 blk_1332906029693_1449 blk_1332906029761_1517
12:36:59,865 INFO  hdfs.StateChange (FSNamesystem.java:computeReplicationWorkForBlock(3321)) - BLOCK* ask xx.xx.xx.102:50086 to replicate blk_1332906029734_1719 to datanode(s) xx.xx.xx.102:50010
12:37:01,416 INFO  hdfs.StateChange (FSNamesystem.java:addStoredBlock(3912)) - BLOCK* NameSystem.addStoredBlock: blockMap updated: xx.xx.xx.102:50010 is added to blk_1332906029734_1719 size 30
14:30:15,537 INFO  hdfs.StateChange (CorruptReplicasMap.java:addToCorruptReplicasMap(55)) - BLOCK NameSystem.addToCorruptReplicasMap: blk_1332906029734 added as corrupt on xx.xx.xx.55:50086 by /xx.xx.xx.55
14:30:15,537 INFO  hdfs.StateChange (FSNamesystem.java:invalidateBlock(2125)) - DIR* NameSystem.invalidateBlock: blk_1332906029734_1719 on xx.xx.xx.55:50086
14:30:15,537 INFO  hdfs.StateChange (FSNamesystem.java:addToInvalidates(2002)) - BLOCK* NameSystem.addToInvalidates: blk_1332906029734 to xx.xx.xx.55:50086
14:30:18,156 INFO  hdfs.StateChange (FSNamesystem.java:invalidateWorkForOneNode(3459)) - BLOCK* ask xx.xx.xx.55:50086 to delete  blk_1332906029734_1719
14:38:47,685 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1
14:44:34,542 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1
14:44:46,937 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1
14:45:15,794 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1
14:45:37,893 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1
14:53:43,656 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1
14:57:31,448 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1
15:17:22,642 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1
15:21:20,961 WARN  namenode.FSNamesystem (FSNamesystem.java:getBlockLocationsInternal(1119)) - Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1, Do you use append on 1.0? It would cause such a thing. Refrain from using append() on the 1.0/0.20-append release as its buggy in some ways.

Although thanks for opening this. We can investigate (but problem may already be fixed in 0.23+ HDFS), I don't this problem because of append usage.

Looks like this a race between markBlockAsCorrupt and processOverReplicatedBlocks.

1) NN detects over replicated block and added to invalidates list for DNn.
2) Before processing invalidates list, BlockScanner found that block corrupted in DNn and reported to NN. 
3) Before acquiring lock, Invalidates list got processed and removed the block from blocksMap from DNn.
4) Now markBlockAsCorrupt started processing. 
   {code}
 // Add this replica to corruptReplicas Map 
      corruptReplicas.addToCorruptReplicasMap(storedBlockInfo, node);
      if (countNodes(storedBlockInfo).liveReplicas()>inode.getReplication()) {
        // the block is over-replicated so invalidate the replicas immediately
        invalidateBlock(storedBlockInfo, node);
      } else {
        // add the block to neededReplication 
        updateNeededReplications(storedBlockInfo, -1, 0);
      }
{code}
 since it found the enough replication and invalidateBlock. It will try to remove the storedBlock if line Replicas are more than one.
 This call will just return, because it was already removed blocksMap.

But it was already added to corruptReplicas Map(shown in the above peice of code).

So, now the counts of corruptReplicas map and blockMap are different about corrupt replicas.

Mostly this exists only on branch-1. 

I think this problem already addressed in Trunk.

code from trunk.
{code}
// Add replica to the data-node if it is not already there
    node.addBlock(storedBlock);

    // Add this replica to corruptReplicas Map
    corruptReplicas.addToCorruptReplicasMap(storedBlock, node, reason);
    if (countNodes(storedBlock).liveReplicas() >= inode.getReplication()) {
      // the block is over-replicated so invalidate the replicas immediately
      invalidateBlock(storedBlock, node);
    } 
{code}

see the first line above. If the block is not already there it is adding to it. I think this should have solve the problem in trunk., I don't think this problem because of append usage.

Looks like this is a race between markBlockAsCorrupt and processOverReplicatedBlocks.

1) NN detects over replicated block and added to invalidates list for DNn.
2) Before processing invalidates list, BlockScanner found that block corrupted in DNn and reported to NN.
3) Before acquiring lock, Invalidates list got processed and removed the block from blocksMap for DNn.
4) Now markBlockAsCorrupt started processing.

// Add this replica to corruptReplicas Map 
      corruptReplicas.addToCorruptReplicasMap(storedBlockInfo, node);
      if (countNodes(storedBlockInfo).liveReplicas()>inode.getReplication()) {
        // the block is over-replicated so invalidate the replicas immediately
        invalidateBlock(storedBlockInfo, node);
      } else {
        // add the block to neededReplication 
        updateNeededReplications(storedBlockInfo, -1, 0);
      }

since it found the enough replication and invalidateBlock. It will try to remove the storedBlock if line Replicas are more than one.
This call will just return, because it was already removed blocksMap.

But it was already added to corruptReplicas Map(shown in the above peice of code).

So, now the counts of corruptReplicas map and blockMap are different about corrupt replicas.

Mostly this issue exists only on branch-1.

I think this problem already addressed in Trunk.

code from trunk.

// Add replica to the data-node if it is not already there
    node.addBlock(storedBlock);

    // Add this replica to corruptReplicas Map
    corruptReplicas.addToCorruptReplicasMap(storedBlock, node, reason);
    if (countNodes(storedBlock).liveReplicas() >= inode.getReplication()) {
      // the block is over-replicated so invalidate the replicas immediately
      invalidateBlock(storedBlock, node);
    }

see the first line above. If the block is not already there, it is adding to it. I think this should have solved the problem in trunk.
, No patch submitted in 1.0.3 time frame.  Please consider contributing in 1.1 context (branch-1).  Thank you., Uma,

Excellent analysis, thanks! That line was added by https://issues.apache.org/jira/browse/HDFS-265 btw, which does indeed make it related to the old append as well?, Yep, 

Actaully Suja has got this issue in our internal branch based on 20.x version. There was one merge in that HDFS-2290. That is added below change
liveReplicas() > inode.getReplication()) ---> liveReplicas() >= inode.getReplication())

And node addition was not there in older versions. 
node.addBlock(storedBlock);

So, This issue will not be there in any of the apache versions. We have corrected it intenally as well.

I will close this issue. If any one facing it, feel free to reopen.

]