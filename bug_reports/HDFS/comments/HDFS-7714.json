[Here are more details on what I've observed.  I saw that the main {{BPServiceActor#run}} loop was active for one NameNode, but for the other one, it had reported the fatal "Initialization failed" error from this part of the code:

{code}
      while (true) {
        // init stuff
        try {
          // setup storage
          connectToNNAndHandshake();
          break;
        } catch (IOException ioe) {
          // Initial handshake, storage recovery or registration failed
          runningState = RunningState.INIT_FAILED;
          if (shouldRetryInit()) {
            // Retry until all namenode's of BPOS failed initialization
            LOG.error("Initialization failed for " + this + " "
                + ioe.getLocalizedMessage());
            sleepAndLogInterrupts(5000, "initializing");
          } else {
            runningState = RunningState.FAILED;
            LOG.fatal("Initialization failed for " + this + ". Exiting. ", ioe);
            return;
          }
        }
      }
{code}

The {{ioe}} was an {{EOFException}} while trying the {{registerDatanode}} RPC.  Lining up timestamps from NN and DN logs, I could see that the NN had restarted at the same time, causing it to abandon this RPC connection, ultimately triggering the {{EOFException}} on the DataNode side.

Most importantly, the fact that it was on the code path with the fatal-level logging means that it would never reattempt registration with this NameNode.  {{shouldRetryInit()}} must have returned {{false}}.  The implementation of {{BPOfferService#shouldRetryInit}} is that it should only retry if the other one already registered successfully:

{code}
  /*
   * Let the actor retry for initialization until all namenodes of cluster have
   * failed.
   */
  boolean shouldRetryInit() {
    if (hasBlockPoolId()) {
      // One of the namenode registered successfully. lets continue retry for
      // other.
      return true;
    }
    return isAlive();
  }
{code}

Tying that all together, this bug happens when the first attempted NameNode registration fails but the second succeeds.  The DataNode process remains running, but with only one live {{BPServiceActor}}.

HDFS-2882 had a lot of discussion of DataNode startup failure scenarios.  I think the summary of that discussion is that the DataNode should in general retry its NameNode registrations, but it should instead abort right away if there is no possibility for registration to be successful.  (i.e. There is a misconfiguration or a hardware failure.)  I think the change we need here is that we should keep retrying the {{registerDatanode}} RPC if there is NameNode downtime or transient connectivity failure.  Other failure reasons should still cause an abort.
, On a related note, I've seen similar symproms when the two namenodes' ctimes in their storage are different. After a datanode registers with one nn, it won't be able to register with the other and cause the actor thread to die. Depending on whom each datanode talk to first, they will be divided into two sets, each of which talking to only one namenode, thus creating a split brain situation.  Of course, running two namenodes with different storage version is a mistake, but I've seen people making this kind of mistake multiple times. Whenever it happened, I wished for a way to start the actor thread back up. The refreshNamenodes dfs admin command does not work for HA configuration., Attaching the patch, handling the EOFException while in {{register()}}.
This is the only place where EOFException during RPC to NameNode would make DN to stop trying to connect to NN.

Also {{isAlive()}} method of {{BPServiceActor}} modified.

Ideally any one of the above would solve the issue.

Please review., Thanks [~cnauroth] for linking the issue back to HDFS-2882. and analyzing it., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12695806/HDFS-7714-001.patch
  against trunk revision 3472e3b.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.

        {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations
                  org.apache.hadoop.hdfs.TestDFSRollback
                  org.apache.hadoop.hdfs.TestDFSUpgrade
                  org.apache.hadoop.hdfs.server.datanode.TestBPOfferService
                  org.apache.hadoop.hdfs.TestRollingUpgrade
                  org.apache.hadoop.net.TestNetworkTopology
                  org.apache.hadoop.hdfs.TestDFSStartupVersions

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9394//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9394//artifact/patchprocess/patchReleaseAuditProblems.txt
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9394//console

This message is automatically generated., Thank you for taking this issue, Vinay.  I think catching {{EOFException}} is a fine approach.  Changing the {{isAlive}} logic has some other side effects, and I believe that's what caused the test failures in the last Jenkins run., Thanks [~cnauroth].
I can see that only {{TestBPOfferService.testNNsFromDifferentClusters}} was failed due to this change.
Others seems to mess-up with some other cluster

Anyway, removed isAlive() changes. Kept only EOFException approach., +1 for patch v002, pending Jenkins.  Thanks again, Vinay., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12697397/HDFS-7714-002.patch
  against trunk revision af08425.

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/9504//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/9504//artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9504//console

This message is automatically generated., Findbug is not related to current patch., Thanks [~cnauroth] for the review.
Committed to trunk and branch-2, FAILURE: Integrated in Hadoop-trunk-Commit #7059 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7059/])
HDFS-7714. Simultaneous restart of HA NameNodes and DataNode can cause DataNode to register successfully with only one NameNode.(Contributed by Vinayakumar B) (vinayakumarb: rev 3d15728ff5301296801e541d9b23bd1687c4adad)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #100 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/100/])
HDFS-7714. Simultaneous restart of HA NameNodes and DataNode can cause DataNode to register successfully with only one NameNode.(Contributed by Vinayakumar B) (vinayakumarb: rev 3d15728ff5301296801e541d9b23bd1687c4adad)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
, FAILURE: Integrated in Hadoop-Yarn-trunk #834 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/834/])
HDFS-7714. Simultaneous restart of HA NameNodes and DataNode can cause DataNode to register successfully with only one NameNode.(Contributed by Vinayakumar B) (vinayakumarb: rev 3d15728ff5301296801e541d9b23bd1687c4adad)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Hdfs-trunk #2032 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2032/])
HDFS-7714. Simultaneous restart of HA NameNodes and DataNode can cause DataNode to register successfully with only one NameNode.(Contributed by Vinayakumar B) (vinayakumarb: rev 3d15728ff5301296801e541d9b23bd1687c4adad)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #101 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/101/])
HDFS-7714. Simultaneous restart of HA NameNodes and DataNode can cause DataNode to register successfully with only one NameNode.(Contributed by Vinayakumar B) (vinayakumarb: rev 3d15728ff5301296801e541d9b23bd1687c4adad)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #2051 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2051/])
HDFS-7714. Simultaneous restart of HA NameNodes and DataNode can cause DataNode to register successfully with only one NameNode.(Contributed by Vinayakumar B) (vinayakumarb: rev 3d15728ff5301296801e541d9b23bd1687c4adad)
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
, SUCCESS: Integrated in Hadoop-Hdfs-trunk-Java8 #97 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/97/])
HDFS-7714. Simultaneous restart of HA NameNodes and DataNode can cause DataNode to register successfully with only one NameNode.(Contributed by Vinayakumar B) (vinayakumarb: rev 3d15728ff5301296801e541d9b23bd1687c4adad)
* hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
* hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
, [~sjlee0] backported this to 2.6.1. I just pushed the commit to 2.6.1 after running compilation. The patch applied cleanly.]