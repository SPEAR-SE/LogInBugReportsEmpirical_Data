[Upload the first patch., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12604223/HDFS-5237.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core:

                  org.apache.hadoop.hdfs.web.TestFSMainOperationsWebHdfs
                  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestInterDatanodeProtocol
                  org.apache.hadoop.hdfs.security.TestDelegationTokenForProxyUser
                  org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement
                  org.apache.hadoop.hdfs.server.namenode.TestAuditLogs
                  org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes
                  org.apache.hadoop.cli.TestHDFSCLI
                  org.apache.hadoop.hdfs.TestFileCreation
                  org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks
                  org.apache.hadoop.hdfs.TestHftpFileSystem
                  org.apache.hadoop.net.TestNetworkTopology
                  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup
                  org.apache.hadoop.hdfs.TestReplication
                  org.apache.hadoop.hdfs.server.namenode.TestHostsFiles
                  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract
                  org.apache.hadoop.hdfs.server.blockmanagement.TestReplicationPolicy
                  org.apache.hadoop.hdfs.web.TestWebHDFS
                  org.apache.hadoop.hdfs.TestDistributedFileSystem

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5010//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5010//console

This message is automatically generated., I don't think this is a great idea.  Hadoop's not been great at separating out a few things:

* The bind addresses and ports for RPC and HTTP.  (Particularly whether to bind on 0.0.0.0 or on a single IP.)
* The way to identify this process to other people so that they can reach it
* How to use DNS to reach other folks

There are, I agree, very minimal defaults here.  Bind to the wildcard.  Identify as {{hostname -f}}.  Use default DNS settings.  Don't talk to DNS explicitly but use the normal OS ways to do it.

{{slave.host.name}} has, I think, made it possible to do some less frequent but still important configurations.  I see this most often as a host having 2 names ("management-name" and "data-name") where "management-name" is the one that's hostname.  I've seen it in use in real configurations (not just in tests) where it did the trick.

Meanwhile, {{DNS.getHosts(String strInterface, String nameserver)}} is a very odd way to configure the hostname that a process should identify as.  It says, "hey, use this other system and do a reverse lookup for yourself with these parameters."  Most of the time, though, when you're in this case, you're already generating your configuration from something that knows exactly what IP/name you want, and it would be simpler and less error-prone to just specify it.

Cheers,

Philip
, Hi Phillip,

I think the idea behind this JIRA is to get rid of DataNodes using totally bogus strings to identify themselves to the NameNode.  This creates a lot of problems for us in the NameNode code because we can't rely on the datanode registration name to be an actual hostname.  We also end up tracking the "real hostname" alongside the "registration name", which is highly confusing to people reading the code.  If {{dfs.datanode.hostname}} was re-purposed to just select which hostname the DN wanted to use, out of several available choices which all resolved to the DN, I think we would all be find with that.  This should be just as simple as just adding a check that the given string actually resolves to the IP address the DN registration was sent on., Colin: I agree--totally reasonable for dfs.datanode.hostname to be a hostname.  (Or, at least, something that you can connect to (ip or hostname).), Agree it's reasonable to validate dfs.datanode.hostname, but Junping's patch removes this config option which will break some configurations that use HDFS-3150 and need to set the reported hostname to something other than the default (for example if getDefaultHost returns a hostname on network A and the clients want to talk to the DN via network B).  Shall we re-purpose this to add dfs.datanode.hostname validation?, Hi Eli and Philip, I think another existing config DFS_DATANODE_DNS_INTERFACE_KEY or "dfs.datanode.dns.interface" already plays the role to select which hostname the DN wanted to use (in DataNode.getHostName(conf)). So it may not be necessary for "dfs.datanode.hostname" to re-purpose for this. Isn't it? 
Per discussion between Colin and me in HDFS-5208, we try to find some alternative ways to get rid of registration name which could be specified for any name by user and proposed for tests with MiniDFSCluster to start with multiple faked nodes before. The simply removal of this config is not working as attached demo patch shows - all MiniDFSCluster related tests are failed as there is no way to fake nodes now. I guess we should keep this config for unit tests only or identify some new ways to achieve fake nodes with removing it. Thoughts?, I'm not so sure that we need to remove this, now.  If it's heavily used in unit tests, and also used by end-users in certain configurations, it's going to be an uphill battle.

Validating it *sounds* like a good idea, but in practice it's going to add up to failing DN registrations if the reported hostname can't be resolved, which doesn't seem that friendly.  DNS is sometimes unreliable like any other network service, after all.

Why don't we add some JavaDoc to DatanodeID explaining the reasons why hostName may in fact not be a real host name?

I also wonder why we need {{DatanodeID#peerHostName}}.  It doesn't seem to be used anywhere, and it definitely muddies the waters in that class.]