[Make sense. There might be some operational impact with disabling DecommissionManager on standby. admins usually update dfs.namenode.hosts.exclude and then call "dfsadmin -refreshNodes" on both active and standby around the same time; in that way if NN fails over, decomm can continue. If DecommissionManager isn't running on standby, nodes will stay in decommission_inprogress state without any progress on standby. As long as admins know to ignore decommission state on standby, that should be ok (even if we keep DecommissionManager running, decommission states between active and standby could be different at any given time)., Related question to Ming's, this means node state will could flip back to decom in progress from decommissioned on failover right? I guess this can happen anyway, but is less likely in the current world.

I'm also curious if this is ameliorated by the decom manager rewrite in HDFS-7411, since in-progress scans are incremental. We could also invest some more effort making it fully incremental if that would help., The change we used is just skipping the costly scan while {{isPopulatingReplQueues}}  is false.  The node state may effectively flip back to decomm-ing on failover, but it won't take long for the decom manager to mark all the decomm-ing nodes as decomm-ed.  I think this is the correct behavior.

In the active's "current" moment, which is the standby's "future" when it has queued IBRs, the active may consider the node still decomm-ing.  It would be bad for the standby to decide it's decomm-ed based on stale info.  Post-failover, the standby will be unable to correct under-replication because decomm-ed nodes are not a valid replication source.]