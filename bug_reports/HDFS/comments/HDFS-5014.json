[Attaching a draft patch, which avoids synchronization, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12593344/HDFS-5014.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4732//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/4732//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4732//console

This message is automatically generated., Attached updated patch to avoid findbug, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12594357/HDFS-5014.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4736//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4736//console

This message is automatically generated., Hi, [~vinayrpet].

Before this patch, both {{BPOfferService#processCommandFromActor}} and {{BPOfferService#updateActorStatesFromHeartbeat}} were synchronized.  This meant that one {{BPServiceActor}} could prevent running commands from the other if the heartbeat response from its NN indicated that it had become the new active.

With this patch, we could have a command in progress, and the other {{BPServiceActor}} takes over as active, but the change isn't visible.  (Basically, one thread won't see the other thread's update of {{BPOfferService#bpServiceToActive}}, and I don't think the addition of {{volatile}} helps.)  If that command is a block invalidate, then we could lose a block erroneously.

I definitely can see how the synchronization could cause the problem you described though.  I don't yet have a suggestion for a different solution, but wanted to mention that the synchronization really does appear to be necessary.

I'm linking this to HDFS-2627, which introduced the synchronization., Thanks Chris for your inputs. I have re-looked the solution and came up with a solution using read-write lock.
Please have a look on this. , Updated patch with read/write lock mechanism instead of synchronization, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12599608/HDFS-5014.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4871//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/4871//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4871//console

This message is automatically generated., bq. Updated patch with read/write lock mechanism instead of synchronization

Nice work!  I think the basic approach is going to work.  However, I think {{updateActorStatesFromHeartbeat}} needs to hold the write lock for the entire method.  Otherwise, we could get some unfortunate interleavings of state in {{bpServiceToActive}}, {{bposThinksActive}}, {{lastActiveClaimTxId}}, and {{isMoreRecentClaim}}.

bq. -1 findbugs. The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

This is somewhat spurious, but your workaround from the earlier patch would still work to silence these warnings.
, {quote}However, I think updateActorStatesFromHeartbeat needs to hold the write lock for the entire method. 
{quote}
Actually if we try holding writeLock() throughout method, then that will be same as adding method level synchronization. where we would get the same issue as this jira.
{quote}we could get some unfortunate interleavings of state in bpServiceToActive, bposThinksActive, lastActiveClaimTxId, and isMoreRecentClaim.{quote}
I dint understand how we could get 'some unfortunate interleavings'. Can you please explain.. I thought since we are reading under lock, we would get correct state always.


Regarding findbugs, I will try to fix it., Adding the Updated patch

1. Covered isMoreRecentClaim and lastActiveClaimTxId under locks

2. covered bpNsInfo under synchronization to avoid the findbugs,, I'm going to be mostly offline until 9/2.  I'm not +1 for the current patch, but I would be +1 if it was changed to hold the write lock for all of {{updateActorStatesFromHeartbeat}}.  I'd like to seek out a second opinion though, because concurrency is hard.  :-)  [~tlipcon], maybe you could take a look since you originally coded HDFS-2627?

Vinay, thank you for reporting the bug and working through the feedback on the patch.

bq. I dint understand how we could get 'some unfortunate interleavings'. Can you please explain.. I thought since we are reading under lock, we would get correct state always.

The challenge is that we are updating multiple pieces of data ({{bpServiceToActive}}, {{bposThinksActive}}, {{lastActiveClaimTxId}}, and {{isMoreRecentClaim}}), and the failover handling logic isn't correct unless those pieces of data are consistent with one another.  Without mutual exclusion, we can end up with {{bposThinksActive}} true even though {{bpServiceToActive}} has changed, or we could end up with {{isMoreRecentClaim}} true, even though a more recent transaction ID has been seen.

In the following examples, assume T1 and T2 are separate threads for separate {{BPServiceActor}} instances/separate name nodes.

Example 1: race on {{bposThinksActive}}

# T1 acquires read lock.
# T1 calculates {{bposThinksActive}} true.
# T1 releases read lock.
# The OS suspends T1 here.  (Note that T1 is holding neither read lock nor write lock at this point.)
# T2 wakes up, processes a heartbeat, and executes all of {{updateActorStatesFromHeartbeat}}.  It determines that T2 is the new active, so it updates {{bpServiceToActive}}.
# The OS suspends T2 here.
# T1 resumes.  It still thinks {{bposThinksActive}} is true, even though the other thread has changed {{bpServiceToActive}}.
# T1 now can fall into the block for {{else if (!nnClaimsActive && bposThinksActive)}} and set {{bpServiceToActive}} null.
# The OS suspends T1 and resumes T2.
# T2 now erroneously skips commands from the active NN, because {{bpServiceToActive}} is null.

Example 2: race on {{isMoreRecentClaim}}

# T1 acquires read lock.
# T1 calculates {{isMoreRecentClaim}} based on {{lastActiveClaimTxId}} 1 and {{txid}} 2 from the heartbeat.
# T1 releases read lock.
# The OS suspends T1 here.  (Note that T1 is holding neither read lock nor write lock at this point.)
# T2 wakes up, processes a heartbeat, and executes all of {{updateActorStatesFromHeartbeat}}.  It updates {{lastActiveClaimTxId}} to 3.
# The OS suspends T2 here.
# T1 resumes.  It still thinks {{isMoreRecentClaim}} is true, even though the other thread has changed {{lastActiveClaimTxId}}.
# T1 now can set {{bpServiceToActive}} to itself instead of going into the split-brain warning path.
# The OS suspends T1 and resumes T2.
# T2 now erroneously skips commands from the active NN, because {{bpServiceToActive}} was changed.

bq. Actually if we try holding writeLock() throughout method, then that will be same as adding method level synchronization. where we would get the same issue as this jira.

My understanding is that you had an actor get stuck in the re-registration loop when it got a {{DNA_REGISTER}} request from a flapping namenode.  Since {{processCommandFromActor}} was synchronized, this also blocked the actor thread for the healthy namenode from making progress.  Therefore, we can solve the problem by allowing concurrent executions of {{processCommandFromActor}}, so that the healthy actor can make progress even if the unhealthy actor gets stuck.  However, we don't need to allow concurrent execution of the heartbeat/HA failover processing (and I think it would be incorrect to do so based on the examples above).

Unlike {{processCommandFromActor}}, the {{updateActorStatesFromHeartbeat}} method only changes local in-memory state.  There is no polling loop or RPC involved, so there is no risk that one actor gets stuck inside this method and blocks the other.  I expect minimal lock contention on the write lock.
, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12599799/HDFS-5014.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4875//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4875//console

This message is automatically generated., Updated patch to solve the concurrency issues, illustrated by chris. Please review.
In both examples, adding a double check in case of state changes would solve the issue.

Removed the lock for processing standby commands to avoid the following case in prev patch.
1. Standby NN was unstable and asked DN to register.
2. DN acquired the readlock and started registering, but by this time Standby NN went down. So DN will keep trying in this point with readlock held.
3. Now any state changes from ANN, will block in getting writeLock().

I feel lock not required while processing commands from standby as no data modification commands will be processed here. Please correct me if I am wrong. :)
  , {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12599907/HDFS-5014.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/4888//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/4888//console

This message is automatically generated., Hi, [~vinayrpet].  I'm coming back to this one after a while.  The latest patch still has some concurrency problems:
* {{updateActorStatesFromHeartbeat}}: It's possible for state to change after releasing the read lock, but before the if statement executes.  The method would then execute logic assuming the old values of {{bpServiceToActive}} and {{lastActiveClaimTxId}}.
* {{processCommandFromActor}}: Even though the read lock is not held during {{processCommandFromStandby}}, it's still possible to have the same problem that you saw in your cluster, but on the active instead of the standby.  If the active requests re-registration of datanodes, and then immediately goes into a bad state or a network partition prevents communication, then datanodes will be stuck inside the re-register polling loop while holding the read lock.  This will prevent the other one from taking over as active, which requires holding the write lock.

I'm starting to think that we can't fix this bug by just tuning locks in {{BPOfferService}}.  Instead, I'm starting to think that we need to work out a way for the re-register polling loops to yield the lock in case of repeated failure, to give the other {{BPServicActor}} a chance.  If a {{BPServiceActor}} yields like this, then it must also have a way to trigger the other {{BPServiceActor}} to repeat its heartbeat *before executing any additional commands*.  It's vital to re-check current state of the other one before proceeding to handle its commands.
, {quote}It's possible for state to change after releasing the read lock, but before the if statement executes. The method would then execute logic assuming the old values of bpServiceToActive and lastActiveClaimTxId.{quote}
But in this case always double check is done if there are any changes in the current call. I dont think this will be a problem
{code:java}+        // double check of any state changes
+        if (bposThinksActive != (bpServiceToActive == actor)
+            || isMoreRecentClaim != (txid > lastActiveClaimTxId)) {
+          // don't update anything here, as another actor have updated the
+          // latest details
+          return;
         }{code}

{quote}processCommandFromActor: Even though the read lock is not held during processCommandFromStandby, it's still possible to have the same problem that you saw in your cluster, but on the active instead of the standby. If the active requests re-registration of datanodes, and then immediately goes into a bad state or a network partition prevents communication, then datanodes will be stuck inside the re-register polling loop while holding the read lock. This will prevent the other one from taking over as active, which requires holding the write lock.{quote}

Yes, I agree .. In extreme case this can happen. But chances of this will be rare when compare to current issue.

{quote}I'm starting to think that we can't fix this bug by just tuning locks in BPOfferService. Instead, I'm starting to think that we need to work out a way for the re-register polling loops to yield the lock in case of repeated failure, to give the other BPServicActor a chance. {quote}
I am attaching a patch for this. I hope this change alone will solve the current issue. But still I would like read/write locks to be in place which helps in allow faster processing of commands during normal cluster state.
, Attached the patch, please review., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12609790/HDFS-5014.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5258//console

This message is automatically generated., Fixed the build failure problem, {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12609819/HDFS-5014.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5261//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5261//console

This message is automatically generated., Hi, Any more update required on this patch..?, Hi [~vinayrpet].  I just wanted to send a quick note to let you know that this patch is still on my TODO list to review.  I haven't forgotten, but I do have some other priorities to get done first.  :-)

Meanwhile, I did want a second set of eyes reviewing this patch anyway, because I think it's a very tricky issue.  Is anyone else out there interested in catching up on the discussion and looking at the current patch?, Thanks Chris. 

Please can someone else too look at the patch. ? Thanks in advance., Thanks for working on this. I think there is no problem in allowing resignation commands alone concurrently right?
I am bit worrying on the above locking and putting double checks to handle specific cases of race conditions. Instead making code complex, how about allowing registaration commands allowing without lock and all other command should go under lock. ex: if command is register, then don't take lock and simply process it. else go with current flow by taking lock on BPService itself. (bit odd to process some commands out of switch, but this looks simpler to me as of now :-) )Do you guys seeing any concurrent issue in processing registrations commands concurrently?  [Correct me if did not follow you guys]

{quote}
 I'm starting to think that we need to work out a way for the re-register polling loops to yield the lock in case of repeated failure, to give the other BPServicActor a chance. If a BPServiceActor yields like this, then it must also have a way to trigger the other BPServiceActor to repeat its heartbeat before executing any additional commands. It's vital to re-check current state of the other one before proceeding to handle its commands.
{quote}
I am not sure what is your idea here. But providing solution with out spreading locks would be great I think. 
Good efforts., Thanks Uma for looking at the Jira.
I think it should work fine with BPOfferService#registrationSucceeded(..) synchronized.
Will post a patch soon after verifying it. , Attached HDFS-5014-v2.patch with the suggested approach.
registerting to namenode outside synchronization of BPOS, but updating to BPOS with synchronized {{BPOS#registrationSucceeded()}}, Please review and let me know your thoughts.. thanks, [~umamaheswararao], thank you for joining the review.

bq. I am not sure what is your idea here.

My last idea was to keep holding the lock during the register attempt, but then release the lock after there is a timeout.  IOW, don't hold the lock during the {{Thread#sleep}} time of {{BPServiceActor#register}}.

bq. how about allowing registaration commands allowing without lock and all other command should go under lock.

Great idea!  That's a much simpler version of what I was trying to achieve.

bq. I think it should work fine with BPOfferService#registrationSucceeded(..) synchronized.

Yes, I agree that {{BPOfferService#registrationSucceeded}} now needs to be synchronized.  [~vinayrpet], thanks for covering this in the most recent patch.

The new patch looks good.  Just one small thing: the log messages used to say whether it was processing a {{DNA_REGISTER}} request from the active or the standby.  With the patch, we lose that information, because the log message is the same regardless of which NN sent the command.  Can we restore active vs. standby in the log message?  That's potentially useful information for troubleshooting., Attaching updated patch for restoring the log.

I thought it would be sufficient to log the claiming state of the actor instead of locking again and checking the state.
Please review., +1 for the patch, pending successful Jenkins test-patch run.

Note that it's possible for the HA state to be null (e.g. if heartbeats are disabled for tests).  I think this is acceptable for the sake of logging.  It would just convert to the string "null".

[~umamaheswararao], any other feedback before this gets committed?  I plan to commit by end of day Tuesday, 11/19 (again assuming a good Jenkins run)., Looks like jenkins didnt detect the latest patch.
Attaching the same patch again to make it happy.., Please give me some time, I am busy with some meeting today(full day).
I would like to review this patch as well. If you are busy at that time, Please let me know I will commit this after my review. 
Thanks for the patch Vinay and thanks for the review Chris., Jenkins running for long time. https://builds.apache.org/job/PreCommit-HDFS-Build/5478/console , dont know whats the problem :(, Patch mostly looks good. I have one comment here to cleanup.
I think, this peace of code unnecesary as this method responsibility only to process non-register commands now. This method does not handle really registartions now. So, either we should indicate that in method name or at least in javadoc at the top saying this is a method to process non-registration commands.
{code}
case DatanodeProtocol.DNA_REGISTER:
-      // namenode requested a registration - at start or if NN lost contact
-      LOG.info("DatanodeCommand action: DNA_REGISTER");
-      actor.reRegister();
+      // this command should be already handled.
       break;
{code}


BTW, I have just triggered a build for you. 
{noformat}
pending  #5485	cancel this build (pending—Waiting for next available executor on Hadoop) ISSUE_NUM=5014
{noformat} Before next revision if it reports any issues please take care of them. Thanks., Thanks Uma, for the comments.
I have updated the java docs and removed unnecessary DNA_REGISTER command.

By this time jenkins was still running. I will update again if any issues from jenkins., Seems like there is an issue with the path. 
We should move the cmd null check before DNA_REGISTER if condition.
{noformat}
2013-11-19 14:08:33,394 ERROR datanode.DataNode (BPServiceActor.java:run(719)) - Exception in BPOfferService for Block pool BP-1297942247-67.195.138.31-1384870112818 (storage id DS-234026112-67.195.138.31-43443-1384870113355) service to localhost/127.0.0.1:48821
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:507)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:745)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:597)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:717)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

This would be the reason for timeouts in Jenkins., Thanks Uma for finding out the failure reason.

Its strange that cmd is null. Need to check.
Here is the updated patch to check for null for cmd before using it., I think it can happen here (I guess).
{code}
DatanodeCommand cmd = blockReport();
 processCommand(new DatanodeCommand[]{ cmd });
{code}
We can check null here itself. But That's ok I think. Let me check the latest patch., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12614728/HDFS-5014-v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5494//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/5494//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5494//console

This message is automatically generated., Attached the findbug fixed patch.

Test failure seems unrelated. same test passed in my local., {color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12614777/HDFS-5014-v2.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5497//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5497//console

This message is automatically generated., +1 on the latest patch. I will commit this patch shortly. , +1 for the latest from me too.  [~vinayrpet], thanks so much for providing a patch for this tricky issue and responding to all of the code review feedback., SUCCESS: Integrated in Hadoop-trunk-Commit #4768 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/4768/])
HDFS-5014. Process register commands with out holding BPOfferService lock. Contributed by Vinay. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1543861)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
, I have committed this to trunk, branch-2 and 2.2, Thanks a lot for the reviews Chris and thanks Vinay for patch, Thanks chris and uma.  , FAILURE: Integrated in Hadoop-Yarn-trunk #398 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/398/])
HDFS-5014. Process register commands with out holding BPOfferService lock. Contributed by Vinay. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1543861)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
, FAILURE: Integrated in Hadoop-Hdfs-trunk #1589 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1589/])
HDFS-5014. Process register commands with out holding BPOfferService lock. Contributed by Vinay. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1543861)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
, FAILURE: Integrated in Hadoop-Mapreduce-trunk #1615 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1615/])
HDFS-5014. Process register commands with out holding BPOfferService lock. Contributed by Vinay. (umamahesh: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1543861)
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java
* /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java
]