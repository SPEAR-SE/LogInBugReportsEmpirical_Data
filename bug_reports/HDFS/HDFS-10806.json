{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "id": "12329603",
            "name": "hdfs",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12329603"
        }],
        "created": "2016-08-26T18:04:11.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Dennis Lattka",
            "key": "taderich",
            "name": "Taderich",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=Taderich",
            "timeZone": "America/Chicago"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2016-10-01T09:27:31.533+0000",
        "customfield_12310222": "1_*:*_1_*:*_3079484268_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i32v2n:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Sat Oct 01 09:27:31 UTC 2016",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "Before applying any StoragePolicy, running any mapreduce jobs will complete as expected. As soon as the StoragePolicy is set (Tested using HOT and ONE_SSD) any mapreduce job will fail.\n\nNOTE: I also tested this with hadoop streaming using two python scripts, one for the mapper and one for the reduce and the error is identical.\n\nSTORAGE POLICY:\n[hdfs@hadoop-vm-client 12:58:41] hdfs storagepolicies -getStoragePolicy -path /\nThe storage policy of /:\nBlockStoragePolicy{ONE_SSD:10, storageTypes=[SSD, DISK], creationFallbacks=[SSD, DISK], replicationFallbacks=[SSD, DISK]}\n\nVERSION:\n[hdfs@hadoop-vm-client 13:02:59] hdfs version\nHadoop 2.7.1.2.4.0.0-169\nSubversion git@github.com:hortonworks/hadoop.git -r 26104d8ac833884c8776473823007f176854f2eb\nCompiled by jenkins on 2016-02-10T06:18Z\nCompiled with protoc 2.5.0\nFrom source with checksum cf48a4c63aaec76a714c1897e2ba8be6\nThis command was run using /usr/hdp/2.4.0.0-169/hadoop/hadoop-common-2.7.1.2.4.0.0-169.jar\n\nERROR:\n[hdfs@hadoop-vm-client 16:25:53] /usr/hdp/current/hadoop-client/bin/yarn --config /usr/hdp/current/hadoop-client/conf jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar randomtextwriter -D mapreduce.randomtextwriter.totalbytes=2147483648000 /benchmarks/Wordcount/Input.11358\n16/08/26 12:58:38 INFO impl.TimelineClientImpl: Timeline service address: http://hadoop-vm-rm.aae.lcl:8188/ws/v1/timeline/\n16/08/26 12:58:38 INFO client.RMProxy: Connecting to ResourceManager at hadoop-vm-rm.aae.lcl/172.16.4.12:8050\nRunning 2000 maps.\nJob started: Fri Aug 26 12:58:39 CDT 2016\n16/08/26 12:58:39 INFO impl.TimelineClientImpl: Timeline service address: http://hadoop-vm-rm.aae.lcl:8188/ws/v1/timeline/\n16/08/26 12:58:39 INFO client.RMProxy: Connecting to ResourceManager at hadoop-vm-rm.aae.lcl/172.16.4.12:8050\n16/08/26 12:58:40 INFO mapreduce.JobSubmitter: Cleaning up the staging area /user/hdfs/.staging/job_1472151637713_0002\norg.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): java.lang.IllegalArgumentException\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.getStorageTypeDeltas(FSDirectory.java:789)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:711)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.unprotectedSetReplication(FSDirAttrOp.java:397)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setReplication(FSDirAttrOp.java:151)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplication(FSNamesystem.java:1968)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setReplication(NameNodeRpcServer.java:740)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setReplication(ClientNamenodeProtocolServerSideTranslatorPB.java:440)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1427)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1358)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy22.setReplication(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setReplication(ClientNamenodeProtocolTranslatorPB.java:349)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n\tat com.sun.proxy.$Proxy23.setReplication(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.setReplication(DFSClient.java:1902)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$9.doCall(DistributedFileSystem.java:517)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$9.doCall(DistributedFileSystem.java:513)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.setReplication(DistributedFileSystem.java:513)\n\tat org.apache.hadoop.mapreduce.split.JobSplitWriter.createFile(JobSplitWriter.java:104)\n\tat org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:77)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:307)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:318)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196)\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)\n\tat org.apache.hadoop.examples.RandomTextWriter.run(RandomTextWriter.java:237)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.examples.RandomTextWriter.main(RandomTextWriter.java:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n16/08/26 12:58:40 ERROR hdfs.DFSClient: Failed to close inode 278124\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hdfs/.staging/job_1472151637713_0002/job.split (inode 278124): File does not exist. Holder DFSClient_NONMAPREDUCE_2142218507_1 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3439)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3529)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3496)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:851)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:536)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1427)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1358)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy22.complete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:462)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n\tat com.sun.proxy.$Proxy23.complete(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2358)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2340)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2304)\n\tat org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:951)\n\tat org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:983)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1086)\n\tat org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2744)\n\tat org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2761)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\n\n",
        "duedate": null,
        "environment": null,
        "fixVersions": [],
        "issuelinks": [{
            "id": "12481799",
            "inwardIssue": {
                "fields": {
                    "issuetype": {
                        "avatarId": 21133,
                        "description": "A problem which impairs or prevents the functions of the product.",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                        "id": "1",
                        "name": "Bug",
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                        "subtask": false
                    },
                    "priority": {
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                        "id": "3",
                        "name": "Major",
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
                    },
                    "status": {
                        "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
                        "id": "6",
                        "name": "Closed",
                        "self": "https://issues.apache.org/jira/rest/api/2/status/6",
                        "statusCategory": {
                            "colorName": "green",
                            "id": 3,
                            "key": "done",
                            "name": "Done",
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
                        }
                    },
                    "summary": "set replication for empty file  failed when set storage policy"
                },
                "id": "12928271",
                "key": "HDFS-9625",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/12928271"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12481799",
            "type": {
                "id": "12310000",
                "inward": "is duplicated by",
                "name": "Duplicate",
                "outward": "duplicates",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310000"
            }
        }],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
            "id": "2",
            "name": "Critical",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/2"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Dennis Lattka",
            "key": "taderich",
            "name": "Taderich",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=Taderich",
            "timeZone": "America/Chicago"
        },
        "resolution": {
            "description": "The problem is a duplicate of an existing issue.",
            "id": "3",
            "name": "Duplicate",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/3"
        },
        "resolutiondate": "2016-10-01T09:28:55.000+0000",
        "status": {
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "id": "5",
            "name": "Resolved",
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "Mapreduce jobs fail when StoragePolicy is set",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2016-10-01T09:28:55.000+0000",
        "versions": [{
            "archived": false,
            "description": "2.7.1 release",
            "id": "12331979",
            "name": "2.7.1",
            "releaseDate": "2015-07-06",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12331979"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-10806/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-10806/watchers",
            "watchCount": 3
        },
        "workratio": -1
    },
    "id": "13000466",
    "key": "HDFS-10806",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13000466"
}