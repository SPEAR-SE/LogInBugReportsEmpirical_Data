{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "id": "12312927",
            "name": "datanode",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12312927"
        }],
        "created": "2011-06-15T13:33:48.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "chakali ranga swamy",
            "key": "ranga swamy",
            "name": "ranga swamy",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=ranga+swamy",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2011-06-15T13:41:44.684+0000",
        "customfield_12310222": "1_*:*_1_*:*_87124755318_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "15179",
        "customfield_12310920": "74408",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i0d3wf:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Mar 19 22:53:03 UTC 2014",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "see sir\ndatanode log socket and datasteam problem unable to upload text file to DFS i deleted tmp folders dfs and mapred again i formated \"hadoop namenode -format\"\nstart-all.sh done then\ndfs folder contains:\ndata node ,name node,secondarynamenode\nmapred: empty\nabout space:-----------------\nlinux-8ysi:/etc/hadoop/hadoop-0.20.2 # df -h\nFilesystem Size Used Avail Use% Mounted on\n/dev/sda5 25G 16G 7.4G 69% /\nudev 987M 212K 986M 1% /dev\n/dev/sda7 42G 5.5G 34G 14% /home\n-------------------------------------------\nhttp://localhost:50070/dfshealth.jsp------------------\n\nNameNode 'localhost:54310'\nStarted: Wed Jun 15 04:13:14 IST 2011\nVersion: 0.20.2, r911707\nCompiled: Fri Feb 19 08:07:34 UTC 2010 by chrisdo\nUpgrades: There are no upgrades in progress.\n\nBrowse the filesystem\nNamenode Logs\nCluster Summary\n10 files and directories, 0 blocks = 10 total. Heap Size is 15.5 MB / 966.69 MB (1%)\nConfigured Capacity : 24.61 GB\nDFS Used : 24 KB\nNon DFS Used : 17.23 GB\nDFS Remaining : 7.38 GB\nDFS Used% : 0 %\nDFS Remaining% : 29.99 %\nLive Nodes : 1\nDead Nodes : 0\n\nNameNode Storage:\nStorage Directory Type State\n/tmp/Testinghadoop/dfs/name IMAGE_AND_EDITS Active\n\nHadoop, 2011.\n----------------------------------------\ncore-site.xml\n---------------------------------\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<!-- Put site-specific property overrides in this file. -->\n\n<configuration>\n\n<property>\n<name>hadoop.tmp.dir<\/name>\n<value>/tmp/Testinghadoop/<\/value>\n<description>A base for other temporary directories.<\/description>\n<\/property>\n\n<property>\n<name>fs.default.name<\/name>\n<value>hdfs://localhost:54310<\/value>\n<description>The name of the default file system. A URI whose\nscheme and authority determine the FileSystem implementation. The\nuri's scheme determines the config property (fs.SCHEME.impl) naming\nthe FileSystem implementation class. The uri's authority is used to\ndetermine the host, port, etc. for a filesystem.<\/description>\n<\/property>\n\n<\/configuration>\n------------------------------------------------\nhdfs-site.xml\n----------------------------------\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<!-- Put site-specific property overrides in this file. -->\n\n<configuration>\n\n<property>\n<name>dfs.permissions<\/name>\n<value>true<\/value>\n<description>\nIf \"true\", enable permission checking in HDFS.\nIf \"false\", permission checking is turned off,\nbut all other behavior is unchanged.\nSwitching from one parameter value to the other does not change the mode,\nowner, or group of files or directories.\n<\/description>\n<\/property>\n\n<property>\n<name>dfs.replication<\/name>\n<value>1<\/value>\n<description>Default block replication.\nThe actual number of replications can be specified when the file is created.\nThe default is used if replication is not specified in create time.\n<\/description>\n<\/property>\n\n<\/configuration>\n---------------------------------------\nmapred-site.xml\n----------------------------------\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<!-- Put site-specific property overrides in this file. -->\n\n<configuration>\n<property>\n<name>mapred.job.tracker<\/name>\n<value>localhost:54311<\/value>\n<description>The host and port that the MapReduce job tracker runs\nat. If \"local\", then jobs are run in-process as a single map\nand reduce task.\n<\/description>\n<\/property>\n<\/configuration>\n----------------------------------------------------------------------------------\n\nplease give suggetions about this error:\n------------------------------------------------------------------------------------------------------------------\nlinux-8ysi:/etc/hadoop/hadoop-0.20.2/conf # hadoop fsck /\nRUN_JAVA\n/usr/java/jre1.6.0_25/bin/java\n.Status: HEALTHY\nTotal size: 0 B\nTotal dirs: 7\nTotal files: 1 (Files currently being written: 1)\nTotal blocks (validated): 0\nMinimally replicated blocks: 0\nOver-replicated blocks: 0\nUnder-replicated blocks: 0\nMis-replicated blocks: 0\nDefault replication factor: 1\nAverage block replication: 0.0\nCorrupt blocks: 0\nMissing replicas: 0\nNumber of data-nodes: 1\nNumber of racks: 1\n\n\nThe filesystem under path '/' is HEALTHY\n\nlinux-8ysi:/etc/hadoop/hadoop-0.20.2/conf # hadoop dfsadmin -report\nRUN_JAVA\n/usr/java/jre1.6.0_25/bin/java\nConfigured Capacity: 26425618432 (24.61 GB)\nPresent Capacity: 7923564544 (7.38 GB)\nDFS Remaining: 7923539968 (7.38 GB)\nDFS Used: 24576 (24 KB)\nDFS Used%: 0%\nUnder replicated blocks: 0\nBlocks with corrupt replicas: 0\nMissing blocks: 0\n\n-------------------------------------------------\nDatanodes available: 1 (1 total, 0 dead)\n\nName: 127.0.0.1:50010\nDecommission Status : Normal\nConfigured Capacity: 26425618432 (24.61 GB)\nDFS Used: 24576 (24 KB)\nNon DFS Used: 18502053888 (17.23 GB)\nDFS Remaining: 7923539968(7.38 GB)\nDFS Used%: 0%\nDFS Remaining%: 29.98%\nLast contact: Wed Jun 15 05:54:00 IST 2011\n\ni got this error:\n----------------------------\n\nlinux-8ysi:/etc/hadoop/hadoop-0.20.2 # hadoop dfs -put spo.txt In\nRUN_JAVA\n/usr/java/jre1.6.0_25/bin/java\n11/06/15 04:50:18 WARN hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)\nat org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)\nat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\nat java.lang.reflect.Method.invoke(Unknown Source)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Unknown Source)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)\n\nat org.apache.hadoop.ipc.Client.call(Client.java:740)\nat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)\nat $Proxy0.addBlock(Unknown Source)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\nat java.lang.reflect.Method.invoke(Unknown Source)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\nat $Proxy0.addBlock(Unknown Source)\nat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2937)\nat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2819)\nat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)\nat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)\n\n11/06/15 04:50:18 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0] nodes == null\n11/06/15 04:50:18 WARN hdfs.DFSClient: Could not get block locations. Source file \"/user/root/In/spo.txt\" - Aborting...\nput: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1\n11/06/15 04:50:18 ERROR hdfs.DFSClient: Exception closing file /user/root/In/spo.txt : org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)\nat org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)\nat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\nat java.lang.reflect.Method.invoke(Unknown Source)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Unknown Source)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)\n\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)\nat org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)\nat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\nat java.lang.reflect.Method.invoke(Unknown Source)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Unknown Source)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)\n\nat org.apache.hadoop.ipc.Client.call(Client.java:740)\nat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)\nat $Proxy0.addBlock(Unknown Source)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\nat java.lang.reflect.Method.invoke(Unknown Source)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\nat $Proxy0.addBlock(Unknown Source)\nat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2937)\nat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2819)\nat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)\nat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)\n\nregards\nRanga Swamy\n8904524975 ",
        "duedate": null,
        "environment": "hadoop -hdfs",
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "chakali ranga swamy",
            "key": "ranga swamy",
            "name": "ranga swamy",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=ranga+swamy",
            "timeZone": "Etc/UTC"
        },
        "resolution": {
            "description": "All attempts at reproducing this issue failed, or not enough information was available to reproduce the issue. Reading the code produces no clues as to why this behavior would occur. If more information appears later, please reopen the issue.",
            "id": "5",
            "name": "Cannot Reproduce",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/5"
        },
        "resolutiondate": "2014-03-19T22:53:03.000+0000",
        "status": {
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "id": "5",
            "name": "Resolved",
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(1",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2014-03-19T22:53:03.000+0000",
        "versions": [{
            "archived": false,
            "description": "",
            "id": "12314204",
            "name": "0.20.2",
            "releaseDate": "2010-02-16",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12314204"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-2076/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-2076/watchers",
            "watchCount": 2
        },
        "workratio": -1
    },
    "id": "12510441",
    "key": "HDFS-2076",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12510441"
}