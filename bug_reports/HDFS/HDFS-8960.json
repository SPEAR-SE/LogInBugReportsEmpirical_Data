{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "id": "12312928",
            "name": "hdfs-client",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12312928"
        }],
        "created": "2015-08-26T18:04:55.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890"
            },
            "displayName": "Benoit Sigoure",
            "key": "tsuna",
            "name": "tsuna",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=tsuna",
            "timeZone": "America/Los_Angeles"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2015-08-26T21:35:06.552+0000",
        "customfield_12310222": null,
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "3.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i2jevb:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Jan 27 17:05:07 UTC 2016",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "Since we upgraded to 2.7.1 we regularly see single-drive failures cause widespread problems at the HBase level (with the default 3x replication target).\n\nHere's an example.  This HBase RegionServer is r12s16 (172.24.32.16) and is writing its WAL to [172.24.32.16:10110, 172.24.32.8:10110, 172.24.32.13:10110] as can be seen by the following occasional messages:\n\n{code}\n2015-08-23 06:28:40,272 INFO  [sync.3] wal.FSHLog: Slow sync cost: 123 ms, current pipeline: [172.24.32.16:10110, 172.24.32.8:10110, 172.24.32.13:10110]\n{code}\n\nA bit later, the second node in the pipeline above is going to experience an HDD failure.\n\n{code}\n2015-08-23 07:21:58,720 WARN  [DataStreamer for file /hbase/WALs/r12s16.sjc.aristanetworks.com,9104,1439917659071/r12s16.sjc.aristanetworks.com%2C9104%2C1439917659071.default.1440314434998 block BP-1466258523-172.24.32.1-1437768622582:blk_1073817519_77099] hdfs.DFSClient: Error Recovery for block BP-1466258523-172.24.32.1-1437768622582:blk_1073817519_77099 in pipeline 172.24.32.16:10110, 172.24.32.13:10110, 172.24.32.8:10110: bad datanode 172.24.32.8:10110\n{code}\n\nAnd then HBase will go like \"omg I can't write to my WAL, let me commit suicide\".\n\n{code}\n2015-08-23 07:22:26,060 FATAL [regionserver/r12s16.sjc.aristanetworks.com/172.24.32.16:9104.append-pool1-t1] wal.FSHLog: Could not append. Requesting close of wal\njava.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[172.24.32.16:10110, 172.24.32.13:10110], original=[172.24.32.16:10110, 172.24.32.13:10110]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:969)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1035)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1184)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:933)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:487)\n{code}\n\nWhereas this should be mostly a non-event as the DFS client should just drop the bad replica from the write pipeline.\n\nThis is a small cluster but has 16 DNs so the failed DN in the pipeline should be easily replaced.  I didn't set {{dfs.client.block.write.replace-datanode-on-failure.policy}} (so it's still {{DEFAULT}}) and didn't set {{dfs.client.block.write.replace-datanode-on-failure.enable}} (so it's still {{true}}).\n\nI don't see anything noteworthy in the NN log around the time of the failure, it just seems like the DFS client gave up or threw an exception back to HBase that it wasn't throwing before or something else, and that made this single drive failure lethal.\n\nWe've occasionally be \"unlucky\" enough to have a single-drive failure cause multiple RegionServers to commit suicide because they had their WALs on that drive.\n\nWe upgraded from 2.7.0 about a month ago, and I'm not sure whether we were seeing this with 2.7 or not \u2013 prior to that we were running in a quite different environment, but this is a fairly new deployment.",
        "duedate": null,
        "environment": "openjdk version \"1.8.0_45-internal\"\nOpenJDK Runtime Environment (build 1.8.0_45-internal-b14)\nOpenJDK 64-Bit Server VM (build 25.45-b02, mixed mode)",
        "fixVersions": [],
        "issuelinks": [{
            "id": "12436190",
            "outwardIssue": {
                "fields": {
                    "issuetype": {
                        "avatarId": 21133,
                        "description": "A problem which impairs or prevents the functions of the product.",
                        "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                        "id": "1",
                        "name": "Bug",
                        "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                        "subtask": false
                    },
                    "priority": {
                        "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
                        "id": "1",
                        "name": "Blocker",
                        "self": "https://issues.apache.org/jira/rest/api/2/priority/1"
                    },
                    "status": {
                        "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                        "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                        "id": "5",
                        "name": "Resolved",
                        "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                        "statusCategory": {
                            "colorName": "green",
                            "id": 3,
                            "key": "done",
                            "name": "Done",
                            "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
                        }
                    },
                    "summary": "Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL"
                },
                "id": "12859403",
                "key": "HBASE-14317",
                "self": "https://issues.apache.org/jira/rest/api/2/issue/12859403"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12436190",
            "type": {
                "id": "10030",
                "inward": "is related to",
                "name": "Reference",
                "outward": "relates to",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
            }
        }],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=tsuna&avatarId=12890",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=tsuna&avatarId=12890",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=tsuna&avatarId=12890",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=tsuna&avatarId=12890"
            },
            "displayName": "Benoit Sigoure",
            "key": "tsuna",
            "name": "tsuna",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=tsuna",
            "timeZone": "America/Los_Angeles"
        },
        "resolution": null,
        "resolutiondate": null,
        "status": {
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "id": "1",
            "name": "Open",
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "statusCategory": {
                "colorName": "blue-gray",
                "id": 2,
                "key": "new",
                "name": "To Do",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2"
            }
        },
        "subtasks": [],
        "summary": "DFS client says \"no more good datanodes being available to try\" on a single drive failure",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2016-01-27T17:05:07.000+0000",
        "versions": [{
            "archived": false,
            "description": "2.7.1 release",
            "id": "12331979",
            "name": "2.7.1",
            "releaseDate": "2015-07-06",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12331979"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-8960/votes",
            "votes": 3
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-8960/watchers",
            "watchCount": 22
        },
        "workratio": -1
    },
    "id": "12859390",
    "key": "HDFS-8960",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12859390"
}