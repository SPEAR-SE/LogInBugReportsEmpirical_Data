{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [],
        "created": "2008-05-29T20:19:02.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Christopher Gillett",
            "key": "ccgillett",
            "name": "ccgillett",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=ccgillett",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2008-07-22T19:16:07.629+0000",
        "customfield_12310222": "1_*:*_1_*:*_113014604136_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "2.0",
        "customfield_12310320": null,
        "customfield_12310420": "16697",
        "customfield_12310920": "107897",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i0itnb:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Dec 28 21:15:46 UTC 2011",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "9/27 update: uploaded the logs, with hopefully all the bits that should be examined. If other things are needed, just let me know. Note that all the paths refer to 0.18.1. This is still an 18.0 installation using the 18.0 core jar, just installed to a non-standard location.\n\n9/26 update: we have successfully reproduced this using Hadoop 0.18 as well. The problem happens on both our own network infrastructure as well as on an Amazon EC2 cluster running CentOS5 images. I'll be attaching the logs Raghu asked for shortly.\n\nA job that used to run correctly on our grid (in 0.15.0) now fails. The failure occurs after the map phase is complete, and about 2/3rds of the way through the reduce phase.   This job is processing a modest amount of input data (approximately 220G)\n\nWhen the error occurs the nodes hosting DataNodes have literally thousands of open socket connections on them.  The DataNode instances are holding large amounts of memory.  Sometimes the DataNodes crash or exit, other times they continue to run.\n\nThe error which gets kicked out from the application perspective is:\n\n08/05/27 11:30:08 INFO mapred.JobClient: map 100% reduce 89%\n08/05/27 11:30:41 INFO mapred.JobClient: map 100% reduce 90%\n08/05/27 11:32:45 INFO mapred.JobClient: map 100% reduce 86%\n08/05/27 11:32:45 INFO mapred.JobClient: Task Id :\n task_200805271056_0001_r_000007_0, Status : FAILED\njava.io.IOException: Could not get block locations. Aborting...\nat org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanode\n Error(DFSClient.java:1832)\nat\n org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1100(DFSClient.java:1487)\nat\n org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1579)\n\nI then discovered that 1 or more DataNode instances on the slave nodes\n are down (we run 1 DataNode instance per machine). The cause for at\n least some of the DataNode failures is a JVM internal error that gets\n raised due to a complete out-of-memory scenario (on a 4G, 4-way machine). \n\nWatching the DataNodes run, I can see them consuming more and more\n memory. For those failures for which there is a JVM traceback, I see (in\n part...NOTE 0.16.4 TRACEBACK):\n#\n# java.lang.OutOfMemoryError: requested 16 bytes for CHeapObj-new. Out\n of swap space?\n#\n# Internal Error (414C4C4F434154494F4E0E494E4C494E450E4850500017),\n pid=4246, tid=2283883408\n#\n# Java VM: Java HotSpot(TM) Server VM (1.6.0_02-b05 mixed mode)\n# If you would like to submit a bug report, please visit:\n# http://java.sun.com/webapps/bugreport/crash.jsp\n#\n--------------- T H R E A D ---------------\nCurrent thread (0x8a942000): JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@3f4f44\" daemon [_thread_in_Java, id=15064]\nStack: [0x881c4000,0x88215000), sp=0x882139e0, free space=318k\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code,\n C=native code)\nV [libjvm.so+0x53b707]\nV [libjvm.so+0x225fe1]\nV [libjvm.so+0x16fdc5]\nV [libjvm.so+0x22aef3]\nJava frames: (J=compiled Java code, j=interpreted, Vv=VM code)\nv blob 0xf4f235a7\nJ java.io.DataInputStream.readInt()I\nj\n org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(Ljava/io/DataOutputStream;Ljava/io/DataInputStream;Ljava/io/DataOutputStream;Ljava/lang/String;Lorg/a\npache/hadoop/dfs/DataNode$Throttler;I)V+126\nj\n org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(Ljava/io/DataInputStream;)V+746\nj org.apache.hadoop.dfs.DataNode$DataXceiver.run()V+174\nj java.lang.Thread.run()V+11\nv ~StubRoutines::call_stub\n--------------- P R O C E S S ---------------\nJava Threads: ( => current thread )\n0x0ae3f400 JavaThread \"process reaper\" daemon [_thread_blocked,\n id=26870]\n0x852e6000 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@e5dce1\" daemon [_thread_in_vm, id=26869]\n0x08a1cc00 JavaThread \"PacketResponder 0 for Block\n blk_-6186975972786687394\" daemon [_thread_blocked, id=26769]\n0x852e5000 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@c40bf8\" daemon [_thread_in_native, id=26768]\n0x0956e000 JavaThread \"PacketResponder 0 for Block\n blk_-2322514873363546651\" daemon [_thread_blocked, id=26767]\n0x852e4400 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@1ca61f9\" daemon [_thread_in_native, id=26766]\n0x09d3a400 JavaThread \"PacketResponder 0 for Block\n blk_8926941945313450801\" daemon [_thread_blocked, id=26764]\n0x852e3c00 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@1e186d9\" daemon [_thread_in_native, id=26763]\n0x0953d000 JavaThread \"PacketResponder 0 for Block\n blk_4785883052769066976\" daemon [_thread_blocked, id=26762]\n0xb13a5c00 JavaThread\n \"org.apache.hadoop.dfs.DataNode$DataXceiver@13d62aa\" daemon [_thread_in_native, id=26761]\n\nThe interesting part here is that if I count the number of JavaThreads\n running org.apache.hadoop.dfs.DataNode I see 4,538 (!) in the\n traceback. The number of threads was surprising.\n\nOther DataNodes just exit without panicking the JVM. In either failure\n mode, the last few lines of the DataNode log file is apparently\n innocuous:\n\n2008-05-27 11:31:47,663 INFO org.apache.hadoop.dfs.DataNode: Datanode 2\n got response for connect ack from downstream datanode with\n firstbadlink as\n2008-05-27 11:31:47,663 INFO org.apache.hadoop.dfs.DataNode: Datanode 2\n forwarding connect ack to upstream firstbadlink is\n2008-05-27 11:31:48,268 INFO org.apache.hadoop.dfs.DataNode: Receiving\n block blk_-2241766430103062484 src: /10.2.14.10:33626 dest:\n /10.2.14.10:50010\n2008-05-27 11:31:48,740 INFO org.apache.hadoop.dfs.DataNode: Receiving\n block blk_313239508245918539 src: /10.2.14.24:37836 dest:\n /10.2.14.24:50010\n2008-05-27 11:31:48,740 INFO org.apache.hadoop.dfs.DataNode: Datanode 0\n forwarding connect ack to upstream firstbadlink is\n2008-05-27 11:31:49,044 INFO org.apache.hadoop.dfs.DataNode: Receiving\n block blk_1684581399908730353 src: /10.2.14.16:51605 dest:\n /10.2.14.16:50010\n2008-05-27 11:31:49,044 INFO org.apache.hadoop.dfs.DataNode: Datanode 0\n forwarding connect ack to upstream firstbadlink is\n2008-05-27 11:31:49,509 INFO org.apache.hadoop.dfs.DataNode: Receiving\n block blk_2493969670086107736 src: /10.2.14.18:47557 dest:\n /10.2.14.18:50010\n2008-05-27 11:31:49,513 INFO org.apache.hadoop.dfs.DataNode: Datanode 1\n got response for connect ack from downstream datanode with\n firstbadlink as\n2008-05-27 11:31:49,513 INFO org.apache.hadoop.dfs.DataNode: Datanode 1\n forwarding connect ack to upstream firstbadlink is\n\nFinally, the task-level output (in userlogs) doesn't reveal much\n either:\n\n2008-05-27 11:38:30,724 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Need 34 map output(s)\n2008-05-27 11:38:30,753 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 done copying\n task_200805271056_0001_m_001976_0 output from worker9.\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1: Got 0 new map-outputs & 0 obsolete\n map-outputs from tasktracker and 0 map-outputs from previous failures\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Got 33 known map output location(s);\n scheduling...\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Scheduled 1 of 33 known outputs (0 slow\n hosts and 32 dup hosts)\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Copying task_200805271056_0001_m_001248_0\n output from worker8.\n2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 Need 33 map output(s)\n2008-05-27 11:38:31,752 INFO org.apache.hadoop.mapred.ReduceTask:\n task_200805271056_0001_r_000007_1 done copying\n task_200805271056_0001_m_001248_0 output from worker8.\n",
        "duedate": null,
        "environment": "Hadoop-0.18.0, 7 node Linux grid (6 DataNodes, 1 master node)\nHadoop-0.18.0, 20 EC2 Linux grid (19 DataNodes, 1 master node)",
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
            "id": "2",
            "name": "Critical",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/2"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Christopher Gillett",
            "key": "ccgillett",
            "name": "ccgillett",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=ccgillett",
            "timeZone": "Etc/UTC"
        },
        "resolution": {
            "description": "All attempts at reproducing this issue failed, or not enough information was available to reproduce the issue. Reading the code produces no clues as to why this behavior would occur. If more information appears later, please reopen the issue.",
            "id": "5",
            "name": "Cannot Reproduce",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/5"
        },
        "resolutiondate": "2011-12-28T21:15:46.000+0000",
        "status": {
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "id": "5",
            "name": "Resolved",
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "DataNode fails to deliver blocks, holds thousands of open socket connections",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2011-12-28T21:15:46.000+0000",
        "versions": [],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-162/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-162/watchers",
            "watchCount": 12
        },
        "workratio": -1
    },
    "id": "12397103",
    "key": "HDFS-162",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12397103"
}