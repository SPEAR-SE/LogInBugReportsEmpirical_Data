{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [{
            "id": "12312928",
            "name": "hdfs-client",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12312928"
        }],
        "created": "2010-06-17T04:27:41.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thanhdo&avatarId=22565",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thanhdo&avatarId=22565",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thanhdo&avatarId=22565",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=thanhdo&avatarId=22565"
            },
            "displayName": "Thanh Do",
            "key": "thanhdo",
            "name": "thanhdo",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=thanhdo",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2010-06-23T04:47:39.169+0000",
        "customfield_12310222": "1_*:*_1_*:*_3444300960_*|*_5_*:*_1_*:*_0",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "15913",
        "customfield_12310920": "113404",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i0jrm7:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Jul 27 01:12:42 UTC 2010",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "Setup:\n--------\n+ # available datanodes = 2\n+ # disks / datanode = 1\n+ # failures = 1\n+ failure type = crash\n+ When/where failure happens = during primary's recoverBlock\n \nDetails:\n----------\nSay client is appending to block X1 in 2 datanodes: dn1 and dn2.\nFirst it needs to make sure both dn1 and dn2  agree on the new GS of the block.\n1) Client first creates DFSOutputStream by calling\n \n>OutputStream result = new DFSOutputStream(src, buffersize, progress,\n>                                            lastBlock, stat, conf.getInt(\"io.bytes.per.checksum\", 512));\n \nin DFSClient.append()\n \n2) The above DFSOutputStream constructor in turn calls processDataNodeError(true, true)\n(i.e, hasError = true, isAppend = true), and starts the DataStreammer\n \n> processDatanodeError(true, true);  /* let's call this PDNE 1 */\n> streamer.start();\n \nNote that DataStreammer.run() also calls processDatanodeError()\n> while (!closed && clientRunning) {\n>  ...\n>      boolean doSleep = processDatanodeError(hasError, false); /let's call this PDNE 2*/\n \n3) Now in the PDNE 1, we have following code:\n \n> blockStream = null;\n> blockReplyStream = null;\n> ...\n> while (!success && clientRunning) {\n> ...\n>    try {\n>         primary = createClientDatanodeProtocolProxy(primaryNode, conf);\n>         newBlock = primary.recoverBlock(block, isAppend, newnodes); /*exception here*/\n>         ...\n>    catch (IOException e) {\n>         ...\n>         if (recoveryErrorCount > maxRecoveryErrorCount) { \n>         // this condition is false\n>         }\n>         ...\n>         return true;\n>    } // end catch\n>    finally {...}\n>    \n>    this.hasError = false;\n>    lastException = null;\n>    errorIndex = 0;\n>    success = createBlockOutputStream(nodes, clientName, true);\n>    }\n>    ...\n \nBecause dn1 crashes during client call to recoverBlock, we have an exception.\nHence, go to the catch block, in which processDatanodeError returns true\nbefore setting hasError to false. Also, because createBlockOutputStream() is not called\n(due to an early return), blockStream is still null.\n \n4) Now PDNE 1 has finished, we come to streamer.start(), which calls PDNE 2.\nBecause hasError = false, PDNE 2 returns false immediately without doing anything\n\n> if (!hasError) { return false; }\n \n5) still in the DataStreamer.run(), after returning false from PDNE 2, we still have\nblockStream = null, hence the following code is executed:\n\nif (blockStream == null) {\n   nodes = nextBlockOutputStream(src);\n   this.setName(\"DataStreamer for file \" + src + \" block \" + block);\n   response = new ResponseProcessor(nodes);\n   response.start();\n}\n \nnextBlockOutputStream which asks namenode to allocate new Block is called.\n(This is not good, because we are appending, not writing).\nNamenode gives it new Block ID and a set of datanodes, including crashed dn1.\nthis leads to createOutputStream() fails because it tries to contact the dn1 first.\n(which has crashed). The client retries 5 times without any success,\nbecause every time, it asks namenode for new block! Again we see\nthat the retry logic at client is weird!\n\n*This bug was found by our Failure Testing Service framework:\nhttp://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html\nFor questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and \nHaryadi Gunawi (haryadi@eecs.berkeley.edu)*",
        "duedate": null,
        "environment": null,
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "id": "3",
            "name": "Major",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thanhdo&avatarId=22565",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thanhdo&avatarId=22565",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thanhdo&avatarId=22565",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=thanhdo&avatarId=22565"
            },
            "displayName": "Thanh Do",
            "key": "thanhdo",
            "name": "thanhdo",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=thanhdo",
            "timeZone": "Etc/UTC"
        },
        "resolution": {
            "description": "All attempts at reproducing this issue failed, or not enough information was available to reproduce the issue. Reading the code produces no clues as to why this behavior would occur. If more information appears later, please reopen the issue.",
            "id": "5",
            "name": "Cannot Reproduce",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/5"
        },
        "resolutiondate": "2010-07-27T01:12:42.000+0000",
        "status": {
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "id": "5",
            "name": "Resolved",
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "DFSClient incorrectly asks for new block if primary crashes during first recoverBlock",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2010-07-27T01:12:42.000+0000",
        "versions": [{
            "archived": true,
            "description": "Append/sync support for Hadoop 0.20",
            "id": "12315103",
            "name": "0.20-append",
            "released": false,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12315103"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-1229/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-1229/watchers",
            "watchCount": 2
        },
        "workratio": -1
    },
    "id": "12467161",
    "key": "HDFS-1229",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12467161"
}