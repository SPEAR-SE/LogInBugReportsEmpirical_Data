{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Rushabh S Shah",
            "key": "shahrs87",
            "name": "shahrs87",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=shahrs87",
            "timeZone": "Etc/UTC"
        },
        "components": [{
            "id": "12312927",
            "name": "datanode",
            "self": "https://issues.apache.org/jira/rest/api/2/component/12312927"
        }],
        "created": "2016-02-29T22:15:27.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Rushabh S Shah",
            "key": "shahrs87",
            "name": "shahrs87",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=shahrs87",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": [{
            "id": "10343",
            "self": "https://issues.apache.org/jira/rest/api/2/customFieldOption/10343",
            "value": "Reviewed"
        }],
        "customfield_12310192": null,
        "customfield_12310220": "2016-03-03T19:19:59.171+0000",
        "customfield_12310222": "1_*:*_3_*:*_239406079_*|*_5_*:*_1_*:*_0_*|*_10002_*:*_3_*:*_1292162215",
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "3.0",
        "customfield_12310320": [{
            "archived": false,
            "description": "2.8.0 release",
            "id": "12329057",
            "name": "2.8.0",
            "releaseDate": "2017-03-22",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12329057"
        }],
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i2txxj:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Aug 25 23:00:25 UTC 2016",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "One of the failed volume shutdown took 3 days to complete.\nBelow are the relevant datanode logs while shutting down a volume (due to disk failure)\n\n{noformat}\n2016-02-21 10:12:55,333 [Thread-49277] WARN impl.FsDatasetImpl: Removing failed volume volumeA/current: \norg.apache.hadoop.util.DiskChecker$DiskErrorException: Directory is not writable: volumeA/current/BP-1788428031-nnIp-1351700107344/current/finalized\n        at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:194)\n        at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:174)\n        at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:108)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.checkDirs(BlockPoolSlice.java:308)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.checkDirs(FsVolumeImpl.java:786)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.checkDirs(FsVolumeList.java:242)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkDataDir(FsDatasetImpl.java:2011)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3145)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$800(DataNode.java:243)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode$7.run(DataNode.java:3178)\n        at java.lang.Thread.run(Thread.java:745)\n2016-02-21 10:12:55,334 [Thread-49277] INFO datanode.BlockScanner: Removing scanner for volume volumeA (StorageID DS-cd2ea223-bab3-4361-a567-5f3f27a5dd23)\n2016-02-21 10:12:55,334 [VolumeScannerThread(volumeA)] INFO datanode.VolumeScanner: VolumeScanner(volumeA, DS-cd2ea223-bab3-4361-a567-5f3f27a5dd23) exiting.\n2016-02-21 10:12:55,335 [VolumeScannerThread(volumeA)] WARN datanode.VolumeScanner: VolumeScanner(volumeA, DS-cd2ea223-bab3-4361-a567-5f3f27a5dd23): error saving org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl@4169ad8b.\njava.io.FileNotFoundException: volumeA/current/BP-1788428031-nnIp-1351700107344/scanner.cursor.tmp (Read-only file system)\n        at java.io.FileOutputStream.open(Native Method)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl.save(FsVolumeImpl.java:669)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.saveBlockIterator(VolumeScanner.java:314)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:633)\n\n2016-02-24 16:05:53,285 [Thread-49277] WARN impl.FsDatasetImpl: Failed to delete old dfsUsed file in volumeA/current/BP-1788428031-nnIp-1351700107344/current\n2016-02-24 16:05:53,286 [Thread-49277] WARN impl.FsDatasetImpl: Failed to write dfsUsed to volumeA/current/BP-1788428031-nnIp-1351700107344/current/dfsUsed\njava.io.FileNotFoundException: volumeA/current/BP-1788428031-nnIp-1351700107344/current/dfsUsed (Read-only file system)\n\t\tat java.io.FileOutputStream.open(Native Method)\n\t\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\t\tat java.io.FileOutputStream.<init>(FileOutputStream.java:162)\n\t\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.saveDfsUsed(BlockPoolSlice.java:247)\n\t\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.shutdown(BlockPoolSlice.java:698)\n\t\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:815)\n\t\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.removeVolume(FsVolumeList.java:328)\n\t\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.checkDirs(FsVolumeList.java:250)\n\t\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkDataDir(FsDatasetImpl.java:2011)\n\t\tat org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3145)\n\t\tat org.apache.hadoop.hdfs.server.datanode.DataNode.access$800(DataNode.java:243)\n\t\tat org.apache.hadoop.hdfs.server.datanode.DataNode$7.run(DataNode.java:3178)\n\t\tat java.lang.Thread.run(Thread.java:745)\n\n2016-02-24 16:05:53,286 [Thread-49277] INFO impl.FsDatasetImpl: Removed volume: volumeA/current\n2016-02-24 16:05:53,286 [Thread-49277] WARN impl.FsDatasetImpl: Completed checkDirs. Found 1 failure volumes.\n2016-02-24 16:05:53,286 [Thread-49277] INFO datanode.DataNode: Deactivating volumes (clear failure=false): volumeA\n2016-02-24 16:05:53,286 [Thread-49277] INFO impl.FsDatasetImpl: Removing volumeA from FsDataset.\n2016-02-24 16:05:53,342 [Thread-49277] INFO common.Storage: Removing block level storage: volumeA/current/BP-1788428031-nnIp-1351700107344\n2016-02-24 16:05:53,345 [Thread-49277] WARN datanode.DataNode: DataNode.handleDiskError: Keep Running: true\n{noformat} \n\nThe datanode waits for the reference count to become zero before shutting down the volume.\n{code:title=FsVolumeImpl.java|borderStyle=solid}\nwhile (this.reference.getReferenceCount() > 0) {\n     if (FsDatasetImpl.LOG.isDebugEnabled()) {\n       FsDatasetImpl.LOG.debug(String.format(\n           \"The reference count for %s is %d, wait to be 0.\",\n           this, reference.getReferenceCount()));\n     }\n     try {\n       Thread.sleep(SLEEP_MILLIS);\n     } catch (InterruptedException e) {\n       throw new IOException(e);\n     }\n   }\n{code}\n\nJust before datanode logged the following line, \n{noformat} \n2016-02-24 16:05:53,285 [Thread-49277] WARN impl.FsDatasetImpl: Failed to delete old dfsUsed file in volumeA/current/BP-1788428031-nnIp-1351700107344/current\n{noformat}\nI saw the following stack trace\n{noformat}\n2016-02-24 16:05:53,017 [DataXceiver for client DFSClient_NONMAPREDUCE_1651663681_1 at /upStreamDNIp:55710 [Receiving block BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832821906]] INFO datanode.DataNode: Exception for BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832909736\njava.io.IOException: Premature EOF from inputStream\n        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:201)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:501)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:895)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:801)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:253)\n        at java.lang.Thread.run(Thread.java:745)\n2016-02-24 16:05:53,018 [PacketResponder: BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832909736, type=LAST_IN_PIPELINE, downstreams=0:[]] INFO datanode.DataNode: PacketResponder: BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832909736, type=LAST_IN_PIPELINE, downstreams=0:[]: Thread is interrupted.\n2016-02-24 16:05:53,018 [PacketResponder: BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832909736, type=LAST_IN_PIPELINE, downstreams=0:[]] INFO datanode.DataNode: PacketResponder: BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832909736, type=LAST_IN_PIPELINE, downstreams=0:[] terminating\n2016-02-24 16:05:53,018 [DataXceiver for client DFSClient_NONMAPREDUCE_1651663681_1 at /upStreamDNIp:55710 [Receiving block BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832821906]] INFO datanode.DataNode: opWriteBlock BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832909736 received exception java.io.IOException: Prematur\ne EOF from inputStream\n2016-02-24 16:05:53,018 [DataXceiver for client DFSClient_NONMAPREDUCE_1651663681_1 at /upStreamDNIp:55710 [Receiving block BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832821906]] ERROR datanode.DataNode: thisDNName:1004:DataXceiver error processing WRITE_BLOCK operation  src: /upStreamDNIp:55710 dst: /thisDNIp\n:1004\njava.io.IOException: Premature EOF from inputStream\n        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:201)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:501)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:895)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:801)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:253)\n        at java.lang.Thread.run(Thread.java:745)\n\n{noformat}\n\n\nOn tracking the block blk_7059462432_1109832821906, I see that the block was created on 2016-02-17 15:06:28,256\n{noformat}\n2016-02-17 15:06:28,928 [DataXceiver for client DFSClient_NONMAPREDUCE_1651663681_1 at /upStreamDNIp:55590 [Receiving block BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832821906]] INFO datanode.DataNode: Receiving BP-1788428031-nnIp-1351700107344:blk_7059462432_1109832821906 src: /upStreamDNIp:55590 dest: /thisDNIp:1004\n{noformat}\n\n\nThe job which created this file was running for more than 7 days and the client eventually failed to renew the delegation token so the lease manager failed to renew lease for this file.\nOnce commitBlockSynchronization kicked in, it tried to recover the block and eventually the DataXceiver thread died decrementing the reference count.",
        "duedate": null,
        "environment": null,
        "fixVersions": [
            {
                "archived": false,
                "description": "2.8.0 release",
                "id": "12329057",
                "name": "2.8.0",
                "releaseDate": "2017-03-22",
                "released": true,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12329057"
            },
            {
                "archived": false,
                "description": "2.7.3 release",
                "id": "12333995",
                "name": "2.7.3",
                "releaseDate": "2016-08-25",
                "released": true,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12333995"
            },
            {
                "archived": false,
                "description": "3.0.0-alpha1 release",
                "id": "12335732",
                "name": "3.0.0-alpha1",
                "releaseDate": "2016-09-03",
                "released": true,
                "self": "https://issues.apache.org/jira/rest/api/2/version/12335732"
            }
        ],
        "issuelinks": [
            {
                "id": "12474390",
                "inwardIssue": {
                    "fields": {
                        "issuetype": {
                            "avatarId": 21133,
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "id": "1",
                            "name": "Bug",
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "subtask": false
                        },
                        "priority": {
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
                            "id": "4",
                            "name": "Minor",
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/4"
                        },
                        "status": {
                            "description": "A patch for this issue has been uploaded to JIRA by a contributor.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/document.png",
                            "id": "10002",
                            "name": "Patch Available",
                            "self": "https://issues.apache.org/jira/rest/api/2/status/10002",
                            "statusCategory": {
                                "colorName": "yellow",
                                "id": 4,
                                "key": "indeterminate",
                                "name": "In Progress",
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/4"
                            }
                        },
                        "summary": "Can not synchronized call method of object and Mockito.spy(object), So UT:testRemoveVolumeBeingWritten passed but maybe deadlock online"
                    },
                    "id": "12988219",
                    "key": "HDFS-10605",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/12988219"
                },
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12474390",
                "type": {
                    "id": "10030",
                    "inward": "is related to",
                    "name": "Reference",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                }
            },
            {
                "id": "12462806",
                "inwardIssue": {
                    "fields": {
                        "issuetype": {
                            "avatarId": 21133,
                            "description": "A problem which impairs or prevents the functions of the product.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
                            "id": "1",
                            "name": "Bug",
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
                            "subtask": false
                        },
                        "priority": {
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "id": "3",
                            "name": "Major",
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3"
                        },
                        "status": {
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "id": "5",
                            "name": "Resolved",
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "statusCategory": {
                                "colorName": "green",
                                "id": 3,
                                "key": "done",
                                "name": "Done",
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
                            }
                        },
                        "summary": "TestFsDatasetImpl#testCleanShutdownOfVolume often fails"
                    },
                    "id": "12956174",
                    "key": "HDFS-10260",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/12956174"
                },
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12462806",
                "type": {
                    "id": "12310050",
                    "inward": "is broken by",
                    "name": "Regression",
                    "outward": "breaks",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310050"
                }
            }
        ],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
            "id": "2",
            "name": "Critical",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/2"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Rushabh S Shah",
            "key": "shahrs87",
            "name": "shahrs87",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=shahrs87",
            "timeZone": "Etc/UTC"
        },
        "resolution": {
            "description": "A fix for this issue is checked into the tree and tested.",
            "id": "1",
            "name": "Fixed",
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1"
        },
        "resolutiondate": "2016-03-18T15:41:35.000+0000",
        "status": {
            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
            "id": "6",
            "name": "Closed",
            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
            "statusCategory": {
                "colorName": "green",
                "id": 3,
                "key": "done",
                "name": "Done",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3"
            }
        },
        "subtasks": [],
        "summary": "Long living DataXceiver threads cause volume shutdown to block.",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2016-12-01T23:20:39.000+0000",
        "versions": [{
            "archived": false,
            "description": "2.7.0 release",
            "id": "12327584",
            "name": "2.7.0",
            "releaseDate": "2015-04-20",
            "released": true,
            "self": "https://issues.apache.org/jira/rest/api/2/version/12327584"
        }],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-9874/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-9874/watchers",
            "watchCount": 16
        },
        "workratio": -1
    },
    "id": "12945635",
    "key": "HDFS-9874",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/12945635"
}