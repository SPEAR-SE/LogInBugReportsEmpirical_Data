{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [],
        "created": "2018-08-17T17:24:32.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=rikeppb100&avatarId=36577",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=rikeppb100&avatarId=36577",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=rikeppb100&avatarId=36577",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=rikeppb100&avatarId=36577"
            },
            "displayName": "Henrique Barros",
            "key": "rikeppb100",
            "name": "rikeppb100",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=rikeppb100",
            "timeZone": "Etc/UTC"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2018-08-17T17:53:06.951+0000",
        "customfield_12310222": null,
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "0.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i3x5y7:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Aug 20 09:51:23 UTC 2018",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "I'm having a random problem with blocks replication with Hadoop 2.6.0-cdh5.15.0\r\nWith Cloudera CDH-5.15.0-1.cdh5.15.0.p0.21\r\n\r\n \r\n\r\nIn my case we are getting this error very randomly (after some hours) and with only one Datanode (for now, we are trying this cloudera cluster for a POC)\r\nHere is the Log.\r\n{code:java}\r\nChoosing random from 1 available nodes on node /default, scope=/default, excludedScope=null, excludeNodes=[]\r\n2:38:20.527 PM\tDEBUG\tNetworkTopology\t\r\nChoosing random from 0 available nodes on node /default, scope=/default, excludedScope=null, excludeNodes=[192.168.220.53:50010]\r\n2:38:20.527 PM\tDEBUG\tNetworkTopology\t\r\nchooseRandom returning null\r\n2:38:20.527 PM\tDEBUG\tBlockPlacementPolicy\t\r\n[\r\nNode /default/192.168.220.53:50010 [\r\n  Datanode 192.168.220.53:50010 is not chosen since the node is too busy (load: 8 > 0.0).\r\n2:38:20.527 PM\tDEBUG\tNetworkTopology\t\r\nchooseRandom returning 192.168.220.53:50010\r\n2:38:20.527 PM\tINFO\tBlockPlacementPolicy\t\r\nNot enough replicas was chosen. Reason:{NODE_TOO_BUSY=1}\r\n2:38:20.527 PM\tDEBUG\tStateChange\t\r\ncloseFile: /mobi.me/development/apps/flink/checkpoints/a5a6806866c1640660924ea1453cbe34/chk-2118/eef8bff6-75a9-43c1-ae93-4b1a9ca31ad9 with 1 blocks is persisted to the file system\r\n2:38:20.527 PM\tDEBUG\tStateChange\t\r\n*BLOCK* NameNode.addBlock: file /mobi.me/development/apps/flink/checkpoints/a5a6806866c1640660924ea1453cbe34/chk-2118/1cfe900d-6f45-4b55-baaa-73c02ace2660 fileId=129628869 for DFSClient_NONMAPREDUCE_467616914_65\r\n2:38:20.527 PM\tDEBUG\tBlockPlacementPolicy\t\r\nFailed to choose from local rack (location = /default); the second replica is not found, retry choosing ramdomly\r\norg.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: \r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:784)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:694)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:601)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalStorage(BlockPlacementPolicyDefault.java:561)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:464)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:395)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:270)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:142)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:158)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1715)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3505)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:694)\r\n\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:219)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:507)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2281)\r\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2277)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2275)\r\n\r\n{code}\r\nThis part makes no sense at all:\r\n\r\n\r\n{code:java}\r\nload: 8 > 0.0{code}",
        "duedate": null,
        "environment": null,
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
            "id": "2",
            "name": "Critical",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/2"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=rikeppb100&avatarId=36577",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=rikeppb100&avatarId=36577",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=rikeppb100&avatarId=36577",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=rikeppb100&avatarId=36577"
            },
            "displayName": "Henrique Barros",
            "key": "rikeppb100",
            "name": "rikeppb100",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=rikeppb100",
            "timeZone": "Etc/UTC"
        },
        "resolution": null,
        "resolutiondate": null,
        "status": {
            "description": "The issue is open and ready for the assignee to start work on it.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
            "id": "1",
            "name": "Open",
            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
            "statusCategory": {
                "colorName": "blue-gray",
                "id": 2,
                "key": "new",
                "name": "To Do",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2"
            }
        },
        "subtasks": [],
        "summary": "Failed to choose from local rack (location = /default); the second replica is not found, retry choosing ramdomly",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2018-08-20T09:54:52.000+0000",
        "versions": [],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-13833/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-13833/watchers",
            "watchCount": 6
        },
        "workratio": -1
    },
    "id": "13179639",
    "key": "HDFS-13833",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13179639"
}