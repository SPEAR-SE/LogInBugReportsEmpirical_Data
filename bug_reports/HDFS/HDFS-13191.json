{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "fields": {
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "aggregatetimeestimate": null,
        "aggregatetimeoriginalestimate": null,
        "aggregatetimespent": null,
        "assignee": null,
        "components": [
            {
                "id": "12329603",
                "name": "hdfs",
                "self": "https://issues.apache.org/jira/rest/api/2/component/12329603"
            },
            {
                "id": "12312928",
                "name": "hdfs-client",
                "self": "https://issues.apache.org/jira/rest/api/2/component/12312928"
            }
        ],
        "created": "2018-02-24T02:15:55.000+0000",
        "creator": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Dennis Huo",
            "key": "dennishuo",
            "name": "dennishuo",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=dennishuo",
            "timeZone": "America/Los_Angeles"
        },
        "customfield_10010": null,
        "customfield_12310191": null,
        "customfield_12310192": null,
        "customfield_12310220": "2018-02-24T06:08:33.768+0000",
        "customfield_12310222": null,
        "customfield_12310230": null,
        "customfield_12310250": null,
        "customfield_12310290": null,
        "customfield_12310291": null,
        "customfield_12310300": null,
        "customfield_12310310": "1.0",
        "customfield_12310320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12310920": "9223372036854775807",
        "customfield_12310921": null,
        "customfield_12311020": null,
        "customfield_12311024": null,
        "customfield_12311120": null,
        "customfield_12311820": "0|i3qjwn:",
        "customfield_12312022": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "customfield_12312026": null,
        "customfield_12312220": null,
        "customfield_12312320": null,
        "customfield_12312321": null,
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312324": null,
        "customfield_12312325": null,
        "customfield_12312326": null,
        "customfield_12312327": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312330": null,
        "customfield_12312331": null,
        "customfield_12312332": null,
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12312335": null,
        "customfield_12312336": null,
        "customfield_12312337": null,
        "customfield_12312338": null,
        "customfield_12312339": null,
        "customfield_12312340": null,
        "customfield_12312341": null,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Mar 05 17:23:29 UTC 2018",
        "customfield_12312720": null,
        "customfield_12312823": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "customfield_12312923": null,
        "customfield_12313422": "false",
        "customfield_12313520": null,
        "description": "{color:red}colored text{color}The org.apache.hadoop.io.DataOutputBuffer is used as an \"optimization\" in many places to allow a reusable form of ByteArrayOutputStream, but requires the caller to be careful to use getLength() instead of getData().length to determine the number of actually valid bytes to consume.\r\n\r\nAt least three places in the path of constructing FileChecksums have incorrect usage of DataOutputBuffer:\r\n\r\n[FileChecksumHelper digesting block MD5s|https://github.com/apache/hadoop/blob/329a4fdd07ab007615f34c8e0e651360f988064d/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/FileChecksumHelper.java#L239]\r\n\r\n[BlockChecksumHelper digesting striped block MD5s to construct block-group checksum|https://github.com/apache/hadoop/blob/329a4fdd07ab007615f34c8e0e651360f988064d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockChecksumHelper.java#L412]\r\n\r\n[MD5MD5CRC32FileChecksum.getBytes()|https://github.com/apache/hadoop/blob/329a4fdd07ab007615f34c8e0e651360f988064d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java#L76]\r\n\r\nThe net effect is that FileChecksum consumes exact BlockChecksums if there are 1 or 2 blocks (at 16 and 32 bytes respectively), but at 3 blocks will round up to 64 bytes, effectively returning the same FileChecksum as if there were 4 blocks and the 4th block happened to have an MD5 exactly equal to 0x00...00. Similarly, BlockGroupChecksum will behave as if there is a power-of-2 number of bytes from BlockChecksums in the BlockGroup.\r\n\r\nThis appears to have been a latent bug for at least 9 years for FileChecksum (and since inception for the implementation of striped files), and works fine as long as HDFS implementations strictly stick to the same internal buffering semantics.\r\n\r\nHowever, this also makes the implementation extremely brittle unless carefully documented. For example, if code is ever refactored to pass around a MessageDigest that consumes block MD5s as they come rather than writing into a DataOutputBuffer before digesting the entire buffer, then the resulting checksum calculations will change unexpectedly.\r\n\r\nAt the same time, \"fixing\" the bug would also be backwards-incompatible, so the bug might need to stick around. At least for the FileChecksum-level calculation, it seems the bug has been latent for a very long time. Since striped files are fairly new, the BlockChecksumHelper could probably be fixed sooner rather than later to avoid perpetuating a bug. The getBytes() method for FileChecksum is more innocuous, so could likely be fixed or left as-is without too much impact either way.\r\n\r\nThe bug can be highlighted by changing the internal buffer-growing semantics of the DataOutputBuffer, or simply returning a randomly-sized byte buffer in getData() while only ensuring the first getLength() bytes are actually present, for example:\r\n\r\n \r\n{code:java}\r\ndiff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java\r\nindex 4c2fa67f8f2..f2df94e898f 100644\r\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java\r\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/DataOutputBuffer.java\r\n@@ -103,7 +103,17 @@ private DataOutputBuffer(Buffer buffer) {\r\n/** Returns the current contents of the buffer.\r\n* Data is only valid to {@link #getLength()}.\r\n*/\r\n- public byte[] getData() { return buffer.getData(); }\r\n+ public byte[] getData() {\r\n+ java.util.Random rand = new java.util.Random();\r\n+ byte[] bufferData = buffer.getData();\r\n+ byte[] ret = new byte[rand.nextInt(bufferData.length) + bufferData.length];\r\n+ System.arraycopy(bufferData, 0, ret, 0, getLength());\r\n+ return ret;\r\n+ }\r\n\r\n{code}",
        "duedate": null,
        "environment": null,
        "fixVersions": [],
        "issuelinks": [],
        "issuetype": {
            "avatarId": 21133,
            "description": "A problem which impairs or prevents the functions of the product.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
            "id": "1",
            "name": "Bug",
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
            "subtask": false
        },
        "labels": [],
        "lastViewed": null,
        "priority": {
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
            "id": "4",
            "name": "Minor",
            "self": "https://issues.apache.org/jira/rest/api/2/priority/4"
        },
        "progress": {
            "progress": 0,
            "total": 0
        },
        "project": {
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310942&avatarId=10094",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310942&avatarId=10094",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310942&avatarId=10094",
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12310942&avatarId=10094"
            },
            "id": "12310942",
            "key": "HDFS",
            "name": "Hadoop HDFS",
            "projectCategory": {
                "description": "Scalable Distributed Computing",
                "id": "10292",
                "name": "Hadoop",
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/10292"
            },
            "self": "https://issues.apache.org/jira/rest/api/2/project/12310942"
        },
        "reporter": {
            "active": true,
            "avatarUrls": {
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452",
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452"
            },
            "displayName": "Dennis Huo",
            "key": "dennishuo",
            "name": "dennishuo",
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=dennishuo",
            "timeZone": "America/Los_Angeles"
        },
        "resolution": null,
        "resolutiondate": null,
        "status": {
            "description": "A patch for this issue has been uploaded to JIRA by a contributor.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/document.png",
            "id": "10002",
            "name": "Patch Available",
            "self": "https://issues.apache.org/jira/rest/api/2/status/10002",
            "statusCategory": {
                "colorName": "yellow",
                "id": 4,
                "key": "indeterminate",
                "name": "In Progress",
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/4"
            }
        },
        "subtasks": [],
        "summary": "Internal buffer-sizing details are inadvertently baked into FileChecksum and BlockGroupChecksum",
        "timeestimate": null,
        "timeoriginalestimate": null,
        "timespent": null,
        "updated": "2018-03-05T22:11:27.000+0000",
        "versions": [],
        "votes": {
            "hasVoted": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-13191/votes",
            "votes": 0
        },
        "watches": {
            "isWatching": false,
            "self": "https://issues.apache.org/jira/rest/api/2/issue/HDFS-13191/watchers",
            "watchCount": 6
        },
        "workratio": -1
    },
    "id": "13140672",
    "key": "HDFS-13191",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13140672"
}