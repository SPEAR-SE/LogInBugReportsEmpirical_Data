{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12443287","self":"https://issues.apache.org/jira/rest/api/2/issue/12443287","key":"HIVE-984","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/3","id":"3","description":"The problem is a duplicate of an existing issue.","name":"Duplicate"},"customfield_12312322":null,"customfield_12310220":"2010-01-07T17:50:46.486+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Wed Feb 24 01:06:59 UTC 2010","customfield_12310420":"73173","customfield_12312320":null,"customfield_12310222":"10002_*:*_1_*:*_2845527695_*|*_1_*:*_2_*:*_3313554604_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2010-02-24T01:06:59.444+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-984/watchers","watchCount":1,"isWatching":false},"created":"2009-12-14T18:15:37.145+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"labels":[],"customfield_12312333":null,"customfield_12310230":"ivy hadoop ","customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"2.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12330437","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12330437","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12457193","key":"HIVE-1192","self":"https://issues.apache.org/jira/rest/api/2/issue/12457193","fields":{"summary":"Build fails when hadoop.version=0.20.1","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/3","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/major.svg","name":"Major","id":"3"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133}}}},{"id":"12330073","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12330073","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12447044","key":"HIVE-1120","self":"https://issues.apache.org/jira/rest/api/2/issue/12447044","fields":{"summary":"In ivy offline mode, don't delete downloaded jars","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}},{"id":"12330444","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12330444","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"inwardIssue":{"id":"12457121","key":"HIVE-1190","self":"https://issues.apache.org/jira/rest/api/2/issue/12457121","fields":{"summary":"Configure build to download Hadoop tarballs from Facebook mirror instead of Apache","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/2","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/critical.svg","name":"Critical","id":"2"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/4","id":"4","description":"An improvement or enhancement to an existing feature or task.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype","name":"Improvement","subtask":false,"avatarId":21140}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2010-02-24T01:06:59.441+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/5","description":"A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/resolved.png","name":"Resolved","id":"5","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[{"self":"https://issues.apache.org/jira/rest/api/2/component/12312593","id":"12312593","name":"Build Infrastructure","description":"Tracks issues with build files."}],"timeoriginalestimate":null,"description":"Folks keep running into this problem when building Hive from source:\n\n\n{noformat}\n[ivy:retrieve]\n[ivy:retrieve] :: problems summary ::\n[ivy:retrieve] :::: WARNINGS\n[ivy:retrieve]          [FAILED     ]\nhadoop#core;0.20.1!hadoop.tar.gz(source): invalid md5:\nexpected=hadoop-0.20.1.tar.gz: computed=719e169b7760c168441b49f405855b72\n(138662ms)\n[ivy:retrieve]          [FAILED     ]\nhadoop#core;0.20.1!hadoop.tar.gz(source): invalid md5:\nexpected=hadoop-0.20.1.tar.gz: computed=719e169b7760c168441b49f405855b72\n(138662ms)\n[ivy:retrieve]  ==== hadoop-resolver: tried\n[ivy:retrieve]\nhttp://archive.apache.org/dist/hadoop/core/hadoop-0.20.1/hadoop-0.20.1.tar.gz\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve]          ::              FAILED DOWNLOADS            ::\n[ivy:retrieve]          :: ^ see resolution messages for details  ^ ::\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve]          :: hadoop#core;0.20.1!hadoop.tar.gz(source)\n[ivy:retrieve]          ::::::::::::::::::::::::::::::::::::::::::::::\n[ivy:retrieve]\n[ivy:retrieve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n{noformat}\n\nThe problem appears to be either with a) the Hive build scripts, b) ivy, or c) archive.apache.org\n\nBesides fixing the actual bug, one other option worth considering is to add the Hadoop jars to the\nHive source repository.","customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12429685","id":"12429685","filename":"HIVE-984.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-07T22:39:14.143+0000","size":25433,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12429685/HIVE-984.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12429292","id":"12429292","filename":"HIVE-984.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-03T05:13:09.881+0000","size":25450,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12429292/HIVE-984.patch"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"122722","customfield_12312823":null,"summary":"Building Hive occasionally fails with Ivy error: hadoop#core;0.20.1!hadoop.tar.gz(source): invalid md5:","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12790272","id":"12790272","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"Here's a workaround:\n\n1) In your hive source directory run 'ant clean'.\n2) remove the contents of ~/.ant/cache/hadoop/core/sources\n3) Download the following files to ~/.ant/cache/hadoop/core/sources:\n\n      hadoop-0.17.2.1.tar.gz\n      hadoop-0.17.2.1.tar.gz.asc\n      hadoop-0.18.3.tar.gz\n      hadoop-0.18.3.tar.gz.asc\n      hadoop-0.19.0.tar.gz\n      hadoop-0.19.0.tar.gz.asc\n      hadoop-0.20.0.tar.gz\n      hadoop-0.20.0.tar.gz.asc\n\n4) For each hadoop-xxx.tar.gz file, compute the sha1 checksum using sha1sum, and verify that it matches the sha1 checksum in the corresponding .asc file.\n\nIf it does not match then the file is corrupt and you need to try downloading it again.\n\nThis step is not absolutely necessary since Ivy will check for you during the build process.\n\n5) Try building Hive again following the instructions on the wiki. You shouldn't have any problems if you verified the checksums.\n\nAs an additional note, if you don't care about support for Hadoop 0.17.2.1, or 0.18, etc, you can disable support for these versions (and skip the download) by removing the references to these versions that shims/build.xml and shims/ivy.xml","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2009-12-14T18:20:16.822+0000","updated":"2009-12-14T18:20:16.822+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12795951","id":"12795951","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"* Make download errors less likely by only downloading the specified version of Hadoop instead of 0.17 through 0.20\n* Add 'ivy-delete-cache' ant target so users can easily recover from corrupt ivy downloads. This target only deletes the hadoop artifacts in the Ivy cache.\n* Add descriptions for all of the top-level targets in build.xml\n* Move some common properties to build.properties.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-03T05:13:10.506+0000","updated":"2010-01-03T05:13:10.506+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12795952","id":"12795952","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"This should be a blocker for 0.5.0\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-03T05:14:20.814+0000","updated":"2010-01-03T05:14:20.814+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12797512","id":"12797512","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"Any chance this can go into 0.5.0?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-07T05:02:15.061+0000","updated":"2010-01-07T05:02:15.061+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12797708","id":"12797708","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=namit","name":"namit","key":"namit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Namit Jain","active":true,"timeZone":"Asia/Kolkata"},"body":"resolve:\n[ivy:retrieve] :: Ivy 2.0.0-rc2 - 20081028224207 :: http://ant.apache.org/ivy/ ::\n:: loading settings :: file = /data/users/njain/hive_commit3/hive_commit3/ivy/ivysettings.xml\n\nBUILD FAILED\n/data/users/njain/hive_commit3/hive_commit3/build.xml:142: The following error occurred while executing this line:\n/data/users/njain/hive_commit3/hive_commit3/build.xml:89: The following error occurred while executing this line:\n/data/users/njain/hive_commit3/hive_commit3/build-common.xml:73: java.lang.IllegalArgumentException: ivy.home must be absolute: ${env.IVY_HOME}\n\n\nI got the following error while compiling after applying the patch","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=namit","name":"namit","key":"namit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Namit Jain","active":true,"timeZone":"Asia/Kolkata"},"created":"2010-01-07T17:50:46.486+0000","updated":"2010-01-07T17:50:46.486+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12797815","id":"12797815","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"* Set ivy.home to ${user.home}/.ant if IVY_HOME is unset.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-07T22:39:14.166+0000","updated":"2010-01-07T22:39:14.166+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12797817","id":"12797817","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"@Namit: Updated patch should fix the compilation error.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-07T22:40:54.668+0000","updated":"2010-01-07T22:40:54.668+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12797890","id":"12797890","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=namit","name":"namit","key":"namit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Namit Jain","active":true,"timeZone":"Asia/Kolkata"},"body":"    [junit] \tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1161)\n    [junit] \tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1029)\n    [junit] \tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:979)\n    [junit] \tat org.apache.hadoop.conf.Configuration.set(Configuration.java:404)\n    [junit] \tat org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:308)\n    [junit] \tat org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:318)\n    [junit] \tat org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:243)\n    [junit] \tat org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:332)\n    [junit] \tat org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:312)\n    [junit] \tat org.apache.hadoop.hive.ql.QTestUtil.<init>(QTestUtil.java:173)\n    [junit] \tat org.apache.hadoop.hive.cli.TestCliDriver.setUp(TestCliDriver.java:39)\n    [junit] \tat junit.framework.TestCase.runBare(TestCase.java:125)\n    [junit] \tat junit.framework.TestResult$1.protect(TestResult.java:106)\n    [junit] \tat junit.framework.TestResult.runProtected(TestResult.java:124)\n    [junit] \tat junit.framework.TestResult.run(TestResult.java:109)\n    [junit] \tat junit.framework.TestCase.run(TestCase.java:118)\n    [junit] \tat junit.framework.TestSuite.runTest(TestSuite.java:208)\n    [junit] \tat junit.framework.TestSuite.run(TestSuite.java:203)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:297)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:672)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:567)\n    [junit] Caused by: java.io.FileNotFoundException: /data/users/njain/hive_commit1/hive_commit1/build/hadoopcore/hadoop-0.20.0/conf/core-site.xml (Too many open files)\n    [junit] \tat java.io.FileInputStream.open(Native Method)\n    [junit] \tat java.io.FileInputStream.<init>(FileInputStream.java:106)\n    [junit] \tat java.io.FileInputStream.<init>(FileInputStream.java:66)\n    [junit] \tat sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)\n    [junit] \tat sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)\n    [junit] \tat com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)\n    [junit] \tat com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:186)\n    [junit] \tat com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:771)\n    [junit] \tat com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)\n    [junit] \tat com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)\n    [junit] \tat com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:225)\n    [junit] \tat com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:283)\n    [junit] \tat javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:180)\n    [junit] \tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1078)\n    [junit] \t... 20 more\n    [junit] Exception: java.io.FileNotFoundException: /data/users/njain/hive_commit1/hive_commit1/build/hadoopcore/hadoop-0.20.0/conf/core-site.xml (Too many open files)\n\n\nI am getting a lot of errors like this","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=namit","name":"namit","key":"namit","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Namit Jain","active":true,"timeZone":"Asia/Kolkata"},"created":"2010-01-08T02:49:28.297+0000","updated":"2010-01-08T02:49:28.297+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12798149","id":"12798149","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"@Namit: \n\nIs /data/users/njain/hive_commit1/hive_commit1/* a valid path on your machine?\n\nThis is failing while running the tests with hadoop 0.20.0?\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-08T19:34:00.722+0000","updated":"2010-01-08T19:34:00.722+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12798195","id":"12798195","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zshao","name":"zshao","key":"zshao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zshao&avatarId=14358","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zshao&avatarId=14358","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zshao&avatarId=14358","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zshao&avatarId=14358"},"displayName":"Zheng Shao","active":true,"timeZone":"America/Los_Angeles"},"body":"-1 on the current patch based on comments from HIVE-487.\nI think it makes sense to compile Hive with every supported major version of hadoop so that we only need to deploy a single set of hive for all versions of hadoop.\n\n{quote}\nTodd Lipcon added a comment - 30/Jul/09 05:34 PM\nPatch looks good for me (just inspected it visually over here)\nOne question: once we use these shims, is it possible that we could have just a single hive distribution which works for all versions of Hadoop? I think we may be able to accomplish this by making the shim jar output be libs/shims/hive_shims-{$hadoop.version.prefix}.jar. Then either through ClassLoader magic or shell wrapper magic, we put the right one on the classpath at runtime based on which hadoop version is on the classpath.\nIs this possible? Having different tarballs of hive for different versions of hadoop makes our lives slightly difficult for packaging.\n{quote}\n\n\nCan we keep building 4 versions of hadoop as hive is doing now?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zshao","name":"zshao","key":"zshao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zshao&avatarId=14358","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zshao&avatarId=14358","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zshao&avatarId=14358","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zshao&avatarId=14358"},"displayName":"Zheng Shao","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-08T22:02:10.679+0000","updated":"2010-01-08T22:02:10.679+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12798226","id":"12798226","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Can we keep building 4 versions of hadoop as hive is doing now?\n\nWhat if I modify the build so that all 4 versions are built when running the 'tar' target, but otherwise only the shim corresponding to hadoop.version is downloaded and built (which would be the case for the 'jar' and 'package' targets)?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-01-08T22:59:29.608+0000","updated":"2010-01-08T22:59:29.608+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12829943","id":"12829943","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zshao","name":"zshao","key":"zshao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zshao&avatarId=14358","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zshao&avatarId=14358","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zshao&avatarId=14358","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zshao&avatarId=14358"},"displayName":"Zheng Shao","active":true,"timeZone":"America/Los_Angeles"},"body":"Given HIVE-1120, I think the need to compile with only one version of hadoop goes down a lot.\nWhat do you think?\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=zshao","name":"zshao","key":"zshao","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=zshao&avatarId=14358","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=zshao&avatarId=14358","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=zshao&avatarId=14358","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=zshao&avatarId=14358"},"displayName":"Zheng Shao","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-02-05T03:39:31.169+0000","updated":"2010-02-05T03:39:31.169+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12833665","id":"12833665","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"bq. Given HIVE-1120, I think the need to compile with only one version of hadoop goes down a lot. What do you think? \n\nThat assumes that folks are able to satisfy the dependency at least once in order to populate\ntheir Ivy cache, and all of the traffic on hive-user seems to indicate this is a lot harder than\nit looks.\n\nRight now we always build shims for four different minor versions of Hadoop: 0.17, 0.18, 0.19\nand 0.20. Users also have the option of building against five different versions of Hadoop\nby setting hadoop.version to one that we don't already include. What's the long term plan\nhere? Are we eventually going to build shims for ten different versions of Hadoop every time\nsomeone runs 'ant package'? \n\nAs I said earlier I think it makes sense to compile shims for as many different versions as possible\nwhen building a release tarball (or days from now when we want to build a Hive POM),\nbut I don't see why we should force a user to do this now when list a specific a Hadoop version\nnumber using hadoop.version.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-02-15T02:56:18.766+0000","updated":"2010-02-15T02:56:18.766+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12834519","id":"12834519","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jvs","name":"jvs","key":"jvs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Sichi","active":true,"timeZone":"Etc/UTC"},"body":"I spoke with Zheng about this and here's what we came up with.  Carl, let me know if this works for you.\n\n* If at all possible, we want to keep building all supported shims as part of ant package to make sure that when a change breaks one, the developer finds out early (before even submitting a bad patch)\n* The long term plan does involve deprecating and eventually dropping support for older Hadoop versions.  The fact that Facebook still has some dependencies on 0.17 probably explains why that is currently the oldest version, but the standard voting procedure can be used at the project level for initiating a deprecation process going forward.\n* Regardless of how many Hadoop versions we support, the current Hadoop+ivy situation is definitely broken, and we need to fix it ASAP since it can be a major impediment to new or existing contributors.\n* Before doing anything else, I'm going to see if a more reliable source than archive.apache.org would address the problem.  I'll test this with my home network tomorrow, which usually fails with archive.apache.org.\n* If a more reliable source would help, then we'll see if we can get mirror.facebook.net to provide all supported Hadoop versions (currently only apache.archive.org has the old ones), and if that's the case, then we'll check in a change to build.properties to make it the default source.\n* If either of the above is not the case, then we can do what you proposed in HIVE-1171 (check the Hadoop dependencies into svn instead).\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jvs","name":"jvs","key":"jvs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Sichi","active":true,"timeZone":"Etc/UTC"},"created":"2010-02-16T22:27:21.266+0000","updated":"2010-02-16T22:27:21.266+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12834544","id":"12834544","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"\nbq.  If at all possible, we want to keep building all supported shims as part of ant package to make sure that when a change breaks one, the developer finds out early (before even submitting a bad patch)\n\nUnless your change specifically mucks with the shim code I think it's unlikely that you're going to introduce a compile time error. It seems more likely that you would cause a test error, and that's something you will only catch if you run the full test suite against all supported versions -- something that we only expect Hudson to do.\n\nWhich brings up another point. How do we configure JIRA/Hudson to automatically test submitted patches? The Hadoop and Pig projects are both setup to do this, but I can't find any references to how it was done. Do either of you know how to set this up, or have objections to doing so?\n\nbq. Before doing anything else, I'm going to see if a more reliable source than archive.apache.org would address the problem. I'll test this with my home network tomorrow, which usually fails with archive.apache.org.\n\nOver the weekend I figured out that there are actually two different reasons why people are encountering errors during the download process, and wanted to make sure that everyone else is aware of this as well:\n\n# Unable to connect to archive.apache.org: We can fix this by adding additional apache mirrors (see http://www.apache.org/mirrors/) to the hadoop-source resolver in ivysettings, and also by letting people know that they can explicitly set the mirror location using the hadoop.mirror property.\n\n# -Dhadoop.version=0.20.1: When people set hadoop.version to 0.20.1 it causes ant to download both 0.20.0 *and* 0.20.1, which is unnecessary since the API does not change between patch releases. But the bigger problem is that 0.20.1's md5 checksum file on archive.apache.org contains an md5 hash along with a bunch of other garbage that breaks ivy. We can fix this either by disabling checksums for archive.apache.org (set ivy.checksums=\"\" on that resolver), or by enhancing the build script so that it ignores patch release numbers and maps 0.20.1 to 0.20.0.\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-02-16T23:19:16.108+0000","updated":"2010-02-16T23:19:16.108+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12834558","id":"12834558","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jvs","name":"jvs","key":"jvs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Sichi","active":true,"timeZone":"Etc/UTC"},"body":"Shims:  you may be right, but I guess the principle is that all source code checked in ought to be covered by the build if possible.  It's arguable that we should actually do even more in this respect (rather than less), since for example in HIVE-1136 we just hit a case where one of my changes was incompatible with an old Hadoop version (nothing to do with shims).  If we built against all supported Hadoop versions as part of ant test, this would have been caught when I ran tests myself (so Zheng would never have had to spend time testing my bad patch and rejecting it).  ant test might be a reasonable place for that, since test time will always be orders of magnitude longer than build time.  (But note:  I'm not proposing to run tests on all versions except in Hudson!)\n\nHudson automatically testing patches:  I don't know the answer to that one, but it sounds like a very high-value automation to me if the resources are available, and my opinion on the version download issue might change if this were working reliably with permanently committed resources.\n\narchive.apache.org:  the default mirroring for Hadoop seems to be 0.18.3, 0.19.2, and 0.20.1 (that's what I see when I browse most of the mirrors), which doesn't match what Hive currently wants (0.17.2.1, 0.18.3, 0.19.0, and 0.20.0).  That's why I was thinking we might need a custom setup on mirror.facebook.net.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jvs","name":"jvs","key":"jvs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Sichi","active":true,"timeZone":"Etc/UTC"},"created":"2010-02-17T00:03:17.123+0000","updated":"2010-02-17T00:03:17.123+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12834970","id":"12834970","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jvs","name":"jvs","key":"jvs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Sichi","active":true,"timeZone":"Etc/UTC"},"body":"I was able to verify that using a reliable server (tested with a privately hosted server of my own) allowed for successful artifact download from my home network.\n\nNext step is to talk to some Facebook peeps to see if we can get what we need set up on mirror.facebook.net.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jvs","name":"jvs","key":"jvs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Sichi","active":true,"timeZone":"Etc/UTC"},"created":"2010-02-17T20:11:40.998+0000","updated":"2010-02-17T20:11:40.998+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12837125","id":"12837125","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jvs","name":"jvs","key":"jvs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Sichi","active":true,"timeZone":"Etc/UTC"},"body":"Mirror is available now; see HIVE-984 for configuration patch to use it.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=jvs","name":"jvs","key":"jvs","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"John Sichi","active":true,"timeZone":"Etc/UTC"},"created":"2010-02-23T07:37:48.044+0000","updated":"2010-02-23T07:37:48.044+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12443287/comment/12837558","id":"12837558","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"Fixed in HIVE-1190.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2010-02-24T01:06:59.432+0000","updated":"2010-02-24T01:06:59.432+0000"}],"maxResults":19,"total":19,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-984/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i0lcsf:"}}