{"expand":"renderedFields,names,schema,operations,editmeta,changelog,versionedRepresentations","id":"12663566","self":"https://issues.apache.org/jira/rest/api/2/issue/12663566","key":"HIVE-5087","fields":{"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/1","id":"1","description":"A problem which impairs or prevents the functions of the product.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype","name":"Bug","subtask":false,"avatarId":21133},"timespent":null,"project":{"self":"https://issues.apache.org/jira/rest/api/2/project/12310843","id":"12310843","key":"HIVE","name":"Hive","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/projectavatar?pid=12310843&avatarId=11935","24x24":"https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12310843&avatarId=11935","16x16":"https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12310843&avatarId=11935","32x32":"https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12310843&avatarId=11935"},"projectCategory":{"self":"https://issues.apache.org/jira/rest/api/2/projectCategory/10292","id":"10292","description":"Scalable Distributed Computing","name":"Hadoop"}},"fixVersions":[{"self":"https://issues.apache.org/jira/rest/api/2/version/12324312","id":"12324312","description":"released","name":"0.12.0","archived":false,"released":true,"releaseDate":"2013-10-15"}],"aggregatetimespent":null,"resolution":{"self":"https://issues.apache.org/jira/rest/api/2/resolution/1","id":"1","description":"A fix for this issue is checked into the tree and tested.","name":"Fixed"},"customfield_12312322":null,"customfield_12310220":"2013-08-26T02:26:37.137+0000","customfield_12312520":null,"customfield_12312323":null,"customfield_12312521":"Tue Aug 26 05:28:09 UTC 2014","customfield_12310420":"343567","customfield_12312320":null,"customfield_12310222":"10002_*:*_3_*:*_2436730277_*|*_1_*:*_3_*:*_2155563217_*|*_5_*:*_1_*:*_0","customfield_12312321":null,"resolutiondate":"2013-10-06T07:20:33.316+0000","workratio":-1,"customfield_12312328":null,"customfield_12312329":null,"customfield_12312923":null,"customfield_12312326":null,"customfield_12312920":null,"customfield_12310300":null,"customfield_12312327":null,"customfield_12312921":null,"customfield_12312324":null,"customfield_12312720":null,"customfield_12312325":null,"lastViewed":null,"watches":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-5087/watchers","watchCount":13,"isWatching":false},"created":"2013-08-14T03:42:19.849+0000","customfield_12310192":null,"customfield_12310191":null,"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/1","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/blocker.svg","name":"Blocker","id":"1"},"labels":["TODOC12"],"customfield_12312333":null,"customfield_12310230":null,"customfield_12312334":null,"customfield_12313422":"false","customfield_12310310":"9.0","customfield_12312331":null,"customfield_12312332":null,"timeestimate":null,"aggregatetimeoriginalestimate":null,"customfield_12311120":null,"customfield_12312330":null,"versions":[],"issuelinks":[{"id":"12395163","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12395163","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"12438744","key":"HIVE-896","self":"https://issues.apache.org/jira/rest/api/2/issue/12438744","fields":{"summary":"Add LEAD/LAG/FIRST/LAST analytical windowing functions to Hive.","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/4","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/minor.svg","name":"Minor","id":"4"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/2","id":"2","description":"A new feature of the product, which has yet to be developed.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype","name":"New Feature","subtask":false,"avatarId":21141}}}},{"id":"12523930","self":"https://issues.apache.org/jira/rest/api/2/issueLink/12523930","type":{"id":"10030","name":"Reference","inward":"is related to","outward":"relates to","self":"https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"},"outwardIssue":{"id":"13129873","key":"HIVE-18425","self":"https://issues.apache.org/jira/rest/api/2/issue/13129873","fields":{"summary":"Document MATCHPATH UDF","status":{"self":"https://issues.apache.org/jira/rest/api/2/status/1","description":"The issue is open and ready for the assignee to start work on it.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/open.png","name":"Open","id":"1","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/2","id":2,"key":"new","colorName":"blue-gray","name":"To Do"}},"priority":{"self":"https://issues.apache.org/jira/rest/api/2/priority/5","iconUrl":"https://issues.apache.org/jira/images/icons/priorities/trivial.svg","name":"Trivial","id":"5"},"issuetype":{"self":"https://issues.apache.org/jira/rest/api/2/issuetype/3","id":"3","description":"A task that needs to be done.","iconUrl":"https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype","name":"Task","subtask":false,"avatarId":21148}}}}],"customfield_12312339":null,"assignee":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"customfield_12312337":null,"customfield_12312338":null,"updated":"2018-01-10T14:49:48.686+0000","customfield_12312335":null,"customfield_12312336":null,"status":{"self":"https://issues.apache.org/jira/rest/api/2/status/6","description":"The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.","iconUrl":"https://issues.apache.org/jira/images/icons/statuses/closed.png","name":"Closed","id":"6","statusCategory":{"self":"https://issues.apache.org/jira/rest/api/2/statuscategory/3","id":3,"key":"done","colorName":"green","name":"Done"}},"components":[],"timeoriginalestimate":null,"description":null,"customfield_10010":null,"timetracking":{},"customfield_12312026":null,"customfield_12312023":null,"customfield_12310320":null,"customfield_12312024":null,"customfield_12312340":null,"attachment":[{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12602028","id":"12602028","filename":"HIVE-5087.1.patch.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T03:24:09.099+0000","size":53536,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12602028/HIVE-5087.1.patch.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12606978","id":"12606978","filename":"HIVE-5087.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-05T06:22:07.594+0000","size":81379,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12606978/HIVE-5087.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12602059","id":"12602059","filename":"HIVE-5087.99.patch.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T17:50:17.367+0000","size":45169,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12602059/HIVE-5087.99.patch.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12602026","id":"12602026","filename":"HIVE-5087.patch.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T02:27:47.060+0000","size":53536,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12602026/HIVE-5087.patch.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12597890","id":"12597890","filename":"HIVE-5087.patch.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-08-14T05:23:00.418+0000","size":79585,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12597890/HIVE-5087.patch.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12607061","id":"12607061","filename":"HIVE-5087-branch0.12.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-06T07:19:01.742+0000","size":81445,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12607061/HIVE-5087-branch0.12.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12602077","id":"12602077","filename":"HIVE-5087-matchpath.1.patch.txt","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-09T02:07:45.178+0000","size":81445,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12602077/HIVE-5087-matchpath.1.patch.txt"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12606977","id":"12606977","filename":"HIVE-5087-matchpath.2.patch","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-05T06:18:13.312+0000","size":81379,"mimeType":"text/plain","content":"https://issues.apache.org/jira/secure/attachment/12606977/HIVE-5087-matchpath.2.patch"},{"self":"https://issues.apache.org/jira/rest/api/2/attachment/12602023","id":"12602023","filename":"regex_path.diff","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T00:31:40.772+0000","size":45558,"mimeType":"text/x-patch","content":"https://issues.apache.org/jira/secure/attachment/12602023/regex_path.diff"}],"aggregatetimeestimate":null,"customfield_12312341":null,"customfield_12312220":null,"customfield_12312022":null,"customfield_12310921":null,"customfield_12310920":"343871","customfield_12312823":null,"summary":"Rename npath UDF to matchpath","creator":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"subtasks":[],"customfield_12310291":null,"reporter":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"customfield_12310290":null,"aggregateprogress":{"progress":0,"total":0},"customfield_12311024":null,"environment":null,"customfield_12313520":null,"customfield_12311020":null,"duedate":null,"customfield_12310250":null,"progress":{"progress":0,"total":0},"comment":{"comments":[{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13739237","id":"13739237","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"Not complete, stupid q tests.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-08-14T05:23:00.421+0000","updated":"2013-08-14T05:23:00.421+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13749796","id":"13749796","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=abreshears%40cloudera.com","name":"abreshears@cloudera.com","key":"abreshears@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alex Breshears","active":true,"timeZone":"Etc/UTC"},"body":"Couple quick questions: what's driving the rename, and what will the new function be named?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=abreshears%40cloudera.com","name":"abreshears@cloudera.com","key":"abreshears@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alex Breshears","active":true,"timeZone":"Etc/UTC"},"created":"2013-08-26T02:26:37.137+0000","updated":"2013-08-26T02:26:37.137+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13749806","id":"13749806","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=abreshears%40cloudera.com","name":"abreshears@cloudera.com","key":"abreshears@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alex Breshears","active":true,"timeZone":"Etc/UTC"},"body":"I guess I could read the patch to get the new name :) regex_path it is.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=abreshears%40cloudera.com","name":"abreshears@cloudera.com","key":"abreshears@cloudera.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alex Breshears","active":true,"timeZone":"Etc/UTC"},"created":"2013-08-26T02:44:32.259+0000","updated":"2013-08-26T02:44:32.259+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13749807","id":"13749807","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"body":"From the last [Hive report|http://www.apache.org/foundation/records/minutes/2013/board_minutes_2013_06_19.txt] to the Apache board\n\n* In late May Teradata requested that the project remove a UDF\n  ('npath') which was included in the 0.11.0 release. Teradata\n  alleges that this UDF violates a US patent they hold as well\n  as their common law trademark. The Hive PMC has referred this issue\n  to the ASF Legal Board.\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=alangates","name":"alangates","key":"alangates","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Alan Gates","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-08-26T02:50:31.143+0000","updated":"2013-08-26T02:50:31.143+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761159","id":"13761159","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"[~rhbutani] Do you think 'regex_path' is good name for this feature? If not can you please suggest an alternative? Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-08T01:40:36.142+0000","updated":"2013-09-08T01:40:36.142+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761173","id":"13761173","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 no tests executed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12602026/HIVE-5087.patch.txt\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/657/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/657/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-657/source-prep.txt\n+ mkdir -p maven ivy\n+ [[ svn = \\s\\v\\n ]]\n+ [[ -n '' ]]\n+ [[ -d apache-svn-trunk-source ]]\n+ [[ ! -d apache-svn-trunk-source/.svn ]]\n+ [[ ! -d apache-svn-trunk-source ]]\n+ cd apache-svn-trunk-source\n+ svn revert -R .\nReverted 'conf/hive-default.xml.template'\nReverted 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java'\nReverted 'service/src/test/org/apache/hive/service/cli/CLIServiceTest.java'\nReverted 'service/src/java/org/apache/hive/service/cli/OperationState.java'\nReverted 'service/src/java/org/apache/hive/service/cli/ICLIService.java'\nReverted 'service/src/java/org/apache/hive/service/cli/EmbeddedCLIServiceClient.java'\nReverted 'service/src/java/org/apache/hive/service/cli/session/HiveSession.java'\nReverted 'service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java'\nReverted 'service/src/java/org/apache/hive/service/cli/session/SessionManager.java'\nReverted 'service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java'\nReverted 'service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java'\nReverted 'service/src/java/org/apache/hive/service/cli/operation/OperationManager.java'\nReverted 'service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIServiceClient.java'\nReverted 'service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java'\nReverted 'service/src/java/org/apache/hive/service/cli/CLIServiceClient.java'\nReverted 'service/src/java/org/apache/hive/service/cli/CLIService.java'\nReverted 'service/src/gen/thrift/gen-py/TCLIService/ttypes.py'\nReverted 'service/src/gen/thrift/gen-cpp/TCLIService_types.cpp'\nReverted 'service/src/gen/thrift/gen-cpp/TCLIService_types.h'\nReverted 'service/src/gen/thrift/gen-rb/t_c_l_i_service_types.rb'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOpenSessionReq.java'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TExecuteStatementReq.java'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOperationState.java'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TProtocolVersion.java'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOpenSessionResp.java'\nReverted 'service/if/TCLIService.thrift'\n++ awk '{print $2}'\n++ egrep -v '^X|^Performing status on external'\n++ svn status --no-ignore\n+ rm -rf build hcatalog/build common/src/gen\n+ svn update\n\nFetching external item into 'hcatalog/src/test/e2e/harness'\nExternal at revision 1520828.\n\nAt revision 1520828.\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nThe patch does not appear to apply with p0 to p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-09-08T02:39:23.703+0000","updated":"2013-09-08T02:39:23.703+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761187","id":"13761187","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 no tests executed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12602028/HIVE-5087.1.patch.txt\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/658/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/658/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-658/source-prep.txt\n+ mkdir -p maven ivy\n+ [[ svn = \\s\\v\\n ]]\n+ [[ -n '' ]]\n+ [[ -d apache-svn-trunk-source ]]\n+ [[ ! -d apache-svn-trunk-source/.svn ]]\n+ [[ ! -d apache-svn-trunk-source ]]\n+ cd apache-svn-trunk-source\n+ svn revert -R .\n++ awk '{print $2}'\n++ egrep -v '^X|^Performing status on external'\n++ svn status --no-ignore\n+ rm -rf\n+ svn update\n\nFetching external item into 'hcatalog/src/test/e2e/harness'\nExternal at revision 1520834.\n\nAt revision 1520834.\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nThe patch does not appear to apply with p0 to p2\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-09-08T03:43:55.410+0000","updated":"2013-09-08T03:43:55.410+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761188","id":"13761188","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"[~brocknoland]\nDO you get what is going on? This patch applies cleanly.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T03:47:23.758+0000","updated":"2013-09-08T03:47:23.758+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761463","id":"13761463","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"Looks like it modifies a file which does not exist?\n\n{noformat}\nIndex: ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/RegexPath.java\n===================================================================\n--- ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/RegexPath.java\t(working copy)\n+++ ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/RegexPath.java\t(working copy)\n@@ -77,15 +77,14 @@\n  * \"tpath\" is available. Path is a collection of rows that represents the matching Path.\n  * </ol>\n  */\n-public class NPath extends TableFunctionEvaluator\n-{\n+public class RegexPath extends TableFunctionEvaluator {\n{noformat}","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-08T14:58:17.778+0000","updated":"2013-09-08T14:58:17.778+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761471","id":"13761471","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"[~brocknoland]\nI did an svn mv, then I edited the file I moved.Is there another way I can send this patch?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T15:32:13.975+0000","updated":"2013-09-08T15:32:13.975+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761476","id":"13761476","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"Sorry, I don't know much about svn mv and then generating a patch which applies with the `patch' command. The way I've handled this scenarios is generating a patch which does the add and then delete. Then on commit do the svn mv as opposed to the add and delete.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-08T15:44:48.182+0000","updated":"2013-09-08T15:44:48.182+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761480","id":"13761480","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"body":"[~cartershanklin] suggested 'matchPath'; imho this is a better name.\nBut if it is too much trouble to change the name gain, 'regex_path' is good too.\n\nHopefully, now that the Java classname is changed away from NPath; \nchanging the function name again (w/o changing the java classname) is a small change.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-08T16:37:21.131+0000","updated":"2013-09-08T16:37:21.131+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761481","id":"13761481","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"I'm really not in the mood to change it at this point. We can alias a function easily enough. I advocated removing this component because I really did not want to deal with it. Somehow, I am dealing with it anyway.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T16:49:10.916+0000","updated":"2013-09-08T16:49:10.916+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761482","id":"13761482","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"This is a blocker. q: who is going to test, +1, and apply this patch? because its ready for review, and the pre-commit tests wont work.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T16:58:06.979+0000","updated":"2013-09-08T16:58:06.979+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761484","id":"13761484","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"[~cartershanklin] and [~rhbutani] I agree that this name might be better. But when I opened this ticket I left it open a long time for comments, no one made any. Now its 11:th hour . Also for reference I am planning to build a new udf for a similar role that does not use regex at all, instead it will use a \"little-language\". This should protect us from the infringment claims that terrordata is making.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T17:00:47.546+0000","updated":"2013-09-08T17:00:47.546+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761492","id":"13761492","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"body":"Just tried to apply the patch. Have the same issue as Brock pointed out. The patch doesn't apply because of \n{noformat}\n--- ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/RegexPath.java\t(working copy)\n+++ ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/RegexPath.java\t(working copy)\n{noformat}\n\nIf you are busy, happy to take this one of your plate and create a new patch...\n\nNot clear about your udf comment; went through a lengthy process with Teradata through our legal on why there is no infringement. Are you saying there are things that can still come up? \nAnd if you look at the current implementation there is no use of regex;I am almost sure they don't use Regexes either. In both cases the interface exposes a Regex like language. As far as my understanding goes, the main claims in their patent are about evaluating Path expressions in constant space and time. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-08T17:44:29.306+0000","updated":"2013-09-08T17:44:29.306+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761496","id":"13761496","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"I have uploaded a new patch which should testnamed 99.patch.txt. [~rhbutani] \n\nThe name does not matter. I opened this ticket and said clearly on the private email chain something to the effect of \"I am calling it regex_path and opening a ticket, if anyone has any other suggestions for names please move the conversation to the ticket. No one said anything.\n\n{noformat}\nNot clear about your udf comment; went through a lengthy process with Teradata through our legal on why there is no infringement. Are you saying there are things that can still come up? \n{noformat}\n\nI turned my attention away from the email chain. I am not clear the infringement issue is resolved. This rename is only to deal with the trademark issue.\n\nAs a result, I want to build a new UDF(I will open a ticket in a couple days) that can be used in to do what NPath does and possibly more, it will be a windowing function but it will not involve specifying patterns in the way npath currently does. It will have different syntax as well.  \n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T18:04:39.059+0000","updated":"2013-09-08T18:04:39.059+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761499","id":"13761499","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:red}Overall{color}: -1 no tests executed\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12602059/HIVE-5087.99.patch.txt\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/662/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/662/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nTests failed with: NonZeroExitCodeException: Command 'bash /data/hive-ptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ [[ -n '' ]]\n+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128'\n+ cd /data/hive-ptest/working/\n+ tee /data/hive-ptest/logs/PreCommit-HIVE-Build-662/source-prep.txt\n+ mkdir -p maven ivy\n+ [[ svn = \\s\\v\\n ]]\n+ [[ -n '' ]]\n+ [[ -d apache-svn-trunk-source ]]\n+ [[ ! -d apache-svn-trunk-source/.svn ]]\n+ [[ ! -d apache-svn-trunk-source ]]\n+ cd apache-svn-trunk-source\n+ svn revert -R .\nReverted 'conf/hive-default.xml.template'\nReverted 'common/src/java/org/apache/hadoop/hive/conf/HiveConf.java'\nReverted 'service/src/test/org/apache/hive/service/cli/CLIServiceTest.java'\nReverted 'service/src/java/org/apache/hive/service/cli/OperationState.java'\nReverted 'service/src/java/org/apache/hive/service/cli/ICLIService.java'\nReverted 'service/src/java/org/apache/hive/service/cli/EmbeddedCLIServiceClient.java'\nReverted 'service/src/java/org/apache/hive/service/cli/session/HiveSession.java'\nReverted 'service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java'\nReverted 'service/src/java/org/apache/hive/service/cli/session/SessionManager.java'\nReverted 'service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java'\nReverted 'service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java'\nReverted 'service/src/java/org/apache/hive/service/cli/operation/OperationManager.java'\nReverted 'service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIServiceClient.java'\nReverted 'service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java'\nReverted 'service/src/java/org/apache/hive/service/cli/CLIServiceClient.java'\nReverted 'service/src/java/org/apache/hive/service/cli/CLIService.java'\nReverted 'service/src/gen/thrift/gen-py/TCLIService/ttypes.py'\nReverted 'service/src/gen/thrift/gen-cpp/TCLIService_types.cpp'\nReverted 'service/src/gen/thrift/gen-cpp/TCLIService_types.h'\nReverted 'service/src/gen/thrift/gen-rb/t_c_l_i_service_types.rb'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOpenSessionReq.java'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TExecuteStatementReq.java'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOperationState.java'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TProtocolVersion.java'\nReverted 'service/src/gen/thrift/gen-javabean/org/apache/hive/service/cli/thrift/TOpenSessionResp.java'\nReverted 'service/if/TCLIService.thrift'\n++ awk '{print $2}'\n++ egrep -v '^X|^Performing status on external'\n++ svn status --no-ignore\n+ rm -rf build hcatalog/build hcatalog/core/build hcatalog/storage-handlers/hbase/build hcatalog/server-extensions/build hcatalog/webhcat/svr/build hcatalog/webhcat/java-client/build hcatalog/hcatalog-pig-adapter/build common/src/gen\n+ svn update\n\nFetching external item into 'hcatalog/src/test/e2e/harness'\nExternal at revision 1520889.\n\nAt revision 1520889.\n+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ patchFilePath=/data/hive-ptest/working/scratch/build.patch\n+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]\n+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh\n+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch\nGoing to apply patch with: patch -p0\npatching file build.properties\npatching file ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\npatching file ql/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java\npatching file ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/RegexPath.java\npatching file ql/src/test/queries/clientpositive/ptf_regexpath.q\npatching file ql/src/test/queries/clientpositive/ptf_register_tblfn.q\npatching file ql/src/test/results/clientpositive/ptf_regexpath.q.out\npatching file ql/src/test/results/clientpositive/ptf_register_tblfn.q.out\npatching file ql/src/test/results/clientpositive/show_functions.q.out\n+ [[ true == \\t\\r\\u\\e ]]\n+ rm -rf /data/hive-ptest/working/ivy /data/hive-ptest/working/maven\n+ mkdir /data/hive-ptest/working/ivy /data/hive-ptest/working/maven\n+ ant -Dtest.continue.on.failure=true -Dtest.silent=false -Divy.default.ivy.user.dir=/data/hive-ptest/working/ivy -Dmvn.local.repo=/data/hive-ptest/working/maven clean package test -Dtestcase=nothing\nBuildfile: /data/hive-ptest/working/apache-svn-trunk-source/build.xml\n\nclean:\n     [echo] Project: hive\n\nclean:\n     [echo] Project: anttasks\n\nclean:\n     [echo] Project: shims\n\nclean:\n     [echo] Project: common\n\nclean:\n     [echo] Project: serde\n\nclean:\n     [echo] Project: metastore\n\nclean:\n     [echo] Project: ql\n\nclean:\n     [echo] Project: contrib\n\nclean:\n     [echo] Project: service\n\nclean:\n     [echo] Project: cli\n\nclean:\n     [echo] Project: jdbc\n\nclean:\n     [echo] Project: beeline\n\nclean:\n     [echo] Project: hwi\n\nclean:\n     [echo] Project: hbase-handler\n\nclean:\n     [echo] Project: testutils\n\nclean:\n     [echo] hcatalog\n\nclean:\n     [echo] hcatalog-core\n\nclean:\n     [echo] hcatalog-pig-adapter\n\nclean:\n     [echo] hcatalog-server-extensions\n\nclean:\n     [echo] webhcat\n\nclean:\n     [echo] webhcat-java-client\n\nclean:\n\nclean:\n     [echo] Project: odbc\n     [exec] rm -rf /data/hive-ptest/working/apache-svn-trunk-source/build/odbc /data/hive-ptest/working/apache-svn-trunk-source/build/service/objs /data/hive-ptest/working/apache-svn-trunk-source/build/ql/objs /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/objs\n\nclean-online:\n     [echo] Project: hive\n\nclean-offline:\n\nivy-init-dirs:\n     [echo] Project: hive\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/maven\n\nivy-download:\n     [echo] Project: hive\n      [get] Getting: http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.3.0/ivy-2.3.0.jar\n      [get] To: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/ivy-2.3.0.jar\n\nivy-probe-antlib:\n     [echo] Project: hive\n\nivy-init-antlib:\n     [echo] Project: hive\n\ncompile-ant-tasks:\n     [echo] Project: hive\n\ncreate-dirs:\n     [echo] Project: anttasks\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/jexl/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hadoopcore\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/test/resources does not exist.\n\ninit:\n     [echo] Project: anttasks\n\nivy-init-settings:\n     [echo] Project: anttasks\n\nivy-resolve:\n     [echo] Project: anttasks\n[ivy:resolve] :: Apache Ivy 2.3.0 - 20130110142753 :: http://ant.apache.org/ivy/ ::\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-anttasks;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound velocity#velocity;1.5 in maven2\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.4/commons-lang-2.4.jar ...\n[ivy:resolve] ..... (255kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-lang#commons-lang;2.4!commons-lang.jar (26ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/velocity/velocity/1.5/velocity-1.5.jar ...\n[ivy:resolve] ....... (382kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] velocity#velocity;1.5!velocity.jar (33ms)\n[ivy:resolve] :: resolution report :: resolve 4735ms :: artifacts dl 78ms\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-anttasks-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-anttasks-default.html\n\nivy-retrieve:\n     [echo] Project: anttasks\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-anttasks\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t2 artifacts copied, 0 already retrieved (638kB/11ms)\n\ncompile:\n     [echo] anttasks\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/ant/build.xml:38: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds\n    [javac] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/classes\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java uses or overrides a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/org/apache/hadoop/hive/ant/DistinctElementsClassPath.java uses unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n\ndeploy-ant-tasks:\n     [echo] Project: hive\n\ncreate-dirs:\n     [echo] Project: anttasks\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/test/resources does not exist.\n\ninit:\n     [echo] Project: anttasks\n\nivy-init-settings:\n     [echo] Project: anttasks\n\nivy-resolve:\n     [echo] Project: anttasks\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-anttasks;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound velocity#velocity;1.5 in maven2\n[ivy:resolve] :: resolution report :: resolve 506ms :: artifacts dl 3ms\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-anttasks-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-anttasks-default.html\n\nivy-retrieve:\n     [echo] Project: anttasks\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-anttasks\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t0 artifacts copied, 2 already retrieved (0kB/10ms)\n\ncompile:\n     [echo] anttasks\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/ant/build.xml:38: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds\n\njar:\n     [echo] anttasks\n     [copy] Copying 1 file to /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/classes/org/apache/hadoop/hive/ant\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/hive-anttasks-0.12.0-SNAPSHOT.jar\n\ninit:\n     [echo] Project: hive\n\ncreate-dirs:\n     [echo] Project: anttasks\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/ant/src/test/resources does not exist.\n\ninit:\n     [echo] Project: anttasks\n\ncreate-dirs:\n     [echo] Project: shims\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/shims\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/shims/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/shims/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/shims/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/shims/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/shims/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/shims/src/test/resources does not exist.\n\ninit:\n     [echo] Project: shims\n\ncreate-dirs:\n     [echo] Project: common\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/common\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/common/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/common/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/common/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/common/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/common/test/resources\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/build/common/test/resources\n\ninit:\n     [echo] Project: common\n\ncreate-dirs:\n     [echo] Project: serde\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/serde\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/serde/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/serde/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/serde/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/serde/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/serde/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/resources does not exist.\n\ninit:\n     [echo] Project: serde\n\ncreate-dirs:\n     [echo] Project: metastore\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/metastore\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/test/resources does not exist.\n\ninit:\n     [echo] Project: metastore\n\ncreate-dirs:\n     [echo] Project: ql\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ql\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ql/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ql/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ql/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ql/test/resources\n     [copy] Copying 4 files to /data/hive-ptest/working/apache-svn-trunk-source/build/ql/test/resources\n\ninit:\n     [echo] Project: ql\n\ncreate-dirs:\n     [echo] Project: contrib\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/contrib\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/contrib/src/test/resources does not exist.\n\ninit:\n     [echo] Project: contrib\n\ncreate-dirs:\n     [echo] Project: service\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/service\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/service/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/service/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/service/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/service/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/service/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/service/src/test/resources does not exist.\n\ninit:\n     [echo] Project: service\n\ncreate-dirs:\n     [echo] Project: cli\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/cli\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/cli/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/cli/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/cli/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/cli/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/cli/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/cli/src/test/resources does not exist.\n\ninit:\n     [echo] Project: cli\n\ncreate-dirs:\n     [echo] Project: jdbc\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/jdbc/src/test/resources does not exist.\n\ninit:\n     [echo] Project: jdbc\n\ncreate-dirs:\n     [echo] Project: beeline\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/beeline\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/beeline/src/test/resources does not exist.\n\ninit:\n     [echo] Project: beeline\n\ncreate-dirs:\n     [echo] Project: hwi\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hwi\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources does not exist.\n\ninit:\n     [echo] Project: hwi\n\ncreate-dirs:\n     [echo] Project: hbase-handler\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/resources does not exist.\n\ninit:\n     [echo] Project: hbase-handler\n\ncreate-dirs:\n     [echo] Project: testutils\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/testutils\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/test\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/test/src\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/test/classes\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/test/resources\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources does not exist.\n\ninit:\n     [echo] Project: testutils\n\ninit:\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/build/hcatalog-0.13.0-SNAPSHOT\n\njar:\n     [echo] Project: hive\n\nivy-init-settings:\n     [echo] Project: shims\n\ncheck-ivy:\n     [echo] Project: shims\n\nivy-resolve:\n     [echo] Project: shims\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-shims;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.zookeeper#zookeeper;3.4.3 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libthrift;0.9.0 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging;1.0.4 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging-api;1.0.4 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-core-asl;1.8.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-mapper-asl;1.8.8 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound commons-io#commons-io;2.4 in maven2\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.3/zookeeper-3.4.3.jar ...\n[ivy:resolve] .............. (749kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.zookeeper#zookeeper;3.4.3!zookeeper.jar (20ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar ...\n[ivy:resolve] ....... (339kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.thrift#libthrift;0.9.0!libthrift.jar (12ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-logging/commons-logging/1.0.4/commons-logging-1.0.4.jar ...\n[ivy:resolve] .. (37kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-logging#commons-logging;1.0.4!commons-logging.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-logging/commons-logging-api/1.0.4/commons-logging-api-1.0.4.jar ...\n[ivy:resolve] .. (25kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-logging#commons-logging-api;1.0.4!commons-logging-api.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar ...\n[ivy:resolve] ..... (222kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.8.8!jackson-core-asl.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar ...\n[ivy:resolve] ............ (652kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.8.8!jackson-mapper-asl.jar (17ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/log4j/log4j/1.2.16/log4j-1.2.16.jar ...\n[ivy:resolve] ......... (470kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] log4j#log4j;1.2.16!log4j.jar(bundle) (14ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/google/guava/guava/11.0.2/guava-11.0.2.jar ...\n[ivy:resolve] ............................ (1609kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.google.guava#guava;11.0.2!guava.jar (34ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-io/commons-io/2.4/commons-io-2.4.jar ...\n[ivy:resolve] .... (180kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-io#commons-io;2.4!commons-io.jar (9ms)\n[ivy:resolve] :: resolution report :: resolve 8023ms :: artifacts dl 154ms\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   9   |   9   |   9   |   0   ||   9   |   9   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-shims-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-shims-default.html\n\nmake-pom:\n     [echo] Project: shims\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/shims/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: shims\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/shims/src/test/resources does not exist.\n\ninit:\n     [echo] Project: shims\n\nivy-retrieve:\n     [echo] Project: shims\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-shims\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t9 artifacts copied, 0 already retrieved (4287kB/21ms)\n\ncompile:\n     [echo] Project: shims\n     [echo] Building shims 0.20\n\nbuild-shims:\n     [echo] Project: shims\n     [echo] Compiling /data/hive-ptest/working/apache-svn-trunk-source/shims/src/common/java;/data/hive-ptest/working/apache-svn-trunk-source/shims/src/0.20/java against hadoop 0.20.2 (/data/hive-ptest/working/apache-svn-trunk-source/build/hadoopcore/hadoop-0.20.2)\n\nivy-init-settings:\n     [echo] Project: shims\n\nivy-resolve-hadoop-shim:\n     [echo] Project: shims\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-shims;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [hadoop0.20.shim]\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-core;0.20.2 in maven2\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound xmlenc#xmlenc;0.52 in maven2\n[ivy:resolve] \tfound commons-httpclient#commons-httpclient;3.0.1 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging;1.0.3 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.3 in maven2\n[ivy:resolve] \tfound commons-net#commons-net;1.4.1 in maven2\n[ivy:resolve] \tfound oro#oro;2.0.8 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jetty;6.1.14 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jetty-util;6.1.14 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#servlet-api-2.5;6.1.14 in maven2\n[ivy:resolve] \tfound tomcat#jasper-runtime;5.5.12 in maven2\n[ivy:resolve] \tfound tomcat#jasper-compiler;5.5.12 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jsp-api-2.1;6.1.14 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jsp-2.1;6.1.14 in maven2\n[ivy:resolve] \tfound org.eclipse.jdt#core;3.1.1 in maven2\n[ivy:resolve] \tfound ant#ant;1.6.5 in maven2\n[ivy:resolve] \tfound commons-el#commons-el;1.0 in maven2\n[ivy:resolve] \tfound net.java.dev.jets3t#jets3t;0.7.1 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging;1.1.1 in maven2\n[ivy:resolve] \tfound net.sf.kosmosfs#kfs;0.3 in maven2\n[ivy:resolve] \tfound junit#junit;4.5 in maven2\n[ivy:resolve] \tfound hsqldb#hsqldb;1.8.0.10 in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-tools;0.20.2 in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-test;0.20.2 in maven2\n[ivy:resolve] \tfound org.apache.ftpserver#ftplet-api;1.0.0 in maven2\n[ivy:resolve] \tfound org.apache.mina#mina-core;2.0.0-M5 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.5.2 in maven2\n[ivy:resolve] \tfound org.apache.ftpserver#ftpserver-core;1.0.0 in maven2\n[ivy:resolve] \tfound org.apache.ftpserver#ftpserver-deprecated;1.0.0-M2 in maven2\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.20.2/hadoop-core-0.20.2.jar ...\n[ivy:resolve] ............................................ (2624kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-core;0.20.2!hadoop-core.jar (52ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-tools/0.20.2/hadoop-tools-0.20.2.jar ...\n[ivy:resolve] ... (68kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-tools;0.20.2!hadoop-tools.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-test/0.20.2/hadoop-test-0.20.2.jar ...\n[ivy:resolve] ........................... (1527kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-test;0.20.2!hadoop-test.jar (35ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar ...\n[ivy:resolve] .. (40kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-cli#commons-cli;1.2!commons-cli.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar ...\n[ivy:resolve] .. (14kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] xmlenc#xmlenc;0.52!xmlenc.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.jar ...\n[ivy:resolve] ...... (273kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-httpclient#commons-httpclient;3.0.1!commons-httpclient.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-codec/commons-codec/1.3/commons-codec-1.3.jar ...\n[ivy:resolve] .. (45kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-codec#commons-codec;1.3!commons-codec.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-net/commons-net/1.4.1/commons-net-1.4.1.jar ...\n[ivy:resolve] .... (176kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-net#commons-net;1.4.1!commons-net.jar (9ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mortbay/jetty/jetty/6.1.14/jetty-6.1.14.jar ...\n[ivy:resolve] ......... (504kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mortbay.jetty#jetty;6.1.14!jetty.jar (18ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mortbay/jetty/jetty-util/6.1.14/jetty-util-6.1.14.jar ...\n[ivy:resolve] .... (159kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mortbay.jetty#jetty-util;6.1.14!jetty-util.jar (17ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/tomcat/jasper-runtime/5.5.12/jasper-runtime-5.5.12.jar ...\n[ivy:resolve] ... (74kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] tomcat#jasper-runtime;5.5.12!jasper-runtime.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/tomcat/jasper-compiler/5.5.12/jasper-compiler-5.5.12.jar ...\n[ivy:resolve] ........ (395kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] tomcat#jasper-compiler;5.5.12!jasper-compiler.jar (12ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar ...\n[ivy:resolve] .... (131kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mortbay.jetty#jsp-api-2.1;6.1.14!jsp-api-2.1.jar (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar ...\n[ivy:resolve] ................. (1000kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mortbay.jetty#jsp-2.1;6.1.14!jsp-2.1.jar (30ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-el/commons-el/1.0/commons-el-1.0.jar ...\n[ivy:resolve] ... (109kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-el#commons-el;1.0!commons-el.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar ...\n[ivy:resolve] ....... (368kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] net.java.dev.jets3t#jets3t;0.7.1!jets3t.jar (26ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar ...\n[ivy:resolve] .... (129kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mortbay.jetty#servlet-api-2.5;6.1.14!servlet-api-2.5.jar (9ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar ...\n[ivy:resolve] .. (11kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] net.sf.kosmosfs#kfs;0.3!kfs.jar (102ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/junit/junit/4.5/junit-4.5.jar ...\n[ivy:resolve] ..... (194kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] junit#junit;4.5!junit.jar (11ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar ...\n[ivy:resolve] ............ (690kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] hsqldb#hsqldb;1.8.0.10!hsqldb.jar (17ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/oro/oro/2.0.8/oro-2.0.8.jar ...\n[ivy:resolve] .. (63kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] oro#oro;2.0.8!oro.jar (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar ...\n[ivy:resolve] ....................................................................................................... (3483kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.eclipse.jdt#core;3.1.1!core.jar (73ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/ant/ant/1.6.5/ant-1.6.5.jar ...\n[ivy:resolve] .................. (1009kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] ant#ant;1.6.5!ant.jar (24ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar ...\n[ivy:resolve] .. (59kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-logging#commons-logging;1.1.1!commons-logging.jar (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/ftpserver/ftplet-api/1.0.0/ftplet-api-1.0.0.jar ...\n[ivy:resolve] .. (22kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.ftpserver#ftplet-api;1.0.0!ftplet-api.jar(bundle) (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/mina/mina-core/2.0.0-M5/mina-core-2.0.0-M5.jar ...\n[ivy:resolve] ........... (622kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.mina#mina-core;2.0.0-M5!mina-core.jar(bundle) (25ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/ftpserver/ftpserver-core/1.0.0/ftpserver-core-1.0.0.jar ...\n[ivy:resolve] ...... (264kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.ftpserver#ftpserver-core;1.0.0!ftpserver-core.jar(bundle) (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/ftpserver/ftpserver-deprecated/1.0.0-M2/ftpserver-deprecated-1.0.0-M2.jar ...\n[ivy:resolve] .. (31kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.ftpserver#ftpserver-deprecated;1.0.0-M2!ftpserver-deprecated.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.5.2/slf4j-api-1.5.2.jar ...\n[ivy:resolve] .. (16kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.slf4j#slf4j-api;1.5.2!slf4j-api.jar (6ms)\n[ivy:resolve] :: resolution report :: resolve 30441ms :: artifacts dl 649ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tjunit#junit;3.8.1 by [junit#junit;4.5] in [hadoop0.20.shim]\n[ivy:resolve] \tcommons-logging#commons-logging;1.0.3 by [commons-logging#commons-logging;1.1.1] in [hadoop0.20.shim]\n[ivy:resolve] \tcommons-codec#commons-codec;1.2 by [commons-codec#commons-codec;1.3] in [hadoop0.20.shim]\n[ivy:resolve] \tcommons-httpclient#commons-httpclient;3.1 by [commons-httpclient#commons-httpclient;3.0.1] in [hadoop0.20.shim]\n[ivy:resolve] \torg.apache.mina#mina-core;2.0.0-M4 by [org.apache.mina#mina-core;2.0.0-M5] in [hadoop0.20.shim]\n[ivy:resolve] \torg.apache.ftpserver#ftplet-api;1.0.0-M2 by [org.apache.ftpserver#ftplet-api;1.0.0] in [hadoop0.20.shim]\n[ivy:resolve] \torg.apache.ftpserver#ftpserver-core;1.0.0-M2 by [org.apache.ftpserver#ftpserver-core;1.0.0] in [hadoop0.20.shim]\n[ivy:resolve] \torg.apache.mina#mina-core;2.0.0-M2 by [org.apache.mina#mina-core;2.0.0-M5] in [hadoop0.20.shim]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|  hadoop0.20.shim |   37  |   30  |   30  |   8   ||   29  |   29  |\n\t---------------------------------------------------------------------\n\nivy-retrieve-hadoop-shim:\n     [echo] Project: shims\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-shims\n[ivy:retrieve] \tconfs: [hadoop0.20.shim]\n[ivy:retrieve] \t29 artifacts copied, 0 already retrieved (14115kB/61ms)\n    [javac] Compiling 17 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/shims/classes\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java uses unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n     [echo] Building shims 0.20S\n\nbuild-shims:\n     [echo] Project: shims\n     [echo] Compiling /data/hive-ptest/working/apache-svn-trunk-source/shims/src/common/java;/data/hive-ptest/working/apache-svn-trunk-source/shims/src/common-secure/java;/data/hive-ptest/working/apache-svn-trunk-source/shims/src/0.20S/java against hadoop 1.1.2 (/data/hive-ptest/working/apache-svn-trunk-source/build/hadoopcore/hadoop-1.1.2)\n\nivy-init-settings:\n     [echo] Project: shims\n\nivy-resolve-hadoop-shim:\n     [echo] Project: shims\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-shims;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [hadoop0.20S.shim]\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-core;1.1.2 in maven2\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound xmlenc#xmlenc;0.52 in maven2\n[ivy:resolve] \tfound com.sun.jersey#jersey-core;1.8 in maven2\n[ivy:resolve] \tfound com.sun.jersey#jersey-json;1.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jettison#jettison;1.1 in maven2\n[ivy:resolve] \tfound stax#stax-api;1.0.1 in maven2\n[ivy:resolve] \tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in maven2\n[ivy:resolve] \tfound javax.xml.bind#jaxb-api;2.2.2 in maven2\n[ivy:resolve] \tfound javax.xml.stream#stax-api;1.0-2 in maven2\n[ivy:resolve] \tfound javax.activation#activation;1.1 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-core-asl;1.7.1 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-mapper-asl;1.7.1 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-jaxrs;1.7.1 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-xc;1.7.1 in maven2\n[ivy:resolve] \tfound com.sun.jersey#jersey-server;1.8 in maven2\n[ivy:resolve] \tfound asm#asm;3.1 in maven2\n[ivy:resolve] \tfound commons-io#commons-io;2.1 in maven2\n[ivy:resolve] \tfound commons-httpclient#commons-httpclient;3.0.1 in maven2\n[ivy:resolve] \tfound junit#junit;3.8.1 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging;1.0.3 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-math;2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging;1.1.1 in maven2\n[ivy:resolve] \tfound commons-digester#commons-digester;1.8 in maven2\n[ivy:resolve] \tfound commons-beanutils#commons-beanutils;1.7.0 in maven2\n[ivy:resolve] \tfound commons-beanutils#commons-beanutils-core;1.8.0 in maven2\n[ivy:resolve] \tfound commons-net#commons-net;1.4.1 in maven2\n[ivy:resolve] \tfound oro#oro;2.0.8 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jetty;6.1.26 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jetty-util;6.1.26 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#servlet-api;2.5-20081211 in maven2\n[ivy:resolve] \tfound tomcat#jasper-runtime;5.5.12 in maven2\n[ivy:resolve] \tfound tomcat#jasper-compiler;5.5.12 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jsp-api-2.1;6.1.14 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#servlet-api-2.5;6.1.14 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jsp-2.1;6.1.14 in maven2\n[ivy:resolve] \tfound org.eclipse.jdt#core;3.1.1 in maven2\n[ivy:resolve] \tfound ant#ant;1.6.5 in maven2\n[ivy:resolve] \tfound commons-el#commons-el;1.0 in maven2\n[ivy:resolve] \tfound net.java.dev.jets3t#jets3t;0.6.1 in maven2\n[ivy:resolve] \tfound hsqldb#hsqldb;1.8.0.10 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-mapper-asl;1.8.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-core-asl;1.8.8 in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-tools;1.1.2 in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-test;1.1.2 in maven2\n[ivy:resolve] \tfound org.apache.ftpserver#ftplet-api;1.0.0 in maven2\n[ivy:resolve] \tfound org.apache.mina#mina-core;2.0.0-M5 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.5.2 in maven2\n[ivy:resolve] \tfound org.apache.ftpserver#ftpserver-core;1.0.0 in maven2\n[ivy:resolve] \tfound org.apache.ftpserver#ftpserver-deprecated;1.0.0-M2 in maven2\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/1.1.2/hadoop-core-1.1.2.jar ...\n[ivy:resolve] ............................................................................................................ (3941kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-core;1.1.2!hadoop-core.jar (76ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-tools/1.1.2/hadoop-tools-1.1.2.jar ...\n[ivy:resolve] ...... (299kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-tools;1.1.2!hadoop-tools.jar (11ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-test/1.1.2/hadoop-test-1.1.2.jar ...\n[ivy:resolve] .................................................. (2712kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-test;1.1.2!hadoop-test.jar (53ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar ...\n[ivy:resolve] ........ (447kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.sun.jersey#jersey-core;1.8!jersey-core.jar(bundle) (13ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar ...\n[ivy:resolve] .... (144kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.sun.jersey#jersey-json;1.8!jersey-json.jar(bundle) (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar ...\n[ivy:resolve] .............. (678kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.sun.jersey#jersey-server;1.8!jersey-server.jar(bundle) (22ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-io/commons-io/2.1/commons-io-2.1.jar ...\n[ivy:resolve] .... (159kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-io#commons-io;2.1!commons-io.jar (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar ...\n[ivy:resolve] .. (56kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-codec#commons-codec;1.4!commons-codec.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/commons/commons-math/2.1/commons-math-2.1.jar ...\n[ivy:resolve] ............... (812kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.commons#commons-math;2.1!commons-math.jar (33ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar ...\n[ivy:resolve] ...... (291kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-configuration#commons-configuration;1.6!commons-configuration.jar (21ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar ...\n[ivy:resolve] .......... (527kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mortbay.jetty#jetty;6.1.26!jetty.jar (26ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar ...\n[ivy:resolve] .... (172kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mortbay.jetty#jetty-util;6.1.26!jetty-util.jar (14ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar ...\n[ivy:resolve] ...... (314kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] net.java.dev.jets3t#jets3t;0.6.1!jets3t.jar (20ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...\n[ivy:resolve] ... (66kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (13ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar ...\n[ivy:resolve] ............... (869kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.sun.xml.bind#jaxb-impl;2.2.3-1!jaxb-impl.jar (35ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.7.1/jackson-jaxrs-1.7.1.jar ...\n[ivy:resolve] .. (17kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.codehaus.jackson#jackson-jaxrs;1.7.1!jackson-jaxrs.jar (9ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar ...\n[ivy:resolve] .. (30kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.codehaus.jackson#jackson-xc;1.7.1!jackson-xc.jar (11ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar ...\n[ivy:resolve] .. (25kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] stax#stax-api;1.0.1!stax-api.jar (16ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar ...\n[ivy:resolve] ... (102kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.2!jaxb-api.jar (21ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar ...\n[ivy:resolve] .. (22kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] javax.xml.stream#stax-api;1.0-2!stax-api.jar (12ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/javax/activation/activation/1.1/activation-1.1.jar ...\n[ivy:resolve] .. (61kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] javax.activation#activation;1.1!activation.jar (15ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/asm/asm/3.1/asm-3.1.jar ...\n[ivy:resolve] .. (42kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] asm#asm;3.1!asm.jar (13ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/junit/junit/3.8.1/junit-3.8.1.jar ...\n[ivy:resolve] ... (118kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] junit#junit;3.8.1!junit.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar ...\n[ivy:resolve] .......... (561kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-collections#commons-collections;3.2.1!commons-collections.jar (31ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar ...\n[ivy:resolve] .... (140kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-digester#commons-digester;1.8!commons-digester.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar ...\n[ivy:resolve] ..... (201kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-beanutils#commons-beanutils-core;1.8.0!commons-beanutils-core.jar (15ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar ...\n[ivy:resolve] .... (184kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-beanutils#commons-beanutils;1.7.0!commons-beanutils.jar (15ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mortbay/jetty/servlet-api/2.5-20081211/servlet-api-2.5-20081211.jar ...\n[ivy:resolve] .... (130kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mortbay.jetty#servlet-api;2.5-20081211!servlet-api.jar (14ms)\n[ivy:resolve] :: resolution report :: resolve 30823ms :: artifacts dl 701ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \torg.codehaus.jackson#jackson-core-asl;1.7.1 by [org.codehaus.jackson#jackson-core-asl;1.8.8] in [hadoop0.20S.shim]\n[ivy:resolve] \torg.codehaus.jackson#jackson-mapper-asl;1.7.1 by [org.codehaus.jackson#jackson-mapper-asl;1.8.8] in [hadoop0.20S.shim]\n[ivy:resolve] \tcommons-logging#commons-logging;1.0.3 by [commons-logging#commons-logging;1.1.1] in [hadoop0.20S.shim]\n[ivy:resolve] \tcommons-codec#commons-codec;1.2 by [commons-codec#commons-codec;1.4] in [hadoop0.20S.shim]\n[ivy:resolve] \tcommons-logging#commons-logging;1.1 by [commons-logging#commons-logging;1.1.1] in [hadoop0.20S.shim]\n[ivy:resolve] \tcommons-codec#commons-codec;1.3 by [commons-codec#commons-codec;1.4] in [hadoop0.20S.shim]\n[ivy:resolve] \tcommons-httpclient#commons-httpclient;3.1 by [commons-httpclient#commons-httpclient;3.0.1] in [hadoop0.20S.shim]\n[ivy:resolve] \torg.apache.mina#mina-core;2.0.0-M4 by [org.apache.mina#mina-core;2.0.0-M5] in [hadoop0.20S.shim]\n[ivy:resolve] \torg.apache.ftpserver#ftplet-api;1.0.0-M2 by [org.apache.ftpserver#ftplet-api;1.0.0] in [hadoop0.20S.shim]\n[ivy:resolve] \torg.apache.ftpserver#ftpserver-core;1.0.0-M2 by [org.apache.ftpserver#ftpserver-core;1.0.0] in [hadoop0.20S.shim]\n[ivy:resolve] \torg.apache.mina#mina-core;2.0.0-M2 by [org.apache.mina#mina-core;2.0.0-M5] in [hadoop0.20S.shim]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t| hadoop0.20S.shim |   62  |   30  |   30  |   11  ||   51  |   28  |\n\t---------------------------------------------------------------------\n\nivy-retrieve-hadoop-shim:\n     [echo] Project: shims\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-shims\n[ivy:retrieve] \tconfs: [hadoop0.20S.shim]\n[ivy:retrieve] \t51 artifacts copied, 0 already retrieved (22876kB/182ms)\n    [javac] Compiling 15 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/shims/classes\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n     [echo] Building shims 0.23\n\nbuild-shims:\n     [echo] Project: shims\n     [echo] Compiling /data/hive-ptest/working/apache-svn-trunk-source/shims/src/common/java;/data/hive-ptest/working/apache-svn-trunk-source/shims/src/common-secure/java;/data/hive-ptest/working/apache-svn-trunk-source/shims/src/0.23/java against hadoop 2.0.5-alpha (/data/hive-ptest/working/apache-svn-trunk-source/build/hadoopcore/hadoop-2.0.5-alpha)\n\nivy-init-settings:\n     [echo] Project: shims\n\nivy-resolve-hadoop-shim:\n     [echo] Project: shims\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-shims;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [hadoop0.23.shim]\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-common;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-annotations;2.0.5-alpha in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-math;2.1 in maven2\n[ivy:resolve] \tfound xmlenc#xmlenc;0.52 in maven2\n[ivy:resolve] \tfound commons-httpclient#commons-httpclient;3.1 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging;1.1.1 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound commons-io#commons-io;2.1 in maven2\n[ivy:resolve] \tfound commons-net#commons-net;3.1 in maven2\n[ivy:resolve] \tfound javax.servlet#servlet-api;2.5 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jetty;6.1.26 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jetty-util;6.1.26 in maven2\n[ivy:resolve] \tfound com.sun.jersey#jersey-core;1.8 in maven2\n[ivy:resolve] \tfound com.sun.jersey#jersey-json;1.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jettison#jettison;1.1 in maven2\n[ivy:resolve] \tfound stax#stax-api;1.0.1 in maven2\n[ivy:resolve] \tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in maven2\n[ivy:resolve] \tfound javax.xml.bind#jaxb-api;2.2.2 in maven2\n[ivy:resolve] \tfound javax.activation#activation;1.1 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-core-asl;1.8.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-mapper-asl;1.8.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-jaxrs;1.8.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-xc;1.8.8 in maven2\n[ivy:resolve] \tfound com.sun.jersey#jersey-server;1.8 in maven2\n[ivy:resolve] \tfound asm#asm;3.2 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.17 in maven2\n[ivy:resolve] \tfound net.java.dev.jets3t#jets3t;0.6.1 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.5 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-digester#commons-digester;1.8 in maven2\n[ivy:resolve] \tfound commons-beanutils#commons-beanutils;1.7.0 in maven2\n[ivy:resolve] \tfound commons-beanutils#commons-beanutils-core;1.8.0 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.5.3 in maven2\n[ivy:resolve] \tfound com.thoughtworks.paranamer#paranamer;2.3 in maven2\n[ivy:resolve] \tfound org.xerial.snappy#snappy-java;1.0.3.2 in maven2\n[ivy:resolve] \tfound net.sf.kosmosfs#kfs;0.3 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.0a in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-auth;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound com.jcraft#jsch;0.1.42 in maven2\n[ivy:resolve] \tfound org.apache.zookeeper#zookeeper;3.4.2 in maven2\n[ivy:resolve] \tfound tomcat#jasper-compiler;5.5.23 in maven2\n[ivy:resolve] \tfound tomcat#jasper-runtime;5.5.23 in maven2\n[ivy:resolve] \tfound commons-el#commons-el;1.0 in maven2\n[ivy:resolve] \tfound javax.servlet.jsp#jsp-api;2.1 in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-mapreduce-client-core;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-yarn-common;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-yarn-api;2.0.5-alpha in maven2\n[ivy:resolve] \tfound com.google.inject.extensions#guice-servlet;3.0 in maven2\n[ivy:resolve] \tfound com.google.inject#guice;3.0 in maven2\n[ivy:resolve] \tfound javax.inject#javax.inject;1 in maven2\n[ivy:resolve] \tfound aopalliance#aopalliance;1.0 in maven2\n[ivy:resolve] \tfound org.sonatype.sisu.inject#cglib;2.2.1-v20090111 in maven2\n[ivy:resolve] \tfound io.netty#netty;3.5.11.Final in maven2\n[ivy:resolve] \tfound com.sun.jersey.jersey-test-framework#jersey-test-framework-grizzly2;1.8 in maven2\n[ivy:resolve] \tfound com.sun.jersey.contribs#jersey-guice;1.8 in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-archives;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-hdfs;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-mapreduce-client-jobclient;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-mapreduce-client-common;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-yarn-client;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-yarn-server-common;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-yarn-server-tests;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-yarn-server-nodemanager;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-yarn-server-resourcemanager;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-yarn-server-web-proxy;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-mapreduce-client-app;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-mapreduce-client-shuffle;2.0.5-alpha in maven2\n[ivy:resolve] \tfound org.apache.hadoop#hadoop-mapreduce-client-hs;2.0.5-alpha in maven2\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.0.5-alpha/hadoop-common-2.0.5-alpha.jar ...\n[ivy:resolve] ..................................................................... (2295kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-common;2.0.5-alpha!hadoop-common.jar (60ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.0.5-alpha/hadoop-common-2.0.5-alpha-tests.jar ...\n[ivy:resolve] .................... (1151kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-common;2.0.5-alpha!hadoop-common.jar(tests) (26ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.0.5-alpha/hadoop-mapreduce-client-core-2.0.5-alpha.jar ...\n[ivy:resolve] ....................... (1325kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-core;2.0.5-alpha!hadoop-mapreduce-client-core.jar (28ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-archives/2.0.5-alpha/hadoop-archives-2.0.5-alpha.jar ...\n[ivy:resolve] .. (20kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-archives;2.0.5-alpha!hadoop-archives.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs/2.0.5-alpha/hadoop-hdfs-2.0.5-alpha.jar ...\n[ivy:resolve] .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... (4241kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-hdfs;2.0.5-alpha!hadoop-hdfs.jar (303ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs/2.0.5-alpha/hadoop-hdfs-2.0.5-alpha-tests.jar ...\n[ivy:resolve] ............................ (1631kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-hdfs;2.0.5-alpha!hadoop-hdfs.jar(tests) (37ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.0.5-alpha/hadoop-mapreduce-client-jobclient-2.0.5-alpha-tests.jar ...\n[ivy:resolve] ....................... (1350kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-jobclient;2.0.5-alpha!hadoop-mapreduce-client-jobclient.jar(tests) (31ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.0.5-alpha/hadoop-mapreduce-client-jobclient-2.0.5-alpha.jar ...\n[ivy:resolve] .. (32kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-jobclient;2.0.5-alpha!hadoop-mapreduce-client-jobclient.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.0.5-alpha/hadoop-mapreduce-client-common-2.0.5-alpha.jar ...\n[ivy:resolve] ........... (579kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-common;2.0.5-alpha!hadoop-mapreduce-client-common.jar (16ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-tests/2.0.5-alpha/hadoop-yarn-server-tests-2.0.5-alpha-tests.jar ...\n[ivy:resolve] .. (39kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-server-tests;2.0.5-alpha!hadoop-yarn-server-tests.jar(tests) (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-app/2.0.5-alpha/hadoop-mapreduce-client-app-2.0.5-alpha.jar ...\n[ivy:resolve] ......... (463kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-app;2.0.5-alpha!hadoop-mapreduce-client-app.jar (13ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.0.5-alpha/hadoop-mapreduce-client-hs-2.0.5-alpha.jar ...\n[ivy:resolve] ... (111kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-hs;2.0.5-alpha!hadoop-mapreduce-client-hs.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/2.0.5-alpha/hadoop-annotations-2.0.5-alpha.jar ...\n[ivy:resolve] .. (16kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;2.0.5-alpha!hadoop-annotations.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar ...\n[ivy:resolve] ...... (297kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-httpclient#commons-httpclient;3.1!commons-httpclient.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-net/commons-net/3.1/commons-net-3.1.jar ...\n[ivy:resolve] ...... (266kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-net#commons-net;3.1!commons-net.jar (11ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar ...\n[ivy:resolve] ... (102kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] javax.servlet#servlet-api;2.5!servlet-api.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n[ivy:resolve] ......... (478kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (14ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.5/commons-lang-2.5.jar ...\n[ivy:resolve] ...... (272kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-lang#commons-lang;2.5!commons-lang.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar ...\n[ivy:resolve] .. (24kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.slf4j#slf4j-api;1.6.1!slf4j-api.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/avro/avro/1.5.3/avro-1.5.3.jar ...\n[ivy:resolve] ...... (257kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.avro#avro;1.5.3!avro.jar (9ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.4.0a/protobuf-java-2.4.0a.jar ...\n[ivy:resolve] ........ (439kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.google.protobuf#protobuf-java;2.4.0a!protobuf-java.jar (13ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/2.0.5-alpha/hadoop-auth-2.0.5-alpha.jar ...\n[ivy:resolve] .. (46kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;2.0.5-alpha!hadoop-auth.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar ...\n[ivy:resolve] .... (181kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.jcraft#jsch;0.1.42!jsch.jar (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.2/zookeeper-3.4.2.jar ...\n[ivy:resolve] ............. (746kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.zookeeper#zookeeper;3.4.2!zookeeper.jar (18ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar ...\n[ivy:resolve] .. (32kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.google.code.findbugs#jsr305;1.3.9!jsr305.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar ...\n[ivy:resolve] .. (17kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.codehaus.jackson#jackson-jaxrs;1.8.8!jackson-jaxrs.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.8.8/jackson-xc-1.8.8.jar ...\n[ivy:resolve] .. (31kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.codehaus.jackson#jackson-xc;1.8.8!jackson-xc.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/asm/asm/3.2/asm-3.2.jar ...\n[ivy:resolve] .. (42kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] asm#asm;3.2!asm.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\n[ivy:resolve] .. (28kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.0.3.2/snappy-java-1.0.3.2.jar ...\n[ivy:resolve] ................. (972kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.0.3.2!snappy-java.jar(bundle) (23ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar ...\n[ivy:resolve] .. (9kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.slf4j#slf4j-log4j12;1.6.1!slf4j-log4j12.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar ...\n[ivy:resolve] ........ (398kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] tomcat#jasper-compiler;5.5.23!jasper-compiler.jar (14ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar ...\n[ivy:resolve] ... (75kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] tomcat#jasper-runtime;5.5.23!jasper-runtime.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar ...\n[ivy:resolve] ... (98kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] javax.servlet.jsp#jsp-api;2.1!jsp-api.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.0.5-alpha/hadoop-yarn-common-2.0.5-alpha.jar ...\n[ivy:resolve] .................. (1050kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-common;2.0.5-alpha!hadoop-yarn-common.jar (25ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar ...\n[ivy:resolve] .. (63kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.google.inject.extensions#guice-servlet;3.0!guice-servlet.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/io/netty/netty/3.5.11.Final/netty-3.5.11.Final.jar ...\n[ivy:resolve] .................... (1106kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] io.netty#netty;3.5.11.Final!netty.jar(bundle) (25ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-api/2.0.5-alpha/hadoop-yarn-api-2.0.5-alpha.jar ...\n[ivy:resolve] .................. (1014kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-api;2.0.5-alpha!hadoop-yarn-api.jar (25ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/google/inject/guice/3.0/guice-3.0.jar ...\n[ivy:resolve] ............ (693kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.google.inject#guice;3.0!guice.jar (17ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/sun/jersey/jersey-test-framework/jersey-test-framework-grizzly2/1.8/jersey-test-framework-grizzly2-1.8.jar ...\n[ivy:resolve] .. (12kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.sun.jersey.jersey-test-framework#jersey-test-framework-grizzly2;1.8!jersey-test-framework-grizzly2.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/sun/jersey/contribs/jersey-guice/1.8/jersey-guice-1.8.jar ...\n[ivy:resolve] .. (14kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.sun.jersey.contribs#jersey-guice;1.8!jersey-guice.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/javax/inject/javax.inject/1/javax.inject-1.jar ...\n[ivy:resolve] .. (2kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] javax.inject#javax.inject;1!javax.inject.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar ...\n[ivy:resolve] .. (4kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] aopalliance#aopalliance;1.0!aopalliance.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/sonatype/sisu/inject/cglib/2.2.1-v20090111/cglib-2.2.1-v20090111.jar ...\n[ivy:resolve] ...... (272kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.sonatype.sisu.inject#cglib;2.2.1-v20090111!cglib.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-client/2.0.5-alpha/hadoop-yarn-client-2.0.5-alpha.jar ...\n[ivy:resolve] .. (28kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-client;2.0.5-alpha!hadoop-yarn-client.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.0.5-alpha/hadoop-yarn-server-common-2.0.5-alpha.jar ...\n[ivy:resolve] .... (148kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-server-common;2.0.5-alpha!hadoop-yarn-server-common.jar (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.0.5-alpha/hadoop-yarn-server-nodemanager-2.0.5-alpha.jar ...\n[ivy:resolve] ........ (404kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-server-nodemanager;2.0.5-alpha!hadoop-yarn-server-nodemanager.jar (16ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.0.5-alpha/hadoop-yarn-server-resourcemanager-2.0.5-alpha.jar ...\n[ivy:resolve] .......... (517kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-server-resourcemanager;2.0.5-alpha!hadoop-yarn-server-resourcemanager.jar (14ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.0.5-alpha/hadoop-yarn-server-web-proxy-2.0.5-alpha.jar ...\n[ivy:resolve] .. (24kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-server-web-proxy;2.0.5-alpha!hadoop-yarn-server-web-proxy.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.0.5-alpha/hadoop-mapreduce-client-shuffle-2.0.5-alpha.jar ...\n[ivy:resolve] .. (20kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-shuffle;2.0.5-alpha!hadoop-mapreduce-client-shuffle.jar (5ms)\n[ivy:resolve] :: resolution report :: resolve 62615ms :: artifacts dl 1043ms\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|  hadoop0.23.shim |   74  |   47  |   47  |   0   ||   77  |   50  |\n\t---------------------------------------------------------------------\n\nivy-retrieve-hadoop-shim:\n     [echo] Project: shims\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-shims\n[ivy:retrieve] \tconfs: [hadoop0.23.shim]\n[ivy:retrieve] \t77 artifacts copied, 0 already retrieved (31997kB/258ms)\n    [javac] Compiling 3 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/shims/classes\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/shims/src/0.23/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java uses or overrides a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n\njar:\n     [echo] Project: shims\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/shims/hive-shims-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-shims;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:20:04 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/shims/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-shims\n[ivy:publish] \tpublished hive-shims to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-shims/0.12.0-SNAPSHOT/jars/hive-shims.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-shims/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: common\n\ncheck-ivy:\n     [echo] Project: common\n\nivy-resolve:\n     [echo] Project: common\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-common;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] downloading /data/hive-ptest/working/ivy/local/org.apache.hive/hive-shims/0.12.0-SNAPSHOT/jars/hive-shims.jar ...\n[ivy:resolve] .... (140kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hive#hive-shims;0.12.0-SNAPSHOT!hive-shims.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\n[ivy:resolve] ..... (235kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\n[ivy:resolve] ... (92kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (7ms)\n[ivy:resolve] :: resolution report :: resolve 2056ms :: artifacts dl 32ms\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   6   |   3   |   3   |   0   ||   6   |   3   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-common-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-common-default.html\n\nmake-pom:\n     [echo] Project: common\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/common/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: common\n\ninit:\n     [echo] Project: common\n\nsetup:\n     [echo] Project: common\n\nivy-retrieve:\n     [echo] Project: common\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-common\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t4 artifacts copied, 2 already retrieved (508kB/5ms)\n\ncompile:\n     [echo] Project: common\n    [javac] Compiling 25 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/common/classes\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/common/src/java/org/apache/hadoop/hive/common/ObjectPair.java uses unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n     [copy] Copying 1 file to /data/hive-ptest/working/apache-svn-trunk-source/build/common/classes\n\njar:\n     [echo] Project: common\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/common/hive-common-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-common;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:20:16 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/common/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-common\n[ivy:publish] \tpublished hive-common to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-common/0.12.0-SNAPSHOT/jars/hive-common.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-common/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: serde\n\ncheck-ivy:\n     [echo] Project: serde\n\nivy-resolve:\n     [echo] Project: serde\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-serde;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] downloading /data/hive-ptest/working/ivy/local/org.apache.hive/hive-common/0.12.0-SNAPSHOT/jars/hive-common.jar ...\n[ivy:resolve] ... (95kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hive#hive-common;0.12.0-SNAPSHOT!hive-common.jar (4ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/mockito/mockito-all/1.8.2/mockito-all-1.8.2.jar ...\n[ivy:resolve] ....................... (1315kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.mockito#mockito-all;1.8.2!mockito-all.jar (27ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/thrift/libfb303/0.9.0/libfb303-0.9.0.jar ...\n[ivy:resolve] ...... (268kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.thrift#libfb303;0.9.0!libfb303.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/avro/avro/1.7.1/avro-1.7.1.jar ...\n[ivy:resolve] ...... (290kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.avro#avro;1.7.1!avro.jar (11ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.7.1/avro-mapred-1.7.1.jar ...\n[ivy:resolve] .... (164kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.avro#avro-mapred;1.7.1!avro-mapred.jar (8ms)\n[ivy:resolve] :: resolution report :: resolve 6153ms :: artifacts dl 73ms\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   14  |   5   |   5   |   0   ||   14  |   5   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-serde-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-serde-default.html\n\nmake-pom:\n     [echo] Project: serde\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/serde/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: serde\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/serde/src/test/resources does not exist.\n\ninit:\n     [echo] Project: serde\n\nivy-retrieve:\n     [echo] Project: serde\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-serde\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t8 artifacts copied, 6 already retrieved (2227kB/25ms)\n\ndynamic-serde:\n\ncompile:\n     [echo] Project: serde\n    [javac] Compiling 325 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/serde/classes\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] Creating empty /data/hive-ptest/working/apache-svn-trunk-source/build/serde/classes/org/apache/hadoop/hive/serde2/typeinfo/package-info.class\n\njar:\n     [echo] Project: serde\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/serde/hive-serde-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-serde;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:20:35 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/serde/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-serde\n[ivy:publish] \tpublished hive-serde to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-serde/0.12.0-SNAPSHOT/jars/hive-serde.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-serde/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: metastore\n\ncheck-ivy:\n     [echo] Project: metastore\n\nivy-resolve:\n     [echo] Project: metastore\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-metastore;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] downloading /data/hive-ptest/working/ivy/local/org.apache.hive/hive-serde/0.12.0-SNAPSHOT/jars/hive-serde.jar ...\n[ivy:resolve] ............ (663kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hive#hive-serde;0.12.0-SNAPSHOT!hive-serde.jar (19ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/antlr/antlr/3.4/antlr-3.4.jar ...\n[ivy:resolve] ................... (1086kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.antlr#antlr;3.4!antlr.jar (24ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar ...\n[ivy:resolve] .... (160kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.antlr#antlr-runtime;3.4!antlr-runtime.jar (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/antlr/ST4/4.0.4/ST4-4.0.4.jar ...\n[ivy:resolve] ..... (231kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.antlr#ST4;4.0.4!ST4.jar (9ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/jolbox/bonecp/0.7.1.RELEASE/bonecp-0.7.1.RELEASE.jar ...\n[ivy:resolve] ... (112kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.jolbox#bonecp;0.7.1.RELEASE!bonecp.jar(bundle) (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar ...\n[ivy:resolve] ... (93kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] commons-pool#commons-pool;1.5.4!commons-pool.jar (7ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/datanucleus/datanucleus-api-jdo/3.2.1/datanucleus-api-jdo-3.2.1.jar ...\n[ivy:resolve] ....... (329kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.datanucleus#datanucleus-api-jdo;3.2.1!datanucleus-api-jdo.jar (11ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/datanucleus/datanucleus-core/3.2.2/datanucleus-core-3.2.2.jar ...\n[ivy:resolve] .............................. (1759kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.datanucleus#datanucleus-core;3.2.2!datanucleus-core.jar (37ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/datanucleus/datanucleus-rdbms/3.2.1/datanucleus-rdbms-3.2.1.jar ...\n[ivy:resolve] ............................. (1728kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.datanucleus#datanucleus-rdbms;3.2.1!datanucleus-rdbms.jar (35ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar ...\n[ivy:resolve] ..... (196kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] javax.jdo#jdo-api;3.0.1!jdo-api.jar (8ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/derby/derby/10.4.2.0/derby-10.4.2.0.jar ...\n[ivy:resolve] .......................................... (2389kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.derby#derby;10.4.2.0!derby.jar (55ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/google/guava/guava/r08/guava-r08.jar ...\n[ivy:resolve] ................... (1088kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.google.guava#guava;r08!guava.jar (25ms)\n[ivy:resolve] :: resolution report :: resolve 10034ms :: artifacts dl 268ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   27  |   12  |   12  |   1   ||   26  |   12  |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-metastore-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-metastore-default.html\n\nmake-pom:\n     [echo] Project: metastore\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: metastore\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/test/resources does not exist.\n\ninit:\n     [echo] Project: metastore\n\nmetastore-init:\n     [echo] Project: metastore\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/gen/antlr/gen-java/org/apache/hadoop/hive/metastore/parser\n\nivy-retrieve:\n     [echo] Project: metastore\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-metastore\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t12 artifacts copied, 14 already retrieved (9838kB/40ms)\n\nbuild-grammar:\n     [echo] Project: metastore\n     [echo] Building Grammar /data/hive-ptest/working/apache-svn-trunk-source/metastore/src/java/org/apache/hadoop/hive/metastore/parser/Filter.g  ....\n\nmodel-compile:\n     [echo] Project: metastore\n    [javac] Compiling 24 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/classes\n     [copy] Copying 1 file to /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/classes\n\ncore-compile:\n     [echo] Project: metastore\n    [javac] Compiling 104 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/classes\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] Creating empty /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/classes/org/apache/hadoop/hive/metastore/parser/package-info.class\n\nmodel-enhance:\n     [echo] Project: metastore\n[datanucleusenhancer] log4j:WARN No appenders could be found for logger (DataNucleus.General).\n[datanucleusenhancer] log4j:WARN Please initialize the log4j system properly.\n[datanucleusenhancer] log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n[datanucleusenhancer] DataNucleus Enhancer (version 3.2.2) for API \"JDO\" using JRE \"1.6\"\n[datanucleusenhancer] DataNucleus Enhancer : Classpath\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/service/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/common/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/serde/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/cli/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/shims/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/classes\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/anttasks/hive-anttasks-0.12.0-SNAPSHOT.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/common/hive-common-0.12.0-SNAPSHOT.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/serde/hive-serde-0.12.0-SNAPSHOT.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/shims/hive-shims-0.12.0-SNAPSHOT.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/activation-1.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/ant-1.6.5.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/asm-3.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-beanutils-1.7.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-beanutils-core-1.8.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-cli-1.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-codec-1.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-collections-3.2.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-configuration-1.6.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-digester-1.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-el-1.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-httpclient-3.0.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-io-2.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-lang-2.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-logging-1.1.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-math-2.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/commons-net-1.4.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/core-3.1.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/ftplet-api-1.0.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/ftpserver-core-1.0.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/ftpserver-deprecated-1.0.0-M2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/hadoop-core-1.1.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/hadoop-test-1.1.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/hadoop-tools-1.1.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/hsqldb-1.8.0.10.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jackson-core-asl-1.8.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jackson-jaxrs-1.7.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jackson-mapper-asl-1.8.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jackson-xc-1.7.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jasper-compiler-5.5.12.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jasper-runtime-5.5.12.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jaxb-api-2.2.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jaxb-impl-2.2.3-1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jersey-core-1.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jersey-json-1.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jersey-server-1.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jets3t-0.6.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jettison-1.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jetty-6.1.26.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jetty-util-6.1.26.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jsp-2.1-6.1.14.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/jsp-api-2.1-6.1.14.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/junit-3.8.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/mina-core-2.0.0-M5.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/oro-2.0.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/servlet-api-2.5-20081211.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/servlet-api-2.5-6.1.14.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/slf4j-api-1.5.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/stax-api-1.0-2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/stax-api-1.0.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/hadoop0.20S.shim/xmlenc-0.52.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/ST4-4.0.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/antlr-3.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/antlr-runtime-3.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/avro-1.7.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/avro-mapred-1.7.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/bonecp-0.7.1.RELEASE.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-cli-1.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-codec-1.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-compress-1.4.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-io-2.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-lang-2.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-logging-1.0.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-logging-api-1.0.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-pool-1.5.4.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/datanucleus-api-jdo-3.2.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/datanucleus-core-3.2.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/datanucleus-rdbms-3.2.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/derby-10.4.2.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/guava-11.0.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/guava-r08.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/hive-common-0.12.0-SNAPSHOT.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/hive-serde-0.12.0-SNAPSHOT.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/hive-shims-0.12.0-SNAPSHOT.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/jackson-core-asl-1.8.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/jackson-mapper-asl-1.8.8.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/jdo-api-3.0.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/libfb303-0.9.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/libthrift-0.9.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/log4j-1.2.16.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/mockito-all-1.8.2.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/slf4j-api-1.6.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/slf4j-log4j12-1.6.1.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/velocity-1.5.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/xz-1.0.jar\n[datanucleusenhancer] >>  /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/zookeeper-3.4.3.jar\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDatabase\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MFieldSchema\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MType\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTable\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MOrder\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MStringList\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartition\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MIndex\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MRole\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MRoleMap\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MMasterKey\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MDelegationToken\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics\n[datanucleusenhancer] ENHANCED (PersistenceCapable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics\n[datanucleusenhancer] DataNucleus Enhancer completed with success for 24 classes. Timings : input=703 ms, enhance=1169 ms, total=1872 ms. Consult the log for full details\n\ncompile:\n     [echo] Project: metastore\n\njar:\n     [echo] Project: metastore\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/hive-metastore-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-metastore;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:21:14 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/metastore/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-metastore\n[ivy:publish] \tpublished hive-metastore to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-metastore/0.12.0-SNAPSHOT/jars/hive-metastore.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-metastore/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: ql\n\ncheck-ivy:\n     [echo] Project: ql\n\nivy-resolve:\n     [echo] Project: ql\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-exec;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-metastore;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.1 in maven2\n[ivy:resolve] \tfound org.iq80.snappy#snappy;0.2 in maven2\n[ivy:resolve] \tfound org.json#json;20090211 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound com.googlecode.javaewah#JavaEWAH;0.3.2 in maven2\n[ivy:resolve] \tfound javolution#javolution;5.5.1 in maven2\n[ivy:resolve] \tfound jline#jline;0.9.94 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] downloading /data/hive-ptest/working/ivy/local/org.apache.hive/hive-metastore/0.12.0-SNAPSHOT/jars/hive-metastore.jar ...\n[ivy:resolve] ..................................................... (3268kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hive#hive-metastore;0.12.0-SNAPSHOT!hive-metastore.jar (76ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.4.1/protobuf-java-2.4.1.jar ...\n[ivy:resolve] ........ (439kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.google.protobuf#protobuf-java;2.4.1!protobuf-java.jar (20ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/iq80/snappy/snappy/0.2/snappy-0.2.jar ...\n[ivy:resolve] .. (47kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.iq80.snappy#snappy;0.2!snappy.jar (9ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/json/json/20090211/json-20090211.jar ...\n[ivy:resolve] .. (44kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.json#json;20090211!json.jar (21ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar ...\n[ivy:resolve] .. (16kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.googlecode.javaewah#JavaEWAH;0.3.2!JavaEWAH.jar (5ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/javolution/javolution/5.5.1/javolution-5.5.1.jar ...\n[ivy:resolve] ........ (385kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] javolution#javolution;5.5.1!javolution.jar(bundle) (11ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/jline/jline/0.9.94/jline-0.9.94.jar ...\n[ivy:resolve] ... (85kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] jline#jline;0.9.94!jline.jar (122ms)\n[ivy:resolve] :: resolution report :: resolve 10402ms :: artifacts dl 305ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tcom.google.guava#guava;r08 by [com.google.guava#guava;11.0.2] in [default]\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   38  |   7   |   7   |   2   ||   36  |   7   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-exec-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-exec-default.html\n\nmake-pom:\n     [echo] Project: ql\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/ql/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: ql\n\ninit:\n     [echo] Project: ql\n\nql-init:\n     [echo] Project: ql\n    [mkdir] Created dir: /data/hive-ptest/working/apache-svn-trunk-source/build/ql/gen/antlr/gen-java/org/apache/hadoop/hive/ql/parse\n\nivy-retrieve:\n     [echo] Project: ql\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-exec\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t10 artifacts copied, 26 already retrieved (5174kB/29ms)\n\nbuild-grammar:\n     [echo] Project: ql\n     [echo] Building Grammar /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g  ....\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:866:5: \n     [java] Decision can match input such as \"Identifier KW_RENAME KW_TO\" using multiple alternatives: 1, 10\n     [java] \n     [java] As a result, alternative(s) 10 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1167:5: \n     [java] Decision can match input such as \"KW_TEXTFILE\" using multiple alternatives: 2, 6\n     [java] \n     [java] As a result, alternative(s) 6 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1167:5: \n     [java] Decision can match input such as \"KW_SEQUENCEFILE\" using multiple alternatives: 1, 6\n     [java] \n     [java] As a result, alternative(s) 6 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1167:5: \n     [java] Decision can match input such as \"KW_ORCFILE\" using multiple alternatives: 4, 6\n     [java] \n     [java] As a result, alternative(s) 6 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1167:5: \n     [java] Decision can match input such as \"KW_RCFILE\" using multiple alternatives: 3, 6\n     [java] \n     [java] As a result, alternative(s) 6 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1180:23: \n     [java] Decision can match input such as \"KW_KEY_TYPE\" using multiple alternatives: 2, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1180:23: \n     [java] Decision can match input such as \"KW_ELEM_TYPE\" using multiple alternatives: 1, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1180:23: \n     [java] Decision can match input such as \"KW_VALUE_TYPE\" using multiple alternatives: 3, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1187:23: \n     [java] Decision can match input such as \"KW_VALUE_TYPE\" using multiple alternatives: 3, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1187:23: \n     [java] Decision can match input such as \"KW_ELEM_TYPE\" using multiple alternatives: 1, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1187:23: \n     [java] Decision can match input such as \"KW_KEY_TYPE\" using multiple alternatives: 2, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1205:29: \n     [java] Decision can match input such as \"KW_PRETTY KW_PARTITION\" using multiple alternatives: 3, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1205:29: \n     [java] Decision can match input such as \"KW_PRETTY {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE..KW_CASCADE, KW_CHANGE..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITIONED..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER..KW_UNARCHIVE, KW_UNDO..KW_UNIONTYPE, KW_UNLOCK..KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 3, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1205:29: \n     [java] Decision can match input such as \"KW_PRETTY Identifier\" using multiple alternatives: 3, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1205:29: \n     [java] Decision can match input such as \"KW_FORMATTED {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE..KW_CASCADE, KW_CHANGE..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITIONED..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER..KW_UNARCHIVE, KW_UNDO..KW_UNIONTYPE, KW_UNLOCK..KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1205:29: \n     [java] Decision can match input such as \"KW_FORMATTED KW_PARTITION\" using multiple alternatives: 1, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1205:29: \n     [java] Decision can match input such as \"KW_FORMATTED Identifier\" using multiple alternatives: 1, 4\n     [java] \n     [java] As a result, alternative(s) 4 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1476:116: \n     [java] Decision can match input such as \"KW_STORED KW_AS KW_DIRECTORIES\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1599:5: \n     [java] Decision can match input such as \"KW_STORED KW_AS KW_INPUTFORMAT\" using multiple alternatives: 5, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1599:5: \n     [java] Decision can match input such as \"KW_STORED KW_AS KW_SEQUENCEFILE\" using multiple alternatives: 1, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1599:5: \n     [java] Decision can match input such as \"KW_STORED KW_AS KW_ORCFILE\" using multiple alternatives: 4, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1599:5: \n     [java] Decision can match input such as \"KW_STORED KW_AS KW_RCFILE\" using multiple alternatives: 3, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): /data/hive-ptest/working/apache-svn-trunk-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g:1599:5: \n     [java] Decision can match input such as \"KW_STORED KW_AS KW_TEXTFILE\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): SelectClauseParser.g:149:5: \n     [java] Decision can match input such as \"KW_NULL DOT {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE..KW_CASCADE, KW_CHANGE..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER..KW_UNARCHIVE, KW_UNDO..KW_UNIONTYPE, KW_UNLOCK..KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): SelectClauseParser.g:149:5: \n     [java] Decision can match input such as \"KW_NULL DOT Identifier\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:127:2: \n     [java] Decision can match input such as \"KW_LATERAL KW_VIEW KW_OUTER\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:25: \n     [java] Decision can match input such as \"LPAREN StringLiteral RPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:25: \n     [java] Decision can match input such as \"LPAREN StringLiteral EQUAL\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:25: \n     [java] Decision can match input such as \"LPAREN StringLiteral COMMA\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_DATE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN BigintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_FALSE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_NOT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_TRUE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN TinyintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN Identifier\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_UNIONTYPE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN SmallintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_CASE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_IF\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_NULL\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN CharSetName\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_STRUCT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN Number\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN StringLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN DecimalLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_CAST\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_MAP\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:179:68: \n     [java] Decision can match input such as \"Identifier LPAREN KW_ARRAY\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_DATE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN BigintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_FALSE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_NOT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_TRUE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN TinyintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN Identifier\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_UNIONTYPE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN SmallintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_CASE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_IF\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_NULL\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN CharSetName\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_STRUCT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN Number\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN StringLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN DecimalLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_CAST\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_MAP\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): FromClauseParser.g:237:16: \n     [java] Decision can match input such as \"Identifier LPAREN KW_ARRAY\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN Number\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL GREATERTHAN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_FALSE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE Identifier\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL GREATERTHANOREQUALTO\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_TRUE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL LESSTHAN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL LESSTHANOREQUALTO\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL DOT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT CharSetName\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE CharSetName\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN CharSetName\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_ARRAY\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL NOTEQUAL\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT StringLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL EQUAL_NS\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN Identifier\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL {DIV..DIVIDE, MOD, STAR}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL BITWISEXOR\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_STRUCT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL EQUAL\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_ARRAY\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_UNIONTYPE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_STRUCT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT Identifier\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_NOT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_NOT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_NOT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_DATE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN TinyintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_UNIONTYPE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL RPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN DecimalLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_NULL\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN BigintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE StringLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN SmallintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL KW_AND\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CAST LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL BITWISEOR\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL KW_BETWEEN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_NULL\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_CASE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_CASE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_CASE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL KW_NOT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN StringLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_ARRAY\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL KW_IN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_FALSE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_STRUCT\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_NULL\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_UNIONTYPE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_TRUE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN {KW_ADD..KW_AFTER, KW_ALTER..KW_ANALYZE, KW_ARCHIVE, KW_AS..KW_CASCADE, KW_CHANGE..KW_COLLECTION, KW_COLUMNS..KW_CREATE, KW_CUBE, KW_CURSOR..KW_DATA, KW_DATABASES, KW_DATETIME..KW_DISABLE, KW_DISTRIBUTE..KW_ELEM_TYPE, KW_ENABLE, KW_ESCAPED, KW_EXCLUSIVE..KW_EXPORT, KW_EXTERNAL, KW_FETCH..KW_FLOAT, KW_FOR..KW_FORMATTED, KW_FULL, KW_FUNCTIONS..KW_GROUPING, KW_HOLD_DDLTIME..KW_IDXPROPERTIES, KW_IGNORE..KW_ITEMS, KW_KEYS..KW_LEFT, KW_LIKE..KW_LONG, KW_MAPJOIN..KW_MINUS, KW_MSCK..KW_NOSCAN, KW_NO_DROP, KW_OF..KW_OFFLINE, KW_OPTION, KW_ORCFILE..KW_OUTPUTFORMAT, KW_OVERWRITE, KW_PARTITION..KW_PLUS, KW_PRETTY..KW_RECORDWRITER, KW_REGEXP..KW_SCHEMAS, KW_SEMI..KW_STRING, KW_TABLE..KW_TABLES, KW_TBLPROPERTIES..KW_TEXTFILE, KW_TIMESTAMP..KW_TOUCH, KW_TRIGGER, KW_TRUNCATE..KW_UNARCHIVE, KW_UNDO..KW_UNION, KW_UNLOCK..KW_VIEW, KW_WHILE, KW_WITH}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT BigintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_IF\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_IF\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_IF\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL AMPERSAND\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL LSQUARE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_MAP\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_MAP\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_MAP\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_DATE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL {KW_LIKE, KW_REGEXP, KW_RLIKE}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE Number\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT Number\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE DecimalLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE TinyintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL {MINUS, PLUS}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE SmallintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN CharSetName CharSetLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE BigintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_DATE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_TRUE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_WHEN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_FALSE\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL KW_IS\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN StringLiteral StringLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN {MINUS, PLUS, TILDE}\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_DATE StringLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NULL KW_OR\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT TinyintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT SmallintLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT KW_CAST\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_CASE KW_CAST\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN LPAREN KW_CAST\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:68:4: \n     [java] Decision can match input such as \"LPAREN KW_NOT DecimalLiteral\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:108:5: \n     [java] Decision can match input such as \"KW_ORDER KW_BY LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:121:5: \n     [java] Decision can match input such as \"KW_CLUSTER KW_BY LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:133:5: \n     [java] Decision can match input such as \"KW_PARTITION KW_BY LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:144:5: \n     [java] Decision can match input such as \"KW_DISTRIBUTE KW_BY LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:155:5: \n     [java] Decision can match input such as \"KW_SORT KW_BY LPAREN\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:172:7: \n     [java] Decision can match input such as \"STAR\" using multiple alternatives: 1, 2\n     [java] \n     [java] As a result, alternative(s) 2 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:185:5: \n     [java] Decision can match input such as \"KW_STRUCT\" using multiple alternatives: 4, 6\n     [java] \n     [java] As a result, alternative(s) 6 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:185:5: \n     [java] Decision can match input such as \"KW_UNIONTYPE\" using multiple alternatives: 5, 6\n     [java] \n     [java] As a result, alternative(s) 6 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:185:5: \n     [java] Decision can match input such as \"KW_ARRAY\" using multiple alternatives: 2, 6\n     [java] \n     [java] As a result, alternative(s) 6 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:267:5: \n     [java] Decision can match input such as \"KW_NULL\" using multiple alternatives: 1, 8\n     [java] \n     [java] As a result, alternative(s) 8 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:267:5: \n     [java] Decision can match input such as \"KW_DATE StringLiteral\" using multiple alternatives: 2, 3\n     [java] \n     [java] As a result, alternative(s) 3 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:267:5: \n     [java] Decision can match input such as \"KW_TRUE\" using multiple alternatives: 3, 8\n     [java] \n     [java] As a result, alternative(s) 8 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:267:5: \n     [java] Decision can match input such as \"KW_FALSE\" using multiple alternatives: 3, 8\n     [java] \n     [java] As a result, alternative(s) 8 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_SORT KW_BY\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_MAP LPAREN\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_ORDER KW_BY\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_GROUP KW_BY\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"KW_BETWEEN KW_MAP LPAREN\" using multiple alternatives: 6, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_INTO\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_CLUSTER KW_BY\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_LATERAL KW_VIEW\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_DISTRIBUTE KW_BY\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:390:5: \n     [java] Decision can match input such as \"{KW_LIKE, KW_REGEXP, KW_RLIKE} KW_INSERT KW_OVERWRITE\" using multiple alternatives: 2, 7\n     [java] \n     [java] As a result, alternative(s) 7 were disabled for that input\n     [java] warning(200): IdentifiersParser.g:514:5: \n     [java] Decision can match input such as \"{AMPERSAND..BITWISEXOR, DIV..DIVIDE, EQUAL..EQUAL_NS, GREATERTHAN..GREATERTHANOREQUALTO, KW_AND, KW_ARRAY, KW_BETWEEN..KW_BOOLEAN, KW_CASE, KW_DOUBLE, KW_FLOAT, KW_IF, KW_IN, KW_INT, KW_LIKE, KW_MAP, KW_NOT, KW_OR, KW_REGEXP, KW_RLIKE, KW_SMALLINT, KW_STRING..KW_STRUCT, KW_TINYINT, KW_UNIONTYPE, KW_WHEN, LESSTHAN..LESSTHANOREQUALTO, MINUS..NOTEQUAL, PLUS, STAR, TILDE}\" using multiple alternatives: 1, 3\n     [java] \n     [java] As a result, alternative(s) 3 were disabled for that input\n\ncompile:\n     [echo] Project: ql\n    [javac] Compiling 919 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] Creating empty /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes/org/apache/hadoop/hive/ql/exec/package-info.class\n    [javac] Creating empty /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes/org/apache/hadoop/hive/ql/io/orc/package-info.class\n    [javac] Creating empty /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes/org/apache/hadoop/hive/ql/udf/generic/package-info.class\n    [javac] Creating empty /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes/org/apache/hadoop/hive/ql/exec/errors/package-info.class\n    [javac] Creating empty /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes/org/apache/hadoop/hive/ql/lockmgr/package-info.class\n     [copy] Copying 1 file to /data/hive-ptest/working/apache-svn-trunk-source/build/ql/classes\n\njar:\n     [echo] Project: ql\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/libthrift-0.9.0.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/thrift/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/commons-lang-2.4.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/commons-lang/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/json-20090211.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/json/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/JavaEWAH-0.3.2.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/javaewah/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/avro-1.7.1.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/avro/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/avro-mapred-1.7.1.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/avro-mapred/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/javolution-5.5.1.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/javolution/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/protobuf-java-2.4.1.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/protobuf-java/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/guava-11.0.2.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/guava/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/snappy-0.2.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/snappy/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/jackson-core-asl-1.8.8.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/jackson-core-asl/classes\n    [unzip] Expanding: /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/lib/default/jackson-mapper-asl-1.8.8.jar into /data/hive-ptest/working/apache-svn-trunk-source/build/jackson-mapper-asl/classes\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/ql/hive-exec-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-exec;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:22:10 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/ql/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-exec\n[ivy:publish] \tpublished hive-exec to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-exec/0.12.0-SNAPSHOT/jars/hive-exec.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-exec/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: contrib\n\ncheck-ivy:\n     [echo] Project: contrib\n\nivy-resolve:\n     [echo] Project: contrib\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-contrib;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-exec;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-metastore;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.1 in maven2\n[ivy:resolve] \tfound org.iq80.snappy#snappy;0.2 in maven2\n[ivy:resolve] \tfound org.json#json;20090211 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound com.googlecode.javaewah#JavaEWAH;0.3.2 in maven2\n[ivy:resolve] \tfound javolution#javolution;5.5.1 in maven2\n[ivy:resolve] \tfound jline#jline;0.9.94 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] downloading /data/hive-ptest/working/ivy/local/org.apache.hive/hive-exec/0.12.0-SNAPSHOT/jars/hive-exec.jar ...\n[ivy:resolve] ............................................................................................................................................ (8858kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hive#hive-exec;0.12.0-SNAPSHOT!hive-exec.jar (116ms)\n[ivy:resolve] :: resolution report :: resolve 6810ms :: artifacts dl 133ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tcom.google.guava#guava;r08 by [com.google.guava#guava;11.0.2] in [default]\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   39  |   1   |   1   |   2   ||   37  |   1   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-contrib-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-contrib-default.html\n\nmake-pom:\n     [echo] Project: contrib\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: contrib\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/contrib/src/test/resources does not exist.\n\ninit:\n     [echo] Project: contrib\n\nsetup:\n     [echo] Project: contrib\n\nivy-retrieve:\n     [echo] Project: contrib\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-contrib\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t1 artifacts copied, 36 already retrieved (8858kB/80ms)\n\ncompile:\n     [echo] Project: contrib\n    [javac] Compiling 39 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/classes\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/contrib/src/java/org/apache/hadoop/hive/contrib/udf/example/UDFExampleStructPrint.java uses unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/contrib/src/java/conf does not exist.\n\njar:\n     [echo] Project: contrib\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/hive-contrib-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-contrib;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:22:19 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/contrib/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-contrib\n[ivy:publish] \tpublished hive-contrib to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-contrib/0.12.0-SNAPSHOT/jars/hive-contrib.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-contrib/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: service\n\ncheck-ivy:\n     [echo] Project: service\n\nivy-resolve:\n     [echo] Project: service\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-service;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-exec;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-metastore;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.1 in maven2\n[ivy:resolve] \tfound org.iq80.snappy#snappy;0.2 in maven2\n[ivy:resolve] \tfound org.json#json;20090211 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound com.googlecode.javaewah#JavaEWAH;0.3.2 in maven2\n[ivy:resolve] \tfound javolution#javolution;5.5.1 in maven2\n[ivy:resolve] \tfound jline#jline;0.9.94 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] :: resolution report :: resolve 6858ms :: artifacts dl 16ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tcom.google.guava#guava;r08 by [com.google.guava#guava;11.0.2] in [default]\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   39  |   0   |   0   |   2   ||   37  |   0   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-service-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-service-default.html\n\nmake-pom:\n     [echo] Project: service\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/service/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: service\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/service/src/test/resources does not exist.\n\ninit:\n     [echo] Project: service\n\nivy-retrieve:\n     [echo] Project: service\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-service\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t0 artifacts copied, 37 already retrieved (0kB/17ms)\n\ncompile:\n     [echo] Project: service\n    [javac] Compiling 151 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/service/classes\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java uses or overrides a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n\njar:\n     [echo] Project: service\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/service/hive-service-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-service;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:22:30 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/service/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-service\n[ivy:publish] \tpublished hive-service to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-service/0.12.0-SNAPSHOT/jars/hive-service.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-service/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: cli\n\ncheck-ivy:\n     [echo] Project: cli\n\nivy-resolve:\n     [echo] Project: cli\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-cli;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-service;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-exec;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-metastore;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.1 in maven2\n[ivy:resolve] \tfound org.iq80.snappy#snappy;0.2 in maven2\n[ivy:resolve] \tfound org.json#json;20090211 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound com.googlecode.javaewah#JavaEWAH;0.3.2 in maven2\n[ivy:resolve] \tfound javolution#javolution;5.5.1 in maven2\n[ivy:resolve] \tfound jline#jline;0.9.94 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] downloading /data/hive-ptest/working/ivy/local/org.apache.hive/hive-service/0.12.0-SNAPSHOT/jars/hive-service.jar ...\n[ivy:resolve] ........................ (1469kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hive#hive-service;0.12.0-SNAPSHOT!hive-service.jar (21ms)\n[ivy:resolve] :: resolution report :: resolve 6675ms :: artifacts dl 55ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tcom.google.guava#guava;r08 by [com.google.guava#guava;11.0.2] in [default]\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   40  |   1   |   1   |   2   ||   38  |   1   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-cli-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-cli-default.html\n\nmake-pom:\n     [echo] Project: cli\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/cli/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: cli\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/cli/src/test/resources does not exist.\n\ninit:\n     [echo] Project: cli\n\nsetup:\n     [echo] Project: cli\n\nivy-retrieve:\n     [echo] Project: cli\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-cli\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t1 artifacts copied, 37 already retrieved (1469kB/15ms)\n\ncompile:\n     [echo] Project: cli\n    [javac] Compiling 4 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/cli/classes\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:71: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac] import sun.misc.Signal;\n    [javac]                ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:72: warning: sun.misc.SignalHandler is Sun proprietary API and may be removed in a future release\n    [javac] import sun.misc.SignalHandler;\n    [javac]                ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:362: warning: sun.misc.SignalHandler is Sun proprietary API and may be removed in a future release\n    [javac]     SignalHandler oldSignal = null;\n    [javac]     ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:363: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac]     Signal interupSignal = null;\n    [javac]     ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:368: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac]       interupSignal = new Signal(\"INT\");\n    [javac]                           ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:369: warning: sun.misc.SignalHandler is Sun proprietary API and may be removed in a future release\n    [javac]       oldSignal = Signal.handle(interupSignal, new SignalHandler() {\n    [javac]                                                    ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:369: warning: sun.misc.SignalHandler is Sun proprietary API and may be removed in a future release\n    [javac]       oldSignal = Signal.handle(interupSignal, new SignalHandler() {\n    [javac]                                                    ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:374: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac]         public void handle(Signal signal) {\n    [javac]                            ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:369: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac]       oldSignal = Signal.handle(interupSignal, new SignalHandler() {\n    [javac]                   ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java:430: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac]         Signal.handle(interupSignal, oldSignal);\n    [javac]         ^\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/RCFileCat.java uses or overrides a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: /data/hive-ptest/working/apache-svn-trunk-source/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java uses unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] 10 warnings\n\njar:\n     [echo] Project: cli\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/cli/hive-cli-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-cli;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:22:37 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/cli/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-cli\n[ivy:publish] \tpublished hive-cli to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-cli/0.12.0-SNAPSHOT/jars/hive-cli.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-cli/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: jdbc\n\ncheck-ivy:\n     [echo] Project: jdbc\n\nivy-resolve:\n     [echo] Project: jdbc\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-jdbc;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-cli;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-service;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-exec;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-metastore;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.1 in maven2\n[ivy:resolve] \tfound org.iq80.snappy#snappy;0.2 in maven2\n[ivy:resolve] \tfound org.json#json;20090211 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound com.googlecode.javaewah#JavaEWAH;0.3.2 in maven2\n[ivy:resolve] \tfound javolution#javolution;5.5.1 in maven2\n[ivy:resolve] \tfound jline#jline;0.9.94 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] downloading /data/hive-ptest/working/ivy/local/org.apache.hive/hive-cli/0.12.0-SNAPSHOT/jars/hive-cli.jar ...\n[ivy:resolve] .. (33kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hive#hive-cli;0.12.0-SNAPSHOT!hive-cli.jar (6ms)\n[ivy:resolve] :: resolution report :: resolve 6976ms :: artifacts dl 33ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tcom.google.guava#guava;r08 by [com.google.guava#guava;11.0.2] in [default]\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   41  |   1   |   1   |   2   ||   39  |   1   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-jdbc-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-jdbc-default.html\n\nmake-pom:\n     [echo] Project: jdbc\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: jdbc\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/jdbc/src/test/resources does not exist.\n\ninit:\n     [echo] Project: jdbc\n\nivy-retrieve:\n     [echo] Project: jdbc\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-jdbc\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t1 artifacts copied, 38 already retrieved (33kB/21ms)\n\ncompile:\n     [echo] Project: jdbc\n    [javac] Compiling 28 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/classes\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n\njar:\n     [echo] Project: jdbc\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/hive-jdbc-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-jdbc;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:22:46 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/jdbc/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-jdbc\n[ivy:publish] \tpublished hive-jdbc to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-jdbc/0.12.0-SNAPSHOT/jars/hive-jdbc.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-jdbc/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: beeline\n\ncheck-ivy:\n     [echo] Project: beeline\n\nivy-resolve:\n     [echo] Project: beeline\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-beeline;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-service;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-exec;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-metastore;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.1 in maven2\n[ivy:resolve] \tfound org.iq80.snappy#snappy;0.2 in maven2\n[ivy:resolve] \tfound org.json#json;20090211 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound com.googlecode.javaewah#JavaEWAH;0.3.2 in maven2\n[ivy:resolve] \tfound javolution#javolution;5.5.1 in maven2\n[ivy:resolve] \tfound jline#jline;0.9.94 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] \tfound commons-io#commons-io;2.4 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging;1.0.4 in maven2\n[ivy:resolve] \tfound commons-logging#commons-logging-api;1.0.4 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libthrift;0.9.0 in maven2\n[ivy:resolve] :: resolution report :: resolve 7569ms :: artifacts dl 19ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tcom.google.guava#guava;r08 by [com.google.guava#guava;11.0.2] in [default]\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   44  |   0   |   0   |   2   ||   42  |   0   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-beeline-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-beeline-default.html\n\nmake-pom:\n     [echo] Project: beeline\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: beeline\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/beeline/src/test/resources does not exist.\n\ninit:\n     [echo] Project: beeline\n\nsetup:\n     [echo] Project: beeline\n\nivy-retrieve:\n     [echo] Project: beeline\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-beeline\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t0 artifacts copied, 42 already retrieved (0kB/12ms)\n\ncompile:\n     [echo] Project: beeline\n    [javac] Compiling 29 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/classes\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/beeline/src/java/org/apache/hive/beeline/SunSignalHandler.java:28: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac] import sun.misc.Signal;\n    [javac]                ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/beeline/src/java/org/apache/hive/beeline/SunSignalHandler.java:29: warning: sun.misc.SignalHandler is Sun proprietary API and may be removed in a future release\n    [javac] import sun.misc.SignalHandler;\n    [javac]                ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/beeline/src/java/org/apache/hive/beeline/SunSignalHandler.java:31: warning: sun.misc.SignalHandler is Sun proprietary API and may be removed in a future release\n    [javac] public class SunSignalHandler implements BeeLineSignalHandler, SignalHandler {\n    [javac]                                                                ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/beeline/src/java/org/apache/hive/beeline/SunSignalHandler.java:44: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac]   public void handle (Signal signal) {\n    [javac]                       ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/beeline/src/java/org/apache/hive/beeline/SunSignalHandler.java:37: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac]     Signal.handle (new Signal (\"INT\"), this);\n    [javac]                        ^\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/beeline/src/java/org/apache/hive/beeline/SunSignalHandler.java:37: warning: sun.misc.Signal is Sun proprietary API and may be removed in a future release\n    [javac]     Signal.handle (new Signal (\"INT\"), this);\n    [javac]     ^\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] 6 warnings\n     [copy] Copying 2 files to /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/classes\n\njar:\n     [echo] Project: beeline\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/hive-beeline-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-beeline;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:22:54 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/beeline/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-beeline\n[ivy:publish] \tpublished hive-beeline to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-beeline/0.12.0-SNAPSHOT/jars/hive-beeline.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-beeline/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: hwi\n\ncheck-ivy:\n     [echo] Project: hwi\n\nivy-resolve:\n     [echo] Project: hwi\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-hwi;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-cli;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-service;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-exec;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-metastore;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.1 in maven2\n[ivy:resolve] \tfound org.iq80.snappy#snappy;0.2 in maven2\n[ivy:resolve] \tfound org.json#json;20090211 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound com.googlecode.javaewah#JavaEWAH;0.3.2 in maven2\n[ivy:resolve] \tfound javolution#javolution;5.5.1 in maven2\n[ivy:resolve] \tfound jline#jline;0.9.94 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jetty;6.1.26 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#jetty-util;6.1.26 in maven2\n[ivy:resolve] \tfound org.mortbay.jetty#servlet-api;2.5-20081211 in maven2\n[ivy:resolve] :: resolution report :: resolve 7300ms :: artifacts dl 19ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tcom.google.guava#guava;r08 by [com.google.guava#guava;11.0.2] in [default]\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   44  |   0   |   0   |   2   ||   42  |   0   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-hwi-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-hwi-default.html\n\nmake-pom:\n     [echo] Project: hwi\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: hwi\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/hwi/src/test/resources does not exist.\n\ninit:\n     [echo] Project: hwi\n\nsetup:\n     [echo] Project: hwi\n\nivy-retrieve:\n     [echo] Project: hwi\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-hwi\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t3 artifacts copied, 39 already retrieved (831kB/14ms)\n\nwar:\n     [echo] Project: hwi\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/hive-hwi-0.12.0-SNAPSHOT.war\n\ncompile:\n     [echo] Project: hwi\n    [javac] /data/hive-ptest/working/apache-svn-trunk-source/hwi/build.xml:67: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds\n    [javac] Compiling 6 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/classes\n\njar:\n     [echo] Project: hwi\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/hive-hwi-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-hwi;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:23:02 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/hwi/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-hwi\n[ivy:publish] \tpublished hive-hwi to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-hwi/0.12.0-SNAPSHOT/jars/hive-hwi.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-hwi/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: hbase-handler\n\ncheck-ivy:\n     [echo] Project: hbase-handler\n\nivy-resolve:\n     [echo] Project: hbase-handler\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-hbase-handler;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound org.apache.hive#hive-exec;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-metastore;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-serde;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-common;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound org.apache.hive#hive-shims;0.12.0-SNAPSHOT in local\n[ivy:resolve] \tfound commons-cli#commons-cli;1.2 in maven2\n[ivy:resolve] \tfound org.apache.commons#commons-compress;1.4.1 in maven2\n[ivy:resolve] \tfound org.tukaani#xz;1.0 in maven2\n[ivy:resolve] \tfound commons-lang#commons-lang;2.4 in maven2\n[ivy:resolve] \tfound log4j#log4j;1.2.16 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-api;1.6.1 in maven2\n[ivy:resolve] \tfound org.slf4j#slf4j-log4j12;1.6.1 in maven2\n[ivy:resolve] \tfound org.mockito#mockito-all;1.8.2 in maven2\n[ivy:resolve] \tfound org.apache.thrift#libfb303;0.9.0 in maven2\n[ivy:resolve] \tfound commons-codec#commons-codec;1.4 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro;1.7.1 in maven2\n[ivy:resolve] \tfound org.apache.avro#avro-mapred;1.7.1 in maven2\n[ivy:resolve] \tfound org.antlr#antlr;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#antlr-runtime;3.4 in maven2\n[ivy:resolve] \tfound org.antlr#ST4;4.0.4 in maven2\n[ivy:resolve] \tfound com.jolbox#bonecp;0.7.1.RELEASE in maven2\n[ivy:resolve] \tfound com.google.guava#guava;r08 in maven2\n[ivy:resolve] \tfound commons-pool#commons-pool;1.5.4 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-api-jdo;3.2.1 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-core;3.2.2 in maven2\n[ivy:resolve] \tfound org.datanucleus#datanucleus-rdbms;3.2.1 in maven2\n[ivy:resolve] \tfound javax.jdo#jdo-api;3.0.1 in maven2\n[ivy:resolve] \tfound org.apache.derby#derby;10.4.2.0 in maven2\n[ivy:resolve] \tfound com.google.protobuf#protobuf-java;2.4.1 in maven2\n[ivy:resolve] \tfound org.iq80.snappy#snappy;0.2 in maven2\n[ivy:resolve] \tfound org.json#json;20090211 in maven2\n[ivy:resolve] \tfound commons-collections#commons-collections;3.2.1 in maven2\n[ivy:resolve] \tfound commons-configuration#commons-configuration;1.6 in maven2\n[ivy:resolve] \tfound com.googlecode.javaewah#JavaEWAH;0.3.2 in maven2\n[ivy:resolve] \tfound javolution#javolution;5.5.1 in maven2\n[ivy:resolve] \tfound jline#jline;0.9.94 in maven2\n[ivy:resolve] \tfound com.google.guava#guava;11.0.2 in maven2\n[ivy:resolve] \tfound com.google.code.findbugs#jsr305;1.3.9 in maven2\n[ivy:resolve] \tfound org.apache.hbase#hbase;0.94.6.1 in maven2\n[ivy:resolve] \tfound com.github.stephenc.high-scale-lib#high-scale-lib;1.1.1 in maven2\n[ivy:resolve] \tfound com.yammer.metrics#metrics-core;2.1.2 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-jaxrs;1.8.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-core-asl;1.8.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-mapper-asl;1.8.8 in maven2\n[ivy:resolve] \tfound org.codehaus.jackson#jackson-xc;1.8.8 in maven2\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hbase/hbase/0.94.6.1/hbase-0.94.6.1-tests.jar ...\n[ivy:resolve] ........................................ (2360kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hbase#hbase;0.94.6.1!hbase.jar(test-jar) (82ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hbase/hbase/0.94.6.1/hbase-0.94.6.1.jar ...\n[ivy:resolve] ............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. (4952kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hbase#hbase;0.94.6.1!hbase.jar (523ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar ...\n[ivy:resolve] ... (93kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.github.stephenc.high-scale-lib#high-scale-lib;1.1.1!high-scale-lib.jar (10ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.1.2/metrics-core-2.1.2.jar ...\n[ivy:resolve] ... (80kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.1.2!metrics-core.jar (10ms)\n[ivy:resolve] :: resolution report :: resolve 12250ms :: artifacts dl 668ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tcom.google.guava#guava;r08 by [com.google.guava#guava;11.0.2] in [default]\n[ivy:resolve] \torg.slf4j#slf4j-api;1.5.10 by [org.slf4j#slf4j-api;1.6.1] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   46  |   3   |   3   |   2   ||   45  |   4   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-hbase-handler-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-hbase-handler-default.html\n\nmake-pom:\n     [echo] Project: hbase-handler\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: hbase-handler\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/test/resources does not exist.\n\ninit:\n     [echo] Project: hbase-handler\n\nsetup:\n     [echo] Project: hbase-handler\n\nivy-retrieve:\n     [echo] Project: hbase-handler\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-hbase-handler\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t6 artifacts copied, 39 already retrieved (7536kB/60ms)\n\ncompile:\n     [echo] Project: hbase-handler\n    [javac] Compiling 13 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/classes\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Creating empty /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/classes/org/apache/hadoop/hive/hbase/package-info.class\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/hbase-handler/src/java/conf does not exist.\n\njar:\n     [echo] Project: hbase-handler\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/hive-hbase-handler-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-hbase-handler;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:23:16 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/hbase-handler/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-hbase-handler\n[ivy:publish] \tpublished hive-hbase-handler to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-hbase-handler/0.12.0-SNAPSHOT/jars/hive-hbase-handler.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-hbase-handler/0.12.0-SNAPSHOT/ivys/ivy.xml\n\nivy-init-settings:\n     [echo] Project: testutils\n\ncheck-ivy:\n     [echo] Project: testutils\n\nivy-resolve:\n     [echo] Project: testutils\n[ivy:resolve] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n[ivy:resolve] :: resolving dependencies :: org.apache.hive#hive-testutils;0.12.0-SNAPSHOT\n[ivy:resolve] \tconfs: [default]\n[ivy:resolve] \tfound junit#junit;4.10 in maven2\n[ivy:resolve] \tfound org.hamcrest#hamcrest-core;1.1 in maven2\n[ivy:resolve] \tfound com.google.code.tempus-fugit#tempus-fugit;1.1 in maven2\n[ivy:resolve] downloading http://repo1.maven.org/maven2/junit/junit/4.10/junit-4.10.jar ...\n[ivy:resolve] ..... (247kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] junit#junit;4.10!junit.jar (9ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/google/code/tempus-fugit/tempus-fugit/1.1/tempus-fugit-1.1.jar ...\n[ivy:resolve] .. (54kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] com.google.code.tempus-fugit#tempus-fugit;1.1!tempus-fugit.jar (6ms)\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.1/hamcrest-core-1.1.jar ...\n[ivy:resolve] ... (74kB)\n[ivy:resolve] .. (0kB)\n[ivy:resolve] \t[SUCCESSFUL ] org.hamcrest#hamcrest-core;1.1!hamcrest-core.jar (6ms)\n[ivy:resolve] :: resolution report :: resolve 2114ms :: artifacts dl 26ms\n[ivy:resolve] \t:: evicted modules:\n[ivy:resolve] \tjunit#junit;4.7 by [junit#junit;4.10] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   4   |   3   |   3   |   1   ||   3   |   3   |\n\t---------------------------------------------------------------------\n[ivy:report] Processing /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/resolution-cache/org.apache.hive-hive-testutils-default.xml to /data/hive-ptest/working/apache-svn-trunk-source/build/ivy/report/org.apache.hive-hive-testutils-default.html\n\nmake-pom:\n     [echo] Project: testutils\n     [echo]  Writing POM to /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/pom.xml\n[ivy:makepom] DEPRECATED: 'ivy.conf.file' is deprecated, use 'ivy.settings.file' instead\n[ivy:makepom] :: loading settings :: file = /data/hive-ptest/working/apache-svn-trunk-source/ivy/ivysettings.xml\n\ncreate-dirs:\n     [echo] Project: testutils\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/test/resources does not exist.\n\ninit:\n     [echo] Project: testutils\n\nsetup:\n     [echo] Project: testutils\n\nivy-retrieve:\n     [echo] Project: testutils\n[ivy:retrieve] :: retrieving :: org.apache.hive#hive-testutils\n[ivy:retrieve] \tconfs: [default]\n[ivy:retrieve] \t3 artifacts copied, 0 already retrieved (376kB/6ms)\n\ncompile:\n     [echo] Project: testutils\n    [javac] Compiling 2 source files to /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/classes\n     [copy] Warning: /data/hive-ptest/working/apache-svn-trunk-source/testutils/src/java/conf does not exist.\n\njar:\n     [echo] Project: testutils\n      [jar] Building jar: /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/hive-testutils-0.12.0-SNAPSHOT.jar\n[ivy:publish] :: delivering :: org.apache.hive#hive-testutils;0.12.0-SNAPSHOT :: 0.12.0-SNAPSHOT :: integration :: Sun Sep 08 14:23:19 EDT 2013\n[ivy:publish] \tdelivering ivy file to /data/hive-ptest/working/apache-svn-trunk-source/build/testutils/ivy-0.12.0-SNAPSHOT.xml\n[ivy:publish] :: publishing :: org.apache.hive#hive-testutils\n[ivy:publish] \tpublished hive-testutils to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-testutils/0.12.0-SNAPSHOT/jars/hive-testutils.jar\n[ivy:publish] \tpublished ivy to /data/hive-ptest/working/ivy/local/org.apache.hive/hive-testutils/0.12.0-SNAPSHOT/ivys/ivy.xml\n\ninit:\n\njar:\n\nmvn-init:\n     [echo] hcatalog-core\n      [get] Getting: http://repo2.maven.org/maven2/org/apache/maven/maven-ant-tasks/2.1.3/maven-ant-tasks-2.1.3.jar\n      [get] To: /data/hive-ptest/working/apache-svn-trunk-source/hcatalog/build/maven-ant-tasks-2.1.3.jar\n\nhive-mvn-publish:\n     [echo] Installing local artifact for maven : shims\n[artifact:install] [INFO] Installing /data/hive-ptest/working/apache-svn-trunk-source/build/shims/hive-shims-0.13.0-SNAPSHOT.jar to /data/hive-ptest/working/maven/org/apache/hive/hive-shims/0.12.0-SNAPSHOT/hive-shims-0.12.0-SNAPSHOT.jar\n[artifact:install] An error has occurred while processing the Maven artifact tasks.\n[artifact:install]  Diagnosis:\n[artifact:install] \n[artifact:install] Error installing artifact 'org.apache.hive:hive-shims:jar': Error installing artifact: File /data/hive-ptest/working/apache-svn-trunk-source/build/shims/hive-shims-0.13.0-SNAPSHOT.jar does not exist\n\nBUILD FAILED\n/data/hive-ptest/working/apache-svn-trunk-source/build.xml:327: The following error occurred while executing this line:\n/data/hive-ptest/working/apache-svn-trunk-source/build.xml:166: The following error occurred while executing this line:\n/data/hive-ptest/working/apache-svn-trunk-source/build.xml:168: The following error occurred while executing this line:\n/data/hive-ptest/working/apache-svn-trunk-source/hcatalog/build.xml:68: The following error occurred while executing this line:\n/data/hive-ptest/working/apache-svn-trunk-source/hcatalog/build-support/ant/deploy.xml:81: The following error occurred while executing this line:\n/data/hive-ptest/working/apache-svn-trunk-source/hcatalog/build-support/ant/deploy.xml:57: The following error occurred while executing this line:\n/data/hive-ptest/working/apache-svn-trunk-source/hcatalog/build-support/ant/deploy.xml:48: Error installing artifact 'org.apache.hive:hive-shims:jar': Error installing artifact: File /data/hive-ptest/working/apache-svn-trunk-source/build/shims/hive-shims-0.13.0-SNAPSHOT.jar does not exist\n\nTotal time: 5 minutes 56 seconds\n+ exit 1\n'\n{noformat}\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-09-08T18:23:22.078+0000","updated":"2013-09-08T18:23:22.078+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761502","id":"13761502","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"This failure is from trunk being broken. Please test and commit manually.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-08T18:33:18.660+0000","updated":"2013-09-08T18:33:18.660+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761504","id":"13761504","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"body":"Patch applies now, but patch doesn't remove NPath.java. \nAlso had to revert change to build.properties. I applied patch on trunk.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-08T18:38:01.506+0000","updated":"2013-09-08T18:38:01.506+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761508","id":"13761508","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"body":"{noformat}\nI turned my attention away from the email chain. I am not clear the infringement issue is resolved. This rename is only to deal with the trademark issue.\n{noformat}\n\nOk understood. You don't have a specific issue with infringement. Right now that is my primary concern.\n\n{noformat}\nThe name does not matter. I opened this ticket and said clearly on the private email chain something to the effect of \"I am calling it regex_path and opening a ticket, if anyone has any other suggestions for names please move the conversation to the ticket. \n{noformat}\n\nOn hindsight, definitely should have communicated better. But you have to understand, it was not clear where this was headed. Was focused on responding to Teradata's complaint. Not a fun experience.. Anyways hopefully the infringement issue is past us now. Appreciate that you made the name change.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=rhbutani","name":"rhbutani","key":"rhbutani","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Harish Butani","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-08T18:57:18.679+0000","updated":"2013-09-08T18:57:18.679+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761555","id":"13761555","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"We did agree to produce a hive release in less then 3 months that did not use the NPath trademark. This jira is to specifically address that. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-09T00:01:50.208+0000","updated":"2013-09-09T00:01:50.208+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761572","id":"13761572","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"[~appodictic] I think the thread that you're referring to was on the private list, so Harish didn't see it. It's my fault that this information wasn't communicated to him earlier, and that I solicited his opinion at such a late stage in the game. Still, I think we should defer to him in this matter since he was the primary implementor of this feature.\n\nI have attached a new patch that changes the name of the function to \"matchpath\" and updates the various testcases. If the tests pass and someone else +1s it I will take care of getting it committed to trunk and backported to branch-0.12.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-09T02:12:57.444+0000","updated":"2013-09-09T02:12:57.444+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13761594","id":"13761594","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"You could have just +1 ed mine and committed it yourself. But anyway +1 on yours.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-09T02:57:26.915+0000","updated":"2013-09-09T02:57:26.915+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13763967","id":"13763967","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"One of the ASF lawyers is running a trademark search on \"matchpath\". If it checks out ok I'll commit this patch.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-11T04:40:45.887+0000","updated":"2013-09-11T04:40:45.887+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13764338","id":"13764338","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"I understand this is an exceptional case, but it is pretty odd that we are doing a trademark search, what about the rest of the udfs?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-11T14:15:43.777+0000","updated":"2013-09-11T14:15:43.777+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13767992","id":"13767992","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"No one has threatened legal action over any of the other UDF names. If that happens I suppose we'll do the same thing.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-16T01:36:02.783+0000","updated":"2013-09-16T01:36:02.783+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13772399","id":"13772399","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"What should we do for 0.12 release if we don't hear from the lawyers in a week ? I am hoping to have release candidates out by then. \nShould we release with this new name that case ? \n(Hello world, does anybody out there have a problem with this new name , please chime in it if you do! :))\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-19T22:32:32.572+0000","updated":"2013-09-19T22:32:32.572+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13772466","id":"13772466","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"We agreed to remove the name npath. If we do not have word from asf legal we should release with the new name we have chosen. We can always changr the name again, but it is imperative we produce a release without the name npath in it. Lets wait as long as possible however. ","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-09-19T23:31:56.887+0000","updated":"2013-09-19T23:31:56.887+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13778062","id":"13778062","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"Can I commit this tomorrow if we haven't heard from them, that this unblocks 0.12 RC ? I am hoping to have an RC out by friday.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-25T20:53:15.829+0000","updated":"2013-09-25T20:53:15.829+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13778099","id":"13778099","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"[~thejas] I requested an update from our lawyer. Let's wait until EOD tomorrow to make a decision on this. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-09-25T21:23:25.321+0000","updated":"2013-09-25T21:23:25.321+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13784693","id":"13784693","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"I am hoping that all serious blockers would get fixed by tomorrow and I can get an 0.12 RC out by friday. Should I commit the changes or should we consider it as non blocker for 0.12 ?\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-03T00:38:31.673+0000","updated":"2013-10-03T00:38:31.673+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13785256","id":"13785256","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"I think we should just commit this. Picking a name of the air is better than using one we know has has legal issues.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-03T15:06:35.062+0000","updated":"2013-10-03T15:06:35.062+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13785282","id":"13785282","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"body":"I am back under the opinion we should just remove this UDF. You could make a sequel to 'office space' based on the story behind this UDF....\n\n'yea... im going to need you to come in on Saturday and rename this udf'\n'yea...im going to need you to come in on sunday because its saturday and I dont know the name yet'\n'yea...im going to need you to come in next saturday because we are not sure if we should rename it yet'\n\nIt would be a block buster for sure.\n\n\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=appodictic","name":"appodictic","key":"appodictic","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Edward Capriolo","active":true,"timeZone":"Etc/UTC"},"created":"2013-10-03T15:38:03.156+0000","updated":"2013-10-03T15:38:03.156+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13785285","id":"13785285","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"body":"I'd be +1 for just removing it.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=brocknoland","name":"brocknoland","key":"brocknoland","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Brock Noland","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-03T15:40:06.101+0000","updated":"2013-10-03T15:40:06.101+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13785459","id":"13785459","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"We are waiting until EOD Friday to commit this. If you are a Hive committer or PMC member and want more information about what's going on, then please send an email to the Hive PMC list. Thanks.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-03T19:21:42.502+0000","updated":"2013-10-03T19:21:42.502+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13786958","id":"13786958","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"Rebasing the old patch in response to some test and PTF changes.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-05T06:18:13.316+0000","updated":"2013-10-05T06:18:13.316+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13786959","id":"13786959","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"body":"Using the right patch name format this time...","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=cwsteinbach","name":"cwsteinbach","key":"cwsteinbach","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Carl Steinbach","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-05T06:22:07.598+0000","updated":"2013-10-05T06:22:07.598+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787271","id":"13787271","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"body":"Thanks Carl for updating the patch +1\nLets check this in to unblock RC so folks can start playing with RC, we can always re-roll if need be later.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-05T17:02:44.523+0000","updated":"2013-10-05T17:02:44.523+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787294","id":"13787294","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"body":"\n\n{color:green}Overall{color}: +1 all checks pass\n\nHere are the results of testing the latest attachment:\nhttps://issues.apache.org/jira/secure/attachment/12606978/HIVE-5087.2.patch\n\n{color:green}SUCCESS:{color} +1 4057 tests passed\n\nTest results: https://builds.apache.org/job/PreCommit-HIVE-Build/1049/testReport\nConsole output: https://builds.apache.org/job/PreCommit-HIVE-Build/1049/console\n\nMessages:\n{noformat}\nExecuting org.apache.hive.ptest.execution.PrepPhase\nExecuting org.apache.hive.ptest.execution.ExecutionPhase\nExecuting org.apache.hive.ptest.execution.ReportingPhase\n{noformat}\n\nThis message is automatically generated.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hiveqa","name":"hiveqa","key":"hiveqa","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=hiveqa&avatarId=17060","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=hiveqa&avatarId=17060","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=hiveqa&avatarId=17060","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=hiveqa&avatarId=17060"},"displayName":"Hive QA","active":true,"timeZone":"America/Chicago"},"created":"2013-10-05T18:39:58.216+0000","updated":"2013-10-05T18:39:58.216+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787309","id":"13787309","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"I will go ahead and commit this without waiting for another 24 hours as many committers have already gone through the patch (pre-rebase). This is the last serious blocker for hive 0.12 . (There is not ETA yet for HIVE-5235, and it is not a regression)\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-05T19:29:48.637+0000","updated":"2013-10-05T19:29:48.637+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787357","id":"13787357","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hive-trunk-hadoop1-ptest #191 (See [https://builds.apache.org/job/Hive-trunk-hadoop1-ptest/191/])\nHIVE-5087 : Rename npath UDF to matchpath (Edward Capriolo and Carl Steinbach via Ashutosh Chauhan) (thejas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1529501)\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/NPath.java\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_matchpath.q\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_npath.q\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_register_tblfn.q\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_matchpath.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_npath.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_register_tblfn.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/show_functions.q.out\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2013-10-05T21:34:34.208+0000","updated":"2013-10-05T21:34:34.208+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787407","id":"13787407","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hive-trunk-hadoop2-ptest #126 (See [https://builds.apache.org/job/Hive-trunk-hadoop2-ptest/126/])\nHIVE-5087 : Rename npath UDF to matchpath (Edward Capriolo and Carl Steinbach via Ashutosh Chauhan) (thejas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1529501)\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/NPath.java\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_matchpath.q\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_npath.q\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_register_tblfn.q\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_matchpath.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_npath.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_register_tblfn.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/show_functions.q.out\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2013-10-05T23:44:52.129+0000","updated":"2013-10-05T23:44:52.129+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787453","id":"13787453","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"This needs to be documented in the wiki.  Also, the wiki contains one mention of npath, in a child of the Home page:  [https://cwiki.apache.org/confluence/display/Hive/Fix+Hive+Unit+Tests+on+Hadoop+2+-+HIVE-3949] which contains this line: \n\nTestCliDriver.ptf_npath.q    HIVE-4721   Resolved\n\n... but the latest patch doesn't change TestCliDriver.ptf_npath.q.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2013-10-06T02:20:46.333+0000","updated":"2013-10-06T02:20:46.333+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787468","id":"13787468","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hive-trunk-hadoop2 #483 (See [https://builds.apache.org/job/Hive-trunk-hadoop2/483/])\nHIVE-5087 : Rename npath UDF to matchpath (Edward Capriolo and Carl Steinbach via Ashutosh Chauhan) (thejas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1529501)\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/NPath.java\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_matchpath.q\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_npath.q\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_register_tblfn.q\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_matchpath.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_npath.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_register_tblfn.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/show_functions.q.out\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2013-10-06T03:34:16.980+0000","updated":"2013-10-06T03:34:16.980+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787473","id":"13787473","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"FAILURE: Integrated in Hive-trunk-h0.21 #2382 (See [https://builds.apache.org/job/Hive-trunk-h0.21/2382/])\nHIVE-5087 : Rename npath UDF to matchpath (Edward Capriolo and Carl Steinbach via Ashutosh Chauhan) (thejas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1529501)\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java\n* /hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/NPath.java\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_matchpath.q\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_npath.q\n* /hive/trunk/ql/src/test/queries/clientpositive/ptf_register_tblfn.q\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_matchpath.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_npath.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/ptf_register_tblfn.q.out\n* /hive/trunk/ql/src/test/results/clientpositive/show_functions.q.out\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2013-10-06T03:34:18.718+0000","updated":"2013-10-06T03:34:18.718+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787519","id":"13787519","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"HIVE-5087-branch0.12.2.patch - The rebased patch for trunk.\nThe trunk patch did not apply on 0.12 branch as it does not have a change (code cleanup changes) in NPath.java that is present in trunk. \n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-06T07:19:01.746+0000","updated":"2013-10-06T07:19:01.746+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787520","id":"13787520","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"Patch committed to 0.12 branch and trunk.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-06T07:20:33.338+0000","updated":"2013-10-06T07:20:33.338+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787522","id":"13787522","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"body":"[~lefty@hortonworks.com] Good point about updating documentation. But unfortunately I didn't find any documentation of the udf in the wiki. \nYou can see the change to ptf_npath.q file in the patch. That corresponds to TestCliDriver.ptf_npath.q\n\n[~rhbutani]  [~ashutoshc] Is there any documentation page that needs to be updated ?","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=thejas","name":"thejas","key":"thejas","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=thejas&avatarId=15902","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=thejas&avatarId=15902","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=thejas&avatarId=15902","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=thejas&avatarId=15902"},"displayName":"Thejas M Nair","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-06T07:29:59.632+0000","updated":"2013-10-06T07:29:59.632+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787888","id":"13787888","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"ABORTED: Integrated in Hive-branch-0.12-hadoop1 #15 (See [https://builds.apache.org/job/Hive-branch-0.12-hadoop1/15/])\nHIVE-5087 : Rename npath UDF to matchpath (Edward Capriolo and Carl Steinbach via Ashutosh Chauhan) (thejas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1529577)\n* /hive/branches/branch-0.12/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\n* /hive/branches/branch-0.12/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java\n* /hive/branches/branch-0.12/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/NPath.java\n* /hive/branches/branch-0.12/ql/src/test/queries/clientpositive/ptf_matchpath.q\n* /hive/branches/branch-0.12/ql/src/test/queries/clientpositive/ptf_npath.q\n* /hive/branches/branch-0.12/ql/src/test/queries/clientpositive/ptf_register_tblfn.q\n* /hive/branches/branch-0.12/ql/src/test/results/clientpositive/ptf_matchpath.q.out\n* /hive/branches/branch-0.12/ql/src/test/results/clientpositive/ptf_npath.q.out\n* /hive/branches/branch-0.12/ql/src/test/results/clientpositive/ptf_register_tblfn.q.out\n* /hive/branches/branch-0.12/ql/src/test/results/clientpositive/show_functions.q.out\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2013-10-07T03:22:11.851+0000","updated":"2013-10-07T03:22:11.851+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13787981","id":"13787981","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"[~thejas], thanks for explaining about TestCliDriver.ptf_npath.q.  Other PTFs are skimpily documented in [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics], so matchpath could go there too.\n\n<Off topic>  Is it possible to change my JIRA username without creating a new account?  It's fine in the JIRA comments but goes to the mailing list as lefty@hortonworks.com.  Of course I'm proud of that former association, but the username is misleading now.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2013-10-07T08:18:32.191+0000","updated":"2013-10-07T08:18:32.191+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13788005","id":"13788005","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"body":"ABORTED: Integrated in Hive-branch-0.12-hadoop2 #9 (See [https://builds.apache.org/job/Hive-branch-0.12-hadoop2/9/])\nHIVE-5087 : Rename npath UDF to matchpath (Edward Capriolo and Carl Steinbach via Ashutosh Chauhan) (thejas: http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1529577)\n* /hive/branches/branch-0.12/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\n* /hive/branches/branch-0.12/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/MatchPath.java\n* /hive/branches/branch-0.12/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/NPath.java\n* /hive/branches/branch-0.12/ql/src/test/queries/clientpositive/ptf_matchpath.q\n* /hive/branches/branch-0.12/ql/src/test/queries/clientpositive/ptf_npath.q\n* /hive/branches/branch-0.12/ql/src/test/queries/clientpositive/ptf_register_tblfn.q\n* /hive/branches/branch-0.12/ql/src/test/results/clientpositive/ptf_matchpath.q.out\n* /hive/branches/branch-0.12/ql/src/test/results/clientpositive/ptf_npath.q.out\n* /hive/branches/branch-0.12/ql/src/test/results/clientpositive/ptf_register_tblfn.q.out\n* /hive/branches/branch-0.12/ql/src/test/results/clientpositive/show_functions.q.out\n","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=hudson","name":"hudson","key":"hudson","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Hudson","active":true,"timeZone":"Etc/UTC"},"created":"2013-10-07T09:14:18.132+0000","updated":"2013-10-07T09:14:18.132+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/13795857","id":"13795857","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"body":"This issue has been fixed and released as part of 0.12 release. If you find further issues, please create a new jira and link it to this one.","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=ashutoshc","name":"ashutoshc","key":"ashutoshc","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?avatarId=10452","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"},"displayName":"Ashutosh Chauhan","active":true,"timeZone":"America/Los_Angeles"},"created":"2013-10-15T23:29:13.964+0000","updated":"2013-10-15T23:29:13.964+0000"},{"self":"https://issues.apache.org/jira/rest/api/2/issue/12663566/comment/14110296","id":"14110296","author":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"body":"The matchpath UDF still needs to be documented.  It belongs in the Windowing and Analytics doc, but could also be mentioned in the UDFs doc.  (Or the UDFs doc could just include a link to Windowing and Analytics.)  HIVE-896 created npath in the first place, in release 0.11.0.\n\n* [Windowing and Analytics Functions | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics]\n* [Hive Operators and User-Defined Functions | https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF]","updateAuthor":{"self":"https://issues.apache.org/jira/rest/api/2/user?username=leftylev","name":"leftylev","key":"lefty@hortonworks.com","avatarUrls":{"48x48":"https://issues.apache.org/jira/secure/useravatar?ownerId=lefty%40hortonworks.com&avatarId=15906","24x24":"https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=lefty%40hortonworks.com&avatarId=15906","16x16":"https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=lefty%40hortonworks.com&avatarId=15906","32x32":"https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=lefty%40hortonworks.com&avatarId=15906"},"displayName":"Lefty Leverenz","active":true,"timeZone":"America/New_York"},"created":"2014-08-26T05:28:09.190+0000","updated":"2014-08-26T05:28:09.190+0000"}],"maxResults":54,"total":54,"startAt":0},"votes":{"self":"https://issues.apache.org/jira/rest/api/2/issue/HIVE-5087/votes","votes":0,"hasVoted":false},"worklog":{"startAt":0,"maxResults":20,"total":0,"worklogs":[]},"customfield_12311820":"0|i1n8zz:"}}